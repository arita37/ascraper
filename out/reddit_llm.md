 
all -  [ Can Langchain help me create notes for a video?  ](https://www.reddit.com/r/LangChain/comments/1b2imy2/can_langchain_help_me_create_notes_for_a_video/) , 2024-02-29-0909
```
Is there any tutorial/guidance available that by using Langchain and an open source LLMs (like but not limited to Huggin
g face),  I can summarise the contents in the video for a particular time frame/section. I may  have a support of transc
ripts. 

Your help is appreciated. Thanks! 
```
---

     
 
all -  [ fetching relevant data from vectorestore ](https://www.reddit.com/r/OpenAIDev/comments/1b2g5rl/fetching_relevant_data_from_vectorestore/) , 2024-02-29-0909
```
Hello , 

I'm using langchain , chromadb and chat gpt 4, I  have loaded some .docx embedded them into a chromadb , then 
(using chat gpt 4) I ask a question regarding something in the documents inside the vectorestrore but the model would ou
tput something else based on its data not mine ? is there a way to fix this ,I would really appreciate any help availabl
e?

more details :

one of the documents is called (for example) subject A 

inside the document , centered in first lin
e there is subject A 

then some words under it , when I ask about subject A , I get answers not related at all to my do
cuments.

I have 4 documents called Subject A example {num} docx , it's ok to fetch anything from them , but unfortunate
ly I get nothing .
```
---

     
 
all -  [ fetching relevant info from vectorestores ](https://www.reddit.com/r/LangChain/comments/1b2g0pl/fetching_relevant_info_from_vectorestores/) , 2024-02-29-0909
```
Hello , 

I have loaded some .docx embedded them into a chromadb , then (using chat gpt 4) I ask a question regarding so
mething in the documents inside the vectorestrore but the model would output something else based on its data not mine ?
 is there a way to fix this ?
```
---

     
 
all -  [ Compare two documents ](https://www.reddit.com/r/LangChain/comments/1b2cfwa/compare_two_documents/) , 2024-02-29-0909
```
I have two pdfs maybe different versions. Document 2 has certain portions changed (the wordings could have been altered)
 is there a way to find the difference between the two semantically. For example document A could be a RFP and document 
B could be a proposal that should adhere to all terms outlined in Document A. I want to find if it does by parsing the m
eaning of both the documents
```
---

     
 
all -  [ [For Hire] Programmer/Web Developer/IT Consultant (Python, PHP, AI, etc.) ](https://www.reddit.com/r/forhire/comments/1b29xvf/for_hire_programmerweb_developerit_consultant/) , 2024-02-29-0909
```
To get in contact, please message me, I don't use the chat thing and might miss you or reply very late. Then we can swit
ch to email/discord/telegram or whatever else. Apologies for starting with this, but many missed it when it was lower.


I'm a programmer/web developer with 14 years of professional experience. I am available for all sorts of programming and
 web development tasks.

I also offer consulting services. If you need something done, but don't know how exactly, I can
 help. I'm an excellent researcher and I communicate well. I will work with you to find the best solution for your probl
em.

My services include, but are not limited to:

* websites

* desktop applications

* AI integration (chatGPT API, la
ngchain, whatever else turns up)

* integration with APIs and other webservices

* all kinds of scripts

* task automati
on

* website optimization

* debugging

* plugins for existing software

* bots (Reddit, Telegram, etc)

* code audits


If you're looking for someone to take care of a variety of different tasks, I can offer continuous support.

My preferr
ed environment is Python with Django, but I work with anything Python or PHP based. I have no problem with learning new 
technologies that are needed for the project.

Rate is $50/h.

Portfolio:

https://bdabkowski.yum.pl

Satisfied customer
s:

https://www.reddit.com/r/testimonials/comments/2e8gqy/pos_uqui_need_a_backend_web_dev_look_no_further/

https://www.
reddit.com/r/testimonials/comments/7fsdze/pos_hiring_uqui_was_an_example_of_how_it_should/

https://www.reddit.com/r/tes
timonials/comments/80pu9l/pos_uqui_great_work_detailed_and_fast/

https://www.reddit.com/r/testimonials/comments/b0nx68/
uqui_is_a_hardworking_intelligent_honest_apps/

https://www.reddit.com/r/testimonials/comments/j3mz3p/uqui_is_a_great_we
b_development_consultant_with/

https://www.reddit.com/r/testimonials/comments/v40ay3/pos_uqui_is_a_great_backend_dev_to
_work_with/

Please note: I am not a designer. To make it clear, it means zero aesthetic sense.
```
---

     
 
all -  [ Scale LLMs for multiple users and Minimum costs ](https://www.reddit.com/r/LangChain/comments/1b29umg/scale_llms_for_multiple_users_and_minimum_costs/) , 2024-02-29-0909
```
Hey guys, I am looking for scaling the LLM Hosting infrastructure. My minimum expectations are to serve at least 100 use
rs per minute with 50 output tokens each and 5000 input tokens. I am hosting a Mistral7B model. 
Thanks in advance.

Wha
t kind of Infra I would need, and what are the optimised options to serve the model using multiple GPUs
```
---

     
 
all -  [ I am looking for partnership business!! ](https://www.reddit.com/r/upwork_posts/comments/1b28vb4/i_am_looking_for_partnership_business/) , 2024-02-29-0909
```
Hi everyone. Nice to meet you all!!

I am currently managing a team named **Sota group** with 15 senior developers in: W
eb/Mobile, Blockchain, AI, and so on.

I am also a developer myself with many years of experience.

**Sota group** is go
ing to sell the services in **US/Canada/EU/Singapore/AU/Japan/Korea** and some other countries. So we are looking for pa
rtners or agents who can find clients for us with % sharing per Agreements and Contracts. Both **long-term and short-ter
m** are accepted.

This is the service list:

**Web development:**

\- Backend: Golang, Python, Nodejs, Java, PHP, Ruby,
 C#, Perl.

\- Frameworks: Gin, Beego, Express, Django, Flask, Spring, Laravel, CodeIgniter, .NET, Jiffy, Dancer...

\- 
Frontend: JS, HTML, CSS

\- Frameworks: React, Refine, Angular, Vue.

**Mobile development:**

Android and IOS: Java, Ko
tlin, Swift, Swift UI, React Native, Flutter.

**AI development:**

AI|ML|LLM|Chatbot|Data science

\- GPT: Python, Lang
chain, LLM (ChatGPT, GPT, GPT 4, OPEN AI…)

\- TensorFlow, Pytorch, Keras, Automl, Scipy…

\- Data visulization: Seaborn
, Plotly

\- NLP: NLTK, Spacy, Gensim…

**Blockchain development:**

\- Decentralized applications (DApps)

\- Account A
bstraction (ERC4337)

\- ERC404 (Mixed ERC20 / ERC721 implementation with native liquidity and fractionalization)

\- BR
C20 (Bitcoin)

\- Tap (Bitcoin)

\- Multisig Bitcoin Wallets

\- Smart Contract Wallet (Smart Wallet)

\- Paymasters

\-
 Meta Transactions/Custom Relayer (Gasless Transactions)

\- MEV Bot

\- Web3/Ethers

\- Truffle/Hardhat/Foundry

\- Sol
idity/Golang (+ Geth/go-ethereum)/Rust

\- Smart Contracts

\- Cryptography

\- Chainlink Oracles

\- Upgradable Smart C
ontracts

\- Smart Contract Testing

\- IPFS/Filecoin

\- NFT Marketplace (ERC721/ERC1155)

\- Private NFTs

\- Dynamic 
NFTs

\- Decentralized Finance (DeFi)

\- DEX (Decentralised Exchange)

\- Metaverse

\- Crypto Token (ERC20)/Cryptocurr
ency

\- Decentralised Storage (Public/Private)

\- Zero-knowledge proofs

\- Ethereum Name Service (ENS)

\- Cross-chai
n communication and messaging (Push/EPNS: Ethereum Push Notification Service)

\- Indexing and Querying Blockchain data 
(The Graph protocol)

\- Access Management

\- Identity Management

\- Block Explorer

\- EVM Development

\- Ethereum/P
olygon (MATIC)/BSC/Tron/Arcana/Base (Coinbase)/Solana/NEAR Protocol

\- Polygon Bridge

\- Cross-Chain

**IOT, Cloud, De
vops...and more!**

If you interested in this proposal. Please contact us to discuss via: [sotagroup.sd@gmail.com](mailt
o:sotagroup.sd@gmail.com)

Let’s grow together!!
```
---

     
 
all -  [ Langchain openai api ](https://www.reddit.com/r/learnpython/comments/1b26k3x/langchain_openai_api/) , 2024-02-29-0909
```
LangChain Openai API

Hey all, I am very new to Python, but I have a great business service idea and need to utilize the
 LangChain Openai API to make it happen...

Currently I am using terminal on my MacBook Air and trying to install LangCh
ain with instructions I found online using:

Pip3 install langchain-openai

Pip3 install langchain[llms]

So for the fir
st command, I was able to install, but it said it was not on PATH

When I tried to upload the second command, it said th
e command does not exist or can't be found, which stumped me because my friend was able to run this via terminal.

Any h
elp or info would be great as I really need to make some progress with this idea.
```
---

     
 
all -  [ Agent just outputs tool output without any editing ](https://www.reddit.com/r/LangChain/comments/1b23qvn/agent_just_outputs_tool_output_without_any_editing/) , 2024-02-29-0909
```
 My problem is my agent is fine with doing this (I intentionally changed the tool to output useless information):

 **> 
Entering new AgentExecutor chain...**

 ***Invoking: \`function\_8\` with \`{}\`*** 

 ***skdfjlsdjflk*** 

**> Finished
 chain.** 

skdfjlsdjflk (this is the output of the entire invocation)

This is my code:

`prompt = ChatPromptTemplate.f
rom_messages(`  
`[`  
`MessagesPlaceholder(variable_name='chat_history'),`  
`('user', '{input}'),`  
`MessagesPlacehol
der(variable_name='agent_scratchpad'),`  
`]`  
`)`  
`chat = AzureChatOpenAI(`  
`temperature=0,`  
`openai_api_version
='2023-12-01-preview',`  
`azure_deployment='****',`  
`).bind(functions=tools)`  
`llm_with_tools = chat.bind(functions
=functions)`  
`agent = (`  
`{`  
 `'input': lambda x: x['input'],`  
 `'chat_history': lambda x: x['chat_history'],`  

 `'agent_scratchpad': lambda x: format_to_openai_function_messages(`  
`x['intermediate_steps']`  
`),`  
`}`  
`| prom
pt`  
`| llm_with_tools`  
`| OpenAIFunctionsAgentOutputParser()`  
`)`  
`agent_executor = AgentExecutor(agent=agent, t
ools=tools, verbose=True)`

`messages.append({`  
 `'role': 'system',`  
 `'content' : guidance})`  
`res = agent_execut
or.invoke(`  
`{`  
 `'input': question,`  
 `'chat_history': message_transform(messages)`  
`}`  
`)`  
`print(res['out
put'])`

Literally just changing the LLM from OpenAI to Gemini on an other project fixes the problem

This is whats happ
ening:

&#x200B;

https://preview.redd.it/iw4p1gqx8blc1.png?width=493&format=png&auto=webp&s=a6174e2731efa0126c39dc99996
ca9eee6560c80

On that other project, the ChatVertexAI would be called again after the tool but not here for some reason
.

What am I missing?
```
---

     
 
all -  [ My book is now listed on Google under the ‘best books on LangChain’ ](https://www.reddit.com/r/developersIndia/comments/1b2373t/my_book_is_now_listed_on_google_under_the_best/) , 2024-02-29-0909
```
 And my book: '***LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs***' final
ly made it to the list of Best books on LangChain by Google. A big thanks to everyone for the support. Being a first tim
e writer and a self-published book, nothing beats this feeling  If you haven't tried it yet, check here :

[https://www.
amazon.in/dp/B0CTHQHT25](https://www.amazon.in/dp/B0CTHQHT25)

https://preview.redd.it/pqr5ey123blc1.png?width=861&forma
t=png&auto=webp&s=6c42d8fd0819362d2308dd6b568d8cf7def55dc1

&#x200B;
```
---

     
 
all -  [ My book is now listed on Google under the ‘best books on LangChain’ ](https://www.reddit.com/r/generativeAI/comments/1b235xc/my_book_is_now_listed_on_google_under_the_best/) , 2024-02-29-0909
```
 And my book: '***LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs***' final
ly made it to the list of Best books on LangChain by Google. A big thanks to everyone for the support. Being a first tim
e writer and a self-published book, nothing beats this feeling  If you haven't tried it yet, check here :

[https://www.
amazon.com/LangChain-your-Pocket-Generative-Applications-ebook/dp/B0CTHQHT25](https://www.amazon.com/LangChain-your-Pock
et-Generative-Applications-ebook/dp/B0CTHQHT25)

&#x200B;

https://preview.redd.it/ifffhy7n2blc1.png?width=850&format=pn
g&auto=webp&s=0c7910bfa36daeb4f555511ec90b4d3db41ebf3b
```
---

     
 
all -  [ My book is now listed on Google under the ‘best books on LangChain’ ](https://www.reddit.com/r/LangChain/comments/1b2331a/my_book_is_now_listed_on_google_under_the_best/) , 2024-02-29-0909
```
And my book: '***LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs***' finall
y made it to the list of Best books on LangChain by Google. A big thanks to everyone for the support. Being a first time
 writer and a self-published book, nothing beats this feeling

If you haven't tried it yet, check here : 

[https://www.
amazon.com/LangChain-your-Pocket-Generative-Applications-ebook/dp/B0CTHQHT25](https://www.amazon.com/LangChain-your-Pock
et-Generative-Applications-ebook/dp/B0CTHQHT25)

https://preview.redd.it/0bm815vw0blc1.png?width=833&format=png&auto=web
p&s=179913ada9855c815375913f3c2c5bae2b4dd1c6
```
---

     
 
all -  [ Vectorstore as retriever: also return scores ](https://www.reddit.com/r/LangChain/comments/1b21o14/vectorstore_as_retriever_also_return_scores/) , 2024-02-29-0909
```
Hi,

at the moment I am retrieving my relevant docs with following code (vectorstore is FAISS.from\_documents):

`retrie
ver= vectorstore.as_retriever(search_type='similarity_score_threshold',search_kwargs={'k': 3, 'score_threshold': 0.82})`


`retriever.get_relevant_documents('QUESTION...?')`

Are the retrieved docs already sorted in a decreasing order? So is
 the most relevant doc the first document in the returned output? Alternatively I want to return the score with this, bu
t don't know how. I can return it with: 

`vectorstore.similarity_search_with_score('Was sind die Grundlagen eines Manag
ementsystems (BCMS)?')`

But with this approach I am not sure how to set 'k' and the 'score\_threshold'.

&#x200B;

Any 
hints on how I can return the scores with the get\_relevant\_docs function?
```
---

     
 
all -  [ Does FAISS (Typescript) not support filters on asRetriever ](https://www.reddit.com/r/LangChain/comments/1b20tge/does_faiss_typescript_not_support_filters_on/) , 2024-02-29-0909
```
Is that correct that FAISS does not support filtering?

This filter is ignored: 

    const retriever = store.asRetrieve
r(undefined, {docType: 'code'});

Where the metadata is

     doc.metadata = { docType: 'code' }

And the documentation 
for the FAISS vector store gives no example of filtering [https://js.langchain.com/docs/integrations/vectorstores/faiss]
(https://js.langchain.com/docs/integrations/vectorstores/faiss)

Is there a way to use FAISS as a store and still only g
et documents that match a filter?
```
---

     
 
all -  [ What storage/ search are you using for retrieval  ](https://www.reddit.com/r/LangChain/comments/1b1x880/what_storage_search_are_you_using_for_retrieval/) , 2024-02-29-0909
```
I have been using faiss but it looks like there are more capabilities in using something like qdrant or weaviate. Their 
hybrid search seems like a good option. 

I am don't know what to switch to. What have you been using and how was your e
xperience? 
```
---

     
 
all -  [ Langchain vs LlamaIndex ](https://www.reddit.com/gallery/1b1qzxt) , 2024-02-29-0909
```
now, fight!

also if anyone knows how to get a streaming loop working in langchain

I get all sorts of abstracted type e
rrors when I try and the documentation makes me want to cry

I just started learning llama index yesterday
```
---

     
 
all -  [ How To Use LangChain With ChatGPT ](https://www.successtechservices.com/how-to-use-langchain-with-chatgpt/) , 2024-02-29-0909
```

```
---

     
 
all -  [ How to Use LangChain with ChatGPT ](https://www.reddit.com/r/ArtificialInteligence/comments/1b1ntdk/how_to_use_langchain_with_chatgpt/) , 2024-02-29-0909
```
Did you know that integrating LangChain with ChatGPT can significantly enhance natural language processing capabilities 
and enable smarter text processing in your projects? LangChain, a powerful framework for building custom ChatGPT AI, com
bined with the AI language model, ChatGPT, can revolutionize the way you work with language data. 

In this article, I w
ill guide you through the process of integrating LangChain and ChatGPT step-by-step, allowing you to leverage their comb
ined power and unlock their full potential for efficient text processing.

https://www.successtechservices.com/how-to-us
e-langchain-with-chatgpt/
```
---

     
 
all -  [ Example unit test for Langchain chat models ](https://www.reddit.com/r/LangChain/comments/1b1kkkq/example_unit_test_for_langchain_chat_models/) , 2024-02-29-0909
```
Something I think that's missing from Langchain documentation is good examples for how to reliably test your chains/chat
s/whatever without actually using a real LLM (costly/slow/unreliable).

I created an example (with Dockerfile included) 
on how to test an LLMChain with a brief conversation including a ConversationBufferWindowMemory.

Please let me know wha
t you think! If you have other requests, let me know.

[https://github.com/ThreeRiversAINexus/sample-langchain-agents/bl
ob/main/fake\_llm\_examples/test\_chat\_convo.py](https://github.com/ThreeRiversAINexus/sample-langchain-agents/blob/mai
n/fake_llm_examples/test_chat_convo.py)

The example in the langchain documentation that this is based on: [https://pyth
on.langchain.com/docs/modules/model\_io/chat/quick\_start](https://python.langchain.com/docs/modules/model_io/chat/quick
_start)
```
---

     
 
all -  [ Codeflash - Optimize your code's performance automatically ](https://www.reddit.com/r/Python/comments/1b1kh2h/codeflash_optimize_your_codes_performance/) , 2024-02-29-0909
```
Hi! I am Saurabh. I love writing fast programs and I've always hated how slow Python code can sometimes be. To solve thi
s problem, I have created Codeflash.

# What My Project Does

`codeflash` is a Python package that uses AI to figure out
 the most performant way to rewrite a Python code. It not only optimizes the performance but also verifies the correctne
ss of the new code, i.e. makes sure that the new code follows exactly the same behavior as your original code. This auto
mates the manual optimization process.

It can improve algorithms, data structures, fix logic, use better optimized libr
aries etc to speed up your code.

Website - [https://www.codeflash.ai/](https://www.codeflash.ai/) , get started here.


PyPi - [https://pypi.org/project/codeflash/](https://pypi.org/project/codeflash/)

If you have a Python project, it shou
ld take you less than 5 minutes to setup codeflash - `pip install codeflash` and `codeflash init`

Codeflash can also op
timize your entire project! Run `codeflash --all` after setting up codeflash, and codeflash will optimize your project, 
function by function, and create PRs on GitHub when it finds an optimization. This is super powerful.  
You can also ins
tall codeflash as a GitHub actions check that runs on every new PR you create, to ensure that all new code is performant
. If codeflash finds that a code can be made more performant, it will create a PR comment with the new optimized code. T
his ensures that your project stays at peak performance everytime.

# How it works

Codeflash works by optimizing the co
de path under a function. So if there is a function `foo(a, b):` , codeflash finds the fastest implementation of the fun
ction foo and all the other functions it calls. The optimization procedure preserves the signature of the function foo a
nd then figures out a new optimized implementation that results in exactly the same return values as the original foo. T
he behavior of the new function is verified to be correct by running your unit tests and generating a bunch of new regre
ssion tests. The runtime of the new code is measured and the fastest one is recommended.

# Target Audience

Codeflash i
s currently the best at optimizing pure-functions without side effects. You can use codeflash to improve performance of 
any custom algorithm, numpy code, pandas code, data processing code etc. It is very general purpose. You should be able 
to optimize anything that can be unit-tested.

You can also try to optimize non-pure functions but you should review the
 new code. We are improving support to more types of functions. Would love to know about your use case and how we can su
pport it!

# Comparison

I am currently unaware of any direct comparison with codeflash on optimizing performance of use
r level code.



Codeflash is still early but has gotten great results already by optimizing open source projects like [
Langchain](https://github.com/langchain-ai/langchain/pull/8151) and many others. I would love you to try codeflash to op
timize your code and let me know how you use it and how we can improve it for you!

Thank you.
```
---

     
 
all -  [ RAG model where the user may mention documents in their messages ](https://www.reddit.com/r/LangChain/comments/1b1k6ed/rag_model_where_the_user_may_mention_documents_in/) , 2024-02-29-0909
```
I have a pretty good LangChain RAG model up and running, and some users asked if it's possible to include the names and 
URLs of the documents they would like the LLM to search for in the query. 

Something like:

Human query: 'Using only th
e data sheet of product X, can you confirm that ....'

Programmatically speaking, I know that I can define the search fi
lters in the retriever, but I am not sure what is the best way to have the LLM detect the document the user is referring
 to first. 

A naive approach here would be to do this outside of the chain, and preprocess the query before sending it 
to the RAG model (alongside any search filter metadata, if applicable). Is that the recommended approach? What about age
nts, would that be too much for this use case?
```
---

     
 
all -  [ How to include metadata of retrieved content in the Output of retriever ](https://www.reddit.com/r/LangChain/comments/1b1k4p7/how_to_include_metadata_of_retrieved_content_in/) , 2024-02-29-0909
```
Hi, 

I have a noob question and hope that someone here can help. 

This is how I set up the agent for my blog.   
embed
dings = OpenAIEmbeddings()  
db = FAISS.load\_local('faiss\_index', embeddings)  
retriever = db.as\_retriever(search\_t
ype='mmr')  
tool = create\_retriever\_tool(  
 retriever,  
 'search\_chandler\_nguyen\_blog',  
 'Searches and returns
 content from Chandler Nguyen's blog. For any questions about what Chandler Nguyen wrote before, you must use this tool!
'  
)  
tools = \[tool\]  
prompt = ChatPromptTemplate.from\_messages(\[  
('system', 'You are a chatbot named Sydney. \
\  
You help people to answer questions about Chandler Nguyen Blog in a thoughtful way. \\  
Respond in a friendly and h
elpful tone, with concise answers if possible. \\  
Use HTML-compatible bullet point format and line breaks (\`<br>\`) f
or long answers where necessary. \\  
Use the following pieces of retrieved context to answer the question. \\  
If you 
do not know the answer, just say that you do not know. \\  
Include the blog post URL or published date as references in
 your answer. \\  
Please try your best as this is very important to me and the user.'),  
 MessagesPlaceholder(variable
\_name='chat\_history'),  
('user', '{input}'),  
 MessagesPlaceholder(variable\_name='agent\_scratchpad'),  
\])  
llm 
= ChatOpenAI(model\_name='gpt-3.5-turbo-0125', temperature=0, streaming=True)  
llm\_with\_tools = llm.bind\_tools(tools
)  
agent = (  
{  
 'input': lambda x: x\['input'\],  
 'agent\_scratchpad': lambda x: format\_to\_openai\_tool\_messag
es(x\['intermediate\_steps'\]),  
 'chat\_history': lambda x: x\['chat\_history'\],  
}  
 | prompt  
 | llm\_with\_tool
s  
 | OpenAIToolsAgentOutputParser()  
)  
agent\_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)  



The issue here is that the retriever output doesn't include metadata like 'URL' or 'published\_date'. I don't know how t
o incorporate that using RunnablePassthrough or RunnableParallel and/or 'lambda' ?  
I know that I can have the metadata
 this way:  
docs = retriever.invoke(query)  
for doc in docs:  
 print(doc.metadata)

&#x200B;
```
---

     
 
all -  [ Improve speed over my LLM App ](https://www.reddit.com/r/LocalLLaMA/comments/1b1jtvw/improve_speed_over_my_llm_app/) , 2024-02-29-0909
```
Hey guys,  
I was wondering I built a Langchain app connecting to a local LLM using Ollama (Mistral 8x7b) I make thousan
ds of requests a day, I have an RTX 4090 but it takes 15 - 25 seconds to get a response with 100% of my GPU used (checki
ng using nvtop). I was wondering if I build a rack and over time, adding more and more GPU Cards will I be able to incre
ase the speed of request processing?
```
---

     
 
all -  [ Langchain / OpenAI to Mistral conversion to query mysql ](https://www.reddit.com/r/LangChain/comments/1b1jfc2/langchain_openai_to_mistral_conversion_to_query/) , 2024-02-29-0909
```
I have the following LangChain/OpenAI that queries a mysql database for data, I have spun up a Mistral LLM thats running
 on my localhost - I have tried to adapt my code to solely use the Mistral LLM, however I get the following error:

Conn
ection could not be made due to the following error: Connection error.

&#x200B;

    def query_maker(user_input):
     
   #OpenAI LLM (working)
        #openaiLLM = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7, openai_api_key=api_key,
 cache=False)
        #Mistral LLM
        LLM = ChatOpenAI(model='mistral', temperature=0.7, base_url='http://localhost
:1234/v1/chat/completions', cache=False)
        prompt_template = PromptTemplate.from_template(
        '{system_prompt
} + '\n' +  {user_input}.')
        chain = LLMChain(llm=LLM, prompt=prompt_template)
        query=chain.run({'system_p
rompt': query_maker_gpt_system_prompt, 'user_input': user_input})
        return query

&#x200B;
```
---

     
 
all -  [ Few-Shot Classification using LangChain  ](https://www.reddit.com/r/LangChain/comments/1b1e1pp/fewshot_classification_using_langchain/) , 2024-02-29-0909
```
Hey everyone, check out this tutorial to understand how Few-Shot Classification (classification with very few labelled s
amples) can be achieved using LangChain's FewShotPromptTemplate https://youtu.be/iWqke_cbapU?si=oyXs7Ex5PctvITra
```
---

     
 
all -  [ Proper Storage Format for Local Retrieval of a ConversationalRetrievalChain QA Object ](https://www.reddit.com/r/LangChain/comments/1b1c85m/proper_storage_format_for_local_retrieval_of_a/) , 2024-02-29-0909
```
I want to store a QA object of type 

    <class 'langchain.chains.conversational_retrieval.base.ConversationalRetrieval
Chain'>

locally. What format should I use to ensure correct retrieval for question answering across different projects?

```
---

     
 
all -  [ What should I learn to get an internship in ML/AI? ](https://www.reddit.com/r/learnmachinelearning/comments/1b1ba2z/what_should_i_learn_to_get_an_internship_in_mlai/) , 2024-02-29-0909
```
I am a 1st year undergrad student. Backstory, I just learnt pandas and have been learning langchain for a while now. I a
m planning of taking CS229 after learning numpy and matplotlib. But my question is what should I learn or what projects 
I should built to get a summer internship? Should I build RAG application or learn something else? Any advice will be he
lpful. Thank you.
```
---

     
 
all -  [ How to load LLMs ](https://www.reddit.com/r/LargeLanguageModels/comments/1b1b96d/how_to_load_llms/) , 2024-02-29-0909
```
Hey there, I am relatively new to working with LLM. So far in order to work with LLMs I've been using libs like langchai
n and ollama that let you load LLM models and use them.

But I wonder how does this libs do that, I've been looking on t
heir repos to understand how does it works, but I wonder if there are some other sources I can take a look on how to do 
that.

I´d like to understand the process it takes to pick the llm file, open it with my code and serve it. Do I go and 
open also the inferences, do I have to tokenize or build my tokenizer first?

thanks a lot!!
```
---

     
 
all -  [ Production ready? ](https://www.reddit.com/r/LangChain/comments/1b194ot/production_ready/) , 2024-02-29-0909
```
Hey guys.  


Some Youtuber criticised LangChain saying it's not production ready. He said the the following: 'It abstra
cts away too many details from you, which makes it super hard to customize for a specific real world use case. Besides, 
it was released before function calling models, so there is no type validation, which is essential in production to prev
ent hallucinations.'  

Do you guys agree here?  


Do you think LangChain can be used in production already or are ther
e crucial steps making LangChain and possibly any other framework a hobby project for now?  


I like the idea of LangGr
aph a lot for example but not sure how much time I should invest here. 
```
---

     
 
all -  [ Deploy Mistral Large to Azure and create a conversation with Python and LangChain ](https://www.reddit.com/r/MistralAI/comments/1b180mk/deploy_mistral_large_to_azure_and_create_a/) , 2024-02-29-0909
```
&#x200B;

https://preview.redd.it/ozoz75uhm3lc1.jpg?width=2400&format=pjpg&auto=webp&s=6a6dc63ed2c78e5e697229cf8616435a8
2f40d9a

[Mistral AI ](https://mistral.ai/)has recently unveiled its most advanced open-source large language model (LLM
) yet, [Mistral Large](https://mistral.ai/news/mistral-large/), alongside its ChatGPT competitor, [Le Chat (beta)](https
://chat.mistral.ai/chat). Le Chat includes other models such as Next, and Small, to let you explore Mistral AI’s capabil
ities. 

This step-by-step guide will show you how to deploy Mistral Large on Azure and start using it immediately with 
LangChain.

[https://neon.tech/blog/deploy-mistral-large-to-azure-and-chat-with-langchain](https://neon.tech/blog/deploy
-mistral-large-to-azure-and-chat-with-langchain)
```
---

     
 
all -  [ Chat doesn`t provide right url, using SiteLoader ](https://www.reddit.com/r/LangChain/comments/1b0w8su/chat_doesnt_provide_right_url_using_siteloader/) , 2024-02-29-0909
```
Hello,

i\`m using siteloader to read data from sitemap.

The whole chat works great but it doesn\`t provide a right lin
k for the product.

How to get it working?  
this is my code: [https://pastebin.com/wxtutj6c](https://pastebin.com/wxtut
j6c)
```
---

     
 
all -  [ I want a reformatted table as output from a PDF file ](https://www.reddit.com/r/LangChain/comments/1b0nhga/i_want_a_reformatted_table_as_output_from_a_pdf/) , 2024-02-29-0909
```
So I have some PDF files with tables not structured properly. Basically some exported pivot tables from excel so they ar
e spaces in between categories which make it impossible to load this data.

I want to use langchain to solve this. 

So 
far this has mixed results. I tried giving the PDFs directly to ChatGPT 4 but it sometimes misses some columns and some 
times for gets a few rows. 

I am looking for how I can use vectorDB to enhance retrieval? Any suggestions? 
```
---

     
 
all -  [ Creating RAG Evaluation dataset manually: What to consider? ](https://www.reddit.com/r/LangChain/comments/1b0n1c9/creating_rag_evaluation_dataset_manually_what_to/) , 2024-02-29-0909
```
Hi,

I want to manually create a Evaluation dataset for RAG with complex Pdfs. I already tried synthetic dataset creatio
n but think you get more reliable evaluation results with human labeled data (e.g. experts on a specific topic, so they 
knlw which questions they would ask and which answers they would expect).

As human annotation is costly I want to discu
ss important Things to consider when starting.

My plan would be: 
- go through Pdf and Ask 10 questions per document
- 
Answer every question based on the informations from the doc

But how would you evaluate the retriever? You somehow woul
d need to also extract the context (for RAGAS it would be 'ground_truths'). For manual annotation, should the annotator 
just copy the page where the relevant context is inside?

Another interesting thing would be how to answer more complex 
questions, e.g. where the true answer is stored on muliple different pages of the document.

Thanks for all suggestions!

```
---

     
 
all -  [ Sql Agents in Langgraph  ](https://www.reddit.com/r/LangChain/comments/1b0mts2/sql_agents_in_langgraph/) , 2024-02-29-0909
```
Has anyone able to implement sql agent in langgraph,? If so can you please share rhe approach
```
---

     
 
all -  [ using private data with openai + account? ](https://www.reddit.com/r/OpenAI/comments/1b0mgo9/using_private_data_with_openai_account/) , 2024-02-29-0909
```
unfortunately i can't code. I'm looking for a tool that will allow me to use private data and query it with openai + (gp
t 4, LLama, etc.)  


I've looked into langchain but my brain is not smart enough, at the moment to utilize it.   I'm go
ing to try to learn some basic programming skills to at least be able to understand how things work but i'm not there ye
t.  Until then, are there any easy to use programs that are open source, can run on my hardware, and have idiot friendly
 user interfaces available?    


what are some open source options that can help me use my data with LLM's?  


thanks 
for any guidance
```
---

     
 
all -  [ Lang chain alternatives? ](https://www.reddit.com/r/LangChain/comments/1b0mcuk/lang_chain_alternatives/) , 2024-02-29-0909
```

So I’m looking to learn how to build ai systems and bots, mostly self taught for projects and research and not for jobs
 as such, I find Langchain to be exactly what I need but most people I’ve seen say it’s terrible and really pointless fo
r even slightly applications and alternatives should be used 

Any ideas or feedback on what to do?

```
---

     
 
all -  [ Unable to split text received via POST request in django ](https://www.reddit.com/r/django/comments/1b0kxtx/unable_to_split_text_received_via_post_request_in/) , 2024-02-29-0909
```
i am facing a weird error. I am trying to split text sent via form in django.

Here is my code in django

    from trans
formers import AutoTokenizer
    from langchain.text_splitter import CharacterTextSplitter
    tokenizer = AutoTokenizer
.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english' )
    text_splitter = CharacterTextSplitter.from_hugg
ingface_tokenizer(
    tokenizer, chunk_size=256, chunk_overlap=0
    )
    def home(request):
      if request.method =
= 'POST':
         input_text = request.POST['input']  
         print('input_text',type(input_text))
         splitted_
text = text_splitter.split_text(str(input_text))
         print('splitted_text_length_outside',len(splitted_text))

The 
length is always 1, which mean text is not split. I have checked that i am receiving text from html form and the type of
 text is str.

&#x200B;

But when i use the same code outside of django such as in jupyter notebook, it work well and sp
lit the text.

&#x200B;

In django I tried \`input\_text.split()\` and that worked. But i am clueless why the langchain 
text splitter is not working.

&#x200B;

    def home(input_text):
      splitted_text = text_splitter.split_text(input_
text)
      print('splitted_text_length_outside',len(splitted_text))
    

&#x200B;

Here is my html form

    <form act
ion='{% url 'home' %}' method='post' id='myForm'>
{% csrf_token %}
    <textarea name='input' id='input' rows='4' cols='
50'></textarea>
    <br>
<button type='submit'>Submit</button>
    </form>


Here is my ajax code

&#x200B;

    $('#myF
orm').submit(function(event) {
    event.preventDefault();
    let formData = $(this).serialize();
    $.ajax({
    'url
': '/',
    'type': 'POST',
    'data': formData,
    'success': function(response) {
    console.log('success');
    },

    'error': function(error) {
    console.log('error',error);
    }});});
    
```
---

     
 
all -  [ RAG Framework playlist ](https://www.reddit.com/r/LangChain/comments/1b0kwvk/rag_framework_playlist/) , 2024-02-29-0909
```
Check out this playlist that covers
1. What is RAG? RAG framework explained with diagram
2. Multi-Document RAG
3. RAG us
ing persisted Vector DB
4. RAG vs Fine-Tuning
5. Saving & Loading Vector DBs
6. RAG FAQs
7. Analyze PDF, CSV, Youtube vi
deo, json, text and GitHub code using RAG

https://youtube.com/playlist?list=PLnH2pfPCPZsJ1qBbf0Fb7onButMjqYa-Z&si=_NgYV
sZ9QaEdaidC

```
---

     
 
all -  [ Returning source documents ](https://www.reddit.com/r/LangChain/comments/1b0hqkz/returning_source_documents/) , 2024-02-29-0909
```
My current implementation returns source documents for every answer, but it can be very slow (numerous users have also c
ommented on slow response times).  Vercel has an [integration](https://sdk.vercel.ai/docs/guides/providers/langchain) wi
th Langchain that seems to work a little faster, but it doesn't return sources, which is a deal-breaker for me.

My impl
ementation is based partly off of Langchain's [Conversational Retrieval Chain](https://js.langchain.com/docs/get_started
/quickstart#conversational-retrieval-chain), but I'm streaming the response back to the client.  This is the only soluti
on that has worked for me, despite being slow.

I'm also using a Pinecone vector db since my data is custom.  Is there a
 way to improve any part of this architecture to improve response times?
```
---

     
 
all -  [ Lang chain alternatives? ](https://www.reddit.com/r/CodingHelp/comments/1b0h5gl/lang_chain_alternatives/) , 2024-02-29-0909
```
So I’m looking to learn how to build ai systems and bots, mostly self taught for projects and research and not for jobs 
as such, I find Langchain to be exactly what I need but most people I’ve seen say it’s terrible and really pointless for
 even slightly applications and alternatives should be used 

Any ideas or feedback on what to do?

```
---

     
 
all -  [ Scrape any website to markdown for RAG ](https://www.reddit.com/r/LangChain/comments/1b0gw81/scrape_any_website_to_markdown_for_rag/) , 2024-02-29-0909
```
I've just launched [UseScraper.com](https://UseScraper.com) which crawls all the pages from any website to markdown or j
son. I made a short post on using it with langchan [https://usescraper.com/blog/langchain-chatgpt-rag-with-your-website-
content](https://usescraper.com/blog/langchain-chatgpt-rag-with-your-website-content)  

```
---

     
 
all -  [ Processing of PDFs ](https://www.reddit.com/r/LangChain/comments/1b0ezm6/processing_of_pdfs/) , 2024-02-29-0909
```
Hi,

We are doing RAG on ecological reports. Most of them are in PDF format. Much of the data is in tables, often with j
oined cells. I have had a lot of difficulty converting these to a text format that ChatGPT would understand. After tryin
g out all available python libraries for PDF to text, I ended up with pymupdf. I use a lot of tricks to extract the tabl
es (because there are often more than one per page) and then convert them to markdown format. The text snippets are then
 uploaded to Azure Search together with a bunch of metadata. It works ok, but processing takes a little time and Azure S
earch is crazy expensive. 

I just got a GPT4 license and tried uploading a report together with a question related to a
 complicated table in the document. The answer was spot on and processing was very fast. 

We cannot use OpenAI privacy 
reasons, otherwise it would be the best solution for my clients.  Azure's OpenAI API does not allow documents to be uplo
aded and MS copilot studio / graph fails in its answers. 

Is there a solution to this problem that I don't know about? 
Has anyone any idea on how OpenAI manages to process PDFs when all other libraries fail? 

&#x200B;
```
---

     
 
all -  [ Chat History implementation in langGraph ](https://www.reddit.com/r/LangChain/comments/1b0ey9g/chat_history_implementation_in_langgraph/) , 2024-02-29-0909
```
Hello,  
I have been using the chat history class of langchain to manage and save persistently the chat\_history of a ch
atbot application. Now I am exploring the langGraph, and I could only find tutorials about the rag application. Do you t
hink that langGraph is suitable also for creating a RAG chatbot with memory integration? And how can it be done??
```
---

     
 
MachineLearning -  [ [D] Graphs + vectordbs? Need your input: Cognee.ai . AI Data Pipelines for Real-World Production (Pa ](https://www.reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/) , 2024-02-29-0909
```
Hey there, Redditors!

I'm back with the latest installment on creating dependable AI data pipelines for real-world prod
uction.

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://top
oteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba40a
ab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines.

After a few months of work
, we integrated cognitive architecture with [keepi.ai](https://www.keepi.ai) 

We aim to explore with our demo:

**1. Co
ntext sanitization**  
The world of AI is fast-moving, and we've realized that the context is becoming a building block 
we refer to as a crucial part of future cognitive architecture.  
**2. Best Practices for AI Memory**  
In this rapidly 
evolving landscape, there are no established best practices. You'll need to make educated bets on tools and processes, k
nowing that things will change. We assume that having traditional data engineering practices + frameworks + classifiers 
and other AI solutions can solve a lot of standard hurdles  
**3. AI Frameworks**  
They are trying to do too much, too 
fast, too broad. We want to find a pattern and a correct layer of abstraction for the AI memory to fit new industry.  



&#x200B;

How does it work? 

The Github repo is l:

  


[How cognee works](https://preview.redd.it/yuiabmyihyjc1.png?
width=1633&format=png&auto=webp&s=4384c4441b615f72caf1e0591c5ab23aee735fab)

Github repo is [here](https://github.com/to
poteretes/cognee)

Next steps:  
I have questions for you:

1. Is context sanitization relevant for you?
2. How do you m
anage metadata? 
3. How do you prepare data for LLMs?
4. Are there any data enrichment steps you perform?

Check out the
 blog post:

[Link to part 4](https://topoteretes.notion.site/Going-beyond-Langchain-Weaviate-Level-4-towards-production
-fe90ff40e56e44c4a49f1492d360173c?pvs=4)

*Remember to give this post an upvote if you found it insightful!*  
*And also
 star our* [Github repo](https://github.com/topoteretes/cognee)
```
---

     
 
MachineLearning -  [ [D] AI projects Suggestions ](https://www.reddit.com/r/MachineLearning/comments/1aunkmw/d_ai_projects_suggestions/) , 2024-02-29-0909
```
Hi Everyone, I need a suggestion to create AI courses for students ( Hands-on AI projects). I am thinking about the late
st AI trends such as Langchain, RAG, and vector databases. In each project, there can be multiple tasks, and the main th
ing is each task should have an automated system in which we can verify whether students have done it correctly or not.


For example: Project with visualization cannot be automatically tested. 

For example: A project with visualization can
not be automatically tested. . em can verify if the length of the text is smaller we can verify that it is correct.
```
---

     
 
MachineLearning -  [ Whats in your RAG setup? [D] ](https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/) , 2024-02-29-0909
```
What frameworks and libraries are you using in your RAG? 

I'm most curious if  LangChain is as popular as it was?

Here
's mine at a high-level: 

*  langchain to use OpenAI for creating embeddings
* Pinecone for storing embedding
* langcha
in to load document splitters and characters splitters for chunking
* Mongo for conversations memory

&#x200B;
```
---

     
 
MachineLearning -  [ [D] What's the best current RAG setup that would work with a local LLM? ](https://www.reddit.com/r/MachineLearning/comments/1ag6bo7/d_whats_the_best_current_rag_setup_that_would/) , 2024-02-29-0909
```
I've tried things like langchain in the past (6-8 months ago) but they were cumbersome and didn't work as expected.

I  
need RAG to get data from various pdfs (long one, 150+ pages) - and i  need a setup that will allow me to add more and m
ore data sources.

I wanna run this locally, can get a 24gb video card (or 2x16gb ones) - so i can run using 33b or smal
ler models.

I  know things in the industry change every 2 weeks, so i'm hoping there's  an easy and efficient way of do
ing RAG (compared to 6 months ago)
```
---

     
 
MachineLearning -  [ [P]: Anukool: My job hunting assistant ](https://www.reddit.com/r/MachineLearning/comments/1adu3tw/p_anukool_my_job_hunting_assistant/) , 2024-02-29-0909
```
Hey Reddit, I've been applying for jobs and found that writing a cover letter for each position was tedious. I also delv
ed into LLM and Langchain, hoping to leverage them for a project to aid in my job hunting. So, I developed Anukool under
 the GPL license. While it's far from perfect, it has proven very useful to me, and I hope it benefits you as well. All 
I have to do is provide it with a pdf containing information about me such as my experience, skills, projects, etc and i
t will use this information along with job description to generate cover letter for me. Since I'm new to ML and LLM, any
 advice or feedback is greatly appreciated, and contributions are also welcome. I plan to utilize Llama-2 soon to furthe
r open-source the project.

Check out the GitHub link, and please star it if you find the project interesting: https://g
ithub.com/dakshesh14/anukool
```
---

     
 
deeplearning -  [ [D] WebVoyager: Navigating Digital Cosmos with LangGraph & Multimodal Models ](https://www.reddit.com/r/deeplearning/comments/1altlca/d_webvoyager_navigating_digital_cosmos_with/) , 2024-02-29-0909
```
Embark on a journey through the digital cosmos with WebVoyager, a groundbreaking Large Multimodal Model (LMM) web agent 
designed to navigate the vastness of the online universe. In collaboration with Langchain, WebVoyager represents a parad
igm shift in autonomous web agents, seamlessly integrating visual and textual information to complete user instructions 
end-to-end by interacting with real-world websites.

Link: [https://medium.com/@andysingal/webvoyager-navigating-digital
-cosmos-with-langgraph-multimodal-models-dace64196c2f](https://medium.com/@andysingal/webvoyager-navigating-digital-cosm
os-with-langgraph-multimodal-models-dace64196c2f)
```
---

     
 
deeplearning -  [ [D] Langchain Elevates with Step-Back Prompting using RAGatouille ](https://www.reddit.com/r/deeplearning/comments/1agtyeh/d_langchain_elevates_with_stepback_prompting/) , 2024-02-29-0909
```
In the dynamic realm of natural language processing, a revolutionary synergy has emerged between Langchain and Step-Back
 Prompting. This article delves into the transformative collaboration, exploring how Langchain’s cutting-edge platform i
ncorporates Step-Back Prompting to redefine language processing capabilities. Join us on a journey of innovation and dis
covery as we unravel the intricacies of this powerful integration. As we explore the uncharted territories of language m
odels, Step-Back Prompting stands as a beacon of progress, promising a journey of nuanced understanding and elevated per
formance in the world of Large Language Models. Welcome to the future of language processing, where inspiration and inno
vation converge in a symphony of words and ideas.

Link: https://medium.com/ai-advances/langchain-elevates-with-step-bac
k-prompting-using-ragatouille-b433e6f200ea
```
---

     
 
deeplearning -  [ Become an AI Developer (Free 9 Part Series) ](https://www.reddit.com/r/deeplearning/comments/1afgp2r/become_an_ai_developer_free_9_part_series/) , 2024-02-29-0909
```
Just sharing a free series I stumbled across on Linkedin - DataCamp's 9-part AI code-along series.

This specific sessio
n linked below is 'Building Chatbots with OpenAI API and Pinecone' but there are 8 others to have a look at and code alo
ng to.

*Start from basics to build on skills with GPT, Pinecone and LangChain to create a chatbot that answers question
s about research papers. Make use of retrieval augmented generation, and learn how to combine this with conversational m
emory to hold a conversation with the chatbot. Code Along on DataCamp Workspace:* [*https://www.datacamp.com/code-along/
building-chatbots-openai-api-pinecone*](https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone)

Find
 all of the sessions at: [https://www.datacamp.com/ai-code-alongs](https://www.datacamp.com/ai-code-alongs)
```
---

     
 
deeplearning -  [ DSPy Explained! ](https://www.reddit.com/r/deeplearning/comments/1adypks/dspy_explained/) , 2024-02-29-0909
```
DSPy is the next big advancement for AI and building applications with LLMs!

Pioneered by frameworks such as LangChain 
and LlamaIndex, we can build much more powerful systems by chaining together LLM calls! This means that the output of on
e call to an LLM is the input to the next, and so on. We can think of chains as programs, with each LLM call analogous t
o a function that takes text as input and produces text as output.

DSPy offers a new programming model, inspired by PyT
orch, that gives you a massive amount of control over these LLM programs. Further the Signature abstraction wraps prompt
s and structured input / outputs to clean up LLM program codebases.

DSPy then pairs the syntax with a super novel compi
ler that jointly optimizes the instructions for each component of an LLM program, as well as sourcing examples of the ta
sk.

Here is my review of the ideas in DSPy, covering the core concepts and walking through the introduction notebooks s
howing how to compile a simple retrieve-then-read RAG program, as well as a more advanced Multi-Hop RAG program where yo
u have 2 LLM components to be optimized with the DSPy compiler! I hope you find it useful!

https://www.youtube.com/watc
h?v=41EfOY0Ldkc
```
---

     
