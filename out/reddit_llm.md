 
all -  [ Unable to get desired results with ChatPromptTemplate and Prompt Caching with Anthropic ](https://www.reddit.com/r/LangChain/comments/1g4ip6z/unable_to_get_desired_results_with/) , 2024-10-16-0913
```
I have a long prompt of instructions that performs as intended when I use PromptTemplate.  
After reading about Prompt C
aching, I tried to implement it with the ChatPromptTemplate, but it did not work as intended. The demo of prompt caching
 uses a book as its context. I have a smaller context but specific instructions.  
Tried fine-tuning the prompt, but the
 model hallucinates badly.

Example: When I ask a question, it does not use the same question to reason/generate the ans
wer.
```
---

     
 
all -  [ Cisco Use Cases, ITSI Best Practices, and More New Articles from Splunk Lantern ](https://www.reddit.com/r/Splunk/comments/1g4g5wh/cisco_use_cases_itsi_best_practices_and_more_new/) , 2024-10-16-0913
```
[Splunk Lantern](https://lantern.splunk.com/) is a Splunk customer success center that provides advice from Splunk exper
ts on valuable data insights, key use cases, and tips on managing Splunk more efficiently.

We also host Getting Started
 Guides for a range of Splunk products, a library of Product Tips, and [Data Descriptor](https://lantern.splunk.com/Data
_Descriptors) articles that help you see everything that’s possible with data sources and data types in Splunk.

This mo
nth, we’re excited to share some articles that show you new ways to get Cisco and AppDynamics integrated with Splunk. We
’ve also updated our  Definitive Guide to Best Practices for IT Service Intelligence (ITSI), and as usual, we’re sharing
 all the rest of the use case, product tip, and data articles that we’ve published over the past month. Read on to find 
out more.

# Splunking with Cisco and AppDynamics

  
Here on the Splunk Lantern team we’ve been busy working with exper
ts in Cisco, AppDynamics, and Spunk to develop articles that show how our products can work together. Here are some of t
he most recent articles we’ve published, and keep watching out for more Cisco and AppD articles over the coming months!


[Monitoring Cisco switches, routers, WLAN controllers and access points](https://lantern.splunk.com/Security/UCE/Founda
tional_Visibility/Security_monitoring/Monitoring_Cisco_switches%2C_routers%2C_WLAN_controllers_and_access_points) shows 
you how to create a comprehensive solution to monitor Cisco network devices in the Splunk platform or in Splunk Enterpri
se Security. Learn how to get set up, create visualizations, and troubleshoot common problems in this new use case artic
le.

https://preview.redd.it/psocr3wu1zud1.png?width=781&format=png&auto=webp&s=d1d3ae3bbf9a8a299deaabf57d011e3d20f95bbd


[Enabling Log Observer Connect for AppDynamics](https://lantern.splunk.com/Observability/Product_Tips/Log_Observer_Con
nect/Enabling_Log_Observer_Connect_for_AppDynamics) teaches you how to configure Log Observer Connect for AppDynamics, a
llowing you to access the right logs in Splunk Log Observer Connect with a single click, all while providing troubleshoo
ting context from AppDynamics.

Looking for more Cisco and AppDynamics use cases? Check out our [Cisco](https://lantern.
splunk.com/Data_Descriptors/Cisco) and [AppDynamics](https://lantern.splunk.com/Data_Descriptors/AppDynamics) data descr
iptor pages for more configuration information, use cases and product tips, and please let us know in the comments what 
other articles you’d like to see!

# ITSI Best Practices

  
The [Definitive Guide to Best Practices for IT Service Inte
lligence](https://lantern.splunk.com/Observability/Product_Tips/IT_Service_Intelligence/The_definitive_guide_to_best_pra
ctices_for_IT_Service_Intelligence) is a must-read resource for ITSI administrators, with essential guidelines that help
 you to unlock the full potential of ITSI. We’ve just updated this resource with fresh articles to help you ensure optim
al operations and exceptional end-user experiences.

[Using dynamic entity rule configurations](https://lantern.splunk.c
om/Observability/Product_Tips/IT_Service_Intelligence/Using_dynamic_entity_rule_configurations) is helpful for anyone wh
o often adds or removes entities from their configurations. Learn how to create a rule configuration that updates immedi
ately and without the need for service configuration changes, reducing the time and risk of error that can result from m
anually reconfiguring entity filter rules.

If you use the ITSI default aggregation policy, you might not know that you 
shouldn’t be using this as your primary aggregation policy. Learn why and how to build policies that better fit your nee
ds in [Utilizing policies other than the default policy](https://lantern.splunk.com/Observability/Product_Tips/IT_Servic
e_Intelligence/Utilizing_policies_other_than_the_default_policy).

[Building your own custom threshold templates](https:
//lantern.splunk.com/Observability/Product_Tips/IT_Service_Intelligence/Building_your_own_custom_threshold_templates) sh
ows you how to use and customize the 33 ITSI out-of-the-box thresholding templates with the ability to configure time po
licies, choose different thresholding algorithms, and adjust sensitivity configurations.

Finally, [Knowing proper adapt
ive threshold configurations](https://lantern.splunk.com/Observability/Product_Tips/IT_Service_Intelligence/Knowing_prop
er_adaptive_threshold_configurations) explains how to best use adaptive thresholding in the most effective way possible,
 helping you to avoid confusing or noisy configurations.

https://preview.redd.it/t1f068yv1zud1.png?width=783&format=png
&auto=webp&s=c37852bd50bd239249bd19ce886a3f83789cd254

These four new articles are just some of many articles in the [De
finitive Guide to Best Practices for IT Service Intelligence](https://lantern.splunk.com/Observability/Product_Tips/IT_S
ervice_Intelligence/The_definitive_guide_to_best_practices_for_IT_Service_Intelligence), so if you’re looking to improve
 how you work with ITSI then don’t miss this helpful resource.

# The Rest of This Month’s New Articles

  
Here’s every
thing else we’ve published over the month:

* [Maximizing performance with the latest Splunk platform capabilities](http
s://lantern.splunk.com/Splunk_Platform/UCE/Security/Applying_the_differentiated_capabilities_of_the_Splunk_platform)
* [
Monitoring LangChain LLM applications with Splunk](https://lantern.splunk.com/Observability/UCE/Unified_Workflows/Enable
_Self-Service_Observability/Monitoring_Langchain_LLM_Applications_with_Splunk_Observability_Cloud)
* [Introduction to th
e Splunk ACS Github Action CI/CD Starter](https://lantern.splunk.com/Splunk_Platform/Product_Tips/Administration/Introdu
ction_to_the_Splunk_ACS_Github_Action_CI%2F%2FCD_Starter)
* [Integrating Kubernetes and Splunk Observability Cloud](http
s://lantern.splunk.com/Observability/UCE/Foundational_visibility/Optimize_Cloud/Integrating_Kubernetes_and_Splunk_Observ
ability_Cloud)
* [Expanding AWS log ingestion capabilities with custom logs in Splunk Data Manager](https://lantern.splu
nk.com/Data_Descriptors/Amazon/Expanding_AWS_log_ingestion_capabilities_with_Splunk_Data_Manager_custom_logs)
* [Using I
ngest Processor to convert JSON logs into metrics](https://lantern.splunk.com/Splunk_Platform/Product_Tips/Extending_the
_Platform/Using_Ingest_Processor_to_convert_JSON_logs_into_metrics)
* [Using generative AI to write and explain SPL sear
ches](https://lantern.splunk.com/Splunk_Platform/Product_Tips/Extending_the_Platform/Using_generative_AI_to_write_and_ex
plain_SPL_searches)

We hope you’ve found this update helpful. Thanks for reading!
```
---

     
 
all -  [ Is RAG Eval Even Possible? ](https://www.reddit.com/r/LangChain/comments/1g4fegp/is_rag_eval_even_possible/) , 2024-10-16-0913
```
I'm asking for a friend.

Just kidding of course. I run an AI tools company, basically APIs for enterprise-grade RAG.  W
e've seen a lot of eval tools, but nothing that actually evals the RAG pipeline.  Most seem focused on the last mile: co
mparing completions to retrievals.

**But RAG breaks down much earlier than that.**   
Did we parse the doc correctly?  

Did we extract correctly?  
Did we chunk correctly?  
Did we add proper metadata to the chunk?  
How performant was the
 search? How about the rerank?

  
Even simple things like how do you generate a correct QA set against a set of documen
ts? That sounds simple. Just ask an LLM. But if you don't have the issues above done perfectly than your QA pairs can't 
be relied upon. 

For example, if your system doesn't perfectly extract a table from a document, then any QA pair built 
on that table will be built on false data.

If anyone is playing with tools that start to tackle these issues, would lov
e your POV.
```
---

     
 
all -  [ Drag and Drop Platform for Agents ](https://www.reddit.com/r/LangChain/comments/1g4fc8e/drag_and_drop_platform_for_agents/) , 2024-10-16-0913
```
Hello,

I've been using LangGraph as a library in python to try and build an agent, however my code got quite disorganiz
ed and hard to debug, so I've been looking into platforms that have the same functionality, but in a diagram/drag and dr
op interface.

I've tried autogpt, flowise, langflow and n8n. However, they've all dropped short in functionality.

Some
 features that I want to use are: read file from my system, write file to my system, use those files for custom prompts,
 display in chat an LLM response, wait for output from chat (so far, most of them had this), sequence controls (if/else,
 loops), run multiple branches concurrently, simple memory system (not memory for chat messages, but sort of like variab
les that you can save a message to, and later use it for something).

Anybody has any suggestions for which platform has
 most of these features and isn't that much of a pain to work with? It's very possible that one of the one I've tried ab
ove is able to do what I want, but I just didn't figure out how, so feel free to correct me.

Or, if you have any sugges
tions for ways to use LangGraph in a more organized matter, whilst being easy to debug every step, please tell. What I m
ean by debug every step, is to be able to see each LLM's response to figure out where a bad output happened.

Thanks for
 any input!
```
---

     
 
all -  [ OpenAI Realtime API with voice detection mode ](https://www.reddit.com/r/LangChain/comments/1g4dtc0/openai_realtime_api_with_voice_detection_mode/) , 2024-10-16-0913
```
Hi, has anyone implemented RealTime API with voice activation detection in langchain? Seems like we have to covert the i
nput into audio file and process it through the API which doesn't give the user experience as in ChatGPT 'Advanced Voice
 Mode'. 
```
---

     
 
all -  [ Does the PGVector integration work with SelfQueryRetriever? ](https://www.reddit.com/r/LangChain/comments/1g49k2m/does_the_pgvector_integration_work_with/) , 2024-10-16-0913
```
Hi all, I'm trying to make a self-querying retriever with my PGVector vector store, following the instructions found on 
[the documentation](https://python.langchain.com/docs/integrations/retrievers/self_query/pgvector_self_query/) (what lit
tle there is, unfortunately) but when I run the code, I can see on my LangSmith trace that the StructuredQueryOutputPars
er receives the json-formatted input with a 'filter', but the output shows no value in the 'filter' key (not even NO\_FI
LTER as it should be). Is this a known issue? I've seen around the web that there are a few months-old posts raising iss
ues with the implementation but the code doesn't throw me any errors, and the documentation gives no explanations other 
than the example code.
```
---

     
 
all -  [ need help creating binary file ](https://www.reddit.com/r/PythonLearning/comments/1g477gy/need_help_creating_binary_file/) , 2024-10-16-0913
```
I have created a python application which will help talk to databases but i am not able to create working binary file, t
here are two files main-file and test_sql_1(second file)

i have provided both here please help me create a binary file

the installed pakages are 


pip install qt-material PySide6 pandas cryptography langchain langchain-core langchain-comm
unity langchain-google-genai  psycopg2 mysqlclient cx_Oracle pyodbc matplotlib

all these are required
**PLEASE HELP**
e
very time create a binary file and it is giveing error about pydantic
leave it and help me create the binary file
file-1

```python
import sys
import json
import os
from PySide6.QtWidgets import (
    QApplication,
    QMainWindow,
    QVBox
Layout,
    QWidget,
    QListWidget,
    QLineEdit,
    QFormLayout,
    QSplitter,
    QPushButton,
    QLabel,
    QL
istWidgetItem,
    QHBoxLayout,
    QRadioButton,
    QButtonGroup,
    QMessageBox,
    QTextEdit,
)

import pandas as 
pd
from cryptography.fernet import Fernet
from PySide6.QtCore import Qt, QThread, Signal
from datetime import datetime
f
rom langchain_core.messages import AIMessage, HumanMessage
from qt_material import apply_stylesheet
import matplotlib
im
port test_sql_1 as langc

CONFIG_FILE = 'db_credentials.json'  # Configuration file for storing DB credentials
KEY_FILE 
= 'secret.key'
CHROMA_DB_FILE = 'chroma\chroma.sqlite3'


# Worker class for handling database connection in a separate 
thread
class DatabaseConnectionWorker(QThread):
    connection_result = Signal(
        bool, object, object, object, ob
ject, object, object, object
    )

    def __init__(self, host, dbname, user, password, port, api_key, db_type):
      
  super().__init__()
        self.host = host
        self.dbname = dbname
        self.user = user
        self.passwor
d = password
        self.port = port
        self.api_key = api_key
        self.db_type = db_type

    def run(self):

        try:
            db_connection = langc.init_database(
                self.user,
                self.password,

                self.host,
                self.port,
                self.dbname,
                self.db_type,
       
     )
            sql_model = langc.get_sql_chain(self.api_key)
            explain_model = langc.get_response(self.api
_key)
            visual_model = langc.generate_plotly_code(self.api_key)
            ehancer_model = langc.better_quest
ion(self.api_key)
            table_names = db_connection.get_usable_table_names()
            table_names_description =
 {
                name: db_connection.get_table_info([name]) for name in table_names
            }
            print(ta
ble_names)
            self.connection_result.emit(
                True,
                db_connection,
               
 sql_model,
                explain_model,
                table_names_description,
                visual_model,
      
          ehancer_model,
                table_names,
            )
        except Exception as e:
            print(f'E
rror: {e}')
            self.connection_result.emit(
                False, None, None, None, None, None, None, None
   
         )  # Emit failure


# Worker class for processing AI responses in a separate thread
class AIProcessingWorkerSQL
(QThread):
    response_generated = Signal(
        str, object, object
    )  # Signal to send generated responses back


    def __init__(self, sql_model, user_text, chat_history, db, schema, db_type):
        super().__init__()
        se
lf.sql_model = sql_model
        self.chat_history = chat_history
        self.user_text = user_text
        self.db = d
b
        self.data = None
        self.schema = schema
        self.sql_command = None
        self.db_type = db_type


    def db_run(self):
        try:
            self.data = self.db.run(self.sql_command, fetch='cursor').fetchall()
    
        return 'work done'
        except Exception as e:
            print(f'Error: {e}')
            self.data = 'the 
SQL query encountered an error'
            return e

    def run(self):
        query_result = ''
        for attempt i
n range(5):
            print('sql worker++++', attempt)
            ai_response = self.sql_model.invoke(
              
  {
                    'schema': self.schema,
                    'chat_history': self.chat_history,
                  
  'question': self.user_text,
                    'db_type': self.db_type,
                }
            )['text']
     
       ai_response = ai_response.replace('```sql', '').replace('```', '')
            self.sql_command = ai_response
   
         query_result = self.db_run()

            if query_result == 'work done':
                break

            ai
_response = self.sql_model.invoke(
                {
                    'schema': self.schema,
                    'cha
t_history': self.chat_history,
                    'question': 'For this question: '
                    + str(self.user
_text)
                    + f' the query '{self.sql_command}' encountered an error: {query_result}',
                  
  'db_type': self.db_type,
                }
            )['text']
            query_result = self.db_run()

           
 if query_result == 'work done':
                break
        print('sql worker++++', query_result)
        self.respon
se_generated.emit(ai_response, self.data, self.user_text)


class AIProcessingWorkerExplain(QThread):
    responce_gener
ated = Signal(str)

    def __init__(
        self, explain_model, user_text, sql_command, data, chat_history, schema
  
  ):
        super().__init__()
        self.chat_history = chat_history
        self.explain_model = explain_model
    
    self.user_text = user_text
        self.sql_command = sql_command
        self.data = data
        self.schema = sch
ema

    def run(self):
        formatted_string = self.explain_model.invoke(
            {
                'schema': se
lf.schema,
                'chat_history': self.chat_history,
                'query': self.sql_command,
               
 'question': self.user_text['enhanced_question'],
                'response': self.data,
            }
        )['text']

        self.responce_generated.emit(formatted_string)


class AIProcessingWorkerUserExpresstion(QThread):
    responce
_ehancer_ = Signal(object)

    def __init__(self, user_input, model, chat_history, schema):
        super().__init__()

        self.user_input = user_input
        self.model = model
        self.chat_history = chat_history
        self.ta
ble_names = schema

    def clean_json(self, text):
        return text.replace('```json', '').replace('```', '')

    d
ef run(self):
        try:
            for attempt in range(5):
                response = self.model.invoke(
          
          {
                        'user_input': self.user_input,
                        'chat_history': self.chat_his
tory,
                        'tables': self.table_names,
                    }
                )['text']
              
  cleaned_response = self.clean_json(response)
                print(cleaned_response)
                print('user worke
r')
                try:
                    result = json.loads(cleaned_response)
                    self.responce_eha
ncer_.emit(result)
                    break
                except json.JSONDecodeError as decode_error:
              
      # Retry with updated input if there's an error
                    retry_input = f'Question: {self.user_input}, Re
sponse: {cleaned_response}, Error: {decode_error}'
                    retry_response = self.model.invoke(
             
           {
                            'user_input': retry_input,
                            'chat_history': self.cha
t_history,
                            'tables': self.table_names,
                        }
                    )['text
']
                    print('user text')
                    cleaned_retry_response = self.clean_json(retry_response)
 
                   result = json.loads(cleaned_retry_response)
                    self.responce_ehancer_.emit(result)
 
                   break

        except Exception as e:
            print(f'Error: {e}')
            self.responce_ehan
cer_.emit('User API key is invalid')


class AIProcessingWorkerGraph(QThread):
    responce_generated = Signal(str)

   
 def __init__(self, visual_model, user_text, data: pd.DataFrame, sql, complate=''):
        super().__init__()
        s
elf.visual_model = visual_model
        self.user_text = user_text
        self.data = data
        self.complate = comp
late
        self.sql_command = sql

    def run(self):
        formatted_string = self.visual_model.invoke(
           
 {
                'question': str(self.user_text)
                if self.complate == ''
                else str(self.
user_text) + ' do not for get that ' + self.complate,
                'sql': self.sql_command,
                'df_metad
ata': self.data.dtypes,
                'sample_data': self.data.head(),
            }
        )['text']
        self.re
sponce_generated.emit(
            formatted_string.replace('```python', '').replace('```', '')
        )


class MainWi
ndow(QMainWindow):
    def __init__(self):
        super(MainWindow, self).__init__()
        self.setWindowTitle('GPT-l
ike Chat with Database Connection')
        self.sql_model = None
        self.explain_model = None
        self.mainLay
out = QVBoxLayout()
        self.message_used_for_sql = None
        self.popup = None
        self.db_schema = None
   
     self.db_connection = None
        self.visual_model = None
        self.sql_db_type = 'postgresql'
        self.tex
t_ehancer = None
        self.db_tables_details = None
        self.db_tables_names = None
        self.key = self.load_
key()
        self.cipher_suite = Fernet(self.key)
        self.db_document = None
        # List widget for chat
      
  self.listWidget = QListWidget()
        self.listWidget.setSelectionMode(QListWidget.NoSelection)
        self.chatHis
tory = []
        self.screen_geometry = QApplication.primaryScreen().geometry()
        # LineEdit for user input
     
   self.lineEdit = QLineEdit()
        self.lineEdit.returnPressed.connect(self.update_chat)

        self.mainLayout.ad
dWidget(self.listWidget)
        self.mainLayout.addWidget(self.lineEdit)

        # Sidebar layout for database connect
ion details
        self.sidebarLayout = QVBoxLayout()

        # Create a form layout for the database connection detai
ls
        formLayout = QFormLayout()
        self.dbTypeGroup = QButtonGroup(self)
        self.postgresRadio = QRadioB
utton('PostgreSQL')
        self.mysqlRadio = QRadioButton('MySQL')
        self.sqliteRadio = QRadioButton('SQLite')
  
      self.oracleRadio = QRadioButton('Oracle')
        self.mssqlRadio = QRadioButton('MSSQL')
        self.dbTypeGroup
.addButton(self.postgresRadio)
        self.dbTypeGroup.addButton(self.mysqlRadio)
        self.dbTypeGroup.addButton(se
lf.sqliteRadio)
        self.dbTypeGroup.addButton(self.oracleRadio)
        self.dbTypeGroup.addButton(self.mssqlRadio)

        self.postgresRadio.setChecked(True)

        # Add radio buttons to form layout
        db_first_layout = QHBox
Layout()
        db_first_layout.addWidget(self.postgresRadio)
        db_first_layout.addWidget(self.mysqlRadio)
      
  db_first_layout.addWidget(self.sqliteRadio)
        formLayout.addRow(QLabel('Database Type:'))
        formLayout.add
Row(db_first_layout)

        db_second_layout = QHBoxLayout()
        db_second_layout.addWidget(self.oracleRadio)
    
    db_second_layout.addWidget(self.mssqlRadio)
        formLayout.addRow(db_second_layout)

        # Connect radio but
ton signals
        self.postgresRadio.toggled.connect(
            lambda: self.on_db_type_changed('postgresql')
      
  )
        self.mysqlRadio.toggled.connect(lambda: self.on_db_type_changed('mysql'))
        self.sqliteRadio.toggled.c
onnect(lambda: self.on_db_type_changed('sqlite'))
        self.oracleRadio.toggled.connect(lambda: self.on_db_type_chang
ed('oracle'))

        # Creating inputs for the connection details with empty default values
        self.hostInput = Q
LineEdit()
        self.dbnameInput = QLineEdit()
        self.userInput = QLineEdit()
        self.passwordInput = QLin
eEdit()
        self.portInput = QLineEdit('5432')
        self.apiKeyInput = QLineEdit()

        # Adding input fields
 to form layout
        formLayout.addRow(QLabel('Host:'), self.hostInput)
        formLayout.addRow(QLabel('Database Na
me:'), self.dbnameInput)
        formLayout.addRow(QLabel('User:'), self.userInput)
        formLayout.addRow(QLabel('Pa
ssword:'), self.passwordInput)
        formLayout.addRow(QLabel('Port:'), self.portInput)
        formLayout.addRow(QLab
el('API Key:'), self.apiKeyInput)

        # 'Connect' button
        self.connectButton = QPushButton('Connect')
      
  self.connectButton.clicked.connect(self.connect_to_database)
        formLayout.addRow(self.connectButton)

        # 
'Load Credentials' button
        self.loadButton = QPushButton('Load Credentials')
        self.loadButton.clicked.conn
ect(self.load_credentials)
        formLayout.addRow(self.loadButton)

        # Status label to show connection result 
(success or not connected)
        self.statusLabel = QLabel('')
        formLayout.addRow(self.statusLabel)

        # 
Add form layout to sidebar
        self.sidebarLayout.addLayout(formLayout)

        # Spacer to push the button to the 
bottom
        self.sidebarLayout.addStretch()

        # Create Chat Button at the bottom
        self.clearChatButton 
= QPushButton('Clear Chat')
        self.clearChatButton.clicked.connect(self.show_clear_chat_confirmation)
        self
.sidebarLayout.addWidget(self.clearChatButton)
        # Sidebar widget
        self.sidebarWidget = QWidget()
        s
elf.sidebarWidget.setLayout(self.sidebarLayout)

        # Chat widget
        self.chatWidget = QWidget()
        self.
chatWidget.setLayout(self.mainLayout)

        # Using QSplitter to divide the sidebar and the main chat area
        se
lf.splitter = QSplitter(Qt.Horizontal)

        # Add sidebar (with a minimum width) and chat widget
        self.splitt
er.addWidget(self.sidebarWidget)
        self.splitter.addWidget(self.chatWidget)

        # Setting sidebar size limita
tions
        self.sidebarWidget.setMinimumWidth(200)
        self.sidebarWidget.setMaximumWidth(350)

        # Control
 the relative sizing - 1:4 means sidebar takes less space than chat
        self.splitter.setStretchFactor(0, 1)
       
 self.splitter.setStretchFactor(1, 4)

        # Create Toggle Button (Fixed on main layout, outside of sidebar)
       
 self.toggleSidebarButton = QPushButton('Close')
        self.toggleSidebarButton.clicked.connect(self.toggle_sidebar)


        # Create a layout to position the toggle button at the top-left corner
        self.toggleLayout = QVBoxLayout()

        self.toggleLayout.addWidget(self.toggleSidebarButton, alignment=Qt.AlignLeft)

        # Create a wrapper widge
t to hold the toggle button and the splitter
        self.wrapperWidget = QWidget()
        self.wrapperLayout = QVBoxLa
yout()
        self.wrapperLayout.addLayout(self.toggleLayout)  # Add toggle button layout
        self.wrapperLayout.ad
dWidget(self.splitter)  # Add the main splitter
        self.wrapperWidget.setLayout(self.wrapperLayout)

        # Set 
the wrapper widget as the central widget
        self.setCentralWidget(self.wrapperWidget)

    def toggle_sidebar(self)
:
        # Check if sidebar is visible
        if self.sidebarWidget.isVisible():
            self.sidebarWidget.hide()

            self.toggleSidebarButton.setText('Open')  # Change button text when hidden
            self.splitter.setSiz
es(
                [0, self.width()]
            )  # Sidebar hidden, chat takes full width
        else:
            s
elf.sidebarWidget.show()
            self.toggleSidebarButton.setText('Close')  # Reset button text when visible
       
     self.splitter.setSizes([400, self.width() - 200])  # Reset the sidebar width

    def show_clear_chat_confirmation(
self):
        # Create a confirmation popup
        confirmation = QMessageBox()
        confirmation.setWindowTitle('C
reate Chat')
        confirmation.setText('Are you sure you want to clear?')
        confirmation.setStandardButtons(QMe
ssageBox.Yes | QMessageBox.No)
        confirmation.setIcon(QMessageBox.Question)

        # Check the user's response
 
       result = confirmation.exec_()

        if result == QMessageBox.Yes:
            self.listWidget.clear()
        
    self.chatHistory = []

    def on_db_type_changed(self, db_type):
        self.sql_db_type = db_type

    def update
_chat(self):
        if not self.sql_model:
            self.add_message_user('Not connected to the database.')
        
    return

        # Get the user's input
        user_text = self.lineEdit.text()
        self.lineEdit.clear()

     
   self.add_message_user(user_text)
        self.message_used_for_sql = user_text
        self.ehnace_user_text = AIProc
essingWorkerUserExpresstion(
            user_text,
            self.text_ehancer,
            self.chatHistory,
       
     self.db_tables_names,
        )
        self.ehnace_user_text.responce_ehancer_.connect(
            self.user_expr
esstion_after_ehnance
        )

        self.ehnace_user_text.start()

    def user_expresstion_after_ehnance(self, use
r_text):
        if user_text == 'User API key is invalid':
            self.statusLabel.setText('ERROR: While Connectin
g to model')
            self.statusLabel.setStyleSheet('color: red')
            return
        self.db_schema = ''
   
     for i in user_text['useful_tables']:
            if i in self.db_tables_names:
                self.db_schema += se
lf.db_tables_details[i] + '\n'

        # Start a separate thread to process AI response
        self.ai_sql_worker = AI
ProcessingWorkerSQL(
            self.sql_model,
            user_text,
            self.chatHistory,
            self.d
b_connection,
            self.db_schema,
            self.sql_db_type,
        )
        self.ai_sql_worker.response_ge
nerated.connect(self.add_ai_response)
        self.ai_sql_worker.start()

    def add_ai_response(self, ai_response, dat
a, user_en):
        if 'User API key is invalid' in ai_response:
            self.statusLabel.setText(
                
'User API key is invalid',
            )
            self.statusLabel.setStyleSheet('color: red')
            return
   
     self.add_ai_sql_message(ai_response, data, user_en)

    def add_message_user(self, text):
        message_widget =
 QWidget()
        message_layout = QHBoxLayout()
        message_layout.setAlignment(Qt.AlignRight)
        message_wid
get.setLayout(message_layout)
        message_layout.addWidget(QLabel(text))
        list_item = QListWidgetItem()
     
   list_item.setSizeHint(message_widget.sizeHint())
        self.listWidget.addItem(list_item)
        self.listWidget.s
etItemWidget(list_item, message_widget)
        self.listWidget.scrollToBottom()

    def add_ai_sql_message(self, text,
 dataframe, user_en):
        message_widget = QWidget()
        message_layout = QHBoxLayout()
        message_layout.s
etAlignment(Qt.AlignLeft)
        message_widget.setLayout(message_layout)
        temp_var = QTextEdit()
        temp_v
ar.setMarkdown('```sql' + text + '```')
        temp_var.setReadOnly(True)
        temp_var.setMaximumSize(
            
int(self.screen_geometry.size().width() * 0.5),
            self.screen_geometry.size().height(),
        )
        mess
age_layout.addWidget(temp_var)
        print(dataframe)
        print(dataframe is not None)

        action_button = No
ne
        action_button_graph = None

        if dataframe is not None and dataframe != 'the sql query made a error':
 
           action_button = self.create_button(
                'Download', lambda: self.handle_action(dataframe)
       
     )
            message_layout.addWidget(action_button)
            action_button_graph = self.create_button(
       
         'Visualize',
                lambda: self.handle_action_graph(
                    pd.DataFrame(dataframe), use
r_en, text
                ),
            )
            message_layout.addWidget(action_button_graph)

        # Event h
andlers for button visibility
        def show_buttons(event):
            if action_button:
                action_butt
on.setVisible(True)
            if action_button_graph:
                action_button_graph.setVisible(True)

        de
f hide_buttons(event):
            if action_button:
                action_button.setVisible(False)
            if acti
on_button_graph:
                action_button_graph.setVisible(False)

        message_widget.enterEvent = show_buttons

        message_widget.leaveEvent = hide_buttons
        list_item = QListWidgetItem()
        list_item.setSizeHint(me
ssage_widget.sizeHint())
        self.listWidget.addItem(list_item)
        self.listWidget.setItemWidget(list_item, mes
sage_widget)
        self.listWidget.scrollToBottom()
        if self.message_used_for_sql:
            self.ai_explain_
worker = AIProcessingWorkerExplain(
                self.explain_model,
                user_en,
                text,
 
               dataframe,
                self.chatHistory,
                self.db_schema,
            )
            se
lf.chatHistory.append(HumanMessage(str(user_en)))
            self.chatHistory.append(AIMessage(text))
            self.
ai_explain_worker.responce_generated.connect(
                self.add_ai_explain_response
            )

            se
lf.ai_explain_worker.start()
            self.message_used_for_sql = None

    def create_button(self, text, action):
  
      button = QPushButton(text)
        button.setVisible(False)
        button.clicked.connect(action)
        return 
button

    def add_ai_explain_response(self, text):
        message_widget = QWidget()
        message_layout = (
     
       QVBoxLayout()
        )  # Use QVBoxLayout for vertical stacking of content
        message_layout.setAlignment(Q
t.AlignLeft)
        self.chatHistory.append(AIMessage(text))

        message_label = QTextEdit()
        # Display the
 HTML content in the QTextEdit
        message_label.setMarkdown(text)
        message_label.setReadOnly(True)
        m
essage_label.adjustSize()

        message_label.setMaximumSize(
            int(self.screen_geometry.size().width() * 0
.5),
            self.screen_geometry.size().height(),
        )
        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        
message_widget.setLayout(message_layout)
        message_layout.addWidget(message_label)

        list_item = QListWidge
tItem()
        list_item.setSizeHint(message_widget.sizeHint())
        self.listWidget.addItem(list_item)
        self
.listWidget.setItemWidget(list_item, message_widget)
        self.listWidget.scrollToBottom()

    def handle_action_gra
ph(self, df, user_en, sql):
        self.graph_df = pd.DataFrame(df)
        self.graph_user_en = user_en
        self.g
raph_sql = sql
        self.run_graph_generation()

    def run_graph_generation(self, complate=''):
        self.graphg
en = AIProcessingWorkerGraph(
            self.visual_model,
            self.graph_user_en,
            self.graph_df,

            self.graph_sql,
            complate,
        )
        self.graphgen.responce_generated.connect(self.create
_math_plot)
        self.graphgen.start()

    def create_math_plot(self, code):
        try:
            # Execute the 
code
            print(code)
            local_scope = {'df': self.graph_df}
            exec(code, {}, local_scope)
   
     except Exception as e:
            print(code)
            print(f'Error executing code: {e}')
            # Log th
e error
            print(f'Error executing code: {e}')
            # Retry the graph generation
            self.run_gr
aph_generation(str(e))

    def handle_action(self, df):
        print('Downloading data...')
        try:
            i
f df is not pd.DataFrame:
                df = pd.DataFrame(list(df))
            # Convert timezone-aware datetime obje
cts to timezone-unaware
            for col in df.select_dtypes(include=['datetimetz']).columns:
                df[col]
 = df[col].apply(
                    lambda x: x.strftime('%Y-%m-%d %H:%M:%S.%f %Z')
                    if pd.notnull(
x)
                    else ''
                )
            for col in df.select_dtypes(include=['timedelta']).columns:

                df[col] = df[col].apply(lambda x: str(x) if pd.notnull(x) else '')

            # Print the DataFrame t
o verify its content

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            excel_filename = f'do
wnloaded_data_{timestamp}.xlsx'
            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
          
      df.to_excel(writer, index=False)

            print(f'Data downloaded successfully as '{excel_filename}'.')
      
      self.statusLabel.setText(
                f'Data downloaded successfully as '{excel_filename}'.'
            )
   
         self.statusLabel.setStyleSheet('color: green;')

            # Open the downloaded Excel file (Windows-specific
)
            if os.name == 'nt':
                os.startfile(excel_filename)
            else:
                print(f
'Please open the file manually: {excel_filename}')

        except Exception as e:
            print(f'Error while downl
oading data: {e}')
            self.statusLabel.setText('Error downloading data.')
            self.statusLabel.setStyle
Sheet('color: red')

    def connect_to_database(self):
        self.statusLabel.setText('Connecting...')
        self.s
tatusLabel.setStyleSheet('color: orange;')
        self.connectButton.setEnabled(False)
        # Start the worker threa
d to connect to the database
        self.db_worker = DatabaseConnectionWorker(
            self.hostInput.text(),
     
       self.dbnameInput.text(),
            self.userInput.text(),
            self.passwordInput.text(),
            se
lf.portInput.text(),
            self.apiKeyInput.text(),
            self.sql_db_type,
        )
        self.db_worker
.connection_result.connect(self.on_connection_result)
        self.db_worker.start()

    def on_connection_result(
    
    self,
        success,
        db_connection,
        sql_model,
        explain_model,
        table_info,
        
visual_model,
        ehancer_model,
        table_names,
    ):
        if success:
            self.db_tables_details 
= table_info
            self.db_tables_names = table_names
            self.db_connection = db_connection
            s
elf.sql_model = sql_model
            self.explain_model = explain_model
            self.visual_model = visual_model
  
          self.text_ehancer = ehancer_model
            self.statusLabel.setText('Success')
            self.statusLabel
.setStyleSheet('color: green;')
            # Save credentials after successful connection
            self.save_credent
ials()
        else:
            self.statusLabel.setText('Not Connected')
            self.statusLabel.setStyleSheet('c
olor: red')
        self.connectButton.setEnabled(True)

    def generate_key(self):
        key = Fernet.generate_key()

        with open(KEY_FILE, 'wb') as key_file:
            key_file.write(key)

    def load_key(self):
        if not 
os.path.exists(KEY_FILE):
            self.generate_key()
        with open(KEY_FILE, 'rb') as key_file:
            ret
urn key_file.read()

    def encrypt(self, data):
        return self.cipher_suite.encrypt(data.encode()).decode()

    
def decrypt(self, data):
        return self.cipher_suite.decrypt(data.encode()).decode()

    def save_credentials(self
):
        credentials = {
            'host': self.hostInput.text(),
            'dbname': self.dbnameInput.text(),
   
         'user': self.userInput.text(),
            'password': self.passwordInput.text(),
            'port': self.port
Input.text(),
            'api_key': self.apiKeyInput.text(),
            'db_type': self.sql_db_type,
        }
       
 encrypted_credentials = {k: self.encrypt(v) for k, v in credentials.items()}
        with open(CONFIG_FILE, 'w') as f:

            json.dump(encrypted_credentials, f)

    def load_credentials(self):
        if os.path.exists(CONFIG_FILE):

            with open(CONFIG_FILE, 'r') as f:
                encrypted_credentials = json.load(f)
                cred
entials = {
                    k: self.decrypt(v) for k, v in encrypted_credentials.items()
                }
         
       self.db_type = credentials.get('db_type', 'postgresql')
                if self.db_type == 'postgresql':
        
            self.postgresRadio.setChecked(True)
                elif self.db_type == 'mysql':
                    self.m
ysqlRadio.setChecked(True)
                elif self.db_type == 'sqlite':
                    self.sqliteRadio.setChecke
d(True)
                elif self.db_type == 'oracle':
                    self.oracleRadio.setChecked(True)
           
     elif self.db_type == 'mssql':
                    self.mssqlRadio.setChecked(True)
                self.hostInput.s
etText(credentials.get('host', ''))
                self.dbnameInput.setText(credentials.get('dbname', ''))
            
    self.userInput.setText(credentials.get('user', ''))
                self.passwordInput.setText(credentials.get('pass
word', ''))
                self.portInput.setText(credentials.get('port', '5432'))
                self.apiKeyInput.set
Text(credentials.get('api_key', ''))
        else:
            self.statusLabel.setText('No saved credentials found.')
 
           self.statusLabel.setStyleSheet('color: red')

    def save_chat_history(self):
        with open('chat_histor
y.json', 'w') as f:
            json.dump([str(msg) for msg in self.chatHistory], f)

    def closeEvent(self, event):
 
       self.save_chat_history()
        event.accept()


app = QApplication(sys.argv)
apply_stylesheet(app, theme='theam
.xml')
window = MainWindow()
window.show()

sys.exit(app.exec())
```


file-2(test_sql_1.py)
```python
from langchain_co
re.prompts import PromptTemplate
from langchain_community.utilities import SQLDatabase
from langchain_google_genai impor
t ChatGoogleGenerativeAI
from langchain.chains import LLMChain


def init_database(
    user: str,
    password: str,
  
  host: str,
    port: str,
    database: str,
    db_type: str = 'postgresql',
) -> SQLDatabase:
    if db_type == 'pos
tgresql':
        db_uri = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}'
    elif db_type == 'mysq
l':
        db_uri = f'mysql+mysqldb://{user}:{password}@{host}:{port}/{database}'
    elif db_type == 'sqlite':
       
 db_uri = f'sqlite:///{database}'
    elif db_type == 'oracle':
        db_uri = f'oracle+cx_oracle://{user}:{password}@
{host}:{port}/{database}'
    elif db_type == 'mssql':
        db_uri = f'mssql+pyodbc://{user}:{password}@{host}:{port}
/{database}'
    return SQLDatabase.from_uri(db_uri)


def get_sql_chain(api_key):
    print('sql_1')
    template = Pro
mptTemplate(
        input_variables=['schema', 'chat_history', 'question', 'db_type'],
        template='')
    print('
sql_2')
    # llm = ChatMistralAI(model='codestral-2405', mistral_api_key=MINSTRAL_API_KEY)
    llm = ChatGoogleGenerati
veAI(
        model='gemini-1.5-flash', google_api_key=api_key, temperature=0
    )
    chain = LLMChain(
        llm=ll
m,
        prompt=template,
    )

    return chain


def get_response(api_key):
    print('nor_1')
    template = Promp
tTemplate(
        input_variables=['schema', 'chat_history', 'query', 'question', 'response'],
        template='')
   
 print('nor_2')
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-flash', google_api_key=api_key, temperature=
0.1
    )
    chain = LLMChain(llm=llm, prompt=template)
    return chain


def generate_plotly_code(api_key) -> str:
  
  print('plotly_1')
    template = PromptTemplate(
        input_variables=['question', 'sql', 'df_metadata', 'sample_da
ta'],
        template='')
    print('plotly_2')
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-flash', goo
gle_api_key=api_key, temperature=0
    )

    chain = LLMChain(llm=llm, prompt=template)

    return chain


def better_
question(api_key):
    template = PromptTemplate(
        input_variables=['user_input', 'chat_history', 'tables'],
    
    template=''
        
    )
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-flash', google_api_key=api_ke
y, temperature=0.1
    )

    chain = LLMChain(llm=llm, prompt=template)

    return chain
``` 
```
---

     
 
all -  [ What are the best practices for loading and splitting Confluence data into a vectorstore for RAG? ](https://www.reddit.com/r/LangChain/comments/1g46j4w/what_are_the_best_practices_for_loading_and/) , 2024-10-16-0913
```
Hello fellow developers,

I'm working on a project that involves integrating our internal Confluence knowledge base with
 a RAG system. I'm facing some challenges and would appreciate your insights:

1. Splitting unstructured data:
   * Init
ially used a basic text splitter with overlapping (suboptimal results)
   * Tried an HTML splitter, but it separates hea
ders from text and cuts off important information - doesn't seem to be the best approach
   * What's the most effective 
approach for maintaining context and relevance?
2. Dealing with outdated content:
   * Our Confluence pages and spaces a
ren't consistently updated
   * How can we ensure our RAG system uses the most current information?<
   * Do you have an
y idea how to fix/improve the 'outdated' data problem?

Has anyone tackled similar issues? I'd love to hear about your e
xperiences and any best practices you've discovered.
```
---

     
 
all -  [ Astute RAG: Fixing RAG’s imperfect retrieval ](/r/AIQuality/comments/1g45nou/astute_rag_fixing_rags_imperfect_retrieval/) , 2024-10-16-0913
```

```
---

     
 
all -  [ How can I add an extra column in the SQL agent response table ? ](https://www.reddit.com/r/LangChain/comments/1g45i24/how_can_i_add_an_extra_column_in_the_sql_agent/) , 2024-10-16-0913
```
Hello everyone , I'm learning about Langgraph agents and developing a project around it.   
  
It's an SQL agent and is 
derived from here \[ [https://docs.smith.langchain.com/tutorials/Developers/agents#sql-agent](https://docs.smith.langcha
in.com/tutorials/Developers/agents#sql-agent) \].  
You can see one of the QnA below .    
My SQL table 'Employee' has N
ame , Location , Experience , Skills , Graduation , Post Graduation , PhD fields only that you guys can also see in the 
AI response.

[Chat with SQL agent](https://preview.redd.it/378uewfsnwud1.png?width=491&format=png&auto=webp&s=219a2738e
e2e55de584a35de3804f2172dcb5cb6)

  
Now , I want to add a column 'Match %' when AI is returning the response to the use
r so that when a user queries for candidate using the job description, he get a column 'Match %' that tells us how much 
% does candidate match.  
  
How can I add this functionality ?
```
---

     
 
all -  [ How to stop generation during streaming? ](https://www.reddit.com/r/LangChain/comments/1g458w6/how_to_stop_generation_during_streaming/) , 2024-10-16-0913
```
I use astream\_events in python to stream model output. Now I want to terminate the model when running a specific tool. 
Just stopping the streaming does not stop the generation and will save no computation costs.

I tried stop sequences but
 they only work when the model generates them itself, not when the tooloutput contains it. So how can I achieve this?
```
---

     
 
all -  [ Need help with understanding Langgraph :) ](https://www.reddit.com/r/LangChain/comments/1g43vje/need_help_with_understanding_langgraph/) , 2024-10-16-0913
```
I have enrolled in the Langgraph course from Langchain academy and I am in the verge of completion 🏁. I could understand
 the concept of graph and states. 🙂

But I have few doubts and that creates roadblocks in my learning journey. 😭

1. Is 
anything created via Langgraph is considered as Agents ? 

2. Is Langgraph designed to work with web frameworks like Dja
ngo, FastAPI or is it just a background process ? 

3. How can I provide human input/feedback via UI to Langgraph (via h
ttp request) ? (If integration with web frameworks is possible) 

4. Is it something that needs to be deployed in Langch
ain cloud and accessed via API ? 

Please help me to understand and help of any kind would be greatly appreciated. 

Tha
nks in advance 🥲👍🏻
```
---

     
 
all -  [ Is hashing data before passing it as prompt to LLM a good practice? ](https://www.reddit.com/r/LocalLLaMA/comments/1g4359h/is_hashing_data_before_passing_it_as_prompt_to/) , 2024-10-16-0913
```
I have a tabular data and I am using langchain's create_csv_agent to query from it. This agent is perfect when used with
 gpt-4o. 

But there's one issue. I don't trust openai with my private data.

Suppose I have a tabular data with column 
names: [name, money, worth], and suppose one row is [skibbidi, 69, 420]; I plan to transform this row like [asd, 6, 8], 
where asd, 6 and 8 map to the respective values. Basically numbers are still numbers and the hierarchy between these num
bers is maintained(since 69 is less than 420 hence 69 = 6 and 420 = 8), the texts on the other hand are transformed into
 some other text.

Is this a good method to maintain data privacy?

I don't have a GPU, and the small models I tested th
rough ollama suck at generating dataframe queries. 


```
---

     
 
all -  [ What gets deployed into LangGraph Cloud? ](https://www.reddit.com/r/LangChain/comments/1g410ef/what_gets_deployed_into_langgraph_cloud/) , 2024-10-16-0913
```
I was reading over the LangGraph docs and wasn’t clear on what happens during deployment. 

When you deploy a graph into
 LangGraph Cloud, what do you get?

An API endpoint you can interact with, or it just runs the Python code and reports b
ack to LangSmith?

How would human in the loop interaction work once the graph was deployed?

Appreciate any insight!
```
---

     
 
all -  [ FloAI: A composable AI Agent Builder (looking for feedback) ](https://www.reddit.com/r/LangChain/comments/1g40v7h/floai_a_composable_ai_agent_builder_looking_for/) , 2024-10-16-0913
```
[Flo](https://github.com/rootflo/flo-ai) was born out of a need for a more streamlined and powerful solution for buildin
g AI agentic workflows. While frameworks like **CrewAI** fell short in offering the flexibility developers needed, and L
angGraph became a challenge to set up and run, Flo provides an ideal middle ground. It’s designed to be the “Keras for T
ensorFlow” of agentic workflows, offering pre-built components for rapid prototyping while allowing deep control for cus
tom, production-level systems. Whether you’re developing simple or intricate workflows, Flo makes AI composability easy 
and powerful.

With its flexible architecture, you can create teams of agents, leverage different router types, and buil
d AI workflows that scale easily. Flo empowers developers to take control of their AI systems, making it a breeze to ada
pt, prototype, and push the boundaries of agentic AI.

Do check the repository and happy to take feedback: [https://gith
ub.com/rootflo/flo-ai](https://github.com/rootflo/flo-ai).  
Give us a star if you think what we plan to build is intere
sting
```
---

     
 
all -  [  Project Alice - v0.2 => open source platform for agentic workflows  ](https://www.reddit.com/r/LangChain/comments/1g3r1ol/project_alice_v02_open_source_platform_for/) , 2024-10-16-0913
```
Hello everyone! A few months ago I launch a project I'd been working on called Project Alice. And today I'm happy to sha
re an incredible amount of progress, and excited to get people to try it out.

To that effect, I've created a few videos
 that show you how to install the platform and an overview of it:

* [Part 1](https://www.youtube.com/watch?v=ojhcb9ADJq
U) (11:38)
* [Part 2](https://www.youtube.com/watch?v=oXGk6g_gPtU) (8:38)

Repository: [Link](https://github.com/Mariano
Molina/project_alice)

# What is it though?

A free open source framework and platform for agentic workflows. It include
s a frontend, backend and a python logic module. It takes 5 minutes to install, no coding needed, and you get a frontend
 where you can create your own agents, chats, task/workflows, etc, run your tasks and/or chat with your agents. You can 
use local models, or most of the most used API providers for AI generation.

You don't need to know how to code at all, 
but if you do, you have full flexibility to improve any aspect of it since its all open source. The platform has been pu
rposefully created so that it's code is comprehensible, easy to upgrade and improve. Frontend and backend are in TS, pyt
hon module uses Pydantic almost to a pedantic level.

It has a total of 22 apis at the moment:

        OPENAI
        O
PENAI_VISION
        OPENAI_IMG_GENERATION
        OPENAI_EMBEDDINGS
        OPENAI_TTS
        OPENAI_STT
        OPENA
I_ASTT
        AZURE
        GEMINI
        GEMINI_VISION
        GEMINI_IMG_GEN => Google's sdk is broken atm
        M
ISTRAL
        MISTRAL_VISION
        MISTRAL_EMBEDDINGS
        GEMINI_STT
        GEMINI_EMBEDDINGS
        COHERE
   
     GROQ
        GROQ_VISION
        GROQ_TTS
        META
        META_VISION
        ANTHROPIC
        ANTHROPIC_VISI
ON
        LM_STUDIO
        LM_STUDIO_VISION
        GOOGLE_SEARCH
        REDDIT_SEARCH
        WIKIPEDIA_SEARCH
     
   EXA_SEARCH
        ARXIV_SEARCH
        GOOGLE_KNOWLEDGE_GRAPH

And an uncountable number of models that you can depl
oy with it.

It is going to keep getting better. If you think this is nice, wait until the next update drops. And if you
 feel like helping out, I'd be super grateful. I'm about to tackle RAG and ReACT capabilities in my agents, and I'm sure
 a lot of people here have some experience with that. Maybe the idea of trying to come up with a (maybe industry?) stand
ard sounds interesting?

Check out the videos if you want some help installing and understanding the frontend. Ask me an
y questions otherwise!
```
---

     
 
all -  [ NaturalAgents - notion-style editor to easily create AI Agents ](https://www.reddit.com/r/OpenSourceAI/comments/1g3qsir/naturalagents_notionstyle_editor_to_easily_create/) , 2024-10-16-0913
```
[NaturalAgents](https://github.com/NaturalAgents/NaturalAgents) is the easiest way to create AI Agents in a notion-style
 editor without code - using plain english and simple macros. It's fully open-source and will be actively maintained.

H
ow this is different from other agent builders -

1. No boilerplate code (imagine langchain for multiple agents)
2. No c
ode experience
3. Can easily share and build with others
4. Readable/organized agent outputs
5. Abstracts agent communic
ations without visual complexity (image large drag and drop flowcharts)

Would love to hear thoughts and feel free to re
ach out if you're interested in contributing!
```
---

     
 
all -  [ Vector Store Usage for RAG ](https://www.reddit.com/r/LangChain/comments/1g3puzu/vector_store_usage_for_rag/) , 2024-10-16-0913
```
I'm a newbie building an RAG application for a simple documentation Q&A where the user enters the URL for a documentatio
n and can ask questions on it. I understand I need a vector store for storing the embeddings. My question is would I nee
d separate collections for every documentation. And if a user enters a documentation that already exists, should it be o
verwritten with new embeddings?
```
---

     
 
all -  [ Does RAG Have a Scaling Problem? ](https://www.reddit.com/r/LlamaIndex/comments/1g3put2/does_rag_have_a_scaling_problem/) , 2024-10-16-0913
```
My team has been digging into the scalability of vector databases for RAG (Retrieval-Augmented Generation) systems, and 
we feel we might be hitting some limits that aren’t being widely discussed.

We tested Pinecone (using both LangChain an
d LlamaIndex) out to 100K pages. We found those solutions started to lose search accuracy in as few as 10K pages. At 100
K pages in the RAG, search accuracy dropped 10-12%.

To be clear, we think this is a vector issue not an orchestrator is
sue. Though we did find particular problems trying to scale LangChain ingestion because of Unstructured (more on the end
 of the piece about that). 

We also tested our approach at [EyeLevel.ai](http://eyelevel.ai/), which does not use vecto
rs at all (I know it sounds crazy), and found only a 2% drop in search accuracy at 100K pages. And showed better accurac
y by significant margins from the outset.

I'm posting our research here to start a conversation on non-vector based app
roaches to RAG. We think there's a big opportunity to do things differently that's still very compatible with orchestrat
ors like LangChain.  We'd love to build a community around it. 

**Here's our research below. I would love to know if an
yone else is exploring non-vector approaches to RAG and of course your thoughts on the research.**

**We explain the res
earch and results on YT as well.**  
[https://youtu.be/qV1Ab0qWyT8?si=dgPL21xAHeI2jXJo](https://youtu.be/qV1Ab0qWyT8?si=
dgPL21xAHeI2jXJo)

[Image: The chart shows accuracy loss at just 10,000 pages of content using a Pinecone vector databas
e with both LangChain and Llamaindex-based RAG applications.  Conversely, EyeLevel's GroundX APIs for RAG show almost no
 loss.](https://preview.redd.it/74oi00ko1sud1.png?width=1600&format=png&auto=webp&s=5f86d97d682631215832ab3f92b34c9641f8
d3e9)

# What’s Inside

In this report, we will review how the test was constructed, the detailed findings, our theories
 on why vector similarity search experienced challenges and suggested approaches to scale RAG without the performance hi
t. We also encourage you to read our [prior research](https://www.eyelevel.ai/post/most-accurate-rag) in which EyeLevel’
s GroundX APIs bested LangChain, Pinecone and Llamaindex based RAG systems by 50-120% on accuracy over 1,000 pages of co
ntent.  

The work was performed by Daniel Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a
 computer scientist and former senior engineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods 
of this test will be open sourced and available shortly. Others are invited to run the data and corroborate or challenge
 these findings. 

# Defining RAG 

**Feel free to skip this section if you’re familiar with RAG.**  

RAG stands for “R
etrieval Augmented Generation”. When you ask a RAG system a query, RAG does the following steps: 

1. Retrieval: Based o
n the query from the user, the RAG system retrieves relevant knowledge from a set of documents. 
2. Augmentation: The RA
G system combines the retrieved information with the user query to construct a prompt. 
3. Generation: The augmented pro
mpt is passed to a large language model, generating the final output. 

The implementation of these three steps can vary
 wildly between RAG approaches. However, the objective is the same: to make a language model more useful by feeding it i
nformation from real-world, relevant documents. 

RAG allows a language model to reference application specific informat
ion from human documents, allowing developers to build tailored and specific products 

# Beyond The Tech Demo 

When mo
st developers begin experimenting with RAG they might grab a few documents, stick them into a RAG document store and be 
blown away by the results. Like magic, many RAG systems can allow a language model to understand books, company document
s, emails, and more. 

However, as one continues experimenting with RAG, some difficulties begin to emerge. 

1. Many do
cuments are not purely textual. They might have images, tables, or complex formatting. While many RAG systems can parse 
complex documents, the quality of parsing varies widely between RAG approaches. We explore the realities of parsing in[ 
another article](https://www.eyelevel.ai/post/most-accurate-rag). 
2. As a RAG system is exposed to more documents, it h
as more opportunities to retrieve the wrong document, potentially causing a degradation in performance   
3. Because of 
technical complexity, the underlying non-determinism of language models, and the difficulty of profiling the performance
 of LLM applications in real world settings, it can be difficult to predict the cost and level of effort of developing R
AG applications. 

In this article we’ll focus on the second and third problems listed above; performance degradation of
 RAG at scale and difficulties of implementation 

# The Test 

To test how much larger document sets degrade the perfor
mance of RAG systems, we first defined a set of 92 questions based on real-world documents.  

[A few examples of the re
al-world documents used in this test, which contain answers to our 92 questions. ](https://preview.redd.it/ojr3ab7s1sud1
.png?width=1429&format=png&auto=webp&s=6ee3ce03d23a9ca0a35b181869c71a213d0da362)

A few examples of the real-world docum
ents used in this test, which contain answers to our 92 questions. 

We then constructed four document sets to apply RAG
 to. All four of these document sets contain the same 310 pages of documents which answer our 92 test questions. However
, each document set also contains a different number of irrelevant pages from miscellaneous documents. We started with 1
,000 pages and scaled up to 100,000 in our largest test. 

[We asked the same questions based on the same set of documen
ts \(blue\), but exposed the RAG system to varying amounts of unrelated documents \(red\). This diagram shows the number
 of relevant pages in each document set, compared to the total size of each document set.](https://preview.redd.it/r9nhj
ahv1sud1.png?width=1431&format=png&auto=webp&s=c09a05becc851cd1199fe39c92ee1753bc9f8950)

We asked the same questions ba
sed on the same set of documents (blue), but exposed the RAG system to varying amounts of unrelated documents (red). Thi
s diagram shows the number of relevant pages in each document set, compared to the total size of each document set.

An 
ideal RAG system would, in theory, behave identically across all document sets, as all document sets contain the same an
swers to the same questions. In practice, however, added information in a docstore can trick a RAG system into retrievin
g the wrong context for a given query. The more documents there are, the more likely this is to happen. Therefore, RAG p
erformance tends to degrade as the number of documents increases. 

In this test we applied each of these three popular 
RAG approaches to the four document sets mentioned above:

* LangChain: a popular python library designed to abstract ce
rtain LLM workflows. 
* LlamaIndex: a popular python library which has advanced vector embedding capability, and advance
d RAG functionality. 
* EyeLevel’s GroundX: a feature complete retrieval engine built for RAG. 

By applying each of the
se RAG approaches to the four document sets, we can study the relative performance of each RAG approach at scale. 

For 
both LangChain and LlamaIndex we employed Pinecone as our vector store and OpenAI’s text-embedding-ada-002 for embedding
. GroundX, being an all-in-one solution, was used in isolation up to the point of generation. All approaches used OpenAI
's gpt-4-1106-preview for the final generation of results. Results for each approach were evaluated as being true or fal
se via human evaluation. 

# The Effect of Scale on RAG 

We ran the test as defined in the previous section and got the
 following results. 

[The performance of different RAG approaches varies greatly, both in base performance and the rate
 of performance degradation at scale. We explore differences in base performance thoroughly in another article ](https:/
/preview.redd.it/o0x2y1dx1sud1.png?width=1600&format=png&auto=webp&s=fe6ce89f4b7657de3221b0e5b93110ecca20c546)

The perf
ormance of different RAG approaches varies greatly, both in base performance and the rate of performance degradation at 
scale. We explore differences in base performance thoroughly in another article 

As can be seen in the figure above, th
e rate at which RAG degrades in performance varies widely between RAG approaches. Based on these results one might expec
t GroundX to degrade in performance by 2% per 100,000 documents, while LCPC and LI might degrade 10-12% per 100,000 docu
ments. The reason for this difference in robustness to larger document sets, likely, has to do with the realities of usi
ng vector search as the bedrock of a RAG system.  

In theory a high dimensional vector space can hold a vast amount of 
information. 100,000 in binary is 17 values long (11000011010100000). So, if we *only* use binary vectors with unit comp
onents in a high dimensional vector space, we could store each page in our 100,000 page set with only a 17 dimensional s
pace. Text-embedding-ada-002, which is the encoder used in this experiment, outputs a 1536-dimension vector. If one calc
ulates 2\^1536 (effectively calculating how many things one could describe using only binary vectors in this space) the 
result would be a number that’s significantly greater than the number of atoms in the known universe. Of course, actual 
embeddings are not restricted to binary numbers; they can be expressed in decimal numbers of very high precision. Even r
elatively small vector spaces can hold a vast amount of information. 

The trick is, how do you get information into a v
ector space *meaningfully*? RAG needs content to be placed in a vector space such that similar things can be searched, t
hus the encoder has to practically organize information into useful regions. It’s our theory that modern encoders don’t 
have what it takes to organize large sets of documents in these vector spaces, even if the vector spaces can theoretical
ly fit a near infinite amount of information. The encoder can only put so much information into a vector space before th
e vector space gets so cluttered that distance-based search is rendered non-performant. 

[There is a big difference bet
ween a space being able to fit information, and that information being meaningfully organized. ](https://preview.redd.it
/inp40j1f2sud1.png?width=948&format=png&auto=webp&s=f7b0c838224196d7d0f24cb6db7abae3dbd8431d)

There is a big difference
 between a space being able to fit information, and that information being meaningfully organized. 

EyeLevel’s GroundX 
doesn’t use vector similarity as its core search strategy, but rather a tuned comparison based on the similarity of sema
ntic objects. There are no vectors used in this approach. This is likely why GroundX exhibits superior performance in la
rger document sets. 

In this test we employed what is commonly referred to as “naive” RAG. LlamaIndex and LangChain all
ow for many advanced RAG approaches, but they had little impact on performance and were harder to employ at larger scale
s. We cover that in another article which will be released shortly.

# The Surprising Technical Difficulty of Scale 

Wh
ile 100,000 pages seems like a lot, it’s actually a fairly small amount of information for industries like engineering, 
law, and healthcare. Initially we imagined testing on much larger document sets, but while conducting this test we were 
surprised by the practical difficulty of getting LangChain to work at scale; forcing us to reduce the scope of our test.
 

To get RAG up and running for a set of PDF documents, the first step is to parse the content of those PDFs into some 
sort of textual representation. LangChain uses libraries from [Unstructured.io](http://unstructured.io/) to perform pars
ing on complex PDFs, which works seamlessly for small document sets. 

Surprisingly, though, the speed of LangChain pars
ing is incredibly slow. Based on our analysis it appears that Unstructured uses a variety of models to detect and parse 
out key elements within a PDF. These models should employ GPU acceleration, but they don’t. That results in LangChain ta
king days to parse a modestly sized set of documents, even on very large (and expensive) compute instances. To get LangC
hain working we needed to reverse engineer portions of Unstructured and inject code to enable GPU utilization of these m
odels. 

It appears that this is a known issue in Unstructured, as seen in the notes below. As it stands, it presents si
gnificant difficulty in scaling LangChain to larger document sets, given LangChain abstracts away fine grain control of 
Unstructured. 

https://preview.redd.it/zebe6lqg2sud1.png?width=1346&format=png&auto=webp&s=393cefee81ed7de8f8b936c0087b
aa0c326fa1d0

[**Source: Github**](https://github.com/Unstructured-IO/unstructured-inference/blob/64cd41c37fe4535702b3be
9c6b58f380ca4c7edd/unstructured_inference/inference/layout.py#L175)

We only made improvements to LangChain parsing up t
o the point where this test became feasible. If you want to modify LangChain for faster parsing, here are some resources
: 

* The default directory loader of LangChain is Unstructured ([source1](https://python.langchain.com/v0.1/docs/module
s/data_connection/document_loaders/file_directory/),[ source2](https://github.com/langchain-ai/langchain/blob/410e9add44
43607618a75827afe1a676fcd7c0a7/libs/community/langchain_community/document_loaders/directory.py#L38)). 
* Unstructured u
ses “hi res” for the PDFs by default if text extraction cannot be performed on the document ([source1](https://github.co
m/langchain-ai/langchain/blob/410e9add4443607618a75827afe1a676fcd7c0a7/libs/community/langchain_community/document_loade
rs/directory.py#L38) ,[ source2](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a5
7bf078/unstructured/partition/strategies.py#L103) ). Other options are available like “fast” and “OCR only”, which have 
different processing intensities 
* “Hi Res” involves: 
   * Converting the pdf into images ([source](https://github.com
/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078/unstructured/partition/pdf_image/pdfminer_pr
ocessing.py#L36)) 
   * Running a layout detection model to understand the layout of the documents ([source](https://git
hub.com/Unstructured-IO/unstructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured_inference/mode
ls/yolox.py#L3)). This model benefits greatly from GPU utilization, but does not leverage the GPU unless ONNX is install
ed ([source](https://onnxruntime.ai/docs/get-started/with-python.html)) 
   * OCR extraction using tesseract (by default
) ([source](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078/unstructured/p
artition/utils/config.py#L103)) which is a very compute intensive process ([source](https://github.com/tesseract-ocr/tes
seract/issues/263)) 
   * Running the page through a table layout model ([source](https://github.com/Unstructured-IO/uns
tructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured_inference/models/tables.py#L140)) 

While
 our configuration efforts resulted in faster processing times, it was still too slow to be feasible for larger document
 sets. To reduce time, we did “hi res” parsing on the relevant documents and “fast” parsing on documents which were irre
levant to our questions. With this configuration, parsing 100,000 pages of documents took 8 hours. If we had applied “hi
 res” to all documents, we imagine that parsing would have taken 31 days (at around 30 seconds per page). 

At the end o
f the day, this test took two senior engineers (one who’s worked at a directorial level at several AI companies, and a m
ulti company CTO with decades of applied experience of AI at scale) several weeks to do the development necessary to wri
te this article, largely because of the difficulty of applying LangChain to a modestly sized document set.  To get LangC
hain working in a production setting, we estimate that the following efforts would be required: 

* Tesseract would need
 to be interfaced with in a way that is more compute and time efficient. This would likely require a high-performance CP
U instance, and modifications to the LangChain source code. 
* The layout and table models would need to be made to run 
on a GPU instance 
* To do both tasks in a cost-efficient manner, these tasks should probably be decoupled. However, thi
s is not possible with the current abstraction of LangChain. 

On top of using a unique technology which is highly perfo
rmant, GroundX also abstracts virtually all of these technical difficulties behind an API. You upload your documents, th
en search the results. That’s it. 

If you want RAG to be even easier, one of the things that makes Eyelevel so compelli
ng is the service aspect they provide to GroundX. You can work with Eyelevel as a partner to get GroundX working quickly
 and performantly for large scale applications. 

# Conclusion 

When choosing a platform to build RAG applications, eng
ineers must balance a variety of key metrics. The robustness of a system to maintain performance at scale is one of thos
e critical metrics. In this head-to-head test on real-world documents, EyeLevel’s GroundX exhibited a heightened level o
f performance at scale, beating LangChain and LlamaIndex. 

Another key metric is efficiency at scale. As it turns out, 
LangChain has significant implementation difficulties which can make the large-scale distribution of LangChain powered R
AG difficult and costly. 

Is this the last word? Certainly not. In future research, we will test various advanced RAG t
echniques, additional RAG frameworks such as Amazon Q and GPTs and increasingly complex and multimodal data types. So st
ay tuned. 

If you’re curious about running these results yourself, please reach out to us at [info@eyelevel.ai.Vector](
mailto:info@eyelevel.ai.Vector) databases, a key technology in building retrieval augmented generation or RAG applicatio
ns, has a scaling problem that few are talking about. 

According to new research by [EyeLevel.ai](https://www.eyelevel.
ai/), an AI tools company, the precision of vector similarity search degrades in as few as 10,000 pages, reaching a 12% 
performance hit by the 100,000-page mark.

The research also tested [EyeLevel’s enterprise-grade RAG platform](https://w
ww.eyelevel.ai/product/apis) which does not use vectors. EyeLevel lost only 2% accuracy at scale.

The findings suggest 
that while vector databases have become highly popular tools to build RAG and LLM-based applications, developers may fac
e unexpected challenges as they shift from testing to production and attempt to scale their applications.  

The work wa
s performed by Daniel Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a computer scientist a
nd former senior engineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods of this test will be 
open sourced and available shortly. Others are invited to run the data and corroborate or challenge these findings. 

My
 team has been digging into the scalability of vector databases for RAG (Retrieval-Augmented Generation) systems, and we
 feel we might be hitting some limits that aren’t being widely discussed.

We tested Pinecone (using both LangChain and 
LlamaIndex) out to 100K pages. We found those solutions started to lose search accuracy in as few as 10K pages. At 100K 
pages in the RAG, search accuracy dropped 10-12%.

We also tested our approach at [EyeLevel.ai](http://eyelevel.ai/), wh
ich does not use vectors at all (I know it sounds crazy), and found only a 2% drop in search accuracy at 100K pages. And
 showed better accuracy by significant margins from the outset.

**Here's our research below. I would love to know if an
yone else is exploring non-vector approaches to RAG and of course your thoughts on the research.**

**We explain the res
earch and results on YT as well.**  
[https://youtu.be/qV1Ab0qWyT8?si=dgPL21xAHeI2jXJo](https://youtu.be/qV1Ab0qWyT8?si=
dgPL21xAHeI2jXJo) 
```
---

     
 
all -  [ I have a python codebase that uses streamlit, want to move to react for easier integration. ](https://www.reddit.com/r/LangChain/comments/1g3milz/i_have_a_python_codebase_that_uses_streamlit_want/) , 2024-10-16-0913
```
As the title states I've been working on a predominately python codebase with a lot of langchian concepts that have been
 written in python and I want to migrate away from streamlit to some sort of react library, please do let me know if the
re are any solutions availabel thank you.
```
---

     
 
all -  [ Python or Typescript for RAG? ](https://www.reddit.com/r/Rag/comments/1g3lgzy/python_or_typescript_for_rag/) , 2024-10-16-0913
```
I am working on a project for my Bachelor's thesis and want to build a Retrieval-Augmented Generation (RAG) system. For 
Langchain and LlamaIndex, there are both Python and TypeScript versions available. I am new to this topic but would like
 to set up a web application in the medium term, which will be programmed in Next.js later on. Since the project will ru
n on a server, does the backend language really matter? Or should I stick to one language, since TypeScript can be used 
in Next.js? The problem I see is that the documentation for Langchain and LlamaIndex in TypeScript is tiny compared to P
ython. As a beginner in this field, will I still be able to manage with TypeScript? I would prefer not to get stuck, but
 I would also like to cover my future projects. Another advantage would be that I could use the code for Obsidian plugin
 development, which can also be done in TypeScript. What do you think? Thanks in advance!
```
---

     
 
all -  [ Has anyone seen AI agents working in production at scale? ](https://www.reddit.com/r/LocalLLaMA/comments/1g3jkct/has_anyone_seen_ai_agents_working_in_production/) , 2024-10-16-0913
```
Has anyone seen AI agents working in production at scale? 

It doesn't matter if you're using the Swarm, langchain, or a
ny other AI agent orchestration framework if the underlying issue is that AI agents too slow, too expensive, and too unr
eliable. I wrote about [AI agent hype vs. reality](https://www.kadoa.com/blog/ai-agents-hype-vs-reality) a while ago, an
d I don't think it has changed yet.

>By combining tightly constrained LLMs, good evaluation data, human-in-the-loop ove
rsight, and traditional engineering methods, we can achieve reliably good results for automating medium-complex tasks.


>Will AI agents automate tedious repetitive work, such as web scraping, form filling, and data entry? Yes, absolutely.


>Will AI agents autonomously book your vacation without your intervention? Unlikely, at least in the near future.

What 
are your real-world use cases and experiences?
```
---

     
 
all -  [ LangGraph 101 - Tutorial with Practical Example ](https://www.reddit.com/r/LangChain/comments/1g3i734/langgraph_101_tutorial_with_practical_example/) , 2024-10-16-0913
```
Hi folks!

It's been a while but I just finished uploading my latest tutorial. I built a super simple, but extremely pow
erful two-node LangGraph app that can retrieve data from my resume and a job description and then use the information to
 respond to any question. It could for example:

* Re-write parts or all of my resume to match the job description.
* Ge
nerate relevant interview questions and provide feedback.
* Write job-specific cover letters.
* etc.

# [>>> Watch here 
<<<](https://youtu.be/7KIrBjQTGLA)

You get the idea! I know the official docs are somewhat complicated, and sometimes b
roken, and a lot of people have a hard time starting out using LangGraph. If you're one of those people or just getting 
started and want to learn more about the library, [check out the tutorial](https://youtu.be/7KIrBjQTGLA)!

Cheers! :)
```
---

     
 
all -  [ Does RAG Have a Scaling Problem? ](https://www.reddit.com/r/Rag/comments/1g3h9w2/does_rag_have_a_scaling_problem/) , 2024-10-16-0913
```
My team has been digging into the scalability of vector databases for RAG (Retrieval-Augmented Generation) systems, and 
we feel we might be hitting some limits that aren’t being widely discussed. 

We tested Pinecone (using both LangChain a
nd LlamaIndex) out to 100K pages. We found those solutions started to lose search accuracy in as few as 10K pages. At 10
0K pages in the RAG, search accuracy dropped 10-12%.

We also tested our approach at [EyeLevel.ai](http://EyeLevel.ai), 
which does not use vectors at all (I know it sounds crazy), and found only a 2% drop in search accuracy at 100K pages. A
nd showed better accuracy by significant margins from the outset. 

**Here's our research below. I would love to know if
 anyone else is exploring non-vector approaches to RAG and of course your thoughts on the research.** 

**We explain the
 research and results on YT as well.**   
[**https://www.youtube.com/watch?v=qV1Ab0qWyT8**](https://www.youtube.com/watc
h?v=qV1Ab0qWyT8)

[Image: The chart shows accuracy loss at just 10,000 pages of content using a Pinecone vector database
 with both LangChain and Llamaindex-based RAG applications.  Conversely, EyeLevel's GroundX APIs for RAG show almost no 
loss.](https://preview.redd.it/3okwujg7fqud1.png?width=1600&format=png&auto=webp&s=dfe1b2bb38f6d4a0fc5e7798ebd60fa23df13
c80)

# What’s Inside

In this report, we will review how the test was constructed, the detailed findings, our theories 
on why vector similarity search experienced challenges and suggested approaches to scale RAG without the performance hit
. We also encourage you to read our [prior research](https://www.eyelevel.ai/post/most-accurate-rag) in which EyeLevel’s
 GroundX APIs bested LangChain, Pinecone and Llamaindex based RAG systems by 50-120% on accuracy over 1,000 pages of con
tent.  

The work was performed by Daniel Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a 
computer scientist and former senior engineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods o
f this test will beopen sourced and available shortly. Others are invited to run the data and corroborate or challenge t
hese findings. 

# Defining RAG 

**Feel free to skip this section if you’re familiar with RAG.**  

RAG stands for “Ret
rieval Augmented Generation”. When you ask a RAG system a query, RAG does the following steps: 

1. Retrieval: Based on 
the query from the user, the RAG system retrieves relevant knowledge from a set of documents. 

1. Augmentation: The RAG
 system combines the retrieved information with the user query to construct a prompt. 

1. Generation: The augmented pro
mpt is passed to a large language model, generating the final output. 

The implementation of these three steps can vary
 wildly between RAG approaches. However, the objective is the same: to make a language model more useful by feeding it i
nformation from real-world, relevant documents. 

[RAG allows a language model to reference application specific informa
tion from human documents, allowing developers to build tailored and specific products ](https://preview.redd.it/z5t4h8a
cfqud1.png?width=1158&format=png&auto=webp&s=ce9d6e5ce5acb057243db14193f90f0a2072ccf2)

# Beyond The Tech Demo 

When mo
st developers begin experimenting with RAG they might grab a few documents, stick them into a RAG document store and be 
blown away by the results. Like magic, many RAG systems can allow a language model to understand books, company document
s, emails, and more. 

However, as one continues experimenting with RAG, some difficulties begin to emerge. 

1. Many do
cuments are not purely textual. They might have images, tables, or complex formatting. While many RAG systems can parse 
complex documents, the quality of parsing varies widely between RAG approaches. We explore the realities of parsing in[ 
another article](https://www.eyelevel.ai/post/most-accurate-rag). 

1. As a RAG system is exposed to more documents, it 
has more opportunities to retrieve the wrong document, potentially causing a degradation in performance   

1. Because o
f technical complexity, the underlying non-determinism of language models, and the difficulty of profiling the performan
ce of LLM applications in real world settings, it can be difficult to predict the cost and level of effort of developing
 RAG applications. 

In this article we’ll focus on the second and third problems listed above; performance degradation 
of RAG at scale and difficulties of implementation 

# The Test 

To test how much larger document sets degrade the perf
ormance of RAG systems, we first defined a set of 92 questions based on real-world documents.  

[A few examples of the 
real-world documents used in this test, which contain answers to our 92 questions. ](https://preview.redd.it/vbm3jcxffqu
d1.png?width=1429&format=png&auto=webp&s=9eed23eb6d7e0c6d8816063e7d3a9d86ad1848e8)

We then constructed four document se
ts to apply RAG to. All four of these document sets contain the same 310 pages of documents which answer our 92 test que
stions. However, each document set also contains a different number of irrelevant pages from miscellaneous documents. We
 started with 1,000 pages and scaled up to 100,000 in our largest test.   
 

[We asked the same questions based on the 
same set of documents \(blue\), but exposed the RAG system to varying amounts of unrelated documents \(red\). This diagr
am shows the number of relevant pages in each document set, compared to the total size of each document set.](https://pr
eview.redd.it/u4d6keqpfqud1.png?width=1431&format=png&auto=webp&s=b0161c312eca9b0633b5df58713f1ed92893cb30)

An ideal RA
G system would, in theory, behave identically across all document sets, as all document sets contain the same answers to
 the same questions. In practice, however, added information in a docstore can trick a RAG system into retrieving the wr
ong context for a given query. The more documents there are, the more likely this is to happen. Therefore, RAG performan
ce tends to degrade as the number of documents increases. 

In this test we applied each of these three popular RAG appr
oaches to the four document sets mentioned above:

* LangChain: a popular python library designed to abstract certain LL
M workflows. 
* LlamaIndex: a popular python library which has advanced vector embedding capability, and advanced RAG fu
nctionality. 
* EyeLevel’s GroundX: a feature complete retrieval engine built for RAG. 

By applying each of these RAG a
pproaches to the four document sets, we can study the relative performance of each RAG approach at scale. 

For both Lan
gChain and LlamaIndex we employed Pinecone as our vector store and OpenAI’s text-embedding-ada-002 for embedding. Ground
X, being an all-in-one solution, was used in isolation up to the point of generation. All approaches used OpenAI's gpt-4
-1106-preview for the final generation of results. Results for each approach were evaluated as being true or false via h
uman evaluation. 

# The Effect of Scale on RAG 

We ran the test as defined in the previous section and got the followi
ng results. 

[The performance of different RAG approaches varies greatly, both in base performance and the rate of perf
ormance degradation at scale. We explore differences in base performance thoroughly in another article ](https://preview
.redd.it/f3uqn8vsfqud1.png?width=1600&format=png&auto=webp&s=69abb4cf7d224a523457ac0b769a6dc2ae0148c7)

As can be seen i
n the figure above, the rate at which RAG degrades in performance varies widely between RAG approaches. Based on these r
esults one might expect GroundX to degrade in performance by 2% per 100,000 documents, while LCPC and LI might degrade 1
0-12% per 100,000 documents. The reason for this difference in robustness to larger document sets, likely, has to do wit
h the realities of using vector search as the bedrock of a RAG system.  

In theory a high dimensional vector space can 
hold a vast amount of information. 100,000 in binary is 17 values long (11000011010100000). So, if we *only* use binary 
vectors with unit components in a high dimensional vector space, we could store each page in our 100,000 page set with o
nly a 17 dimensional space. Text-embedding-ada-002, which is the encoder used in this experiment, outputs a 1536-dimensi
on vector. If one calculates 2\^1536 (effectively calculating how many things one could describe using only binary vecto
rs in this space) the result would be a number that’s significantly greater than the number of atoms in the known univer
se. Of course, actual embeddings are not restricted to binary numbers; they can be expressed in decimal numbers of very 
high precision. Even relatively small vector spaces can hold a vast amount of information. 

The trick is, how do you ge
t information into a vector space *meaningfully*? RAG needs content to be placed in a vector space such that similar thi
ngs can be searched, thus the encoder has to practically organize information into useful regions. It’s our theory that 
modern encoders don’t have what it takes to organize large sets of documents in these vector spaces, even if the vector 
spaces can theoretically fit a near infinite amount of information. The encoder can only put so much information into a 
vector space before the vector space gets so cluttered that distance-based search is rendered non-performant. 

[There i
s a big difference between a space being able to fit information, and that information being meaningfully organized. ](h
ttps://preview.redd.it/uzbfvsyufqud1.png?width=948&format=png&auto=webp&s=5dc7fad4f9e976a595281bac95e34f64e472613a)

Eye
Level’s GroundX doesn’t use vector similarity as its core search strategy, but rather a tuned comparison based on the si
milarity of semantic objects. There are no vectors used in this approach. This is likely why GroundX exhibits superior p
erformance in larger document sets. 

In this test we employed what is commonly referred to as “naive” RAG. LlamaIndex a
nd LangChain allow for many advanced RAG approaches, but they had little impact on performance and were harder to employ
 at larger scales. We cover that in another article which will be released shortly.

# The Surprising Technical Difficul
ty of Scale 

While 100,000 pages seems like a lot, it’s actually a fairly small amount of information for industries li
ke engineering, law, and healthcare. Initially we imagined testing on much larger document sets, but while conducting th
is test we were surprised by the practical difficulty of getting LangChain to work at scale; forcing us to reduce the sc
ope of our test. 

To get RAG up and running for a set of PDF documents, the first step is to parse the content of those
 PDFs into some sort of textual representation. LangChain uses libraries from [Unstructured.io](http://Unstructured.io) 
to perform parsing on complex PDFs, which works seamlessly for small document sets. 

Surprisingly, though, the speed of
 LangChain parsing is incredibly slow. Based on our analysis it appears that Unstructured uses a variety of models to de
tect and parse out key elements within a PDF. These models should employ GPU acceleration, but they don’t. That results 
in LangChain taking days to parse a modestly sized set of documents, even on very large (and expensive) compute instance
s. To get LangChain working we needed to reverse engineer portions of Unstructured and inject code to enable GPU utiliza
tion of these models. 

It appears that this is a known issue in Unstructured, as seen in the notes below. As it stands,
 it presents significant difficulty in scaling LangChain to larger document sets, given LangChain abstracts away fine gr
ain control of Unstructured. 

https://preview.redd.it/dhchak3xfqud1.png?width=1346&format=png&auto=webp&s=2e1e6b84f79d8
89c901f05140771f68bb0f00150

[**Source: Github**](https://github.com/Unstructured-IO/unstructured-inference/blob/64cd41c
37fe4535702b3be9c6b58f380ca4c7edd/unstructured_inference/inference/layout.py#L175)

We only made improvements to LangCha
in parsing up to the point where this test became feasible. If you want to modify LangChain for faster parsing, here are
 some resources: 

* The default directory loader of LangChain is Unstructured ([source1](https://python.langchain.com/v
0.1/docs/modules/data_connection/document_loaders/file_directory/),[ source2](https://github.com/langchain-ai/langchain/
blob/410e9add4443607618a75827afe1a676fcd7c0a7/libs/community/langchain_community/document_loaders/directory.py#L38)). 
*
 Unstructured uses “hi res” for the PDFs by default if text extraction cannot be performed on the document ([source1](ht
tps://github.com/langchain-ai/langchain/blob/410e9add4443607618a75827afe1a676fcd7c0a7/libs/community/langchain_community
/document_loaders/directory.py#L38) ,[ source2](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5
788dcb5b3a7a7a57bf078/unstructured/partition/strategies.py#L103) ). Other options are available like “fast” and “OCR onl
y”, which have different processing intensities 
* “Hi Res” involves: 
   * Converting the pdf into images ([source](htt
ps://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078/unstructured/partition/pdf_im
age/pdfminer_processing.py#L36)) 
   * Running a layout detection model to understand the layout of the documents ([sour
ce](https://github.com/Unstructured-IO/unstructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured
_inference/models/yolox.py#L3)). This model benefits greatly from GPU utilization, but does not leverage the GPU unless 
ONNX is installed ([source](https://onnxruntime.ai/docs/get-started/with-python.html)) 
   * OCR extraction using tesser
act (by default) ([source](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078
/unstructured/partition/utils/config.py#L103)) which is a very compute intensive process ([source](https://github.com/te
sseract-ocr/tesseract/issues/263)) 
   * Running the page through a table layout model ([source](https://github.com/Unst
ructured-IO/unstructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured_inference/models/tables.py
#L140)) 

While our configuration efforts resulted in faster processing times, it was still too slow to be feasible for 
larger document sets. To reduce time, we did “hi res” parsing on the relevant documents and “fast” parsing on documents 
which were irrelevant to our questions. With this configuration, parsing 100,000 pages of documents took 8 hours. If we 
had applied “hi res” to all documents, we imagine that parsing would have taken 31 days (at around 30 seconds per page).
 

At the end of the day, this test took two senior engineers (one who’s worked at a directorial level at several AI com
panies, and a multi company CTO with decades of applied experience of AI at scale) several weeks to do the development n
ecessary to write this article, largely because of the difficulty of applying LangChain to a modestly sized document set
.  To get LangChain working in a production setting, we estimate that the following efforts would be required: 

* Tesse
ract would need to be interfaced with in a way that is more compute and time efficient. This would likely require a high
-performance CPU instance, and modifications to the LangChain source code. 
* The layout and table models would need to 
be made to run on a GPU instance 
* To do both tasks in a cost-efficient manner, these tasks should probably be decouple
d. However, this is not possible with the current abstraction of LangChain. 

On top of using a unique technology which 
is highly performant, GroundX also abstracts virtually all of these technical difficulties behind an API. You upload you
r documents, then search the results. That’s it. 

If you want RAG to be even easier, one of the things that makes Eyele
vel so compelling is the service aspect they provide to GroundX. You can work with Eyelevel as a partner to get GroundX 
working quickly and performantly for large scale applications. 

# Conclusion 

When choosing a platform to build RAG ap
plications, engineers must balance a variety of key metrics. The robustness of a system to maintain performance at scale
 is one of those critical metrics. In this head-to-head test on real-world documents, EyeLevel’s GroundX exhibited a hei
ghtened level of performance at scale, beating LangChain and LlamaIndex. 

Another key metric is efficiency at scale. As
 it turns out, LangChain has significant implementation difficulties which can make the large-scale distribution of Lang
Chain powered RAG difficult and costly. 

Is this the last word? Certainly not. In future research, we will test various
 advanced RAG techniques, additional RAG frameworks such as Amazon Q and GPTs and increasingly complex and multimodal da
ta types. So stay tuned. 

If you’re curious about running these results yourself, please reach out to us at info@eyelev
el.ai.Vector databases, a key technology in building retrieval augmented generation or RAG applications, has a scaling p
roblem that few are talking about. 

According to new research by [EyeLevel.ai](https://www.eyelevel.ai/), an AI tools c
ompany, the precision of vector similarity search degrades in as few as 10,000 pages, reaching a 12% performance hit by 
the 100,000-page mark.

The research also tested [EyeLevel’s enterprise-grade RAG platform](https://www.eyelevel.ai/prod
uct/apis) which does not use vectors. EyeLevel lost only 2% accuracy at scale.

The findings suggest that while vector d
atabases have become highly popular tools to build RAG and LLM-based applications, developers may face unexpected challe
nges as they shift from testing to production and attempt to scale their applications.  

The work was performed by Dani
el Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a computer scientist and former senior en
gineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods of this test will be open sourced and av
ailable shortly. Others are invited to run the data and corroborate or challenge these findings. 
```
---

     
 
all -  [ Help scaling LLM classifications and validations ](https://www.reddit.com/r/LangChain/comments/1g3h1hd/help_scaling_llm_classifications_and_validations/) , 2024-10-16-0913
```
I'm working on an application that will classify 10 million records according to pretty well-defined standards. Ideally,
 I'd classify them with a LLM and LangChain, then run validation on it to double check the classifications. Here's the i
ssue: I'm a little lost. I've built smaller-scale RAG systems, but I have no idea how to do this at scale. Any help woul
d be greatly appreciated.

Big apologies if I shouldn't be posting the question here.
```
---

     
 
all -  [ Framework recommendation for a RAG project with LaTeX source files ](https://www.reddit.com/r/LangChain/comments/1g3gza0/framework_recommendation_for_a_rag_project_with/) , 2024-10-16-0913
```
Hi, I want to build a custom RAG application for question-answering on PDF files where I already have the LaTeX source.


  
The files have been parsed into LaTeX fragments and I have extracted metadata such as equation labels + LaTeX source
, what LaTeX source corresponds to which pages, and so on. All of this data is stored in a database. Different user will
 have access to different subsets of the LaTeX files.

The AI component will be build into an existing website (django),
 i.e. I would ideally like to answer questions such as:

-  Can you explain concept Y (it should be answered by any part
 of the material that has been made available)

-  Can you tell me what this file is about (the request will contain the
 file id)

- Can you explain what this page is about? (the request will contain the file id and page number,; but the ap
plication needs to understand that 'this page'  corresponds to the context associated with the file id and page number)




My current approach uses django vectordb plus custom queries/chunking but this is not very good. Ideally I would like
 an application where I can 'handfeed' it the LaTeX sources and metadata and use the library to do the RAG part. It is i
mportant that it replies with references to relevant text (if any) and it should also function in conversational mode.


I would like it to work with different endpoints and it is important that I can stream the response to the end-user (i.e
. it should respond with an iterator or similar).

Any guides or library recommendations would be highly appreciated!
```
---

     
 
all -  [ Similar tool like LangChain in GO ](https://www.reddit.com/r/golang/comments/1g3gqli/similar_tool_like_langchain_in_go/) , 2024-10-16-0913
```
Hey, I found this langChan tool from twitter. I'm more familiar to Golang, Does anyone know if there is alternative to t
his in Golang  
[https://python.langchain.com/docs/integrations/tools/](https://python.langchain.com/docs/integrations/t
ools/)
```
---

     
 
all -  [ Multi-Hop Agent with Langchain, Llama3, and Human-in-the-Loop for the Google Frames Benchmark ](https://www.reddit.com/r/LocalLLaMA/comments/1g3g365/multihop_agent_with_langchain_llama3_and/) , 2024-10-16-0913
```
In this notebook, I walk through how to create an agent using Langchain to solve the complex **Google Frames Benchmark**
 dataset. This agent leverages Wikipedia as a knowledge base to handle multi-hop reasoning tasks, with human reviewers p
roviding feedback via **Argilla** to improve its performance.

The **Frames-Benchmark** dataset is useful for building a
nd testing multi-hop retrieval and reasoning models. It consists of 824 challenging questions that require information r
etrieval from multiple Wikipedia articles (anywhere from 2 to 15 articles). These questions span diverse topics such as 
history, science, and health and are labeled based on reasoning types—numerical, tabular, multiple constraints, temporal
, and post-processing.

The human-in-the-loop feedback through **Argilla** helps make the agent’s thought process more t
ransparent and easier to refine with prompts.

Baseline results for the dataset show an accuracy range from **41% with b
asic prompting** to **66% for multi-step retrieval and reasoning**, indicating a lot of room for further improvements.


[LINK TO THE NOTEBOOK](https://github.com/argilla-io/argilla-cookbook/blob/main/multihop_langchain_frames_benchmark.ipyn
b)
```
---

     
 
all -  [ Build Your Own ChatGPT Clone with LangChain, Streamlit, and Ollama ](https://www.reddit.com/r/u_bluebashllc/comments/1g3ffq1/build_your_own_chatgpt_clone_with_langchain/) , 2024-10-16-0913
```
[Build Your Own ChatGPT Clone with LangChain, Streamlit, and Ollama](https://preview.redd.it/7dx6thfk1qud1.png?width=192
0&format=png&auto=webp&s=423a0defa28fb16eb889b261d42c7864750d613e)

Building your own [ChatGPT clone](https://www.blueba
sh.co/blog/build-chatgpt-clone-ollama-langchain-streamlit-rag/) is now easier with tools like Ollama, LangChain, and Str
eamlit RAG! Ollama provides an efficient way to run language models locally, while LangChain handles the integration and
 orchestration of various AI components. Use **RAG with LangChain** to provide context to the bot—allowing the chatbot t
o use information grabbed in real-time from uploaded files, offering far more accurate answers. Deploy the app by runnin
g your Streamlit interface, and voilà—you’ve created your own personal ChatGPT-like chatbot for real-world use!

**Impor
tant Tools**

**Ollama:** the central language model doing the talking.

**Streamlit:** Easy framework for writing web a
pplications. Could be helpful in building a front end for the chatbot.

**LangChain:** It integrates RAG, whereby you al
low your bot to retrieve information from other documents you have loaded.

**How to Build:**

**Set up environment**

I
nstall LangChain, Streamlit, and Ollama packages in your system.

**Create an Interface on Streamlit:** This is where th
e users will interact with the bot. There would be an uploading file feature where the users can upload files that will 
be used to provide context to the bot.

**Use RAG with LangChain:** The chatbot will use information grabbed in real-tim
e from uploaded files through LangChain and give far more accurate answers.

**Deploy the App:** Finally, you would just
 run your Streamlit app, and voilà. You've created your own personal ChatGPT-like chatbot for use.
```
---

     
 
all -  [ create_csv_agent fails to work on llama 3.1 70b through ollama. Which open-source model is compatibl ](https://www.reddit.com/r/LangChain/comments/1g3ex5n/create_csv_agent_fails_to_work_on_llama_31_70b/) , 2024-10-16-0913
```
I need a private model that works well on create_csv_agent. Gpt-4o was doing wonders but I don't want to entrust my comp
any's data to openai. I need a private in house model that can achieve this. What should I do? Which model worked for yo
u guys?
```
---

     
 
all -  [ How do you currently handle tasks that your AI agents cannot complete? ](https://www.reddit.com/r/LangChain/comments/1g3en5x/how_do_you_currently_handle_tasks_that_your_ai/) , 2024-10-16-0913
```
What challenges or inefficiencies do you face when integrating human intervention?
```
---

     
 
all -  [ RAG metadata ](https://www.reddit.com/r/LangChain/comments/1g3d0vg/rag_metadata/) , 2024-10-16-0913
```
Hello everyone! 

  
I'm building a RAG agent in LangGraph for production scale. On a small text files I used to send th
e whole content to the LLM so it generates some metadata about it, but for production level that won't be efficient beca
use the PDF size could be large. On the other hand if I created the metadata from the PDF's chunks they will be meaning 
less!! so how do you achieve that?

Thanks in advance, great community. <3

Co-founder, Shaareable Apps


```
---

     
 
all -  [  Major updates to XeroFlow node-based LLM tool ](https://www.reddit.com/r/ollama/comments/1g39vzo/major_updates_to_xeroflow_nodebased_llm_tool/) , 2024-10-16-0913
```
I just finished up a bunch of updates fixing issues dealing with the installer and a couple other minor issues in the pr
ogram. If you're not familiar with this tool it is a node-based llm tool that allows you to set up workflows to interact
 with and process data utilizing multiple llms. One of the updates it has now is it is able to utilize custom Vector dat
abases utilizing RAG and Langchain.    
For those of you who don't use Windows, just make sure you have python and pip i
nstalled and install using the requirements.txt file, once this is done then you can just run the [main.py](http://main.
py) file directly.

check it out here: [https://github.com/Xerophayze/XeroFlow](https://github.com/Xerophayze/XeroFlow)


https://preview.redd.it/3n41uzsx3oud1.png?width=752&format=png&auto=webp&s=b51db5d1e695a470f717bcbee12ac2c727424497

ht
tps://preview.redd.it/vh99khhq3oud1.png?width=1130&format=png&auto=webp&s=fe7d2b8216d295ce0fe6323f2d0623edd0c10acc

http
s://preview.redd.it/qshhx1iq3oud1.png?width=752&format=png&auto=webp&s=05bfb8aee6f3bd913ef68193947cd12e889d52bc

https:/
/preview.redd.it/pkxh0ghq3oud1.png?width=752&format=png&auto=webp&s=bb3ad0debd767995f625eafcb7e6023e35ba6e6a

https://pr
eview.redd.it/qe6m3ghq3oud1.png?width=857&format=png&auto=webp&s=5f6981fea7e4f314faed32ae4e451ad5fbcd1975


```
---

     
 
all -  [ Latest version of XeroFlow Node based LLM tool ](https://www.reddit.com/r/StableDiffusion/comments/1g39t3b/latest_version_of_xeroflow_node_based_llm_tool/) , 2024-10-16-0913
```
Hey everyone! Just fixed the issues with the installer for our tool. Make sure you have Python 3.10.9 installed; it shou
ld come with pip, but if not, you may need to install it separately. Once that's set, run the `setup.bat` file to instal
l all required dependencies—this may take a few minutes. When it’s done, you can launch the program by running `run.bat`
.

Also, I've left the API endpoints set up; just add your own API keys for OpenAI or Groq, and if you’re running a loca
l install or have Ollama on your system, simply adjust the IP address or hostname in the URL for those APIs.

This tool 
includes a vector database utilizing RAG and Langchain, with a sample document (an 80-90k word sci-fi novel that was wri
tten by the long form content node!) included in the database folder for you to check out. I'll be putting together a tu
torial video soon, so stay tuned!  
[https://github.com/Xerophayze/XeroFlow](https://github.com/Xerophayze/XeroFlow)

ht
tps://preview.redd.it/ccyzt0wp2oud1.png?width=1069&format=png&auto=webp&s=c6e27de025751899e508849e0fb1b9297921e7c2

http
s://preview.redd.it/c8xvguvp2oud1.png?width=1130&format=png&auto=webp&s=a7126f20e8c9844e0bf329ff6c7085ed8407a875

https:
//preview.redd.it/rft6bvvp2oud1.png?width=752&format=png&auto=webp&s=8cc31d58f5534c49c0ccc2e7a9709793cae2a919

https://p
review.redd.it/cv7p9gwp2oud1.png?width=752&format=png&auto=webp&s=5190ad7a57036e2fd037e248437d3ff8bf8f3ef5

https://prev
iew.redd.it/494kpuvp2oud1.png?width=857&format=png&auto=webp&s=a1e234ec05dc62b1820147e4f053d4a5a8ff3bd3


```
---

     
 
all -  [ Need help to create template for RAG app ](https://www.reddit.com/r/LangChain/comments/1g39p3h/need_help_to_create_template_for_rag_app/) , 2024-10-16-0913
```
I am creating a RAG application where I will be giving LLM a large set of responses array of objects where there will be
 multiple questions and it's answer provided by the user.  
For example :

    { 'responses': [ 
    { 'answers': [ 
   
   { 
        'question': 'WHAT IS YOUR NAME',
         'response': 'ABCD'
       }, {
       'question': 'What is your 
AGE',
       'response': '30' 
      } 
    ] },
     { 'answers': [ 
      { 'question': 'WHAT IS YOUR NAME',
       'r
esponse': 'ABCD'
       },
         { 'question': 'What is your AGE',
           'response': '30'
         } 
      ] },

      ...
     ]
    }

This is my current template :-

    Use the following pieces of context to answer the question.

      If you don't know the answer, just say that you don't know, don't try to make up an answer.
      Always say 'tha
nks for asking!' at the end of the answer.
      Don't provide any code for doing just provide the output.
      Conside
r the given context as document don't provide answer as a json data consider it as document.
      which contain informa
tion about all the responses for a particular form.
      The answer to a particular question is inside the response fie
ld, always provide the question along with response and make sure not to repeat the question.
      The given context is
 a array of user responses and each object is a response responded by a particular user.
      Consider all the users re
sponse before answering.
      {context}
    
      Question: {question}
      Helpful Answer:

Also whenever I ask it q
uestions it provides the answer on the basis of top 2 matches, if the number of questions are like 10, and If I ask it t
o 'give me all the questions present in the form' it gives only 2.
```
---

     
 
all -  [ Do I need to pay for unit test with langsmith library? ](https://www.reddit.com/r/LangChain/comments/1g33fsy/do_i_need_to_pay_for_unit_test_with_langsmith/) , 2024-10-16-0913
```
Pretty simple question I think?

I was reading the following:

[https://docs.smith.langchain.com/how\_to\_guides/evaluat
ion/unit\_testing](https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing)

But I don't get if I need to
 use the LangSmith web app to see the results or I can simply use the langsmith library to run my unit tests and get the
 results without the tracing.. Is this doable?

Is the use of the web app mandatory? Or langsmith can be used solely as 
a library without the tracing?

Thank you in advance!
```
---

     
 
all -  [ Which framework between haystack, langchain and llamaindex, or others? ](https://www.reddit.com/r/Rag/comments/1g31urm/which_framework_between_haystack_langchain_and/) , 2024-10-16-0913
```
The use case is the following.
Database: vector database with 10k scientific articles.
User needs: the user will need th
e chatbot both for advanced research on the dataset and chat with those results. 

Please let me know your advices!! 
```
---

     
 
all -  [ [Hiring][Remote] Software Engineer ](https://www.reddit.com/r/jobbit/comments/1g31h8x/hiringremote_software_engineer/) , 2024-10-16-0913
```
* Full-time opportunity
* Remote
* Compensation - $125k - $190k
* 4+ years of experience
* Experience with one or more o
f the following areas (or related technologies): 
   * Modern web/backend technologies: React, Typescript, Node, Next, P
ython
   * AI techniques and workflows: LLMs, RAG, Langchain, CoreML
   * Mobile development: Flutter (Dart), iOS (Swift
), Android (Kotlin)
   * Cloud platforms/infrastructure: GCP, AWS, Azure, Docker
   * Databases: Relational (PostgreSQL,
 MySQL) or Non-relational (MongoDB, Firestore)

Apply - [https://peerlist.io/company/squint/careers/software-engineer/jo
bhkknpn86qnjqbk2jaqlrngdperq](https://peerlist.io/company/squint/careers/software-engineer/jobhkknpn86qnjqbk2jaqlrngdper
q)
```
---

     
 
all -  [ [Hiring][Remote] Software Engineer ](https://www.reddit.com/r/RemoteJobHunters/comments/1g31erw/hiringremote_software_engineer/) , 2024-10-16-0913
```
* Full-time opportunity
* Remote 
* Compensation - $125k - $190k
* 4+ years of experience
* Experience with one or more 
of the following areas (or related technologies): 
   * Modern web/backend technologies: React, Typescript, Node, Next, 
Python
   * AI techniques and workflows: LLMs, RAG, Langchain, CoreML
   * Mobile development: Flutter (Dart), iOS (Swif
t), Android (Kotlin)
   * Cloud platforms/infrastructure: GCP, AWS, Azure, Docker
   * Databases: Relational (PostgreSQL
, MySQL) or Non-relational (MongoDB, Firestore)

Apply - [https://peerlist.io/company/squint/careers/software-engineer/j
obhkknpn86qnjqbk2jaqlrngdperq](https://peerlist.io/company/squint/careers/software-engineer/jobhkknpn86qnjqbk2jaqlrngdpe
rq)
```
---

     
 
MachineLearning -  [ [D] How are folks building conversational Retrieval Augmented Generation apps ](https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/) , 2024-10-16-0913
```
I've read through various resources such as:  
- [https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/](htt
ps://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/)  
- [https://python.langchain.com/docs/tutorials/qa\_cha
t\_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)  
- [https://langchain-ai.github.io/langgraph
/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) 
 
- [https://docs.llamaindex.ai/en/stable/module\_guides/deploying/chat\_engines/](https://docs.llamaindex.ai/en/stable/
module_guides/deploying/chat_engines/)  
- [https://huggingface.co/datasets/nvidia/ChatRAG-Bench](https://huggingface.co
/datasets/nvidia/ChatRAG-Bench) 

But these feel overly reductive, since they don't address complexities like:  
1) when
 to retrieve vs. just respond immediately to reduce latency  
2) rely on existing context previously retrieved in the co
nversation instead of retrieving again at the current turn  
3) partition LLM context between retrieved information and 
past conversation history.

I'm sure some teams already have good systems for this, would appreciate pointers!
```
---

     
 
MachineLearning -  [ Built a web agent which call fill Google forms based on the user details [P] ](https://www.reddit.com/r/MachineLearning/comments/1fozud5/built_a_web_agent_which_call_fill_google_forms/) , 2024-10-16-0913
```
GitHub repo : [https://github.com/shaRk-033/web-agent](https://github.com/shaRk-033/web-agent)

Tried to solve it using 
two approaches:

# 1: Basic Scraping and Filling

This is the straightforward approach. The agent scrapes the form’s HTM
L and uses fixed XPaths to find and fill in the required fields.

* It pulls the form’s HTML, locates the fields with se
t XPaths, and inputs the answers. It’s a direct and simple method.
* If the form changes or an element isn’t where it’s 
expected, the process can fail and may need manual adjustments.

[basic approach](https://preview.redd.it/5e8g4a1k4xqd1.
png?width=1055&format=png&auto=webp&s=d8e984e4feaee2f0453b08c8696768c40a2a5c20)

2. Using LangChain Agents and tool call
ing

* LangChain Agent**:** The agent handles everything by using the LLM’s reasoning to decide what to do next, includi
ng generating those tricky XPaths.
* Error Handling**:** If something goes wrong (like an element not found), the agent 
tries again with better XPaths until it gets the job done.

[using langchain agents](https://preview.redd.it/948i88pl4xq
d1.png?width=782&format=png&auto=webp&s=ed1e6c19efec9f4cbbbd6ab5a22558f221cf745f)

Any recommendations to improve this w
ould be welcome. Also, if anyone has ideas on building similar web agents to automate other tasks, it would be great to 
hear them. :)
```
---

     
 
MachineLearning -  [ [P] Swapping Embedding Models for an LLM ](https://www.reddit.com/r/MachineLearning/comments/1fktvbj/p_swapping_embedding_models_for_an_llm/) , 2024-10-16-0913
```
How tightly coupled is an embedding model to a language model?

Taking an example from Langchain's tutorials, they use O
llama's _nomic-embed-text_ for embedding and _Llama3.1_ for the understanding and Q/A. I don't see any documentation abo
ut Llama being built on embeddings from this embedding model. 

Intuition suggests that a different embedding model may 
produce outputs of other sizes or produce a different tensor for a character/word, which would have an impact on the res
ults of the LLM. So would changing an embedding model require retraining/fine-tuning the LLM as well?

I need to use a e
mbedding model for code snippets and text. Do I need to find a specialized embedding model for that? If yes, how will ll
ama3.1 ingest the embeddings?
```
---

     
 
deeplearning -  [ What is the best approach for Parsing and Retrieving Code Context Across Multiple Files in a Hierarc ](https://www.reddit.com/r/deeplearning/comments/1fh58oz/what_is_the_best_approach_for_parsing_and/) , 2024-10-16-0913
```
I want to implement a Code-RAG system on a code directory where I need to:

* Parse and load all the files from folders 
and subfolders while excluding specific file extensions.
* Embed and store the parsed content into a vector store.
* Ret
rieve relevant information based on user queries.

However, I’m facing two major challenges:

**File Parsing and Loading
:** What’s the most efficient method to parse and load files in a hierarchical manner (reflecting their folder structure
)? Should I use Langchain’s directory loader, or is there a better way? I came across the Tree-sitter tool in Claude-dev
’s repo, which is used to build syntax trees for source files—would this be useful for hierarchical parsing?

**Cross-Fi
le Context Retrieval:** If the relevant context for a user’s query is spread across multiple files located in different 
subfolders, how can I fine-tune my retrieval system to identify the correct context across these files? Would reranking 
resolve this, or is there a better approach?

**Query Translation:** Do I need to use Something like Multi-Query or RAG-
Fusion to achieve better retrieval for hierarchical data?

\[I want to understand how tools like [continue.dev](http://c
ontinue.dev/) and [claude-dev](https://github.com/saoudrizwan/claude-dev) work\]
```
---

     
