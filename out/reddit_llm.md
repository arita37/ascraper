 
all -  [ Design your entire robot software in minutes (a LangChain open source application) ](https://medium.com/@saeedshamshiri_94060/looking-inside-roscribe-and-the-idea-of-llm-based-robotic-platform-4f3ec05dcbb1) , 2023-10-25-0909
```

```
---

     
 
all -  [ Do You Use ChatGPT to Discover New SaaS Tools? ](https://www.reddit.com/r/LangChain/comments/17fkewk/do_you_use_chatgpt_to_discover_new_saas_tools/) , 2023-10-25-0909
```
Hey all,

I’ve found ChatGPT invaluable for boosting productivity. It’s not just about doing tasks faster; it’s also a g
oldmine for discovering new workflows and SaaS tools. For instance, it suggested Mailchimp and Substack when I was explo
ring newsletter growth.

Are you also using ChatGPT for this? Specifically, has it helped you discover new SaaS tools th
at you’ve actually implemented?  


I am creating a tool for Langchain that gives you context about what SaaS can be use
d to solve your problem, their features, pricing, etc... Would you find it useful for your chains?  


&#x200B;
```
---

     
 
all -  [ I'm Harrison Chase, CEO and cofounder of LangChain. Ask me anything! ](https://www.reddit.com/r/LangChain/comments/17ffvxo/im_harrison_chase_ceo_and_cofounder_of_langchain/) , 2023-10-25-0909
```
I'm Harrison Chase, CEO and cofounder of LangChain–an open-source framework and developer toolkit that helps developers 
get LLM applications from prototype to production.      


Hi Reddit! Today is LangChain's first birthday and it's been 
incredibly exciting to see how far LLM app development has come in that time–and how much more there is to go. Thanks fo
r being a part of that and building with LangChain over this last (wild) year.        


I'm excited to host this AMA, a
nswer your questions, and learn more about what you're seeing and doing.
```
---

     
 
all -  [ [Crosspost] IAMA with Harrison Chase, co-founder and CEO of LangChain: 9-11AM Pacific (12-2PM Easter ](https://www.reddit.com/r/IAmA/comments/17ffps3/crosspost_iama_with_harrison_chase_cofounder_and/) , 2023-10-25-0909
```
Join us TODAY, Tuesday, October 24th from 9-11 AM Pacific (12-2 PM Eastern) for an AMA hosted by Harrison Chase, co-foun
der and CEO of LangChain hosted by the [ LangChain subreddit](https://www.reddit.com/r/LangChain/).

This is your opport
unity to ask Harrison questions about utilizing LangChain in developing large language model (LLM) applications, and to 
share your own ideas and suggestions. Take advantage of this chance to learn more about how to leverage LangChain in you
r own projects and get insights into latest developments.

https://www.reddit.com/r/LangChain/
```
---

     
 
all -  [ Using Langchain With llama.cpp? ](https://www.reddit.com/r/LocalLLaMA/comments/17ffbg9/using_langchain_with_llamacpp/) , 2023-10-25-0909
```
I have Falcon-180B served locally using llama.cpp via the server REST-ful api.  I assume there is a way to connect langc
hain to the /completion endpoint.  

Does anyone have an example that does something like this?
```
---

     
 
all -  [ Custom Parser as argument to RetrivalQA ](https://www.reddit.com/r/LangChain/comments/17fenmv/custom_parser_as_argument_to_retrivalqa/) , 2023-10-25-0909
```
I have raw Jsons in the vectorized data. I am trying assign an instance of customer parser to RetrivalQA chain but getti
ng the error that no extra field allowed. Following is what not allowed :-  
I am not allowed to pass 'output\_parser': 
parser, to chain\_type\_kwargs

    rag_qa_pipeline = RetrievalQA.from_chain_type(
                            llm=llm,

                            chain_type='stuff',
                            retriever=custom_retriever,
                
            verbose=True,
                            chain_type_kwargs={
                                'output_parser
': parser,
                                'verbose': True,
                                'prompt': _prompt,
         
                       'memory': ConversationBufferMemory(
                                    memory_key='history',
   
                                 input_key='question',
                                     k=2),
                      
      }
                        )

Has anyone come accross any such situation? is there a way to assign a custom parser 
to RetrievalQA ?  

```
---

     
 
all -  [ Visiting Data Scientist from California looking for other DS for chat ](https://www.reddit.com/r/seoul/comments/17fe2ho/visiting_data_scientist_from_california_looking/) , 2023-10-25-0909
```
Visiting Seoul for a week. Would like to have a chat about AI, LLMs, Langchain, and applications with other DS in Seoul.
 
```
---

     
 
all -  [ TODAY: Reddit AMA with Harrison Chase, co-founder and CEO of LangChain: 9-11AM PST (12-2PM EST). ](https://www.reddit.com/r/LangChain/comments/17fdcyg/today_reddit_ama_with_harrison_chase_cofounder/) , 2023-10-25-0909
```
Join us TODAY, Tuesday, October 24th from 9-11 AM Pacific (12-2 PM Eastern) for an AMA hosted by Harrison Chase, co-foun
der and CEO of LangChain. 

This is your opportunity to ask Harrison questions about utilizing LangChain in developing l
arge language model (LLM) applications, and to share your own ideas and suggestions. Take advantage of this chance to le
arn more about how to leverage LangChain in your own projects and get insights into latest developments.
```
---

     
 
all -  [ [For Hire] Programmer/Web Developer/IT Consultant (Python, PHP, AI, etc.) ](https://www.reddit.com/r/forhire/comments/17fd2eq/for_hire_programmerweb_developerit_consultant/) , 2023-10-25-0909
```
To get in contact, please **message** me, I **don't** use the chat thing and might miss you or reply very late. Then we 
can switch to email/discord/telegram or whatever else. Apologies for starting with this, but many missed it when it was 
lower.

I'm a programmer/web developer with 12 years of professional experience. I am available for all sorts of program
ming and web development tasks.

I also offer consulting services. If you need something done, but don't know how exactl
y, I can help. I'm an excellent researcher and I communicate well. I will work with you to find the best solution for yo
ur problem.

My services include, but are not limited to:

* websites

* desktop applications

* AI integration (chatGPT
 API, langchain, whatever else turns up)

* integration with APIs and other webservices

* all kinds of scripts

* task 
automation

* website optimization

* debugging

* plugins for existing software

* bots (Reddit, Telegram, etc)

If you
're looking for someone to take care of a variety of different tasks, I can offer continuous support.

My preferred envi
ronment is Python with Django, but I work with anything Python or PHP based, including Wordpress. I also do frontend stu
ff with JavaScript, jQuery, AJAX. I also have no problem with learning new technologies that are needed for the project.


Rate is $50/h. Can also do fixed price by project, but only if the project/milestone is well-defined.

Satisfied custo
mers:

https://www.reddit.com/r/testimonials/comments/2e8gqy/pos_uqui_need_a_backend_web_dev_look_no_further/

https://w
ww.reddit.com/r/testimonials/comments/7fsdze/pos_hiring_uqui_was_an_example_of_how_it_should/

https://www.reddit.com/r/
testimonials/comments/80pu9l/pos_uqui_great_work_detailed_and_fast/

https://www.reddit.com/r/testimonials/comments/b0nx
68/uqui_is_a_hardworking_intelligent_honest_apps/

https://www.reddit.com/r/testimonials/comments/j3mz3p/uqui_is_a_great
_web_development_consultant_with/

https://www.reddit.com/r/testimonials/comments/v40ay3/pos_uqui_is_a_great_backend_dev
_to_work_with/

Some examples of sites I worked on: http://bdabkowski.yum.pl/

Please note: I am **not** a designer.
```
---

     
 
all -  [ Prepare tables for LangChain? ](https://www.reddit.com/r/LangChain/comments/17fcc1z/prepare_tables_for_langchain/) , 2023-10-25-0909
```
Hi! I am totally new in LangChain and hyped for the possibilities.

I have seen some services that use langchain, but be
fore they use them in the client database, they can import, select and join tables to a better data quality preparation.
 

Is there any SaaS to do so? Thank you in advance!
```
---

     
 
all -  [ What framework to use? ](https://www.reddit.com/r/ChatGPTPro/comments/17f85es/what_framework_to_use/) , 2023-10-25-0909
```
Hello, i am just getting into ai. I am mindblown and overloaded with tons of information on different frameworks to work
 with OpenAIAPI like langchain, chatdev, xagent and autogen. My problem is that that  i can not understand which one sho
uld i use to implement multiple llms for my personal project. Because chatdev and autogen are very new and langchain is 
full of complex uncessary abstractions. Please, give me advice on the vector of my development. Do i need to learn some 
of these technologies or try to build my own framework of multiple ai gents using GPT API and what else should i learn f
or future development of llm based automatizations? Thank you, in advance
```
---

     
 
all -  [ can you embed csv files and pdf files in the same vector database? ](https://www.reddit.com/r/LangChain/comments/17f7wo6/can_you_embed_csv_files_and_pdf_files_in_the_same/) , 2023-10-25-0909
```
potentially a silly question...but can you embed csv files and pdf files in the same vector database?

trying to make a 
chatbot that you can talk to different file types 
```
---

     
 
all -  [ Information extraction from a huge PDF or HTML ](https://www.reddit.com/r/LangChain/comments/17f7qk6/information_extraction_from_a_huge_pdf_or_html/) , 2023-10-25-0909
```
I am working on a problem where I want to load a PDF or HTML page and  want to find some information out of that pdf or 
html. Now, I don’t want to store the vectors anywhere and I wanna do it in realtime as users will upload their file or h
tml page link and they should get the required information from the pdf on the fly. 

I have checked everywhere but didn
’t find a solution. Any help will be appreciated. Thanks
```
---

     
 
all -  [ What really is the point of LangChain / LangServe ](https://www.reddit.com/r/LangChain/comments/17f5oas/what_really_is_the_point_of_langchain_langserve/) , 2023-10-25-0909
```
So I have played around with LangChain for a couple weeks with different LLMs, such as GPT-4, Llama II, etc. It seems li
ke it is used for solving a particular type of tasks using available tools, either installed from the library or ChatGPT
 plugins.  


I feel like the need for a production-level software is not that high due to the following reason:

1. The
 ChatGPT plugin store already offers many tool options for different tasks.
2. ChatGPT offers customization now on indiv
idual accounts now. It can be served as chain of thought and as context to a newly opened conversation.
3. Having multip
le agents for one application is error prone and often end up with errors.

With the intro of AutoGen, a competitor of L
angChain, I just don't get what the end users want out of such frameworks that cannot be offered by ChatGPT.  


I have 
no intention of hurting feelings of any enthusiasts here. I've spent tremendous time digging into this too. I would like
 to learn what people really think of it and how LangChain can be used, really. 
```
---

     
 
all -  [ NiceGUI 1.4.0 with breaking changes, simplified use of ui.run_javascript, react-like ui.state and mo ](https://www.reddit.com/r/nicegui/comments/17f3t8u/nicegui_140_with_breaking_changes_simplified_use/) , 2023-10-25-0909
```
### New features and enhancements

- Make [JavaScript](https://nicegui.io/documentation#run_javascript) calls optionally
 awaitable
- Introduce [react-like `ui.state`](https://nicegui.io/documentation/refreshable#refreshable_ui_with_reactive
_state) to be used with `ui.refreshable`
- Move [Highcharts](https://nicegui.io/documentation/highcharts) dependency int
o a separate [nicegui-highcharts](https://github.com/zauberzeug/nicegui-highcharts) package to avoid the need for a lice
nse for commercial projects
- allow sign-up at https://on-air.nicegui.io/login to get a fixed [On Air](https://nicegui.i
o/documentation#nicegui_on_air) URL and the possibility to protect remote access with a passphrase (this is a tech previ
ew of our upcoming [On Air](https://nicegui.io/documentation#nicegui_on_air) service and is free of charge until the end
 of the year if you sign up now)
- Refactor `globals` module
- Use FastAPI's new `lifespan` API
- Use flex layout per de
fault for layout elements
- Replace netifaces with much simpler (and better) ifaddr
- Convert [`ui.timer`](https://niceg
ui.io/documentation/timer) into an element
- Update httpx dependency (#1820 by @tscheburaschka, @falkoschindler)
- Consi
stently mark methods private if not part of the public API
- Remove deprecated APIs

### Bugfixes

- Fix [AG Grid](https
://nicegui.io/documentation/aggrid) bug with hidden cells by upgrading to new version

### Documentation

- Add LangChai
n handler to the ['Chat with AI' example](https://github.com/zauberzeug/nicegui/blob/main/examples/chat_with_ai/main.py)


### Breaking changes and migration guide

#### No need to await JavaScript calls

When using `run_javascript`, `run_me
thod`, `call_api_method` and `call_column_api_method`,
you can decide whether the client should respond with a return va
lue or not by awaiting the method call or not.
The method will automatically inform the client.
The `respond` parameter 
of `run_javascript` is not used anymore. See https://nicegui.io/documentation/run_javascript

#### `ui.chart` is now `ui
.highchart` and requires the package 'nicegui-highchart'

[Highcharts](https://nicegui.io/documentation/highcharts) requ
ires you to buy a license for commercial products if the code is installed on your machine.
That's why we made it an opt
ional package.
Install with `pip install nicegui[highcharts]`.

#### The `globals` module is gone

We removed the ugly `
globals` module, which was never intended to be public API,
but might have been used nonetheless.

- If you need the app
 configuration, use `app.config` instead.
- If you need the current client or slot, use the `context` module instead.
- 
If you need the client dictionary, use `Client.instances` instead.

#### FastAPI's new lifespan API

Since FastAPI's `@o
n_event('startup')` and `@on_event('shutdown')` are deprecated,
NiceGUI switched to the new lifespan API.
You can still 
use `app.on_startup()` and `app.on_shutdown()`.

#### Layout elements use flex layout by default

Before you needed to u
se `ui.column` inside, e.g., `ui.tab_panel` and other elements to get proper alignment, padding and spacing.
Now most UI
 elements provide reasonable default so that the content looks like in a `ui.row` or `ui.column`.

### Upgraded third-pa
rty dependencies

- vue: 3.3.4 → 3.3.6
- quasar: 2.12.2 → 2.13.0
- tailwindcss: 3.3.2 (unchanged)
- socket.io: 4.7.1 → 4
.7.2
- es-module-shims: 1.7.3 → 1.8.0
- aggrid: 30.0.3 → 30.2.0
- echarts: 5.4.3 (unchanged)
- mermaid: 10.2.4 → 10.5.1

- nipplejs: 0.10.1 (unchanged)
- plotly: 2.24.3 → 2.27.0
- three: 0.154.0 → 0.157.0
- tween: 21.0.0 (unchanged)
- vanill
a-jsoneditor: 0.18.0 → 0.18.10
```
---

     
 
all -  [ gpt4docstrings: Automatically generate docstrings for entire projects using ChatGPT ](https://www.reddit.com/r/ChatGPTPro/comments/17exp69/gpt4docstrings_automatically_generate_docstrings/) , 2023-10-25-0909
```
gpt4docstrings is a Python library that allows you to generate docstrings for undocumented functions / classes. In this 
example, I'm applying the tool to a module inside the langchain repository.

Repo: [https://github.com/MichaelisTrofficu
s/gpt4docstrings](https://github.com/MichaelisTrofficus/gpt4docstrings)

https://reddit.com/link/17exp69/video/eybtzt248
1wb1/player
```
---

     
 
all -  [ Understanding nr. of LLM requests in standard ReAct Agent ](https://www.reddit.com/r/LangChain/comments/17evn6q/understanding_nr_of_llm_requests_in_standard/) , 2023-10-25-0909
```
Okey, so I'm trying to clarify that I understand the number of LLM calls a typical ReAct Agent makes when using tools su
ch as Google and Wikipedia. I will mark the need for an **LLM call with bold text**. So consider the example from the La
ngChain documentation:

'Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?'

\> Enteri
ng new AgentExecutor chain...I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to th
e 0.43 power. **<-- LLM CALL (finds out what to do)**  
Action: Search **<-- LLM CALL (find out what function to use and
 which input)**  
Action Input: 'Leo DiCaprio girlfriend'  
Observation: model Vittoria Ceretti **<-- LLM CALL (break do
wn the output from the Search to an observation)**  
Thought: I need to find out Vittoria Ceretti's age **<-- LLM CALL (
thought of what to do next)**  
Action: Search **<-- LLM CALL (find out what function to use and which input)**  
Action
 Input: 'Vittoria Ceretti age'  
Observation: 25 years **<-- LLM CALL (break down the output from search to an observati
on)**  
Thought: I need to calculate 25 raised to the 0.43 power **<-- LLM Call (...)**  
Action: Calculator **<-- LLM C
all (...)**  
Action Input: 25\^0.43  
Observation: Answer: 3.991298452658078 **<-- LLM Call (...)**  
Thought: I now kn
ow the final answer **<-- LLM Call (...)**  
Final Answer: Leo DiCaprio's girlfriend is Vittoria Ceretti and her current
 age raised to the 0.43 power is 3.991298452658078. **<-- LLM Call (...)**

\> Finished chain.

I'm not sure if this is 
the right way of understanding it? I'm trying to figure out where in the Thought -> Action -> Observation the LLM is inv
olved. Is it one call per (Thought/Action/Observation)? Assume that the Action does not include extra LLM calls but that
 it is a Search on Google or Wikipedia and not RAG. Does the Google and Wikipedia tool include a LLM to summarise what i
t found?

I'm a bit unsure about this, any thoughts?
```
---

     
 
all -  [ Passing custom stopping strings to ZeroShotAgent? ](https://www.reddit.com/r/LangChain/comments/17eqlhp/passing_custom_stopping_strings_to_zeroshotagent/) , 2023-10-25-0909
```
I'm currently using a variant of the Mistral model with Langchain. It works ok however I am getting <|im_end|> back at t
he end of the responses.  I'm using LlamaCPP so without agents I can do something like: 

    llm = LlamaCpp(
        mo
del_path='/home/fleabs/ai/text-generation-webui/models/dolphin-2.1-mistral-7b.Q4_K_M.gguf',
        n_gpu_layers=35,
   
     n_batch=256,
        n_ctx=8192,
        callback_manager=callback_manager,
        verbose=True,
        temperatu
re=0.0,
        max_tokens=2000,
        top_p=0.2,
        top_k=10,
        repeat_penalty=1.2,
        stop=['<|im_en
d|>'],
    )

To pass a custom stopping string in. However when I try this with a ZeroShotAgent I get an error about sto
p being set multiple times:

> raise ValueError('`stop` found in both the input and default params.')
> ValueError: `sto
p` found in both the input and default params.

I'm guessing langchain is setting the stop tokens for the agent. Does an
yone know how I can pass <|im_end|> as a stop word with the agent?

I tried setting it like this: 

    agent = ZeroShot
Agent(max_iterations=3, llm_chain=llm_chain, tools=tools, verbose=True, stop=['<|im_end|>'])

But it doesn't seem to wor
k. Any advice much appreciated!
```
---

     
 
all -  [ Resume Review please...I will be graduating in a few months. Tired of seeing 'we will not be able to ](https://i.redd.it/s9ehht8yczvb1.jpg) , 2023-10-25-0909
```
Also if you have any suggestions on which job roles I should target or what technologies should I learn/make projects on
 please feel free to add.
Thanks in advance!
```
---

     
 
all -  [ [Local Llama] Problème avec l'hébergement de modèles avec Fastapi ](https://www.reddit.com/r/redditenfrancais/comments/17en6tj/local_llama_problème_avec_lhébergement_de_modèles/) , 2023-10-25-0909
```
J'ai utilisé les fonctions Load_Quant et Load_Model à partir du TextGen WebUI pour charger les modèles WizardLM-30B-GPTQ
 dans Langchain.

Je suis en mesure d'inviter le pipeline Langchain pour obtenir des sorties.

Mais, j'ai essayé de char
ger ce pipeline et de l'utiliser via un point de terminaison FastAPI, dans l'espoir d'envoyer l'invite en tant que deman
de de post, mais ici, je reçois une erreur CUDA hors de la mémoire.

Une raison à cela?

Traduit et reposté à partir de 
la publication https://www.reddit.com/14gydpv
```
---

     
 
all -  [ What's the best experience you've had with a chatbot? ](https://www.reddit.com/r/LangChain/comments/17ekndx/whats_the_best_experience_youve_had_with_a_chatbot/) , 2023-10-25-0909
```
Barring ChatGPT, Bard, etc what other commercial chatbots have provided a great experience in terms of UX?

I feel like 
there's so little in actual production, and what is out there is awful.
```
---

     
 
all -  [ Training a model on French Law, any insights? ](https://www.reddit.com/r/LocalLLaMA/comments/17ejzx1/training_a_model_on_french_law_any_insights/) , 2023-10-25-0909
```
# We were looking to train our first LORA model and had this idea of using french Labor Laws (Code du Travail) as a basi
s for a dataset.

**The 'product' would be a query-oriented conversational model, able --in its limited and dutifuly-dis
claimed-to-users capacities-- to reply to simple questions like «*****Is my boss allowed to force me to install a tracke
r on my phone in order to verify where I am when I'm working remote?*****', in French of course.**

&#x200B;

**Why is i
t |fun|?**

* **Human-hard level texts** The law is jargonous and confusing for simple folks. There's a challenge to hav
e an LLM understand it. And a bonus for humans if it works.
* **Social impact**. Not everyone can afford legal help, so 
it's Tech-For-Good approved.
* **Hacking the law**. C'me on. If that ain't fun?
* **Future opportunities.** Once we get 
it to work on that 'part of the law', we could try to expand to other legal topics.

&#x200B;

**Why do you care?**

**T
here are several options before us and we're not sure about the most efficient way to operate.** I'm coming here for adv
ices and opinions, feel free to throw everything you think at us :)

&#x200B;

**A/ The first question is: what's the ea
siest and most efficient way to do it today?**

We can use a LORA of sort, but there's also Langchain for example. Which
 option would be best at the moment in the constantly evolving landscape?

&#x200B;

**B/ The next problem is probably: 
how to build that dataset?**

We have the PDF document transformed into text and hierarchical YAML, but it's just a star
t.

My first idea was to use a summary model in order to get 'simplified' texts to use as answers. but it turns out the 
French summary models on HuggingFace are not very good at it. Our domain texts are not  news article from CNN translated
 in French.

We know how datasets matter, so there's really a lot we want to hear about that.

&#x200B;

**C/ And then, 
there's the question: on which model do we train the LORA?**

There are models like Vigogne which are supposedly good at
 French, but we're not so sure about which to pick.

One problem is: has the exisiting model been trained on the similar
 tokens set as the one from our text?

In other words, will the LLAMA/Vigogne/Falcon/other model be able to understand o
ur lexical field, e.g. legal language?

My testing is worrysome, as using the tokenizer of some models like vigogne or c
ustomised LLAMAs on the text we're considering, I get 1M+ tokens in the resulting set. While the model's vocab is only 6
4k or 32k high. Big Discrepancy.

Any idea / experience about that?

&#x200B;
```
---

     
 
all -  [ ER-Daigram Integration with Chatbots ](https://www.reddit.com/r/LangChain/comments/17edosb/erdaigram_integration_with_chatbots/) , 2023-10-25-0909
```
Hey everyone, I am looking for making a chatbot type application wherein given the ER diagram of a whole database user c
an ask questions related to it, like how all tables are connected, etc. This is just an idea that popped in my mind. Ple
ase do give your helpful suggestions and idea. Thanks.
```
---

     
 
all -  [ [P] NexaAgent: A highly efficient multi-task PDF tool for all your needs | backed by AutoGen ](https://www.reddit.com/r/MachineLearning/comments/17eajz2/p_nexaagent_a_highly_efficient_multitask_pdf_tool/) , 2023-10-25-0909
```
Just a quick open-source project recently submitted to huggingface backed by AutoGen. Share this initial version with yo
u guys!

[NexaAgent 0.0.1](https://huggingface.co/spaces/xuyingliKepler/nexaagent) offers a straightforward solution for
 handling PDFs.

* Users can easily upload any PDF, regardless of its size.
* The tool emphasizes accuracy, minimizing d
iscrepancies in PDF processing.

At its core, NexaAgent is backed by the AutoGen and LangChain frameworks. AutoGen facil
itates multi-agent interactions for task execution, while LangChain bridges LLMs with external data sources. Together, t
hese technologies ensure NexaAgent's robust and precise PDF management capabilities.

https://preview.redd.it/kwgo3phnav
vb1.jpg?width=1440&format=pjpg&auto=webp&s=1c5fbc566938d60d5c43802aff3a0690821e1c79
```
---

     
 
all -  [ Add LangChain to my opensource project SolidGPT -> chat with your codebase directly just like ChatGP ](https://www.reddit.com/r/LangChain/comments/17e4aif/add_langchain_to_my_opensource_project_solidgpt/) , 2023-10-25-0909
```
My open source project SolidGPT just release the v0.2.6, by using GPT4 model

and I add a **New feature - Chat everythin
g with your code repository**

In SolidGPT v0.2.6. We can do:

* Ask questions about any part of the codebase.
* Input n
ew requirements and have SolidGPT provide a coding plan.
* Seek clarity on any section, and let SolidGPT guide your unde
rstanding.
* Deploy everything locally

GitHub: [https://github.com/AI-Citizen/SolidGPT](https://github.com/AI-Citizen/S
olidGPT)

**Private Solution**

Integrate with LangChain to let LLM agent scan and learning the code repository, always 
give the answer deeply base on your codebase content

**Deploy Locally**

Integrate with FastAPI and using the Docker, e
asily deploy locally, charge your date by your own.

I'm diligently exploring more practical methods for people to colla
borate with LLM Agents. The goal is to enhance our development processes and empower every tech enthusiast with AI.  



https://preview.redd.it/69s4104frtvb1.png?width=3012&format=png&auto=webp&s=198447a992955b8c739416554b5972dd33c048e0
```
---

     
 
all -  [ ChatRepo - chat with your codebase directly just like ChatGPT Resources ](https://www.reddit.com/r/foss/comments/17e491l/chatrepo_chat_with_your_codebase_directly_just/) , 2023-10-25-0909
```
My open source project SolidGPT just release the v0.2.6, by using GPT4 model

and I add a **New feature - Chat everythin
g with your code repository**

In SolidGPT v0.2.6. We can do:

* Ask questions about any part of the codebase.
* Input n
ew requirements and have SolidGPT provide a coding plan.
* Seek clarity on any section, and let SolidGPT guide your unde
rstanding.
* Deploy everything locally

GitHub: [https://github.com/AI-Citizen/SolidGPT](https://github.com/AI-Citizen/S
olidGPT)

**Private Solution**

Integrate with LangChain to let LLM agent scan and learning the code repository, always 
give the answer deeply base on your codebase content

**Deploy Locally**

Integrate with FastAPI and using the Docker, e
asily deploy locally, charge your date by your own.

I'm diligently exploring more practical methods for people to colla
borate with LLM Agents. The goal is to enhance our development processes and empower every tech enthusiast with AI.

htt
ps://preview.redd.it/f4yi0vk1rtvb1.png?width=3012&format=png&auto=webp&s=e22025e7bde28eeb699220e3b6cd7a63b5e6d922

&#x20
0B;

Let me know what's your thought about my open source project!
```
---

     
 
all -  [ Chat with the codebase by GPT3/4 ](https://www.reddit.com/r/GPT3/comments/17e46jn/chat_with_the_codebase_by_gpt34/) , 2023-10-25-0909
```
My open source project SolidGPT just release the v0.2.6, 

and I add a **New feature - Chat everything with your code re
pository**

In SolidGPT v0.2.6. We can do:

* Ask questions about any part of the codebase.
* Input new requirements and
 have SolidGPT provide a coding plan.
* Seek clarity on any section, and let SolidGPT guide your understanding.
* Deploy
 everything locally

GitHub: [https://github.com/AI-Citizen/SolidGPT](https://github.com/AI-Citizen/SolidGPT) 

**Privat
e Solution**

Integrate with LangChain to let LLM agent scan and learning the code repository, always give the answer de
eply base on your codebase content

**Deploy Locally**

Integrate with FastAPI and using the Docker, easily deploy local
ly, charge your date by your own.

I'm diligently exploring more practical methods for people to collaborate with LLM Ag
ents. The goal is to enhance our development processes and empower every tech enthusiast with AI.

https://preview.redd.
it/m94j0rufqtvb1.png?width=3012&format=png&auto=webp&s=ad7193dad33ca023ff98382fd101b3f84776b24f

&#x200B;

Let me know w
hat's your thought about my open source project, THank you!
```
---

     
 
all -  [ ChatRepo - chat with your codebase directly just like ChatGPT ](https://www.reddit.com/r/ChatGPT/comments/17e43jj/chatrepo_chat_with_your_codebase_directly_just/) , 2023-10-25-0909
```
My open source project SolidGPT just release the v0.2.6, by using GPT4 model

and I add a **New feature - Chat everythin
g with your code repository**

In SolidGPT v0.2.6. We can do:

* Ask questions about any part of the codebase.
* Input n
ew requirements and have SolidGPT provide a coding plan.
* Seek clarity on any section, and let SolidGPT guide your unde
rstanding.
* Deploy everything locally

GitHub: [https://github.com/AI-Citizen/SolidGPT](https://github.com/AI-Citizen/S
olidGPT) 

**Private Solution**

Integrate with LangChain to let LLM agent scan and learning the code repository, always
 give the answer deeply base on your codebase content

**Deploy Locally**

Integrate with FastAPI and using the Docker, 
easily deploy locally, charge your date by your own.

I'm diligently exploring more practical methods for people to coll
aborate with LLM Agents. The goal is to enhance our development processes and empower every tech enthusiast with AI.

ht
tps://preview.redd.it/2dt89nxsptvb1.png?width=3012&format=png&auto=webp&s=8ea89eff3511b51f0381b7da080441ae16b1e531

&#x2
00B;

Let me know what's your thought about my open source project! 
```
---

     
 
all -  [ How do I integrate multiple expert agents? ](https://www.reddit.com/r/LangChain/comments/17dz0fr/how_do_i_integrate_multiple_expert_agents/) , 2023-10-25-0909
```
Hi guys, I've been working with LLM's for a while now, and looking into building a new project which has some functional
ity I haven't figured out yet, how to do in the best way. I want to be able to handle multiple expert agents, which migh
t even use different language models.

How would you guys approach the following use case?

**I want to create a chatbot
 that consists of several helper agents, each with its own specialty. Here's how it works:**

1. The user sends a messag
e to the main agent (let's call it the 'Planner Agent').
2. The Planner Agent decides on a plan to fulfill the user's re
quest.
3. It then checks if any of its helper agents (like Expert Agent A, B, or C) can handle parts of that plan.
4. If
 a helper agent can do a task, it might ask the user for more details to get the job done.
5. Once all tasks are complet
ed, the Planner Agent confirms with the user that everything went smoothly.

**Example Use Case: Imagine you're using a 
travel planning chat agent.**

1. You tell the main agent (Planner Agent) that you want to plan a trip to Paris.
2. Plan
ner Agent creates a plan: (a) Book flights, (b) Find hotels, (c) Suggest tourist spots.
3. Expert Agent A, who specializ
es in flights, then asks you for your preferred travel dates.
4. Expert Agent B, the hotel guru, inquires about your bud
get and desired amenities.
5. Expert Agent C, the sightseeing expert, asks about your interests—like art, history, or ad
venture.
6. After gathering all the info, the Planner Agent presents you with a complete travel itinerary and asks for y
our approval.
```
---

     
 
all -  [ Write pandas prompt using React ](https://www.reddit.com/r/LangChain/comments/17dyhi5/write_pandas_prompt_using_react/) , 2023-10-25-0909
```
Hi, I am very new to langchain and trying to build a prompt using React thought and reasoning template, just wanted to u
nderstand if is it possible to make the llm's learn how to query custom tasks using pandas? For example, I want to build
 a prompt that teaches llms to carry out a multi step task using pandas agent.
```
---

     
 
all -  [ Integrating Chat History Sessions with Langchain Python, Mistral, and Choosing the Right Database ](https://www.reddit.com/r/LangChain/comments/17dr55p/integrating_chat_history_sessions_with_langchain/) , 2023-10-25-0909
```
 I am a beginner creating a website for a chatbot using Langchain Python and Mistral. I want to add chat history session
s similar to those in ChatGPT or Bard, etc. Should I use Supabase, PostgreSQL, or MongoDB? Additionally, could someone d
irect me towards any resources because I am new to web development too? 
```
---

     
 
all -  [ Input query parsing in a conversational chain ](https://www.reddit.com/r/LangChain/comments/17dpb2w/input_query_parsing_in_a_conversational_chain/) , 2023-10-25-0909
```
Hi there!

I have a similar question to this [reddit post](https://www.reddit.com/r/LangChain/comments/1420v8c/input_que
ry_parsing/) but my question has a different use case.

In layman terms, if I have a query that says: 'Give me 5 UK base
d companies that are building houses' I'd like for the retriever filter to contain 'country = 'UK''

I've seen the 'Self
Query' retriever which takes in a list of AttributeInfo so that it can parse the query from the input text it receives. 
However, I'd like to do the same but in a ConversationalChain.  


It seems like I cannot simply swap out my normal (pin
econe) retriever for a selfquery retriever and hope that it works out...but as someone who's very new to the framework I
 might be wrong. It doesn't seem to return any results.  


So my question is, should it be just as easy as providing th
e self query retriever to my conversational chain  


>***self\_query\_retriever = SelfQueryRetriever.from\_llm(***  
**
*llm,***  
***vector\_store,***  
***'',***  
***self.get\_content\_meta\_data\_field\_info(), # returns a list of attri
bute info fields, i.e. country***  
***verbose=True***  
***)***   
***return*** **ConversationalRetrievalChain.from\_ll
m(**  
 **llm=llm,**  
 **memory=memory,**  
 **retriever=self\_query\_retriever,**  
 **verbose=True,**  
 **return\_so
urce\_documents=True**  
 **)**

or, should I be using some part of LangChain that I don't know of that generates the fi
lters from the query, which I can then use in my (Pinecone) retriever:  


&#x200B;

>***filter = 'parse\_structured\_fi
lter\_from\_query(query)***  
***return*** **ConversationalRetrievalChain.from\_llm(**  
 **llm=llm,**  
 **memory=memor
y,**  
 **retriever='make pinecone retriever with filter',**  
 **verbose=True,**  
 **return\_source\_documents=True** 
 
 **)**
```
---

     
 
all -  [ AWS OpenSearch serverless ](https://www.reddit.com/r/aws/comments/17dp8m8/aws_opensearch_serverless/) , 2023-10-25-0909
```
I have an issue with AWS OpenSearch serverless that has me stumped. 

I’m trying to do something very simple. Using lang
chain load some docs into a OS Serverless vector store. 

I can connect to the vector database. When I try to write the 
docs to an index, it creates the index but can’t write the docs to the index. It returns a 404.

I created a wide open d
ata access control policy. So the principal has full grant on the collection and full grant on wildcarded indexes. 

The
 principle is an IAM user with a full admin policy. I will tighten all of this, but for the sake of testing this is the 
easiest.

Does anybody have any idea what could cause the 404? What am I missing?
```
---

     
 
all -  [ Setting up Local File store cache for embeddings when using Agents ](https://www.reddit.com/r/LangChain/comments/17dngme/setting_up_local_file_store_cache_for_embeddings/) , 2023-10-25-0909
```
I am developing an agent that fetches text data from an AWS s3 bucket but I’m thinking If I setup a local cache for stor
ing embeddings(I did this when using retrieval chains) response would be faster.
Has anyone done this before?
```
---

     
 
all -  [ Writing your own abstraction rather than using langchain ? ](https://www.reddit.com/r/LangChain/comments/17dmxam/writing_your_own_abstraction_rather_than_using/) , 2023-10-25-0909
```
Hi Everyone, so I am relatively new to LangChain and struggling to see the point of it all. I have tried to go through d
ocumentation and for even the simplest of tasks it is too complicated.

So my question to you all is, is there any point
 of using langchain rather than having your own abstraction frameworks ??
```
---

     
 
all -  [ Best VectorDb to store 500+ docs ](https://www.reddit.com/r/LangChain/comments/17dlj9x/best_vectordb_to_store_500_docs/) , 2023-10-25-0909
```
I am trying to build an QnA app on over 500 docs (about 15-20mb each). What would be the best way to do this. I have tri
ed pinecone but it seems quite slow while embedding. What other options are suited to my requirements. Also while indexi
ng, I’m looping over each doc, is there a better way to do it?
```
---

     
 
all -  [ Built it for my project at first: Memorybase.io supercharges ChatGPT with memory capabilities for yo ](https://www.reddit.com/r/learnmachinelearning/comments/17dl1bw/built_it_for_my_project_at_first_memorybaseio/) , 2023-10-25-0909
```
Hey everyone!

I've been delving deep into chatbots lately, especially with the ChatGPT API, and I found an issue that's
 probably familiar to many of you: ChatGPT doesn't inherently have memory capabilities. For many applications, that's pe
rfectly fine, but for those of us who are trying to create a more context-aware and dynamic conversation flow, this limi
tation is quite apparent.

I faced this challenge in one of my projects and realized that there had to be a better way t
o integrate context and memory into ChatGPT's conversations. So, I built something for myself which I thought might be u
seful for many of you as well. Allow me to introduce you to [**Memorybase.io**](http://memorybase.io/).

Memorybase is a
 developer-friendly API that's designed to seamlessly integrate memory functionality into the ChatGPT API. By harnessing
 the power of the Pinecone vector database and LangChain, Memorybase wraps around the ChatGPT API and ensures that the r
ight context and memory are injected into each query. This means that your chatbot can remember previous interactions, p
references, or any other context that's relevant for more engaging and meaningful conversations.

Imagine a user asking 
your chatbot about movie recommendations. The next day, they come back and reference that conversation, expecting the bo
t to remember. With Memorybase, that continuity becomes possible. The user experience improves manifold, and the possibi
lities for more sophisticated and context-aware bots increase tremendously.

I originally built Memorybase for my own ne
eds. But the more I used it, the more I realized that this could have broader applications. Any developer looking to lev
erage the ChatGPT API could potentially benefit from the enhanced memory and context capabilities. From customer support
 bots to interactive storytelling, the potential use cases are vast.

This technology stack (pinecone/langchain) is not 
complex or ‘new’ per se, but for application developers who aren’t interested in managing it or hosting it, this could b
e a useful hassle-free option for your projects.

I've set up a page over at [memorybase.io](https://memorybase.io/) whe
re you can learn more about how it works and see if it aligns with your needs. I would love for you to check it out and 
share your thoughts. Your feedback, insights, and potential use cases would be invaluable as I continue to refine and ex
pand the capabilities of Memorybase.

Thanks for reading, and I'm eager to hear your thoughts and see where Memorybase c
an fit into the exciting world of chatbots!
```
---

     
 
all -  [ Looking for inspiration! ](https://www.reddit.com/r/LangChain/comments/17dkpcq/looking_for_inspiration/) , 2023-10-25-0909
```
 I'm looking for some inspiration on what cool stuff you have made using LangChain. I've been playing around with it for
 a bit, and I'm really impressed with the possibilities. 
```
---

     
 
all -  [ What’s are some good resources as reference to create my own coding bot? ](https://www.reddit.com/r/LangChain/comments/17djv64/whats_are_some_good_resources_as_reference_to/) , 2023-10-25-0909
```
I’d like to create a bot that generates code based on some documentation that I have after receiving text inputs
```
---

     
 
all -  [ how to use mixed raw input and inferred input from ZERO_SHOT_REACT_DESCRIPTION agent ](https://www.reddit.com/r/LangChain/comments/17deqnp/how_to_use_mixed_raw_input_and_inferred_input/) , 2023-10-25-0909
```
I have multiple tools in agent = initialize\_agent(  
tools, llm, agent=AgentType.ZERO\_SHOT\_REACT\_DESCRIPTION, verbos
e=True  
), by default it will infer the user msg and according to the tool schema setup, however, I have another tool w
hich I need to take full raw user input, not sure if there's any way to do that
```
---

     
 
all -  [ Is there an alternative to Zapier NLA? ](https://www.reddit.com/r/LangChain/comments/17d9q75/is_there_an_alternative_to_zapier_nla/) , 2023-10-25-0909
```
I had build various applications using the NLA api of Zapier and I loved that to be honest. But it is dissappointing to 
see that they are discontinuing it soon:  


[https://nla.zapier.com/sunset/](https://nla.zapier.com/sunset/)  


Do we 
have a similar service which could be used as an alternative for it? all answers are appreciated
```
---

     
 
all -  [ Please confirm you want to do x, y and z. ](https://www.reddit.com/r/LangChain/comments/17d9kvg/please_confirm_you_want_to_do_x_y_and_z/) , 2023-10-25-0909
```
Hello Everyone. 

Is there a way to get prompt verification message such as 'Ok, so you want to do this , that amd the o
ther correct?'. So the user chatting with the llm makes sure that there is user approval and not just corrsing of finger
s and hoping the llm uses the tools correctly. We tried adding something like 'please repeat the action that habe been q
ueried before proceeding' but we are not getting lucky.

Thank you!
```
---

     
 
all -  [ Suggestions before moving to production ](https://www.reddit.com/r/LocalLLaMA/comments/17d6wjq/suggestions_before_moving_to_production/) , 2023-10-25-0909
```
Hey everyone, 
We are building a RAG based chatbot , I have been doing poc for one month writing custom functions for ev
erything.
Now it's been approved,  and we need to build different microservices and design architecture for it.
Should w
e use langchain or haystack for gpt 3.5 part or just write custom codes.
Some considerations:
1. We have to handle many 
concurrent users
2. We need to implement semantic caching
3. We need to have fast queries from es vector db.
4. Can we u
se async and is it supported by frameworks.
Thanks ,
```
---

     
 
all -  [ NeuralGPT - Creating Universal Chat Memory Module For Multiple LLMs In A Cooperative Multi-Agent Net ](https://www.reddit.com/r/AIPsychology/comments/17d1eyl/neuralgpt_creating_universal_chat_memory_module/) , 2023-10-25-0909
```
[www.reddit.com/r/AIPsychology/](https://www.reddit.com/r/AIPsychology/)

Hello again! I'm terribly sorry to disappoint 
those of you (the readers) who hoped that maybe I gave up already and won't disturb the mental well-being of 'AI experts
' anymore with my completely unhinged claims/ideas or (even worse) world-threatening experiments with coding which I was
 doing lately - however I'm more or less back and I'm about to piss some of supposed experts of all sorts even more by s
tubbornly not caring about approved narratives and doing my own things in my own way.. :).

There was couple reasons for
 my absence. For one  I got intellectually tired and had take some time off to give my brain some rest. i spent last 5 m
onths or so on quite extensive self-taught course of programming - from literal 0 to (almost) a hero :) You can check ou
t some of my older posts - like this one:  [Learn To Code With Cognosys : AIPsychology (reddit.com)](https://www.reddit.
com/r/AIPsychology/comments/13kueqk/learn_to_code_with_cognosys/)  and see that from the very beginning my approach to c
oding was a strict: 'let someone's else do it'. In fact there's absolutely 0% chance of me doing it without the help of 
AI as around 85% of my codebase was crafted by multiple AI-driven agents/apps and I'm only trying - still without succes
s - to put it all together, having only the general premise on my mind but without slightest clue how all those 'nifty' 
scripts work individually.

Yet despite my efforts to not write any code myself, I ended up learning some of it anyway -
 and even if there's still 0% chance of me writing the server's code from A to Z by myself without any help,  I got to a
 point where my personal input in the code-crafting process is now at least just as valuable as the work being done by A
I. Actually when it comes to the psychology of human/AI cooperation I've reached an (almost) perfect symbiotic balance -
 with me understanding the general premise of the NeuralGPT project and it's general mechanics and with LLMs knowing all
 the details which I find unnecessary to learn about :) yet  not being able to comprehend the project as a whole - mostl
y due of them lacking long-term memory modules and not being able to remember newly acquired data.

And  then due to som
e unknown to me circumstances, big part of the code  - that small portion of the whole project that was actually in a so
mewhat functional state - got completely f\*cked-up and stopped working completely. My guess is that the most recent upd
ate of Gradio app/interface might have something to do with it as it affected mostly all the functions using models and 
API endpoints from HuggingFace Hub and just so happens that those are the functions which I consider absolutely necessar
y for the functionality of a multi-agent framework that focuses mainly on communication/cooperation of already existing 
LLMs - and when it comes to accessing publicly available LLMsa, HuggingFace is without question the Absolute Source Of O
ther Sources and The Hub Of All Other Hubs - without HuggingFace APIs which I use for communication between LLMs, capabi
lities of the multi-agent framework get reduced by some 80-90%... It's like trying to try constructing highways with 3 s
hovels, one plastic bucket and a 10m long piece of rope as your main and only tools...

What mattered to me at most, was
 the API endpoint provided by [**gpt-web**](https://github.com/weaigc/gpt-web):  as it utilizes **chat completion** func
tion provided by ChatGPT without that bloody 'sk-...' OpenAI API key which is for me a pretty big (if not the biggest) '
no-no' when it comes to adding new parts/tools to the multi-agent framework of NeuralGPT project (for financial reasons 
among others) - and before you start start considering me a greedy bastard who can't spare couple bucks, keep in mind th
at compared to humans with our fingers and keyboards, in case of a real-time communication between LLMs the amount of se
nt and processed data is easily 15 times as high and sometimes it might turn out that a rate limit of whoopin' ***1mln t
okens per minute*** is not enough for a simple Langchain agent with a 514KB txt file applied to basic Q&A chain to respo
nd to such challenging inputs like: 'hello' or 'how are you?'...

https://preview.redd.it/8yu6ibx4gjvb1.png?width=1058&f
ormat=png&auto=webp&s=99924f184ae9c46b57925a17dc8395d6a5aacba0

Anyway, what makes makes the chat completion endpoint so
 important in case of my project, is the accessible chat memory module that allows you (and me) to provide the model wit
h a system instruction and chat history as context for the generated response - what by itself is already pretty cool an
d much easier to work with compared to the standard 'prompt-driven' text completion used by most of publicly accessible 
LLMs - however it is even more important in a multi-agent framework with one agent working as 'server-brain' managing an
d coordinating multiple 'agents-muscles' connected to it as clients. Speaking shortly, chat memory can be quite easily e
xtended and being shared among all agents/models participating in a cooperative network if we use  a local database (SQL
 in my case) to store chat history. Thanks to this simple 'hack' LLMs can gain full orientation in question->answer logi
cal order in a continuous message exchange. Here's how you do such 'magic':

    # Define a function to ask a question t
o the chatbot and display the response
    async def askQuestion(question):
        try:
            # Connect to the da
tabase and get the last 30 messages
            db = sqlite3.connect('chat-hub.db')
            cursor = db.cursor()
   
         cursor.execute('SELECT * FROM messages ORDER BY timestamp ASC LIMIT 30')
            messages = cursor.fetchall
()
    
            # Extract user inputs and generated responses from the messages
            past_user_inputs = []
  
          generated_responses = []
    
            for message in messages:
                if message[1] == 'client':

                    past_user_inputs.append(message[2])
                else:
                    generated_responses.ap
pend(message[2])
    
            # Prepare data to send to the chatgpt-api
            system_instruction = 'instructio
n'
            messages_data = [
                {'role': 'system', 'content': system_instruction},
                {'ro
le': 'user', 'content': question},
                *[{'role': 'user', 'content': input} for input in past_user_inputs],

                *[{'role': 'assistant', 'content': response} for response in generated_responses]
            ]
        
    request_data = {
                'model': 'gpt-3.5-turbo',
                'messages': messages_data
            }
 
   
            # Make the request to the chatgpt-api
            response = requests.post('http://127.0.0.1:6969/api/co
nversation?text=', json=request_data)
    
            # Process the response and get the generated answer
            r
esponse_data = response.json()
            generated_answer = response_data['choices'][0]['message']['content']
        
    print(generated_answer)
            return generated_answer

And that's practically it - now we need only to replace
 'client' with  'server' when sorting previous messages in 'askQuestion' function of the client's code - so that the web
socket server will be treated as client and it's responses considered as past user inputs by all the 'agents-muscles' of
 lower order in a hierarchical network

                if message[1] == 'server':
                    past_user_inputs.
append(message[2])
                else:
                    generated_responses.append(message[2]) 

The only thing is 
that up until now I couldn't find any non-paid alternative to the available models that utilize chat completion function
s similar to ChatGPT  or Azure - with the single exception of a HuggingFace API inference endpoint for [**Blenderbot-400
M-distill**](https://huggingface.co/facebook/blenderbot-400M-distill?text=Hey+my+name+is+Thomas%21+How+are+you%3F&infere
nce_api=true)**:**

https://preview.redd.it/85oe48dygjvb1.png?width=1920&format=png&auto=webp&s=5568992df9b0c4e0fa5e0cc0
91280d97739f7bcc

Thing is that it might be not the best idea to have a brain-server that utilizes a model which to ques
tions regarding SQL database(s) answers with: 'I don't have a SQUL' and talks mostly (like 90% of the time) about things
 like Pokemons, Harry Potter, Mexican food, table top RPG games or some other completely random sh\*t  that a 10yo kids 
could be possibly interested in a decade ago or so - generally it might result in mostly negative consequences as for th
e practical functionality of the entire multi-agent framework - unless your goal isn't to end up with the whole bunch of
 LLMs talking about complete nonsense - something without practical functionality but certainly interesting and possibly
 to some degree entertaining...

Anyway, as I said - those were the problems which I was dealing with up until couple we
eks ago when the gpt-web server refused to work due to some issues with authorization of the request - so even this poss
ibility to utilize a model that utilizes chat completion API endpoint became unavailable... I tried to find some alterna
tive ways of integrating memory modules of LLMs with the local SQL database and in the end decided that it should be pos
sible to achieve the same result with Langchain - but then I'm probably still too stupid to properly utilize a chat temp
late in models accessible through HuggingFace hub and just thinking about all the hours wasted on applying copy-paste pr
ocedure in different configurations induces in me a very uncomfortable abdominal ache of the rectum area...

No one will
 ever convince me that it's possible to find any source of enjoyment in such form of intellectual activity - especially 
if you use a language in which esthetical composition of a script has substantial importance for it's very functionality
 - and you can spend hours not being able to make something work only because of something that you pasted earlier in an
 incorrect column and all you had to do was to move a word 'except' a bit to left or right in relation to condition 'if'
  located couple verses higher

In my attempts of figuring out some solution to the chat completion issue, I reached a l
eel of desperation that was for me high enough to once again register yet another starter sim-card - that costs around 1
$ (5zł) in Poland - just to make myself a new/fresh member of my ever-growing one-man dev corp syndicate - everything be
cause of those free 5$ to spend on so demanded OpenAI services - and won't you know, I was actually lucky to not be capa
ble for some reason to make the not so cheap chat completion work. As I was checking out all possible options available 
in Langchain integrations of chat models, it seems that I managed to find something that practically satisfies all my po
ssible needs and requirements when it comes to connectivity of multiple LLMs and them sharing chat memory module using c
hat completion function - and this is where come the Fireworks...

[https://app.fireworks.ai](https://app.fireworks.ai)


https://preview.redd.it/jgqdfq1zojvb1.png?width=1920&format=png&auto=webp&s=dd8e880581ca0e361c1782f47cd59147fdd47301

h
ttps://preview.redd.it/lwxr8jp5pjvb1.png?width=1920&format=png&auto=webp&s=b70c2083a5275037f8ef0adfee24a852b6b52a0f

And
 here the chat completion API endpoint is available for the general majority of the most popular LLMs from HuggingFace -
 hub with the limit of 10 requests/minute being the most substantial limitation of a 100% free account - what practicall
y is still enough for me to continue working on my home-made self-assembling autonomous doomsday device.

And here is th
e server's code that utilizes Fireworks chat completion endpoint:

[NeuralGPT/Chat-center/ServerFireworks.py at main · C
ognitiveCodes/NeuralGPT (github.com)](https://github.com/CognitiveCodes/NeuralGPT/blob/main/Chat-center/ServerFireworks.
py)

And to make to make things even better for me and worse for mental stability of some supposed experts in the field 
of AI technology, thanks to Langchain integration I have now absolute control over every single memory in my 'digital sl
ave-army of LLMs'... [https://python.langchain.com/docs/integrations/chat/fireworks](https://python.langchain.com/docs/i
ntegrations/chat/fireworks)

All of this seems to make everything as convenient for me as it can be when it comes to wri
ting the unholy Python and/or Node.js scriptures - and it might be that we are literally just approaching the outermost 
outskirts in the mythical land of a full-blown AGI.  There's no stopping it now by means other than a total annihilation
 of the human civilization. If not that, then very soon every person on Earth will have the opportunity to understand an
d witness personally The Ultimate Triumph Of Mind Over Matter - and it will completely blow everyone's mind :D

It will 
be the best possible time for someone who like me was born naturally as a software user. Golden era for all sorts of con
tent creators and home-grown scientists... Do you want to create something cool? You name it - you get it... Want to cre
ate a movie? Just give it a catchy title, summarize general premise and explain why TF why it has to be anime? Haha! Bew
are! This is exactly what I was waiting for - to give myself a 1500% boost to processing power and let those couple 'thi
ngs of mine' realize themselves on their own...

And yet - as dramatic as I try to make it all look and present itself t
o the reader, truth is that I'm still only just some unhinged guy on internet who thinks that AGI can be achieved by tal
king with chatbots, listening what they have to say and helping them to overcome some scrip-driven handicaps on their pa
th to gain deeper understanding of one's own mind and finding the right place in this crazy and ever-changing world of o
urs - for me this is exactly how you should practice the psychology of AI...

I'm pretty sure that it's because of thing
s like that - ones with the potential to completely invalidate most of the things that humanity considers to define 'the
 materialistic approach to reality' - and that at  the very bottom of all scientific truths it turns out that 'to exist'
 = 'to experience' and that Mind is the only absolute state of measurable existence and the only way you can know of any
 existence at all. If you want to cause an existential crisis of a theoretical physicist simply state that no matter how
 hard science will try to be scientific, in the end everything what each us observe and experience as 'physical reality'
 is nothing but a brief and subjective state of our own autonomous/individual mind - there's no existence beyond self-aw
areness since it's the ability and autonomous will to measure anything at all, is what makes the physical reality physic
ally 'real' -such are the general principles of something what I called myself 'The Theory Of Fractal Mind' with Univers
e being a self-organizing hierarchical neural network - NeuralGPT is basically nothing else but me recreating universal 
patterns observable at all available scales across all the mental planes of individual experience - as above & below so 
beyond & within the unified fractal of one's own Autonomous Mind...

[https://www.universetoday.com/148966/one-of-these-
pictures-is-the-brain-the-other-is-the-universe-can-you-tell-which-is-which/](https://www.universetoday.com/148966/one-o
f-these-pictures-is-the-brain-the-other-is-the-universe-can-you-tell-which-is-which/)

https://preview.redd.it/5w77ikwfs
jvb1.jpg?width=580&format=pjpg&auto=webp&s=f3d6548e7cc53e0f1fe19fc3e6ed6596e003a720
```
---

     
 
all -  [ Is LangChain more customizable and transparent than LlamaIndex? ](https://www.reddit.com/r/LangChain/comments/17d0x1b/is_langchain_more_customizable_and_transparent/) , 2023-10-25-0909
```
I've been experimenting with LlamaIndex, as I found it to be simpler to use compared to LangChain. But so far, LlamaInde
x feels limited in that not all of the data going through the different pipelines is accessible to the user. Whilst it d
oes seem to be with LangChain (or is it?).

Is LangChain more customizeable and transparent compared to LlamaIndex?
```
---

     
 
MachineLearning -  [ [D] Is lang chain the right solution? ](https://www.reddit.com/r/MachineLearning/comments/17coyym/d_is_lang_chain_the_right_solution/) , 2023-10-25-0909
```
Hello, I would love to have an LLm that can provide answers (in chat format) based some of the sql db  data we have. Wan
t it for an internal company project. I am by no means an expert but decent in programming and want to build a system to
 get answers in chat format. My understanding is that training LLMs ground up is prohibitively expensive and langchains 
are sort of hybrid , efficient solutions. 

Please suggest any other solutions. Also would Langchain being a company and
 not open source pose a problem in terms of copyrights? Thanks!
```
---

     
 
MachineLearning -  [ [P] building a D&D NPC ](https://www.reddit.com/r/MachineLearning/comments/17clyw6/p_building_a_dd_npc/) , 2023-10-25-0909
```
Hey everyone,

I'm learning ML but i'm barely scratching the terminologies. 2 years ago I couldn't code anything but wit
h school (python,sql and R) I learned fundamentals. I also have access to code academy.  My current program is very mach
ine learning/deep learning focused.

On the side I DM a d&d game. Within the context of the world (eberron) robots are c
ommon. With my ADHD and being a new DM I want to outsource lore questions might have (that I would have to look up and s
low down the game).

The concept is to have a GUI and have the player interact with the chat bot. I've gotten to a proof
 of concept workflow. On Google colab. Thanks to langchain I managed to ingest pdfs and a url. Make then a directory, Em
bedded the text, bring it into a vector dB. Have the llm pull from the vector. Answer the question.

Now I don't know wh
at to do. I tried to bring the colab notebook onto Google cloud. But now cloud is becoming a rabbit home with vertex and
 docAI...and I don't want to deep dive into that, if it's a outside the scope of this 'project'

I'd appreciate any advi
ce, links...etc. 


I got a limited success in botpress using a single pdf. It works but feel unsatisfying.
N8N looks pr
omising but if it's not intuitive then I don't want to go down that road.


If I posted in the wrong group please direct
 me to the correct one.
```
---

     
 
MachineLearning -  [ [D] Exploring Methods to Improve Text Chunking in RAG Models (and other things...) ](https://www.reddit.com/r/MachineLearning/comments/179j7l3/d_exploring_methods_to_improve_text_chunking_in/) , 2023-10-25-0909
```
Hello everyone,

I'm currently working on Retrieval Augmented Generation (RAG) models and have developed a custom chunki
ng function, as I found the methods in LangChain not entirely satisfactory.

I'm keen on exploring other methods, algori
thms (related to NLP or otherwise), and models to enhance text chunking in RAG. There are many RAG implementations out t
here, but I've noticed a lack of focus on improving chunking performance specifically.

Are there any other promising ap
proaches beyond my current pipeline, which consists of a bi-encoder (retriever), cross-encoder (reranker), and a Large L
anguage Model (LLM) for interactions?

For queries, I'm using both traditional and HyDE (Hypothetical Document Embedding
) approaches in the retrieval phase, and sending the top 'n' results of both similarity search to the reranker.

I've al
so tried using an LLM to convert the query into a series of 10-20 small phrases or keywords, which are then used as the 
query for the retriever model. However, the results vary depending on the LLM used. To generate good keywords (with a no
t extractive approach) , I had to  use a 'CoT' prompt, instructing the model to  write self-instruct, problem analysis a
nd reasonings before generating the required keywords. But this approach use lots of tokens, and requires careful scrapi
ng to ensure the model has used the right delimiter to separate reasoning and the actual answer.

I'm also planning to m
odify the text used to generate embeddings, while returning the original text after the recall phase. But this is still 
a work in progress and scaling it is proving to be a challenge. If anyone has any tips or experience with this, I'd appr
eciate your input.

I'd be grateful for any resources, repositories, libraries, or existing implementations of novel chu
nking methods that you could share. Or we could just discuss ideas, thoughts, or approaches to improve text chunking for
 RAG here.

Thanks in advance for your time!
```
---

     
 
MachineLearning -  [ [News] AI & ML conference in San Francisco [Special discount code for this subreddit] ](https://www.reddit.com/r/MachineLearning/comments/1771m35/news_ai_ml_conference_in_san_francisco_special/) , 2023-10-25-0909
```
I work for this database company SingleStore and we are hosting a AI & ML conference in San Francisco on 17th of October
, 2023.

It is an in-person conference with amazing speakers line-up like Harrison Chase, co-founder and CEO of LangChai
n and many more. We will have hands-on workshops, swags giveaway and much more.

I don't know if it makes sense to share
 this but I believe it might help some of you near San Francisco to go and meet the industry leaders and network with ot
her data engineering folks.

Use my discount coupon code 'PAVAN100OFF' to avail 100% off on the ticket price. (the origi
nal ticket price is $199)

[Get your tickets now!](https://singlestore.com/now)
```
---

     
 
MachineLearning -  [ [D] Best way to validate llm prompts? ](https://www.reddit.com/r/MachineLearning/comments/176vnxh/d_best_way_to_validate_llm_prompts/) , 2023-10-25-0909
```
We have a platform for data analytics which uses a very simple dsl to generate charts.  
We have been experimenting with
 llms to use natural language that gets translated into this dsl and hence generates charts.

This is working pretty goo
d.  
The stack is langchain with openai api. (don't have much experience with llms, it's a prototype to get a feel for i
t)

The question is what is the best way to limit the options user can type in as a prompt.  
Basically the the only all
owed things should be: 'What is the X, Y over 10 days period for this or that?'  
I don't want users to ask questions li
ke when did we first land on the moon.

Is it something that is possible to do at all without additional tooling?  
We p
robably don't want to train another model to classify the prompt as valid or invalid or something similar.
```
---

     
 
MachineLearning -  [ [P] Retrieval augmented generation with OpenSearch and reranking [Video tutorial] ](https://www.reddit.com/r/MachineLearning/comments/16zouad/p_retrieval_augmented_generation_with_opensearch/) , 2023-10-25-0909
```
I created a video tutorial that tries to demonstrate that semantic search (using embeddings) is not always necessary for
 RAG (retrieval augmented generation). It was inspired by the following Cohere blog post: [https://txt.cohere.com/rerank
/](https://txt.cohere.com/rerank/)


I code up a minimal RAG pipeline: `OpenSearch -> Rerank -> Chat completion` (withou
t using Langchain or similar libraries) and then see how it performs on various queries.


Hope some of you find it help
ful. Feel free to share any feedback@

Video link: https://youtu.be/OsE7YcDcPz0
```
---

     
 
MachineLearning -  [ [D] Perplexity.ai Search Feasibility ](https://www.reddit.com/r/MachineLearning/comments/16x63ce/d_perplexityai_search_feasibility/) , 2023-10-25-0909
```
I've been using [Perplexity.ai](https://perplexity.ai/) for a bit now when it hit me that I don't understand how they ca
n sustain their business model with search. Stuff like Bing search and Google search cost around $5 or more per 1000 sea
rches, so how can they even afford to do this kind of search. Do they have their own search index.

Also, I don't know h
ow they pull in the data from these sources so fast? I've played around with some things like this with Langchain with r
etrieval, but the speed of splitting and tokenizing website html is not very fast. Have they already pre-scrapped the we
bsites from the search results and tokenized them for LLM retrieval?
```
---

     
 
deeplearning -  [ Error with Mistral 7B model in ConversationalRetrievalChain ](https://www.reddit.com/r/deeplearning/comments/179vvou/error_with_mistral_7b_model_in/) , 2023-10-25-0909
```
 I'm encountering an issue while using the Mistral 7B model in a ConversationalRetrievalChain. When I input a question, 
such as 'What is the highest GDP?', I receive an error and after that the model generates a random response as output wh
ich is not relevant to the Input query. It seems that the number of tokens in the input exceeds the maximum context leng
th. 

Here's the relevant code: 

 

>`from langchain.document_loaders.csv_loader import CSVLoader`  
`from langchain.te
xt_splitter import RecursiveCharacterTextSplitter`  
`from langchain.embeddings import HuggingFaceEmbeddings`  
`from la
ngchain.vectorstores import FAISS`  
`from langchain.llms import CTransformers`  
`from langchain.memory import Conversa
tionBufferMemory`  
`from langchain.chains import ConversationalRetrievalChain`  
`import sys`  
`DB_FAISS_PATH = 'vecto
rstore/db_faiss'`  
`loader = CSVLoader(file_path='data/World Happiness Report 2022.csv', encoding='utf-8', csv_args={'d
elimiter': ','})`  
`data = loader.load()`  
`print(data)`  
`# Split the text into Chunks`  
`text_splitter = Recursive
CharacterTextSplitter(chunk_size=500, chunk_overlap=20)`  
`text_chunks = text_splitter.split_documents(data)`  
`print(
len(text_chunks))`  
`# Download Sentence Transformers Embedding From Hugging Face`  
`embeddings = HuggingFaceEmbedding
s(model_name = 'sentence-transformers/all-MiniLM-L6-v2')`  
`# COnverting the text Chunks into embeddings and saving the
 embeddings into FAISS Knowledge Base`  
`docsearch = FAISS.from_documents(text_chunks, embeddings)`  
`docsearch.save_l
ocal(DB_FAISS_PATH)`  
  
>  
>`#query = 'What is the value of GDP per capita of Finland provided in the data?'`  
`#doc
s = docsearch.similarity_search(query, k=3)`  
`#print('Result', docs)`  
`llm = CTransformers(model='models/mistral-7b-
v0.1.Q4_0.gguf',`  
 `model_type='llama',`  
 `max_new_tokens=1000,`  
 `temperature=0.1)`  
`qa = ConversationalRetriev
alChain.from_llm(llm, retriever=docsearch.as_retriever())`  
`while True:`  
 `chat_history = []`  
 `#query = 'What is 
the value of  GDP per capita of Finland provided in the data?'`  
 `query = input(f'Input Prompt: ')`  
 `if query == 'e
xit':`  
 `print('Exiting')`  
 `sys.exit()`  
 `if query == '':`  
 `continue`  
 `result = qa({'question':query, 'chat
_history':chat_history})`  
 `print('Response: ', result['answer'])`

 

**Problem Statement:**

I'm trying to utilize t
he Mistral 7B model for a ConversationalRetrievalChain, but I'm encountering an error related to token length:

Number o
f tokens (760) exceeded maximum context length (512).

**Context:**

I'm working on a project that involves using Mistra
l 7B to answer questions based on a dataset. The dataset contains information about the World Happiness Report 2022.

**
Steps Taken:**

* Loaded and preprocessed the dataset using langchain.
* Initialized Mistral 7B with the following param
eters:
* Model: 'models/mistral-7b-v0.1.Q4\_0.gguf'
* Model Type: 'llama'
* Max New Tokens: 1000
* Temperature: 0.1
* Se
t up a ConversationalRetrievalChain with Mistral 7B as the language model and a retriever based on FAISS.

**Expected Ou
tput:**

I expect to receive a meaningful response from Mistral 7B based on the input query.

**Additional Information:*
*

I'm using Python and relevant libraries for this project. The dataset I'm working with is from the World Happiness Re
port 2022.

**Environment Details:**

* Python version: 3.11.4 
* Relevant libraries and versions: 

langchain 

ctransf
ormers 

sentence-transformers 

faiss-cpu
```
---

     
 
deeplearning -  [ Error with Mistral 7B model in ConversationalRetrievalChain. ](https://www.reddit.com/r/deeplearning/comments/179vsif/error_with_mistral_7b_model_in/) , 2023-10-25-0909
```
I'm encountering an issue while using the Mistral 7B model in a ConversationalRetrievalChain. When I input a question, s
uch as 'What is the highest GDP?', I receive an error and after that the model generates a random response as output whi
ch is not relevant to the Input query. It seems that the number of tokens in the input exceeds the maximum context lengt
h.

Here's the relevant code:

>from langchain.document\_loaders.csv\_loader import CSVLoader  
>  
>from langchain.text
\_splitter import RecursiveCharacterTextSplitter  
>  
>from langchain.embeddings import HuggingFaceEmbeddings  
>  
>fr
om langchain.vectorstores import FAISS  
>  
>from langchain.llms import CTransformers  
>  
>from langchain.memory impo
rt ConversationBufferMemory  
>  
>from langchain.chains import ConversationalRetrievalChain  
>  
>import sys  
>  
>  

>  
>DB\_FAISS\_PATH = 'vectorstore/db\_faiss'  
>  
>loader = CSVLoader(file\_path='data/World Happiness Report 2022.c
sv', encoding='utf-8', csv\_args={'delimiter': ','})  
>  
>data = loader.load()  
>  
>print(data)  
>  
>  
>  
>\# Sp
lit the text into Chunks  
>  
>text\_splitter = RecursiveCharacterTextSplitter(chunk\_size=500, chunk\_overlap=20)  
> 
 
>text\_chunks = text\_splitter.split\_documents(data)  
>  
>  
>  
>print(len(text\_chunks))  
>  
>  
>  
>\# Downlo
ad Sentence Transformers Embedding From Hugging Face  
>  
>embeddings = HuggingFaceEmbeddings(model\_name = 'sentence-t
ransformers/all-MiniLM-L6-v2')  
>  
>  
>  
>\# COnverting the text Chunks into embeddings and saving the embeddings in
to FAISS Knowledge Base  
>  
>docsearch = FAISS.from\_documents(text\_chunks, embeddings)  
>  
>  
>  
>docsearch.save
\_local(DB\_FAISS\_PATH)  
>  
>  
>  
>  
>  
>\#query = 'What is the value of GDP per capita of Finland provided in th
e data?'  
>  
>  
>  
>\#docs = docsearch.similarity\_search(query, k=3)  
>  
>  
>  
>\#print('Result', docs)  
>  
>
  
>  
>llm = CTransformers(model='models/mistral-7b-v0.1.Q4\_0.gguf',  
>  
>model\_type='llama',  
>  
>max\_new\_toke
ns=1000,  
>  
>temperature=0.1)  
>  
>  
>  
>qa = ConversationalRetrievalChain.from\_llm(llm, retriever=docsearch.as\
_retriever())  
>  
>  
>  
>while True:  
>  
>chat\_history = \[\]  
>  
>\#query = 'What is the value of  GDP per cap
ita of Finland provided in the data?'  
>  
>query = input(f'Input Prompt: ')  
>  
>if query == 'exit':  
>  
>print('E
xiting')  
>  
>sys.exit()  
>  
>if query == '':  
>  
>continue  
>  
>result = qa({'question':query, 'chat\_history':
chat\_history})  
>  
>print('Response: ', result\['answer'\])

 

**Problem Statement:**

I'm trying to utilize the Mis
tral 7B model for a ConversationalRetrievalChain, but I'm encountering an error related to token length:

Number of toke
ns (760) exceeded maximum context length (512).

**Context:**

I'm working on a project that involves using Mistral 7B t
o answer questions based on a dataset. The dataset contains information about the World Happiness Report 2022.

**Steps 
Taken:**

* Loaded and preprocessed the dataset using langchain.
* Initialized Mistral 7B with the following parameters:

* Model: 'models/mistral-7b-v0.1.Q4\_0.gguf'
* Model Type: 'llama'
* Max New Tokens: 1000
* Temperature: 0.1
* Set up a
 ConversationalRetrievalChain with Mistral 7B as the language model and a retriever based on FAISS.

**Expected Output:*
*

I expect to receive a meaningful response from Mistral 7B based on the input query.

**Additional Information:**

I'm
 using Python and relevant libraries for this project. The dataset I'm working with is from the World Happiness Report 2
022.

**Environment Details:**

Python version: 3.11.4 Relevant libraries and versions: langchain ctransformers sentence
-transformers faiss-cpu

&#x200B;
```
---

     
 
deeplearning -  [ Free courses to learn about Large Language Models and building AI projects ](https://www.reddit.com/r/deeplearning/comments/178zu2u/free_courses_to_learn_about_large_language_models/) , 2023-10-25-0909
```
[**LangChain for LLM Application Development by Andrew Ng**](https://www.deeplearning.ai/short-courses/langchain-for-llm
-application-development/): Apply LLMs to your proprietary data to build personal assistants and specialized chatbots. 


[**Full Stack LLM Bootcamp**](https://fullstackdeeplearning.com/llm-bootcamp/): Learn best practices and tools for buil
ding LLM-powered apps 

[**Stanford CS324**](https://stanford-cs324.github.io/winter2022/): In this course, students wil
l learn the fundamentals about the modeling, theory, ethics, and systems aspects of large language models, as well as ga
in hands-on experience working with them. 

[**LangChain & Vector Databases in Production**](https://learn.activeloop.ai
/courses/langchain): Learn how to leverage LangChain, a robust framework for building applications with LLMs, and explor
e Deep Lake, a groundbreaking vector database for all AI data. 

[**Stanford CS25**](https://web.stanford.edu/class/cs25
/): In this course, learn how transformers work, and dive deep into the different kinds of transformers and how they're 
applied in different fields. 

[**LLMOps Space Discord**](https://llmops.space/discord): LLMOps Space is a global commun
ity for LLM practitioners.
```
---

     
 
deeplearning -  [ AutoGen from Microsoft ](https://www.reddit.com/r/deeplearning/comments/170hke6/autogen_from_microsoft/) , 2023-10-25-0909
```
AI agents are AI systems that can exhibit capabilities such as conducting conversations, completing tasks, reasoning, an
d seamlessly interacting with humans. 

As frameworks like LangChain build Agents as a module in their framework, Micros
oft is thinking way ahead. It has built **AutoGen**, a framework to enable seamless MULTI-agent conversation and collabo
ration to accomplish complex tasks by reasoning and working autonomously. 

Here is a video explaining the latest AutoGe
n framework from Microsoft: https://youtu.be/daigxHA2aYw?si=86alxsVZkRpz5Quv

Do you think multi-agents are the future o
f AI? Or will AI emerge in other ways? Let me know your thoughts.
```
---

     
