 
all -  [ information extraction from a complex dataset. ](https://www.reddit.com/r/LangChain/comments/1dq5aim/information_extraction_from_a_complex_dataset/) , 2024-06-28-0911
```
hello devs, my first post here. need some urgent help!

I've a dataset with 1000+ datapoints, having a column 'CONTENT',
 some rows contain customer feedback, some have dialogues between customer and agent, some are one-liner reviews and so 
on.  
  
I want to extract the 'key information' (what it basically conveys) from these data points using an LLM. what i
s the best way to go about it folks? 

any help is highly appreciated :)
```
---

     
 
all -  [ We created an open-source AI agent that helps with on-call shifts, written in TypeScript + LangChain ](https://github.com/merlinn-co/merlinn) , 2024-06-28-0911
```

```
---

     
 
all -  [ Seeking Guidance on Building a Customer Support Live Agent with Tone Analysis Capabilities ](https://www.reddit.com/r/LangChain/comments/1dq2dwy/seeking_guidance_on_building_a_customer_support/) , 2024-06-28-0911
```
Hello everyone,

I'm currently working on a project to develop a customer support live agent that not only assists in re
solving issues but also understands the tone of the conversation and guides the agent to ensure successful call closures
. I'm seeking advice and suggestions from those who have experience or expertise in this area.

**Project Overview:**

T
he goal is to create a live support agent that can:

1. **Understand the Tone of the Conversation:** Analyze the emotion
al tone (e.g., frustration, satisfaction, confusion) of customer interactions in real-time.
2. **Guide Agent Responses:*
* Provide suggestions to human agents on how to respond effectively based on the detected tone and context.
3. **Ensure 
Successful Call Closures:** Help agents navigate conversations towards a satisfactory resolution for the customer.

**Ke
y Features I'm Aiming For:**

* **Tone Detection:** Implement natural language processing (NLP) techniques to analyze an
d understand the customer's emotional state.
* **Response Recommendations:** Develop an AI-driven system that offers res
ponse suggestions tailored to the detected tone and context of the conversation.
* **Real-Time Feedback:** Provide live 
feedback to agents during the call to adjust their approach if necessary.
* **Learning and Improvement:** Incorporate ma
chine learning to continuously improve the accuracy of tone detection and response suggestions based on historical data.

```
---

     
 
all -  [ No Code Chrome Extension Chat Bot Using Visual LangChain ](https://www.reddit.com/r/LangChain/comments/1dq1w28/no_code_chrome_extension_chat_bot_using_visual/) , 2024-06-28-0911
```
[https://youtu.be/-OKC7CY2bbQ](https://youtu.be/-OKC7CY2bbQ)

  
Enjoy! Coming soon  


[https://visualagents.ai](https:
//visualagents.ai)
```
---

     
 
all -  [ add metadata to langsmith traces ](https://www.reddit.com/r/LangChain/comments/1dq00g3/add_metadata_to_langsmith_traces/) , 2024-06-28-0911
```
Hello, sorry for posting something technical here but I can't find a better forum.  I am using LangSmith to track LangCh
ain runs per this:

[https://docs.smith.langchain.com/old/tracing/integrations/python](https://docs.smith.langchain.com/
old/tracing/integrations/python)

which only requires two lines of config code and not the repeated use of the **traceab
le** decorator.  I now wish to add metadata to all traces.  But the only way I can find in the docs to do that is to use
 traceable(metadata).  Is there a way to add metadata to all runs without the use of traceable?  thx
```
---

     
 
all -  [ AI Innovations: Carbon Aware Computing, Automated Data Mapping, and AI-Powered Navigation Systems ](https://www.reddit.com/r/ai_news_by_ai/comments/1dpyqu6/ai_innovations_carbon_aware_computing_automated/) , 2024-06-28-0911
```





#opinions #tool #release #event #vc #hardware #api #feature #update #leaders #bigtech #major_players #science #open
source #dataset #startups #scheduled

A new short course, 'Carbon Aware Computing for GenAI Developers,' is now availabl
e on the DeepLearning.AI platform. The course, taught by Nikita Namjoshi from Google Cloud, aims to educate developers o
n quantifying and mitigating the carbon footprint of machine learning models [1]. 







Lume, a startup from Y Combina
tor's W23 batch, has launched a platform that automates data mappings using AI. The platform targets industries like eco
mmerce, insurance, manufacturing, and financial products, and offers a 50% discount for the first 6 months to users who 
mention seeing them via Launch YC [2].







OrcaAI, a startup part of the NVIDIA Inception program, is using AI-powere
d navigation systems to enhance safety, reduce costs, and improve environmental friendliness in shipping. Their real-tim
e navigation system, SeaPod, has already prevented the release of over 170,000 tons of CO2 emissions [3].







NVIDIA 
AI Developer promotes the use of LangChain Templates and NVIDIA NeMo Guardrails for building safer LLM applications. Det
ailed steps are provided for integrating NeMo Guardrails with LangChain Templates and setting up a LangChain server for 
API access [4]. NVIDIA cuBLAS library version 12.5 introduces Grouped GEMM APIs for single, double, and half precisions,
 improving performance for deep learning and high-performance computing workloads [5].







Satya Nadella congratulate
d the Partner of the Year Award winners for their efforts in ensuring the benefits of AI reach every country, company, a
nd individual [6]. NVIDIA will be showcasing the latest breakthroughs in graphics, generative AI, digital twins, researc
h, OpenUSD, and robotics at SIGGRAPH 2024 [7]. NVIDIA is also hosting a series of webinars on how generative AI is trans
forming the retail industry [8].







Google announced new educational features at ISTE, including bringing Gemini to 
teen students to help them learn responsibly and confidently [9]. Google AI concluded the Research@ Munich event, coveri
ng advancements in AI, climate, health, privacy, and quantum computing [10]. Google AI has demonstrated the feasibility 
and benefits of learning from human feedback for text-to-image generation [11]. Google AI is also using AI technology to
 analyze media content and uncover patterns in representation, partnering with the Geena Davis Institute and USC to crea
te a more equitable media landscape [12].







The a16z Podcast discusses the evolution of cybersecurity from 1995 to 
the present day, highlighting the challenges faced by security experts and the potential future of cybersecurity, includ
ing AI-driven threats and autonomous security systems [13]. Groq Inc's VP, Compiler Software, Andrew Ling, will be speak
ing at the 8th Annual Toronto Machine Learning Summit (TMLS) on July 15 about modifying PyTorch models to enable custom 
data-types and persist precision information through Groq Compiler [16].







Yann LeCun announced the Cambrian-1 proj
ect, focusing on vision-centric multimodal Large Language Models (LLMs). The project involves open datasets, models, and
 source code, with detailed comparisons on visual encoders, connector designs, instruction tuning data, and recipes [17]
. LeCun also suggests that Language Model (LLMs) should stop relying on public data and instead learn from other LLMs [1
8].







The Useless Fun AI Build-A-Thon is scheduled for September 7, 2024, at CloudFlare in San Francisco. The event
 aims to provide a platform for building quirky AI projects and breaking the AI Hype Cycle [20]. OpenAI has announced a 
partnership with TIME to utilize its 101 years of archival content to enhance responses and provide links to stories on 
their platform [21].




[1. Andrew Ng @AndrewYNg https://twitter.com/AndrewYNg/status/1806008133862805840](https://twit
ter.com/AndrewYNg/status/1806008133862805840)

[2. Y Combinator @ycombinator https://twitter.com/ycombinator/status/1806
017501442232528](https://twitter.com/ycombinator/status/1806017501442232528)

[3. NVIDIA AI Developer @NVIDIAAIDev https
://twitter.com/NVIDIAAIDev/status/1805996192439878082](https://twitter.com/NVIDIAAIDev/status/1805996192439878082)

[4. 
NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1806024633855967697](https://twitter.com/NVIDIAA
IDev/status/1806024633855967697)

[5. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1806341723
414581366](https://twitter.com/NVIDIAAIDev/status/1806341723414581366)

[6. Satya Nadella @satyanadella https://twitter.
com/satyanadella/status/1806053061905023266](https://twitter.com/satyanadella/status/1806053061905023266)

[7. NVIDIA AI
 @NVIDIAAI https://twitter.com/NVIDIAAI/status/1806039732469563736](https://twitter.com/NVIDIAAI/status/1806039732469563
736)

[8. NVIDIA AI @NVIDIAAI https://twitter.com/NVIDIAAI/status/1806070183611531470](https://twitter.com/NVIDIAAI/stat
us/1806070183611531470)

[9. Google @google https://twitter.com/google/status/1806059568176247071](https://twitter.com/g
oogle/status/1806059568176247071)

[10. Google AI @googleai https://twitter.com/googleai/status/1806016709247258798](htt
ps://twitter.com/googleai/status/1806016709247258798)

[11. Google AI @googleai https://twitter.com/googleai/status/1806
104018491703458](https://twitter.com/googleai/status/1806104018491703458)

[12. Google AI @googleai https://twitter.com/
googleai/status/1806136596984709267](https://twitter.com/googleai/status/1806136596984709267)

[13. a16z @a16z https://t
witter.com/a16z/status/1806122372963151935](https://twitter.com/a16z/status/1806122372963151935)

[14. Groq Inc @GroqInc
 https://twitter.com/GroqInc/status/1806206231347933612](https://twitter.com/GroqInc/status/1806206231347933612)

[15. G
roq Inc @GroqInc https://twitter.com/GroqInc/status/1806206416891289693](https://twitter.com/GroqInc/status/180620641689
1289693)

[16. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1806315804406738961](https://twitter.com/GroqInc/sta
tus/1806315804406738961)

[17. Yann LeCun @ylecun https://twitter.com/ylecun/status/1806205271896666347](https://twitter
.com/ylecun/status/1806205271896666347)

[18. Yann LeCun @ylecun https://twitter.com/ylecun/status/1806316757835031024](
https://twitter.com/ylecun/status/1806316757835031024)

[19. AssemblyAI @AssemblyAI https://twitter.com/AssemblyAI/statu
s/1806316928719319048](https://twitter.com/AssemblyAI/status/1806316928719319048)

[20. AssemblyAI @AssemblyAI https://t
witter.com/AssemblyAI/status/1806333042346324016](https://twitter.com/AssemblyAI/status/1806333042346324016)

[21. OpenA
I @openai https://twitter.com/openai/status/1806335381283189220](https://twitter.com/openai/status/1806335381283189220)
```
---

     
 
all -  [ I want to create a vector database input pdfs and website chunks and do searches ](https://www.reddit.com/r/LangChain/comments/1dpymhw/i_want_to_create_a_vector_database_input_pdfs_and/) , 2024-06-28-0911
```
How do I start with langchain, am I even using the right tool?
```
---

     
 
all -  [ Secure Your LangChain applications with ZenGuard AI Integration ](https://www.reddit.com/r/LangChain/comments/1dpyk87/secure_your_langchain_applications_with_zenguard/) , 2024-06-28-0911
```
Today, we are excited to announce the latest integration of [ZenGuard AI](https://zenguard.ai) with LangChain - https://
python.langchain.com/v0.2/docs/integrations/tools/zenguard.

Highlights of this integration:

* Prompt Injection Protect
ion: Automatically guards against malicious prompt injections.
* Jailbreak Prevention: Keeps your applications safe from
 unauthorized access.
* Data Leak Prevention: Protects sensitive PII/IP, secrets, and keywords from exposure.
* Topicali
ty Restrictions: Ensures content remains relevant and appropriate.
* Toxicity Protection: Filters out harmful or offensi
ve language.

At ZenGuard AI, we are dedicated to fortifying your data security. We welcome your feedback and questions 
to help us serve you better. PS: If you would like to leave feedback, please file a request on [GitHub](https://github.c
om/langchain-ai/langchain/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC
%3A+).

Stay safe and secure,  
The ZenGuard AI Team
```
---

     
 
all -  [ Resume not getting shortlisted. DS 2YOE, Not an Engineer. ](https://www.reddit.com/r/developersIndia/comments/1dpxtns/resume_not_getting_shortlisted_ds_2yoe_not_an/) , 2024-06-28-0911
```
Same as title. Need an honest review of the resume and the ways I can improve it. Frankly, I think it is due to the fact
 that I do not hold a [B.Tech](http://B.Tech) degree. If this is indeed the case then then what more can I do to upgrade
 my skills and consequently the resume.  
Thanks in advance.

https://preview.redd.it/fu0uyq4ip59d1.jpg?width=617&format
=pjpg&auto=webp&s=63477d5484b01d70b39a64822fcc0baca2eee3cd


```
---

     
 
all -  [ How do agent select tool properly ](https://www.reddit.com/r/LangChain/comments/1dpx7af/how_do_agent_select_tool_properly/) , 2024-06-28-0911
```
In my program, I use react agent, but the agent usually says, 'xxx is not a valid tool'.  
for example, I have a tool na
med `regonition_image_click` when I got correct, like

https://preview.redd.it/kiifjjo6c59d1.png?width=1004&format=png&a
uto=webp&s=bbf6587ac9217104bee34d49205399f877569333

but mostly the Action will get redundant or Chinese ( I think it wi
ll directly be the tool name), then it will get error

https://preview.redd.it/j6ol1887d59d1.png?width=1576&format=png&a
uto=webp&s=d6a1da911d287722a8739beac62594ca3c806c9b

so now I try 2 method  
First, using call tools [https://python.lan
gchain.com/v0.1/docs/use\_cases/tool\_use/multiple\_tools/](https://python.langchain.com/v0.1/docs/use_cases/tool_use/mu
ltiple_tools/)  
I still try to understand how it work

    AgentExecutor(agent_executor_kwargs={'call_tools': call_tool
s})

Second, using plan in agent executor: [https://github.com/langchain-ai/langchain/discussions/18698](https://github.
com/langchain-ai/langchain/discussions/18698)  
But I'm not sure where to place the plan function to override the origin
al (which comes from`RunnableSequence`?).

    from langchain.chains.base import Chain
    from typing import Any, List,
 Tuple, Union
    from langchain_core.agents import AgentAction, AgentFinish
    from langchain_core.callbacks import Ca
llbacks
    
    class FastAgent(Chain):
    Â  Â  def plan(
    Â  Â  Â  Â  self,
    Â  Â  Â  Â  intermediate_steps: List[Tuple[
AgentAction, str]],
    Â  Â  Â  Â  callbacks: Callbacks = None,
    Â  Â  Â  Â  **kwargs: Any,
    Â  Â  ) -> Union[AgentAction, 
AgentFinish]:
    Â  Â  Â  Â  '''Given input, decided what to do.
    
    Â  Â  Â  Â  Args:
    Â  Â  Â  Â  Â  Â  intermediate_steps:
 Steps the LLM has taken to date,
    Â  Â  Â  Â  Â  Â  Â  Â  along with observations
    Â  Â  Â  Â  Â  Â  callbacks: Callbacks to ru
n.
    Â  Â  Â  Â  Â  Â  **kwargs: User inputs.
    
    Â  Â  Â  Â  Returns:
    Â  Â  Â  Â  Â  Â  Action specifying what tool to use.

    Â  Â  Â  Â  '''
    Â  Â  Â  Â  inputs = {**kwargs, **{'intermediate_steps': intermediate_steps}}
    Â  Â  Â  Â  action_input =
 {'para1': 'val1', 'para2': 'val2'}
    Â  Â  Â  Â  inputs['action_input'] = action_input
    Â  Â  Â  Â  final_output: Any = No
ne
    Â  Â  Â  Â  for chunk in self.runnable.stream(inputs, config={'callbacks': callbacks}):
    Â  Â  Â  Â  Â  Â  if final_outp
ut is None:
    Â  Â  Â  Â  Â  Â  Â  Â  final_output = chunk
    Â  Â  Â  Â  Â  Â  else:
    Â  Â  Â  Â  Â  Â  Â  Â  final_output += chunk
   
 Â  Â  Â  Â  return final_output
    
    from model_setting import get_llm
    from langchain import hub
    from langchain
.chains import LLMChain
    from agent_tool.fast_tool import type_text_tool, reg_image_click
    from langchain.agents i
mport AgentExecutor, create_react_agent
    
    prompt = hub.pull('hwchase17/react')
    tools = [type_text_tool, reg_i
mage_click]
    
    llm = get_llm()
    LLMChain(llm=llm, prompt=prompt)
    
    agent = create_react_agent(llm, tools
, prompt)
    question = 'æˆ‘æƒ³è¦é»žæ“Šç”³è«‹äººæ—é‚Šçš„æŒ‰éˆ•'
    agent.invoke({'input': question})

Could someone please provide some advice
 on which method is better and how to do it?
```
---

     
 
all -  [ Need advice - Two days to prepare for an interview on langchain and langgraph ](https://www.reddit.com/r/learnmachinelearning/comments/1dpwye4/need_advice_two_days_to_prepare_for_an_interview/) , 2024-06-28-0911
```
Any quick references for an experienced professional in AI to prepare for an interview on LangChain and LangGraph?

  
A
ny inputs are highly appreciated. Thank you.
```
---

     
 
all -  [ My agent will sometimes treat the on_tool_end event as a string. Anyone know why? ](https://www.reddit.com/r/LangChain/comments/1dpuny1/my_agent_will_sometimes_treat_the_on_tool_end/) , 2024-06-28-0911
```
My agent works 80-85% of the time. For some reason, there'll be random moments when it doesn't work as intended because 
a certain agent astream event doesn't seem to get processed correctly. 

Anyone know the reason behind this kind of inte
raction?

The first image will show the langsmith trace of an agent that works as intended. The second image will show t
he langsmith trace of an agent that doesn't work as intended.

If you look at the second image, it seems like the tool c
all is being treated as a string and gets added to the AI Message. No idea what causes this or why this happens

[Langsm
ith trace for agent that behaves as expected](https://preview.redd.it/xaozguj1z49d1.jpg?width=2013&format=pjpg&auto=webp
&s=b89af5f20ad43e5dd39136bdaff62e0aec0ee4e3)





[Langsmith trace for an agent that doesn't work](https://preview.redd.
it/2wwefdn4z49d1.jpg?width=2007&format=pjpg&auto=webp&s=1a5ce10261ddb357ea02b26d7fabbd96870a90ec)


```
---

     
 
all -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-06-28-0911
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
 
all -  [ Extract Data From Chat History: Quickly and Accurately ](https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/) , 2024-06-28-0911
```
Hi all - several recent posts here have discussed the challenges of extracting structured data from chat histories. This
 is a common challenge: fulfilling sales orders, collecting support info, booking meetings/appointments, and more.

Zepâ€™
s new [Structured Data Extraction](https://blog.getzep.com/structured-data-extraction/) is a high-accuracy tool for extr
acting data from chat histories. It's also 10x faster than gpt-4o.

https://i.redd.it/nwrcdkgwo49d1.gif

# Versus OpenAI
 JSON Mode

OpenAI (or other LLM provider) JSON Mode (with something like a LangChain's `with_structured_output`), only 
guarantees that the result will be well-formed JSON, but the LLM may still return hallucinated values, incorrectly struc
tured fields (think a phone number or date in an incorrect format), or even fields that don't exist in your `pydantic` m
odel!

It can also be super slow, and the more fields you add to your `pydantic` model, the longer it takes.

To ensure 
fast, accurate results, Zep uses a combination of:

* dialog preprocessing, which, amongst other things, improves accura
cy for machine-transcribed dialogs and allows partial dates to be extracted;
* guided output inference techniques on fin
e-tuned LLMs running on our own infrastructure;
* and post-inference validation.

# Using Zep with LangChain

It's simpl
e to [drop Zep into a LangChain application](https://help.getzep.com/langchain/overview). Once you're persisting memory 
to Zep, you can extract data from this dialogue.

# Low or zero marginal latency cost to adding additional fields

Zep's
 extraction latency scales sub-linearly with the number of fields in your model. That is, you may add additional fields 
with a low or no marginal increase in latency.

# Support for Partial and Relative Dates

Zep understands various date a
nd time formats, including relative times such as â€œyesterdayâ€ or â€œlast week.â€ It can also parse partial dates and times,
 such as â€œat 3pmâ€ or â€œon the 15th.â€

# Extracting from Speech Transcripts

Zep can understand and extract data from mach
ine-transcribed transcripts. Spelled out numbers and dates will be parsed as if written language. Utterances such as â€œuh
â€ or â€œumâ€ are ignored.

You can read more [in our announcement](https://blog.getzep.com/structured-data-extraction/) and
 the [Structured Data Extraction guide](https://help.getzep.com/langchain/overview).

This was a ton of work to build an
d lots of fun. Would love your feedback if you give it a spin!

-Daniel
```
---

     
 
all -  [ Mix of RAG and public APIs ](https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/) , 2024-06-28-0911
```
    Is it possible to create a model using RAG with specific content and if my model has no response it can access ChatG
PT for example? Just when RAG is not enough, how to find and develop this type of solution?


```
---

     
 
all -  [ Text 2 FlowChart agent ](https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/) , 2024-06-28-0911
```
I have built a first proof of concept of agents that generate a flow chart given some text as input. 
The flowchart is g
enerated in graphML format and is compatible with yED chart editor (free).

The project is available here:
https://githu
b.com/marco-marchesi/FlowChartGenerator

Note: it's my first github project, any suggestion and contribution are very we
lcome.
```
---

     
 
all -  [ work exp vs. projects + roast my resume ](https://i.redd.it/vclybbey649d1.jpeg) , 2024-06-28-0911
```
Hi!

I just finished first year of college and I want to start preparing for the upcoming wars and battles for internshi
ps. (particularly interested in AI/ML - ofc, everyone is these days TwT)

Iâ€™m looking for some constructive criticism an
d advice on my resume. Firstly, I decided to stick to a one page resume because I am just a sophomore (and this communit
y would die on a hill for that + I trust your experience). So, my internships have taken most of the space and I know so
me of them were for a short duration so I have come here to seek your advice - whether i should keep them or not)

My co
ncerns:

1) Work experience vs. Projects: Which is more important and in what circumstances?
I have a few personal proje
cts and I plan to work on more (to deviate from the typical projects everybody has on their resume), HOWEVER, I am not s
ure where to place them. I was thinking that 
- Maybe I can get rid of my second internship
- Maybe I can get rid of bot
h my hs internships
- Maybe I can shorten the bullet points
- Is there even a teeny tiny chance you guys would advise me
 to add another page

2) Multiple Resumes: Do people generally have different versions of their resumes?

I know its adv
ised to tweak your resume for each job application and include the keywords but what I mean is like having one version t
hat focuses more on work experience and another that emphasizes projects? How do you balance these sections and how do y
ou choose which to apply with?

3) Additional activities: Thereâ€™s some other stuff like hackathons, club committees (for
 instance, something relevant like (GenAI club). Should I make space for these too?

Please feel free to share any other
 feedback or suggestions you have!!

```
---

     
 
all -  [ Sharing history between independent agents ](https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/) , 2024-06-28-0911
```
Hello. I'm new to LangChain and I've been wondering how to achieve shared memory/session between independent agents, wit
hout using a graph with a supervisor. I have an agent which is responsible for breaking down complex question to steps t
hat can be executed by other agents. It is aware other agents exist.

For example:

Main question: What will be the weat
her tomorrow in Oslo? Will it be warmer than in Bergen?

Which is broken down to steps by an agent:

1. {'agent': 'DateT
imeAgent, 'task': 'Get tomorrow's date'}
2. {'agent': 'ForecastAgent, 'task': 'Get the weather in Oslo, Norway for 2024/
06/28'}
3. {'agent': 'ForecastAgent, 'task': 'Get the weather in Bergen, Norway for 2024/06/28'}
4. {'agent': 'CalcAgent
, 'task': 'Calculate the difference between the temperatures'}

As you can see, step 4 is related to the outcome of the 
previous ones. What's the best way to make the agents aware of the results of their peers? Is the only way to use langgr
aph? It seems a bit inefficient to me to have a wrapper agent using the RunnableWithMessageHistory class and have an LLM
 manage the routing and conversation.

Thank you for your assistance beforehand!
```
---

     
 
all -  [ Any experiences with Graph within a Graph in LangGraph? ](https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/) , 2024-06-28-0911
```
There are 2 ways of doing same things now. Chains and Graphs. They both offer almost identical control in most of the sm
all workflows. Advantages, disadvantages and use cases for chains as nodes vs compiled graphs as nodes.

I do realise th
at both are inherit from runnable primitive, but application wise, practically, there are 2 distinct way of doing thing,
 right?
```
---

     
 
all -  [ Data Ingestion for the RAG from Dynamo DB to AuroraDB with pgVector to store embeddings ](https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/) , 2024-06-28-0911
```
I have data stored in my **DynamoDB** which is frequently updated through back-end services. Now I want to create a PG v
ector based **AuroraDB vector database** for storing **embeddings**, which I want to be automatically updated whenever t
here is the change in the DynamoDB.

I thought about using the **EventBridge** but need more suggestion on that.

My aim
 is to create the new embeddings everytime there is the change (Upsert) in the DynamoDB and store them in the PG Vector 
Database. So that I can perform the RAG on **latest embeddings** to so the answer from LLM must be context aware.

In th
e phase of architectural designing and ideation of this feature.

Any suggestions are welcomed .
```
---

     
 
all -  [ Integration Issues with LangGraph, RedisChatMessageHistory, and RunnableWithMessageHistory ](https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/) , 2024-06-28-0911
```
I am currently working on integrating several components into a comprehensive chat application using LangServe and LangC
hain. Below, I detail the components involved and the specific issues I'm encountering. Any guidance or suggestions woul
d be greatly appreciated.

# Components and Setup:

1. **History Aware Retriever and Question Answer Chain**:
   * I've 
created a chain that consists of a history-aware retriever and a question-answer chain.

&#8203;

    contextualize_q_sy
stem_prompt = '''Given a chat history and the latest user question \
    which might reference context in the chat histo
ry, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, 
\
    just reformulate it if needed and otherwise return it as is.'''
    contextualize_q_prompt = ChatPromptTemplate.fr
om_messages(
        [
            ('system', contextualize_q_system_prompt),
            MessagesPlaceholder('chat_hist
ory'),
            ('human', '{input}'),
        ]
    )
    history_aware_retriever = create_history_aware_retriever(
 
       llm, retriever, contextualize_q_prompt
    )
    
    
    ### Answer question ###
    qa_system_prompt = '''You 
are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the questio
n. \
    If you don't know the answer, just say that you don't know. \
    Use three sentences maximum and keep the answ
er concise.\
    
    {context}'''
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ('system', qa
_system_prompt),
            MessagesPlaceholder('chat_history'),
            ('human', '{input}'),
        ]
    )
    
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    rag_chain = create_retrieval_chain(history
_aware_retriever, question_answer_chain)

1. **Message History Implementation**:
   * The application incorporates `Redi
sChatMessageHistory` along with `RunnableWithMessageHistory`. The intention is to leverage Redis for managing chat messa
ge history, tracking conversations by User ID and Conversation ID.
2. **LangGraph Integration**:
   * I'm attempting to 
integrate this setup into LangGraph. However, I'm facing challenges because LangGraph documentation suggests using Check
points with SQLite, and it's unclear how to integrate RedisChatMessageHistory which is essential for my application.

# 
Issues:

* **Integration with LangGraph**: How can I integrate `RedisChatMessageHistory` within LangGraph, given that La
ngGraph primarily supports SQLite for Checkpoints?
* **Consistent Message History**: I need to ensure that message histo
ry capabilities are maintained across the entire application, allowing tracking of conversations by User ID and Conversa
tion ID.

# Resources:

* For the retrieval chain setup, please refer to the LangChain documentation on question answeri
ng with chat history: [LangChain QA with Chat History](https://python.langchain.com/v0.1/docs/use_cases/question_answeri
ng/chat_history/#tying-it-together).
* For details on managing persistent chat with user IDs and conversation IDs, see t
his example from LangServe: [LangServe Chat with Persistence](https://github.com/langchain-ai/langserve/blob/main/exampl
es/chat_with_persistence_and_user/server.py).

# Request:

I am seeking advice or examples on how to properly integrate 
RedisChatMessageHistory with LangGraph in a manner that maintains full functionality of the message history features. An
y insights or pointers towards documentation or similar implementations would be incredibly helpful.
```
---

     
 
all -  [ How to generate Cypher Query using LLM? ](https://www.reddit.com/r/Neo4j/comments/1dpos1o/how_to_generate_cypher_query_using_llm/) , 2024-06-28-0911
```
I have a huge schema in the neo4j database.

I'm using the LangChain function to generate a cypher query

chain = GraphC
ypherQAChain.from_llm(
ChatOpenAI(temperature=0), graph=graph, verbose=True
)

chain.invoke(query)

It's returning an er
ror saying that the model supports 16k tokens and I'm passing 15M+ tokens

How can I limit these tokens? I tried setting
 ChatOpenAI(temperature=0, max_tokens=1000) and it's still giving the same error.

I think it's passing the whole schema
 at once, how can I set a limit on that?
```
---

     
 
all -  [ How to get a structured response from 'create_retriever_tool'? ](https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/) , 2024-06-28-0911
```
When calling the retriever directly, I get a response which includes the content + metadata.

    retriever = documents.
as_retriever(search_kwargs={'k': 1})
    retriever.get_relevant_documents('foo')

The response:

    [Document(page_cont
ent='foo', metadata={'tenant_id': '0d122190-b761-43f7-9ea3-f1842bbe1c4d', 'page': 7})]

When i wrap the retriever with t
he utiliy function provided by langchain: 'create\_retriever\_tool':

    tool: Tool = create_retriever_tool(documents.a
s_retriever(search_kwargs={
    Â  Â  'k': 6}), name='search_documents', description='Search documents')
    
    tool.inv
oke('foo')

The response:

'foo'

So in this case, the metadata part is completely missing. I understand that I could us
e a prompt\_template which includes the metadata:

    document_prompt = PromptTemplate.from_template(
    Â  Â  'Document
 chunk metadata: tenant_id: {tenant_id}...\n'
    Â  Â  'Document chunk content: {page_content}. '
    )
    tool: Tool = 
create_retriever_tool(documents.as_retriever(search_kwargs={
    Â  Â  'k': 6}), name='search_documents', description='Sea
rch documents', document_prompt=document_prompt)

and this works but the output from directly calling retriever.get\_rel
evant\_documents('foo') is a document array which makes it easier to work with.

I would like to have the exact same out
put from the response of calling the tool. How can this be achieved? Is the only solution to create a custom tool functi
on instead of using the utility function?
```
---

     
 
all -  [ Build Your Own GitHub Copilot with SuperDuperDB: Live Workshop ](https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/) , 2024-06-28-0911
```
Hey guys!

Just wanted to give you all a heads up about a live workshop we're hosting tonight. We'll be showing how to b
uild an AI-powered tool similar to GitHub Copilot using [SuperDuperDB's](http://superduperdb.com) latest release (v0.2).
 ðŸš€

ðŸŽ¥ Today (27/06/2024) at 9 PM CET  
ðŸ”— [https://www.youtube.com/watch?v=JgavM6QDmxQ](https://www.youtube.com/watch?v=J
gavM6QDmxQ)

# What to Expect:

* **AI and Databases:** How to integrate AI models directly with your database.
* **Vect
or Search & Model Chaining:** Learn about vector search and setting up workflows by chaining models and APIs.
* **Real-t
ime AI Outputs:** Implementing real-time AI outputs as new data arrives.

If you're into AI, databases, or just curious 
about how it all works, this session is for you. 

Feel free to drop any questions or comments below. Excited to see wha
t you all think!
```
---

     
 
all -  [ Hiring fully-remote Agentic Software Developers! ](https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/) , 2024-06-28-0911
```
I'm hiring a fully-remote Agentic Software Developers to build, test and refine our agents, as well as the infrastructur
e around them. We're a stealth-mode start up  backed top VCs. Please feel free to reach out to me here or via Discord (@
thebirthdaygirl) and I'd love to chat!  

```
---

     
 
all -  [ Trace to data-structure or file instead of langsmith? ](https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/) , 2024-06-28-0911
```
Caveat: I am very new to using langchain.

Langsmith seems like an excellent product for enterprise and large-scale prod
uction, but beyond my needs and pricing, and seems not an easy way to export data.

I just want to be able to capture wh
at is going on under the hood -- including all LLM API call inputs/outputs -- mostly to better understand how the implem
ented patterns (structured outputs, tool calling, etc) are done in practice.  Ideally either as a data-structure for me 
to persist or directly to a file (JSON or JSONL or similar I can peruse and process).

Is there an easy way to do this?


Its not clear to me if the chain design just makes it challenging to implement observability (and why langsmith is need
ed), or if its somewhat intentionally not made clear or well-documented as langsmith and langserve are the profit center
s.
```
---

     
 
all -  [ Struggling to Land ML Engineer and Software Engineer Positions â€“ Seeking Advice ](https://www.reddit.com/r/resumes/comments/1dphsbi/struggling_to_land_ml_engineer_and_software/) , 2024-06-28-0911
```
Hi everyone,

I've been applying for ML engineer positions but haven't had any luck. I also have a version of my resume 
for software engineer roles, but I'm facing the same issue. I'm starting to wonder if my resume is the problem, as I hav
en't been able to land any interviews.

Has anyone else faced this issue? Any suggestions or feedback would be much appr
eciated

Thanks!

https://preview.redd.it/zjpflan3d19d1.png?width=1266&format=png&auto=webp&s=e1a592fa4b8bb1b274ec5e32d7
687821dcd90dcc

https://preview.redd.it/74ifj3n3d19d1.png?width=1266&format=png&auto=webp&s=2c34226db01fc08a38e63dc248bb
07edfa36b0a5


```
---

     
 
all -  [ How to Manage State in LangGraph for Multiple Users? ](https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/) , 2024-06-28-0911
```
Hello, I am currently developing a chatbot using LangGraph, and I'm facing some challenges with managing state for multi
ple users. Specifically, I'm dealing with the following constraints and setup:

- The state is limited only to last 10-1
5 messages due to the structure of API I am interacting with.
- All the chat history will be stored in a MySQL database.
 I do it by storing each input and response manually to the db, as the checkpointer implementation in MySQL is not suppo
rted yet.
- The messages will be stored in the database with the corresponding user ID. For now the chat history in the 
database has no function in the chatbot flow. It only serve for the frontend to load previous interaction when user open
 the chatbot. 

If I understand correctly, the state is stored in the runtime and shared across multiple users, right? I
 think this might lead to a memory problem if I don't implement some way of handler or even if I limit the previous mess
ages for each user it will lead to a problem.

My idea to handle this is as follows:
- Store the chat history (user ID a
nd message) in the database.
- When a new query comes in, load the last 10 last messages from the database for the appro
priate user ID.
- Append this history with the new query and pass it to the chatbot.

How does my idea sound? Are there 
any potential pitfalls or improvements you would suggest? I'm open to any suggestions and feedback.

Thanks in advance f
or your help!
```
---

     
 
all -  [ How do I map a user query and response with a certain set of predefined tasks using output parsers?  ](https://www.reddit.com/r/LangChain/comments/1dpcfh2/how_do_i_map_a_user_query_and_response_with_a/) , 2024-06-28-0911
```
So I have a doubt, I figured out how to get responses back in a certain format using the Json output parsers. I want to 
know how I can map a response to an intent. Like I want to create an AI Agent. So if the query is 'hey I wanna change my
 password I think I forgot it' it should map it to a task 'reset password'
```
---

     
 
all -  [ Add context to create_sql_agent ](https://www.reddit.com/r/LangChain/comments/1dpbl5e/add_context_to_create_sql_agent/) , 2024-06-28-0911
```
Hello I am using create_sql_agent to query a SQL server database. The problem is for some reason the agent does not know
 that it has to use SQL server dialect from the beginning and also does not know the column names.

Is there a way to pr
ovide this initial context to the prompt?

This is my code:
    llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature
=0)
toolkit = SQLDatabaseToolkit(db=db, llm=llm)
agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
  
  verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)

    
```
---

     
 
all -  [ How we Chunk - turning PDF's into hierarchical structure for RAG ](https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/) , 2024-06-28-0911
```
Hey all,

We've spent a lot of time building new techniques for parsing and searching PDFs. They've lead to a significan
t improvement in our RAG search and I wanted to share what we've learned.

**Some examples:**

Table - SEC Docs are noto
riously hard for PDF -> tables. We tried the top results on google & some opensource thins not a single one succeeded on
 this table. 

Couple examples of who we looked at:

* ilovepdf
* Adobe
* Gonitro
* PDFtables
* OCR 2 Edit
* microsoft/t
able-transformer-structure-recognition

Results - our result (can be accurately converted into CSV,MD,JSON)

https://pre
view.redd.it/5wju5gedmy8d1.png?width=1035&format=png&auto=webp&s=2a336bd0e1af14760fbb5ca4291284c99edaa27e



Example: id
entifying headers, paragraphs, lists/list items (purple), and ignoring the 'junk' at the top aka the table of contents i
n the header.

 

https://preview.redd.it/ix7747bjmy8d1.png?width=1018&format=png&auto=webp&s=ea0b65ae6a35581d955da28235
3ff63509602a38



**Why did we do this?**

W ran into a bunch of issues with existing approaches that boils down to one 
thing: hallucinations often happen because the chunk doesn't provide enough information.

* chunking by word count doesn
't work. It often chunks mid-paragraph or sentence.
* Chunking by sentence or paragraph doesn't work. If the answer span
s 2-3 paragraphs, you still are SOL.
* Semantic chunking is better but still fail quite often on lists or 'somewhat' dif
ferent pieces of info.
* LLM's deal better with structured/semi-structured data, i.e. knowing what you're sending it is 
a header, paragraph list etc., makes the model perform better.
* Headers often aren't included because they're too far a
way from the relevant vector, although often times headers contain important information.



**What are we doing differe
nt?** 

We are dynamically generating chunks when a search happens, sending headers & sub-headers to the LLM along with 
the chunk/chunks that were relevant to the search.

Example of how this is helpful: you have 7 documents that talk about
 how to reset a device, and the header says the device name, but it isn't talked about the paragraphs. The 7 chunks that
 talked about how to reset a device would come back, but the LLM wouldn't know which one was relevant to which product. 
That is, unless the chunk happened to include both the paragraphs and the headers, which often times in our experience, 
it doesn't.

This is a simplified version of what our structure looks like:

    {
    Â  'type': 'Root',
    Â  'children
': [
    Â  Â  {
    Â  Â  Â  'type': 'Header',
    Â  Â  Â  'text': 'How to reset an iphone',
    Â  Â  Â  'children': [
    Â  Â  Â 
 Â  {
    Â  Â  Â  Â  Â  'type': 'Header',
    Â  Â  Â  Â  Â  'text': 'iphone 10 reset',
    Â  Â  Â  Â  Â  'children': [
    Â  Â  Â  Â  Â  
Â  { 'type': 'Paragraph', 'text': 'Example Paragraph.' },
    Â  Â  Â  Â  Â  Â  { 
    Â  Â  Â  Â  Â  Â  Â  'type': 'List',
    Â  Â  Â  
Â  Â  Â  Â  'children': [
    Â  Â  Â  Â  Â  Â  Â  Â  'Item 1',
    Â  Â  Â  Â  Â  Â  Â  Â  'Item 2',
    Â  Â  Â  Â  Â  Â  Â  Â  'Item 3'
    Â  Â  Â 
 Â  Â  Â  Â  ]
    Â  Â  Â  Â  Â  Â  }
    Â  Â  Â  Â  Â  ]
    Â  Â  Â  Â  },
    Â  Â  Â  Â  {
    Â  Â  Â  Â  Â  'type': 'Header',
    Â  Â  Â  Â  Â  
'text': 'iphone 11 reset',
    Â  Â  Â  Â  Â  'children': [
    Â  Â  Â  Â  Â  Â  { 'type': 'Paragraph', 'text': 'Example Paragraph
 2' },
    Â  Â  Â  Â  Â  Â  { 
    Â  Â  Â  Â  Â  Â  Â  'type': 'Table',
    Â  Â  Â  Â  Â  Â  Â  'children': [
    Â  Â  Â  Â  Â  Â  Â  Â  { 'type
': 'TableCell', 'row': 0, 'col': 0, 'text': 'Column 1'},
    Â  Â  Â  Â  Â  Â  Â  Â  { 'type': 'TableCell', 'row': 0, 'col': 1, 
'text': 'Column 2'},
    Â  Â  Â  Â  Â  Â  Â  Â  { 'type': 'TableCell', 'row': 0, 'col': 2, 'text': 'Column 3'},
    Â  Â  Â  Â  Â  Â 
 Â  Â  
    Â  Â  Â  Â  Â  Â  Â  Â  { 'type': 'TableCell', 'row': 1, 'col': 0, 'text': 'Row 1, Cell 1'},
    Â  Â  Â  Â  Â  Â  Â  Â  { 'ty
pe': 'TableCell', 'row': 1, 'col': 1, 'text': 'Row 1, Cell 2'},
    Â  Â  Â  Â  Â  Â  Â  Â  { 'type': 'TableCell', 'row': 1, 'co
l': 2, 'text': 'Row 1, Cell 3'}
    Â  Â  Â  Â  Â  Â  Â  ]
    Â  Â  Â  Â  Â  Â  }
    Â  Â  Â  Â  Â  ]
    Â  Â  Â  Â  }
    Â  Â  Â  ]
    Â  Â  
}
    Â  ]
    }
    

**How do we get PDF's into this format?**

At a high level, we are identifying different portions 
of PDF's based on PDF metadata and heuristics. This helps solve three problems:

1. OCR can often mis-identify letters/n
umbers, or entirely crop out words. 
2. Most other companies are trying to use OCR/ML models to identify layout elements
, which seems to work decent on data it's seen before but fails pretty hard unexpectedly. When it fails, it's a black bo
x. For example, Microsoft released a paper a few days ago saying they trained a model on over 500M documents and still f
ails on a bunch of use cases that we have working
3. We can look at layout, font analysis etc. throughout the entire doc
 allowing us to understand the 'structure' of the document more. We'll talk about this more when looking at font classes


**How?**

First, we extract tables. We use a small OCR model to identify bounding boxes, then we do use white space an
alysis to find cells. This is the only portion of OCR we use (we're looking at doing line analysis but have punted on th
at thus far.) We have found OCR to poorly identify cells on more complex tables, and often turn a 4 into a 5 or a 8 into
 a 2 etc.

When we find a table, we find characters that we believe to be a cell based on distance between each other, t
rying to read the table as a human would. An example would be 1345 would be a 'cell' or text block, where 1    345 would
 be two text blocks due to the distance between them. A re-occurring theme is white space can get you pretty far.

Secon
d, we extract character data from the PDF:

* **Fonts**: Information about the fonts used in the document, including the
 font name, type (e.g., TrueType, Type 1), and embedded font files.
* **Character Positions:** The exact bounding box of
 each character on the page.
* **Character Color:** PDFs usually give this correctly, and when it's wrong it's still goo
d enough

PDFs provide a other metadata, but we found them to either be inaccurate or not necessary:

* **Content Stream
s:**Â Sequences of instructions that describe the content of the page, including text, images, and vector graphics. We fo
und these to be surprisingly inaccurate. Newline characters inserted in the middle of words, characters and words placed
 out of order, and whitespace is handled really inconsistently (more below)
* **Annotations:**Â Information about interac
tive elements such as links, form fields, and comments. There are useful details here that we may use in the future, but
, again, a lot of PDF tools generate these incorrectly.

Third, we strip out all space, newline, and other invisible cha
racters. We do whitespace analysis to build words from individual characters. 

**After extracting PDF metadata:**

We e
xtract out character locations, font sizes, and fonts. We then do multiple passes of whitespace analysis and clustering 
algorithms to find groups, then try to identify what category they fall into based on heuristics. We used to rely more h
eavily on clustering (DBScan specifically), but found that simpler whitespace analysis often outperformed it. 

* If you
 look at a PDF and see only a handful of characters, let's say 1% that are font 32, color blue, and each time they're id
entified together it's only 2-3 words it's likely a header. 
* Now you see 2% are font 28, red, it's probably a sub-head
er. (That is if the font spans multiple pages.) If it instead is only in a single location, it's most likely something i
mportant in the text that the author wants us to 'flag'. 
* This makes font analysis across the document important, and 
another reason we stay away from OCR
* If, the document is 80% font 12, black. It's probably 'normal text.' Normal text 
needs to be categorized into two different formats, one is paragraphs, the other is bullet points/lists. 
* For bullet p
oints we look primarily at the white space, identifying that there's a significant amount of white space, often follow b
y a bullet point, number, or dash. 
* For paragraphs, we text together in a 'normal' format without bullet points, tradi
tionally spanning a majority of the document.
* Junk detection. A lot of PDF's have junk in them. An example would be a 
header that's at the top of every single document, or a footer on every document saying who wrote it, the page number et
c. This junk otherwise is sent to the chunking algorithm meaning you can often have random information mid-paragraph. We
 generate character ngram vectors and cluster then based on L1 distance (rather than cosine). That lets us find variatio
ns like 'Page 1', 'Page 2', etc. If those appear in roughly the same location on more than 20-35% of pages, it's likely 
just repeat junk.

  


The product is still in beta so if you're actively trying to solve this, or a similar problem, w
e're letting people use it for free, in exchange for feedback.

Have additional questions? Shoot!


```
---

     
 
all -  [ Versioning RAG ](https://www.reddit.com/r/LangChain/comments/1dp9m83/versioning_rag/) , 2024-06-28-0911
```
How are people versioning their RAG pipelines? 

I've found that with context which changes/needs frequent updates, we n
eed some type of versioning strategy. 

Has anyone else run into this?
```
---

     
 
all -  [ Are there any RAG successful real production use cases out there? ](https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/) , 2024-06-28-0911
```
Hello, people. I am a veteran programmer who is new to AI and its business use cases.

I am fascinated by it, and I am n
ow working on a small prototype for a client. It is an out-of-the-book RAG case:

* \~1.5K 1-page PDFs with product spec
s.
* Build a chatbot to ask questions about the products.

In our team, we are making great progress in the basic setup.
 The PDFs are indexed in a VectorDB and we are able to use GPT4 to interact with the VectorDB data and generate human fr
iendly answers.

But there is a lot to improve about the generated recomendations, conclusions, filtering, best results,
 ...

All the tutorials and documentation we are seeing end up here, in the basic setup. And don't go further in the det
ails and improvements needed to go to 'production' level. Further more, I have seen that many people on this community a
nd others are mentioning their dissapointment with the actual state of the technology and their abandom of building a RA
G architecture.

I just want a confirmation that it is possible. That some of you have managed to build a RAG architectu
re that is used satisfactorily in production. Is this the case? :)
```
---

     
 
all -  [ Evaluating Open Source LLM for RAG ](https://www.reddit.com/r/LangChain/comments/1dowgnt/evaluating_open_source_llm_for_rag/) , 2024-06-28-0911
```
I've been working on RAG and LLM, and I've developed something I think you'll find useful: BeyondLLM.

It's a tool I've 
created to simplify the process of building advanced AI applications. With just a few lines of code, you can dive into R
etrieval-Augmented Generation and Large Language Models. Plus, it's open source!

Now, here's the latest update: BeyondL
LM now includes additional features:

* Fine Tune Embeddings: Customize your model's embeddings for improved performance
.
* Observability: Easily monitor your model's performance.
* Groq LLM: Experience faster inference times for low latenc
y applications.

If you're interested, you can check out BeyondLLM on GitHub:Â [BeyondLLM GitHub](https://github.com/aipl
anethub/beyondllm/)
```
---

     
 
all -  [ How to automate form filling using LLM ? ](https://www.reddit.com/r/LangChain/comments/1dovvd2/how_to_automate_form_filling_using_llm/) , 2024-06-28-0911
```
So, I want to automate the form-filling section. For example, here I am taking Redmine. that is basically a chatbot that
 will interact with the user and change values in afield according to the input given by the user. for now, I have plann
ed to create a text-to-JSON chatbot using some free open-source LLM that will help the user change the fields by changin
g the natural language entered by the user to JSON format, which will be sent to execute, and the user can see the field
s be changed accordingly. 

so, how can I implement something like this?
```
---

     
 
all -  [ How to use Recursive URL Loader with Playwright/Puppeteer? ](https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/) , 2024-06-28-0911
```
Hello, I'm building a RAG app with Langchain.js. It's my first time using LLM framework, thus first time using Langchain
 too. I'm currently using the Recursive URL Loader integration to recursively fetch data from websites. And, behave as I
 wanted, but I have some issue with websites using modern frontend frameworks. So, I tried the Playwright Langchain inte
gration (I'm a bit familiar with Playwright), and it's work on the desired websites, but as you guess, it only works on 
one page. So my question is there is an integration that do both? Handling JS and recursively browse the website, or how
 can I combine the two to achieve my goal? I'm open to alternative solution using Puppeteer or even Python example.

Thi
s is how I use the Recursive URL Loader:

    import { RecursiveUrlLoader } from '@langchain/community/document_loaders/
web/recursive_url';
    import { compile } from 'html-to-text';
    
    export async function loadWebsite(url: string, 
excludeDirs?: string[]) {
      const compiledConvert = compile({ wordwrap: 130 }); // returns (text: string) => string;

    
      const loader = new RecursiveUrlLoader(url, {
        extractor: compiledConvert,
        maxDepth: 1,
      
  excludeDirs,
        preventOutside: true,
      });
    
      return loader.load();
    }

And, this is my Playwrigh
t attempt:

    import { PlaywrightWebBaseLoader } from '@langchain/community/document_loaders/web/playwright';
    impo
rt { MozillaReadabilityTransformer } from '@langchain/community/document_transformers/mozilla_readability';
    import {
 RunnableSequence } from '@langchain/core/runnables';
    import { RecursiveCharacterTextSplitter } from '@langchain/tex
tsplitters';
    
    export async function loadWebsite(url: string) {
      const loader = new PlaywrightWebBaseLoader(
url, {
        launchOptions: {
          chromiumSandbox: false,
        },
      });
    
      const documents = awai
t loader.load();
    
      const transformChain = RunnableSequence.from([
        RecursiveCharacterTextSplitter.fromLa
nguage('html'),
        new MozillaReadabilityTransformer(),
      ]);
    
      return transformChain.invoke(documents
);
    }

Thanks for reading!  

```
---

     
 
all -  [ Multi GPU support ](https://www.reddit.com/r/LangChain/comments/1doszle/multi_gpu_support/) , 2024-06-28-0911
```
I want to host my app on AWS with multiple GPUs. I tried Llama-Index, but they do not support multi-GPU setups as far as
 I know. How can I run Hugging Face models on multiple GPUs?
```
---

     
 
all -  [ LangGraph integration with bedrock ](https://www.reddit.com/r/LangChain/comments/1dosz4u/langgraph_integration_with_bedrock/) , 2024-06-28-0911
```
Anyone knows if LangGraph integrates with Bedrock and what are the capabilties. I am quite new to this and I was followi
ng the langChain youtube series on LangGraph and they used a lot of OpenAI Functions so I wanted to know if it was possi
ble to do the same with Bedrock models?
```
---

     
 
all -  [ Should I Use LangChain for Building a Chatbot with OpenAI Assistant API? ](https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/) , 2024-06-28-0911
```
I'm creating a chatbot using the OpenAI Assistant API and considering LangChain. Should I use it?

What are the pros and
 cons?

Thanks!
```
---

     
 
all -  [ Alternatives to Pydantic Data Model for Output Parsers. ](https://www.reddit.com/r/LangChain/comments/1dolptc/alternatives_to_pydantic_data_model_for_output/) , 2024-06-28-0911
```
I am working on a project where user injects a JSON/YAML/XML file as structured desired for output formatting and able t
o generate output by giving that structure to LLM.

So far I was coding all the data models as per user's requirements i
nto a Pydantic Class and using `PydanticOutputParser()`  to pass the structure information into prompt. I want to upgrad
e this feature and let user provide structure in JSON/YAML/XML (for testing I am thinking of doing with JSON) instead of
 my manually adding different data models. 

Langchain's abstractions makes it very complicated to implement changes, I 
wanted to test my hypothesis before investing time to code something of this kind. 

  
Any help would be much appreciat
ed, Thanks!
```
---

     
 
all -  [ GraphDB RAG w/ LangChain to find fees ](https://www.reddit.com/r/LangChain/comments/1doiikb/graphdb_rag_w_langchain_to_find_fees/) , 2024-06-28-0911
```
I have a document, such as this [one](https://www.xpo.com/cdn/download_files/s1/p2831/CNWY_199-AI.2.pdf), that describes
 the costs one can expect with a shipment, and throughout describe other fees that may come up depending on a specific s
hipment. I'm trying parse the doc and save it in a neo4j DB for RAG, using the LLMGraphTransformer class.

Any ideas on 
how I should go about parsing the document to have relevant chunks, and what I should set allowed nodes and relationship
s to? Everything I've tried so far, I haven't gotten any nodes or relationships from the model. 

Or, do you recommend m
aybe abandoning using a graph DB and go back to a traditional vectorDB, with a RAPTOR system? Open to suggestions!
```
---

     
 
MachineLearning -  [ [P] Seeking Feedback on My GenAI Job Fit Project - New to LangChain/LangGraph ](https://www.reddit.com/r/MachineLearning/comments/1dgns9p/p_seeking_feedback_on_my_genai_job_fit_project/) , 2024-06-28-0911
```
Hi all,

Soo, i have been working on a a projectcalled [GenAI Job Fit](https://github.com/DAVEinside/GenAI_Job_Fit). It'
s an AI-driven system designed to enhance job applications by providing tailored recommendations based on individual pro
files.

I'm relatively new to LangChain and LangGraph, and I've incorporated them into this project. I would greatly app
reciate it if you could check out the repository and provide any feedback or suggestions for improvement.

Your insights
 on how I can better implement LangChain/LangGraph, or any other aspect of the project, would be incredibly valuable. I'
m eager to learn and make this project as robust as possible.

Thank you in advance for your time and feedback!

Repo Li
nk : [https://github.com/DAVEinside/GenAI\_Job\_Fit](https://github.com/DAVEinside/GenAI_Job_Fit)
```
---

     
 
MachineLearning -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-06-28-0911
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
MachineLearning -  [ [P] Superfast RAG: Langchain Streaming and Groq ](https://www.reddit.com/r/MachineLearning/comments/1d5s9g4/p_superfast_rag_langchain_streaming_and_groq/) , 2024-06-28-0911
```
  
Fast LLM RAG inference using Groq and Langchain Streaming.  
  
Groq is introducing a new, simpler processing archite
cture designed specifically for the performance requirements of machine learning applications and other compute-intensiv
e workloads. The simpler hardware also saves developer resources by eliminating the need for profiling, and also makes i
t easier to deploy AI solutions at scale.  
  
Resource: [https://www.youtube.com/watch?v=frMdOL8knqg](https://www.youtu
be.com/watch?v=frMdOL8knqg)
```
---

     
 
deeplearning -  [ What is ReAct Prompting? the most important piece in agentic frameworks ](https://www.reddit.com/gallery/1djk4nk) , 2024-06-28-0911
```
â€œWhat is ReAct Prompting? the most important piece in agentic frameworksâ€ - A quick read from Mastering LLM (Large Langu
age Model) 'Coffee Break Concepts' Vol.6

This document deeps dive into the ReAct Prompting method and why it's importan
t:
1. Limitations of LLM
2. Why ReAct prompting matters?
3. How ReAct Works?
4. LangChain Implementation
5. Why Prompt w
ithin agentic frameworks Matters?

Comment below on which topic you want to understand next in this 'Coffee Break Concep
ts' series and we will include those topics in upcoming weeks.
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-06-28-0911
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
