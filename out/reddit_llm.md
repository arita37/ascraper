 
all -  [ List of FREE and Best Selling Discounted Courses ](https://www.reddit.com/r/udemyfreebies/comments/1ebdd7x/list_of_free_and_best_selling_discounted_courses/) , 2024-07-25-0911
```
# Udemy Free Courses for 25 July 2024

Note : Coupons might expire anytime, so enroll as soon as possible to get the cou
rses for FREE.

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11803/)Learn Basics of Obsidian: Mastering Study Not
es
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11804/)Learn Basics of Obsidian: Mastering Study Notes in Arabic

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11805/)Complete Generative AI Course With Langchain and Huggingface

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11806/)Building Gen AI App 12+ Hands-on Projects with Gemini Pro
* [
REDEEM OFFER ](https://idownloadcoupon.com/udemy/11807/)AWS Business Essentials – The Business Value of AWS \[2024\]
* [
REDEEM OFFER ](https://idownloadcoupon.com/udemy/11808/)Basics of Obsidian: The Canvas Plugin in Arabic
* [REDEEM OFFER 
](https://idownloadcoupon.com/udemy/11809/)Learn Python for Data Science for Beginners in Arabic
* [REDEEM OFFER ](https
://idownloadcoupon.com/udemy/11810/)Learn Basics of Obsidian: The Canvas Plugin
* [REDEEM OFFER ](https://idownloadcoupo
n.com/udemy/11811/)زد من انتاجيتك عن طريق برنامج اوبسيديان
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11812/)R 
Programming for Complete Beginners
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11813/)Improve your Productivity 
by using Obsidian
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11814/)Management Information Systems Student’s Jo
urney
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11815/)Learn Obsidian from Scratch
* [REDEEM OFFER ](https://i
downloadcoupon.com/udemy/11816/)Learn Python for Data Science for Complete Beginners
* [REDEEM OFFER ](https://idownload
coupon.com/udemy/11799/)TRADING META TRANSACTION
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11798/)Menguasai Go
ogle Ads untuk Pemula hingga Mahir 2024
* WAX Blockchain Game Front-End w/ React, Redux & Saga Part 3
* [REDEEM OFFER](h
ttps://idownloadcoupon.com/udemy/11797/)
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11796/)Learn Expert Systems
 for BI and Business Analytics
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11795/)Starting with IoT Simulations

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11800/)Diseño UX Potenciado por la Ciencia de los Hábitos
* [REDEEM 
OFFER ](https://idownloadcoupon.com/udemy/11801/)WAX Blockchain Game Front-End w/ React, Redux & Saga Part 1
* [REDEEM O
FFER ](https://idownloadcoupon.com/udemy/11794/)Executive Diploma of Chief Technology Officer
* [REDEEM OFFER ](https://
idownloadcoupon.com/udemy/11793/)400 Tableau Interview Questions Practice Test
* [REDEEM OFFER ](https://idownloadcoupon
.com/udemy/11792/)AI For Warehouse Management Certificate Odoo 17 Program
* [REDEEM OFFER ](https://idownloadcoupon.com/
udemy/11791/)Control of Manufacturing Processes with Odoo 17 AI-Powered
* [REDEEM OFFER ](https://idownloadcoupon.com/ud
emy/11790/)Six Sigma Green Belt Practice Exam
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11789/)From Zero to He
ro: Master Microservices with ASP.NET Core
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11237/)Social Media Maste
ry 2023| Increase Customer Conversion Rate
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/3570/)Professional Diplom
a in Agile and Scrum
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/8176/)Microsoft Excel – Beginner to Advance wit
h Example
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/2649/)Migra un Sitio Web de WordPress a otro Dominio o Hos
ting
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/4070/)Excel VBA / Makros Programmierung. Automatisierung mit Ex
cel
* Mastering ChatGPT (AI) and PowerPoint presentation
* [REDEEM OFFER](https://idownloadcoupon.com/udemy/8200/)
* [RE
DEEM OFFER ](https://idownloadcoupon.com/udemy/2807/)Cómo Crear una Página web con WordPress y Elementor 2024
* [REDEEM 
OFFER ](https://idownloadcoupon.com/udemy/11012/)TOEFL Preparation: Listening Section
* [REDEEM OFFER ](https://idownloa
dcoupon.com/udemy/8980/)Complete SmartPhone Graphic Design – 3 in 1 Course
* [REDEEM OFFER ](https://idownloadcoupon.com
/udemy/2646/)Cómo Crear una Tienda Online con WordPress y WooCommerce
* [REDEEM OFFER ](https://idownloadcoupon.com/udem
y/2799/)WP Rocket 2024: Mejora la Velocidad de Carga en WordPress
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/10
597/)Web3 Development Essentials
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11788/)Parallel Computing in Julia

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/11787/)Professional Diploma in Public Relations and PR Management
* 
[REDEEM OFFER ](https://idownloadcoupon.com/udemy/11786/)SAFe® 6.0: From Zero to Hero in Scaled Agile Framework®
* [REDE
EM OFFER ](https://idownloadcoupon.com/udemy/11785/)Comprehensive Machine Learning Practice Test: Skill Mastery
* [REDEE
M OFFER ](https://idownloadcoupon.com/udemy/9811/)Ethically Hack the Planet Part 4
* [REDEEM OFFER ](https://idownloadco
upon.com/udemy/10543/)350+ CSS Practice Tests & Interview Questions \[April. 2024\]
* [REDEEM OFFER ](https://idownloadc
oupon.com/udemy/10085/)Master Ruby Scripting with (Practice test only) for Hacking
* [REDEEM OFFER ](https://idownloadco
upon.com/udemy/10786/)Computer Science MetaBootcamp: Beginner to Intermediate 2024
* [REDEEM OFFER ](https://idownloadco
upon.com/udemy/10827/)Google Ads 2024: How to Drive Sales With PPC!

GET MORE FREE ONLINE COURSES WITH CERTIFICATE – [CL
ICK HERE](https://idownloadcoupon.com/)
```
---

     
 
all -  [ Your Free Personal AI Companion for Emotional Support ](https://v.redd.it/ka68j1xn1jed1) , 2024-07-25-0911
```
Excited to share my AI Companion project—a supportive, empathetic companion available via call, now enhanced with guided
 sleep sessions, meditation exercises, and mental health tools. It respects privacy, keeps no personal data, and aims to
 make mental health support accessible to all. Your feedback shapes its evolution—let's make companionship and well-bein
g tools more accessible!
```
---

     
 
all -  [ How to return similarity scores using retriever.get_relevant_documents(query) ](https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/) , 2024-07-25-0911
```
Hi I want to do metadata filtering first and then retrieve the document  
Code:

    langchain_chroma = Chroma(
        
        client=self.persistent_client,
                collection_name=self.COLLECTION_NAME,
                embedding_f
unction=self.embedding_function  # Use the variable containing the collection name
            )

    retriever = langch
ain_chroma.as_retriever(search_type='similarity',search_kwargs={'k': 1, 'filter': cond}) 
    query = 'What is patient f
amily Medical history in reverse cronological order?'
    res = retriever.get_relevant_documents(query)
    res

This is
 not returning scores, Whereas If use ,  


    res = langchain_chroma.similarity_search_with_score(query)

then i am ge
tting score as well but how to do metadata filtering here?





  

```
---

     
 
all -  [ Langchain vs LlamaIndex ](https://www.reddit.com/r/LlamaIndex/comments/1eb25ir/langchain_vs_llamaindex/) , 2024-07-25-0911
```
Hello guys I wondering what are the differences between Langchain and LlamaIndex? I am not asking about what’s best but 
I want to know when to use each one? Can you give me some advices and tips?
Thank you
```
---

     
 
all -  [ LangChain VS LangGraph: Git ](https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/) , 2024-07-25-0911
```
At the time of posting,

LangChain repository's `master` branch is

```
Cloning into 'langchain'...
remote: Enumerating 
objects: 137116, done.
remote: Counting objects: 100% (5275/5275), done.
remote: Compressing objects: 100% (481/481), do
ne.
remote: Total 137116 (delta 5003), reused 4829 (delta 4794), pack-reused 131841
Receiving objects: 100% (137116/1371
16), 224.32 MiB | 4.70 MiB/s, done.
Resolving deltas: 100% (101282/101282), done.
Updating files: 100% (7595/7595), done
.
```

and LangGraph repository's `main` branch is

```
Cloning into 'langgraph'...
remote: Enumerating objects: 10436, 
done.
remote: Counting objects: 100% (1815/1815), done.
remote: Compressing objects: 100% (1015/1015), done.
remote: Tot
al 10436 (delta 1090), reused 1371 (delta 774), pack-reused 8621
Receiving objects: 100% (10436/10436), 327.76 MiB | 3.1
3 MiB/s, done.
Resolving deltas: 100% (6828/6828), done.
```

For comparision, this is React's `main` brach is

```
Clon
ing into 'react'...
remote: Enumerating objects: 326918, done.
remote: Counting objects: 100% (813/813), done.
remote: C
ompressing objects: 100% (324/324), done.
remote: Total 326918 (delta 470), reused 718 (delta 422), pack-reused 326105
R
eceiving objects: 100% (326918/326918), 532.16 MiB | 5.97 MiB/s, done.
Resolving deltas: 100% (232896/232896), done.
```

and it doesn't even have rich text files like .ipynb.

There are couple of observations.
1. Maintaining an open-source 
repository with Jupyter Notebooks is not for easy, I think. Any updates to libraries used need notebooks to rerun and re
flect latest outputs. Even if there is no change in output, the git diff changes drastically. I have heard about nbdime 
but have no idea about it.
2. LangGraph repo is bigger in size than LangChain after decompressing.
   ```
   du -sh lang
graph
   475M	langgraph
   
   du -sh langchain
   459M	langchain```
   This size by du depends on multiple factors, blo
ck size being on of them.

What did you find interesting? Do share more insights and fun facts about the projects!
```
---

     
 
all -  [ Reverting back to planning node based on a condition ](https://www.reddit.com/r/LangChain/comments/1eazoaz/reverting_back_to_planning_node_based_on_a/) , 2024-07-25-0911
```
I am trying to make a planning agent using langgraph in which we can revert back to the planner node using conditions fr
om other agent nodes . I am stuck on the reverting back function.
```
---

     
 
all -  [ Request for Guidance  ](https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/) , 2024-07-25-0911
```
Hi all. I am an LLM enthusiast trying to use GGUF version of Llama 3.1 for summarisation task. 

I am using Q4_K_M model
 from this repo:
MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF
Link: https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1
-8B-Instruct-GGUF

I used the following code to load the model:
```
from langchain_community.llms import LlamaCpp
from l
angchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler

callback_manager = CallbackManager([Stre
amingStdOutCallbackHandler()])
n_gpu_layers = -1  
n_batch = 2048 

# Make sure the model path is correct for your syste
m!
llm = LlamaCpp(
    model_path='./Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf',
    n_gpu_layers=n_gpu_layers,
    n_ctx =
 32768,
    rope_freq_scale=0.25,
    temperature = 0,
    n_batch=n_batch,
    callback_manager=callback_manager,
    v
erbose=True,  # Verbose is required to pass to the callback manager
)
```

When I pass long inputs to this model and ins
truct it to summarise it, it just blabbers with random and repitive texts/numbers.

How do I resolve this. Requesting fo
r guidance.

(PS: Tried Rope_freq_scale with values 0.125, 0.25, 1, 4, 8. But they were not so good, even comparing to t
he above results)
```
---

     
 
all -  [ Llama 3.1 LangChain integration codes explained  ](https://www.reddit.com/r/ArtificialInteligence/comments/1eayapj/llama_31_langchain_integration_codes_explained/) , 2024-07-25-0911
```
This demo talks about how to use Llama 3.1 with LangChain to build Generative AI applications: https://youtu.be/LW64o3Yg
bE8?si=1nCi7Htoc-gH2zJ6
```
---

     
 
all -  [ Llama 3.1 using LangChain  ](/r/LangChain/comments/1eay7kz/llama_31_using_langchain/) , 2024-07-25-0911
```

```
---

     
 
all -  [ Easiest way to implement reranking in Langchain and Java ](https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/) , 2024-07-25-0911
```
i am using in memory vector database where I get scoring of responses. 

  
Now i want to implement reranking to get the
 most accurate responses. 

  
What would be the easiest way to implmement this in Java Langchain. 
```
---

     
 
all -  [ What is your LLM Stack? ](https://www.reddit.com/r/SmythOS/comments/1eawark/what_is_your_llm_stack/) , 2024-07-25-0911
```
I’ll go first!   
Vector database: Quadrant  
Orchestration: Haystack and llamaindex  
LLM: OpenAI GPT 4, LLAMA 70B(loca
l), Gemini Pro 1.5  
LLM Framework: Langchain or direct integration

What’s yours?


```
---

     
 
all -  [ How to customize the Chat Format LangChain uses for my specific LLM? ](https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/) , 2024-07-25-0911
```
Hello, I am starting to use LangChain and have a question, for which I did not find a response in the documentation.

Fr
om my understanding, each LLM is trained with a different *chat format* to separate AI and user messages. For instance, 
I am currently developing with Phi3 which uses the following format for AI messages: `<|assistant|>Assistant Message<|en
d|>`.

How can I pass some parameters to tell LangChain to use this format? Above all, is this handled by the `LLM` clas
s or by the `Message` class?

I make an example to make my point clearer. When I use the following code

    ChatPromptT
emplate.from_messages(
                [
                    ('system', 'Behave like this...'),
                    ('hu
man', '{input}'),
                ]
            )

How can I tell LangChain to insert `<|user|>` at the beginning of the
 user message? I do not see any parameter to pass to the `HumanMessage` object. 
```
---

     
 
all -  [ Long term memory for agents? ](https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/) , 2024-07-25-0911
```
Does anyone have best practices to share on implementing long term memory for agents? E.g., personalization based on cha
t history. Based on the memgpt paper it seems best practices would be to have a secondary agent that can read/write long
 term context into a database, like a Redis cache. Curious if  anyone has tuned a model for this? 
```
---

     
 
all -  [ Having problem with langchain loader ](https://www.reddit.com/r/datascience/comments/1eaien5/having_problem_with_langchain_loader/) , 2024-07-25-0911
```
I have the data in JSON format I’m trying to use the jsonloader but apparently I need a download and import a jq module 
and that’s where my problem is. I have pip installed jq but when it’s time to import it, I get a no module error and yes
 it’s installed in venv that I am working in. Has anyone had this problem before 
```
---

     
 
all -  [ Exciting News from Meta [Llama 3.1 is Here] ](https://www.reddit.com/r/LangChain/comments/1eageaq/exciting_news_from_meta_llama_31_is_here/) , 2024-07-25-0911
```
Meta has just released its latest LLM model, Llama 3.1, marking a significant step in accessible artificial intelligence
. Here are the key points from the announcement:

1. **405B version.** There is a new Llama 3.1 405B version. That’s rig
ht *405 Billion parameters.*
2. **Expanded context length**: Now all llama 3.1 models offer a context length of **128K t
okens**, 16 times its previous 8K context length from Llama 3. This allows for more advanced use cases, such as long-for
m text summarization, multilingual conversational agents, and coding assistants
3. **Model evaluations**: The model eval
uations released by Meta are as follows:

[Llama 405B](https://preview.redd.it/zjcxaf93jbed1.png?width=3201&format=png&a
uto=webp&s=31191792e788799899102d882d3170acc34ea19b)

[Llama 8B](https://preview.redd.it/h1x4jcy6jbed1.png?width=3201&fo
rmat=png&auto=webp&s=4fb34e2d110345a34e1715d16be8951d0edc637b)

**4. API Coming Soon:** Users will be able to access and
 utilize Llama 3.1 models through [awanllm.com](http://awanllm.com/) soon. Stay tuned for updates in this post!

Source:
 [https://ai.meta.com/blog/meta-llama-3-1/](https://ai.meta.com/blog/meta-llama-3-1/)
```
---

     
 
all -  [ MongoDB as vectorstore  ](https://www.reddit.com/r/LangChain/comments/1eaffzv/mongodb_as_vectorstore/) , 2024-07-25-0911
```
What are your thoughts on using MongoDB as vectorstore for your apps.

I was working on prototype locally for the most o
f its time but right now we are moving to hosting on streamlit, what are your recommendations for vectorstores.

```
---

     
 
all -  [ Looking for an opensource framework to manage agents ](https://www.reddit.com/r/LangChain/comments/1eaedlh/looking_for_an_opensource_framework_to_manage/) , 2024-07-25-0911
```
I want to give my client the option to construct new agents and create flows for input and output like vectorizing input
 and parsing output and storing it in a database. Is there any opensource tool with a UI that can do this?
The language 
it's written in doesn't really matter, all options are welcome.
```
---

     
 
all -  [ I build a RAG-based multi-tenant AI Code Assistant with OpenAI, LangChain, Postgres and PG Vector ](https://www.reddit.com/r/LangChain/comments/1eadv0e/i_build_a_ragbased_multitenant_ai_code_assistant/) , 2024-07-25-0911
```
I am rather new to the AI space (my background is data infrastructure), so I am documenting my journey as I'm learning. 
This time I built an AI Code Assistant that uses RAG to answer questions about different repositories.

I blogged everyt
hing I learned while building this - Schema design, use of LangChain (tbh, not sure it was a good choice...), choice of 
models, streaming chat UX...

You can see the app here: [https://code-assist-nile.vercel.app](https://code-assist-nile.v
ercel.app)  
And the blog: [https://www.thenile.dev/blog/building\_code\_assistant](https://www.thenile.dev/blog/buildin
g_code_assistant)  
The code is here: [https://github.com/niledatabase/niledatabase/tree/main/examples/ai/code\_assist/]
(https://github.com/niledatabase/niledatabase/tree/main/examples/ai/code_assist/)
```
---

     
 
all -  [ Forced function calling in vertex ai gemini?? ](https://www.reddit.com/r/LangChain/comments/1ead44f/forced_function_calling_in_vertex_ai_gemini/) , 2024-07-25-0911
```
Hello everyone,

I want to force function calling with Gemini. Does anyone know how to do this?

I have checked the docu
mentation for Vertex AI and Langchain but couldn't find any information. In the Vertex AI docs, I found a parameter that
 could be passed, but I don't know how to pass it using Langchain. I am using `ChatVertexAI().bind_tools()`.

Regards!
```
---

     
 
all -  [ What do you think are better alternatives to Pinecone Vector DB? ](https://www.reddit.com/r/SmythOS/comments/1eabnkj/what_do_you_think_are_better_alternatives_to/) , 2024-07-25-0911
```
I have tried using pinecone, usually with frameworks like Langchain and llama-index and I saw that pinecone doesn't appe
ar to support storing documents alongside your vectors so what I tried to do is actually cram snippets of the document i
nto the metadata, but the metadata is limited to something really really small, so the maximum document length gets cons
trained. 

If you use another database alongside pinecone and just want to retrieve uuids or something, it's fine, but i
t struck me as a very weird omission in their design. 
```
---

     
 
all -  [ Is Qdrant cloud Production Ready? ](https://www.reddit.com/r/LangChain/comments/1eaabwv/is_qdrant_cloud_production_ready/) , 2024-07-25-0911
```
Guys,

Has anybody used Qdrant in the cloud, especially Azure and has gone live and in production? We are trying to inse
rt 884 points with a production grade cluster in azure eastus and it takes about 6-8 seconds and that too with gRPC. Htt
p takes even longer.

We are absolutely sure that this is the time taken by Qdrant Remote Client provided by their offic
ial package because we have enabled all the logging and can pin-point which operation takes time.

We created a support 
ticket with the Qdrant team as well, but have been ghosted by them.  

Wondering if Qdrant is right choice and if it is,
 how do people insert points faster? We do have metadata and chunk text in the point. 
```
---

     
 
all -  [ Multi-agent-DataAnalysis AI-Driven Data Analysis System ](https://www.reddit.com/r/LangChain/comments/1ea7g23/multiagentdataanalysis_aidriven_data_analysis/) , 2024-07-25-0911
```
# # Advanced AI-Driven Data Analysis System: A LangGraph Implementation



## Project Overview



I've developed a sophi
sticated data analysis system that leverages the power of LangGraph, showcasing its capabilities in integrating various 
AI architectures and methodologies. This system is designed to serve as a comprehensive example of how LangGraph can be 
used to streamline complex data analysis tasks by orchestrating multiple AI agents and architectural patterns.

https://
preview.redd.it/lntq41fap9ed1.jpg?width=610&format=pjpg&auto=webp&s=7b2f31c450c495ebe2eb3d5a1e75522223ae1267

## Key Fea
tures



- \*\*LangGraph-Powered Architecture\*\*: The system demonstrates LangGraph's flexibility by incorporating:

  
- Supervisor agents for overseeing the analysis process

  - Chain-of-thought reasoning for complex problem-solving

  -
 Critic agents for quality assurance and error checking



- \*\*Innovative Note Taker Agent\*\*: A standout feature tha
t highlights LangGraph's extensibility:

  - Continuously records the current state of the project

  - Provides a more 
efficient alternative to transmitting complete historical information

  - Enhances the system's ability to maintain con
text and continuity across different analysis stages



- \*\*Adaptive Workflow\*\*: Showcases LangGraph's dynamic routi
ng capabilities, adjusting the analysis approach based on the data and task at hand.



## Why It's a Valuable LangGraph
 Example



This implementation serves as an excellent case study for LangGraph users by demonstrating:



1. Integratio
n of diverse AI agent types within a unified framework

2. Efficient state management using the innovative Note Taker ag
ent

3. Real-world application of LangGraph in complex data analysis scenarios



## Contribution to LangGraph



I am e
ager to contribute this project as an example in the official LangGraph repository. My goals are to:



- Provide a comp
rehensive, real-world example of LangGraph's capabilities

- Help other developers understand advanced LangGraph impleme
ntations

- Contribute to the growth and adoption of LangGraph in the AI community



## Project Repository



For a dee
per dive into the codebase, architecture, and implementation details, please visit the project's GitHub repository:



\
[AI-Driven Data Analysis System on GitHub\](https://github.com/starpig1129/Multi-agent-DataAnalysis)





I welcome feed
back and collaboration to refine this example for potential inclusion in the LangGraph documentation or example collecti
on.



## Next Steps



1. I am open to adapting the project to better align with LangGraph's documentation standards.


2. I would appreciate guidance on the best way to submit this as a potential official example.

3. I'm eager to collabor
ate with the LangGraph community to enhance this example and make it as valuable as possible for other users.



Feel fr
ee to explore the repository, and I look forward to any feedback or suggestions for improving this as a LangGraph exampl
e!


```
---

     
 
all -  [ Using ChatGPT Vision API with LangChain in JavaScript ](https://www.reddit.com/r/learnjavascript/comments/1ea7cxr/using_chatgpt_vision_api_with_langchain_in/) , 2024-07-25-0911
```
Made this short tutorial about using ChatGPT Vision API with LangChain in JavaScript

* [https://www.js-craft.io/blog/vi
sion-api-langchain-javascript/](https://www.js-craft.io/blog/vision-api-langchain-javascript/)

Hope you will find it us
eful and any feedback is welcomed!

PS: I think is was of the most fun examples I've recently worked on. Very easy to us
e and has huge product potential. Especially when compared with how things were with TensorFlow.js  [https://www.js-craf
t.io/blog/tensorflowjs-detect-multiple-objects-from-image-coco-ssd-1/](https://www.js-craft.io/blog/tensorflowjs-detect-
multiple-objects-from-image-coco-ssd-1/)
```
---

     
 
all -  [ Tool Calling tutorial for LangChain.js ](https://www.reddit.com/r/LangChain/comments/1ea79dp/tool_calling_tutorial_for_langchainjs/) , 2024-07-25-0911
```
Made this 2 part tutorial about Tool Calling in LangChain.js

* Part 1️: [https://www.js-craft.io/blog/tool-calling-lang
chain-js/](https://www.js-craft.io/blog/tool-calling-langchain-js/)
* Part 2: [https://www.js-craft.io/blog/tool-calling
-langchain-js-toolmessage-schemas/](https://www.js-craft.io/blog/tool-calling-langchain-js-toolmessage-schemas/)

Hope y
ou will find it useful and any feedback is welcomed!



PS: I think it was one of the most time-consuming tutorials to m
ake, as things here are not quite intuitive. At least for me :) 
```
---

     
 
all -  [ Master ve iş arasında tercih ](https://www.reddit.com/r/CodingTR/comments/1ea6wwu/master_ve_iş_arasında_tercih/) , 2024-07-25-0911
```
Merhabalar, ben 2024 haziran ayında sabancı üniversitesi bilgisayar mühendisliği bölümünden mezun oldum ve hali hazırda 
part time olarak çalıştığım şirkette 1 temmuz itibari ile full time olarak devam ettim. Şirkette web front end ekibindey
im ancak daha çok internal projelere bakıyorum ve full stack çalışıyorum bunun yanında da arge tarafında langchain gibi 
toollarla llmler ile uğraşıyorum. Şirkette uzaktan çalışma imkanı mevcut ve yeni mezun birine 3 asgari ücretten biraz fa
zla ödüyor. Bununla beraber 2024-2025 fall dönemi için yine sabancı üniversitesinde master kabulü almış bulunuyorum. Bu 
master kabulü burslu ancak burs için haftada en az 20 saat akademik çalışmalara katılmamı bekliyorlar. Bunlar öğrenciler
e soru çözümü sınav/ödev okuma gibi ekstra işler oluyor tabi bir de bunun yanında master tezi ya da paperlar vs için ara
ştırma  gerçekleştirmem bekleniyor. ikisini aynı anda götürmem pek mümkün görünmüyor ve master sektörde ne kadar önemli 
pek bir bilgim yok. Bu konudaki düşünceleriniz nedir sizce hangisini tercih etmek kariyerim açısında daha mantıklı ?
```
---

     
 
all -  [ txtai: Open-source vector search and RAG for minimalists ](https://www.reddit.com/r/Python/comments/1ea6t89/txtai_opensource_vector_search_and_rag_for/) , 2024-07-25-0911
```
txtai is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows. 

**What
 it does**

txtai was created back in 2020 starting with semantic search of medical literature. It has since grown into 
a framework for vector search, retrieval augmented generation (RAG) and large language model (LLM) orchestration/workflo
ws.

The goal of txtai is to be simple, performant, innovative and easy-to-use. It had vector search before many current
 projects existed. Semantic Graphs were added in 2022 before the Generative AI wave of 2023/2024. GraphRAG is a hot topi
c but txtai had examples of using graphs to build search contexts back in 2022/2023.

There is a commitment to quality a
nd performance, especially with local models. For example, it's vector embeddings component streams vectors to disk duri
ng indexing and uses mmaped arrays to enable indexing large datasets locally on a single node. txtai's BM25 component is
 built from the scratch to work efficiently in Python leading to 6x better memory utilization and faster search performa
nce than the BM25 Python library most commonly used.

I often see others complain about AI/LLM/RAG frameworks, so I want
ed to share this project as many don't know it exists.

Link to source (Apache 2.0): [https://github.com/neuml/txtai](ht
tps://github.com/neuml/txtai)

**Target Audience**

Developers who want to build AI/LLM/RAG applications with a simple a
pproach.

**Comparison**

txtai is similar to LangChain and LlamaIndex. From a vector database standpoint, it's similar 
to ChromaDB.   
  
See the following post for a detailed comparison.

[https://www.reddit.com/r/txtai/comments/1e5nw3t/v
ector\_search\_rag\_landscape\_a\_review\_with\_txtai/](https://www.reddit.com/r/txtai/comments/1e5nw3t/vector_search_ra
g_landscape_a_review_with_txtai/)
```
---

     
 
all -  [ RAG for JSON ](https://www.reddit.com/r/LangChain/comments/1ea5lco/rag_for_json/) , 2024-07-25-0911
```
Can we apply RAG to JSON files. Currently we are using unstructured for parsing different types of file types. But, it d
oesn't have integration with json file. Can anyone experienced this before?
```
---

     
 
all -  [ Applying RAG to Large-Scale Code Repositories - Guide ](https://www.reddit.com/r/LangChain/comments/1ea4nhz/applying_rag_to_largescale_code_repositories_guide/) , 2024-07-25-0911
```
The article discusses various strategies and techniques for implementing RAG to large-scale code repositories, as well a
s potential benefits and limitations of the approach as well as show how RAG can improve developer productivity and code
 quality in large software projects: [RAG with 10K Code Repos](https://www.codium.ai/blog/rag-for-large-scale-code-repos
/)
```
---

     
 
all -  [ Docker image with Ollama and langchain  ](https://www.reddit.com/r/LangChain/comments/1ea4ccw/docker_image_with_ollama_and_langchain/) , 2024-07-25-0911
```
Hi, 
Need help in finding a docker image containing both Ollama and langchain to ease the creation/development of use ca
ses. 
```
---

     
 
all -  [ Human In The Loop In Langgraph ](https://www.reddit.com/r/LangChain/comments/1ea3p2i/human_in_the_loop_in_langgraph/) , 2024-07-25-0911
```
    from langgraph_state import GraphState
    from langgraph.graph import END, StateGraph
    from langgraph.checkpoint
.memory import MemorySaver
    from nodes import build_strategy, human_feedback, decision_node, rag_node, database_searc
h, web_search_node, sql_search_node, state_printer
    from edges import rag_database_websearch_sqlseqrch, strategy_deci
sion
    workflow = StateGraph(GraphState)
    workflow.add_node('build_strategy', build_strategy)
    workflow.add_node
('human_feedback', human_feedback)
    workflow.add_node('decision_node', decision_node)
    workflow.add_node('rag_node
', rag_node)
    workflow.add_node('database_search', database_search)
    workflow.add_node('web_search_node', web_sear
ch_node)
    workflow.add_node('sql_search_node', sql_search_node)
    workflow.add_node('state_printer', state_printer)

    workflow.set_entry_point('build_strategy')
    workflow.add_edge('build_strategy', 'human_feedback')
    workflow.a
dd_conditional_edges(
    'human_feedback',
    strategy_decision,
    {
    'yes': 'decision_node',
    'no': 'build_st
rategy'
    },
    )
    workflow.add_conditional_edges(
    'decision_node',
    rag_database_websearch_sqlseqrch,
    
{
    'rag': 'rag_node',
    'database_search': 'database_search',
    'web_search': 'web_search_node',
    'sql_search'
: 'sql_search_node',
    'state_printer': 'state_printer'
    },
    )
    workflow.add_conditional_edges(
    'rag_node
',
    rag_database_websearch_sqlseqrch,
    {
    'rag': 'rag_node',
    'database_search': 'database_search',
    'web
_search': 'web_search_node',
    'sql_search': 'sql_search_node',
    'state_printer': 'state_printer'
    },
    )
    
workflow.add_conditional_edges(
    'database_search',
    rag_database_websearch_sqlseqrch,
    {
    'rag': 'rag_node'
,
    'database_search': 'database_search',
    'web_search': 'web_search_node',
    'sql_search': 'sql_search_node',
  
  'state_printer': 'state_printer'
    },
    )
    workflow.add_conditional_edges(
    'web_search_node',
    rag_datab
ase_websearch_sqlseqrch,
    {
    'rag': 'rag_node',
    'database_search': 'database_search',
    'web_search': 'web_s
earch_node',
    'sql_search': 'sql_search_node',
    'state_printer': 'state_printer'
    },
    )
    workflow.add_con
ditional_edges(
    'sql_search_node',
    rag_database_websearch_sqlseqrch,
    {
    'rag': 'rag_node',
    'database_
search': 'database_search',
    'web_search': 'web_search_node',
    'sql_search': 'sql_search_node',
    'state_printer
': 'state_printer'
    },
    )
    workflow.add_edge('state_printer', END)
    memory = MemorySaver()
    app = workflo
w.compile(checkpointer=memory, interrupt_before=['human_feedback'])

I have created a graph with human feedback using la
nggraph. After **strategy node** is executed. It should interrupt before **human\_feedback node** and ask from the user 
that if the strategy made by the agent is correct or wrong in case if it is correct it will proceed to the next nodes an
d if not then it will go to strategy node again.

For my first condition in case it is correct it is working fine. But w
hen it is wrong the strategy node executes but donot go to the next nodes and the program terminates.

This is the issue
 if anyone can help. Thanks
```
---

     
 
all -  [ Troubleshooting string Errors in LangChain ](https://www.reddit.com/r/LangChain/comments/1ea2inm/troubleshooting_string_errors_in_langchain/) , 2024-07-25-0911
```
Hi all,

I'm using the LangChain React agent infrastructure and encountering various string-related issues.   
For insta
nce, I often get errors like:

* 'The model produced invalid content. Consider modifying your prompt if you are seeing t
his error persistently.'
* 'Could not parse LLM output.'
* 'An output parsing error occurred.'

Currently, I have some a
d-hoc methods to fix the outputs, which involve a lot of string manipulation. I'm wondering if this is the right approac
h to handle these issues.

I tried using the handle\_parsing\_errors argument, but it doesn't seem to be a good solution
.

What do you think?
```
---

     
 
all -  [ CS RAG CHATBOT for a hardware e-com company ](https://www.reddit.com/r/LangChain/comments/1ea0qw5/cs_rag_chatbot_for_a_hardware_ecom_company/) , 2024-07-25-0911
```
I would love to get your expertise and advice on building a RAG chatbot for an e-commerce company. I'm currently explori
ng Graph-RAG and hybrid search, but I'm feeling overwhelmed by the amount of data. The company has about 100 products, a
long with data such as blogs, articles, FAQs, etc., which sometimes reference specific products. I would like to know ho
w I can move forward with this project. Any help is much appreciated.

Thanks!
```
---

     
 
all -  [ Make your own Intelligent Investment Analyst Agent ](https://www.reddit.com/r/LangChain/comments/1e9ys5f/make_your_own_intelligent_investment_analyst_agent/) , 2024-07-25-0911
```
Hey everyone! I’m excited to share a new project: an Investment Research Project leveraging CrewAI and Composio to condu
ct investment research, analyze data, and provide investment recommendations.

**Objectives**  
This project sets up 
a system of agents to streamline investment research and analysis, ultimately generating insightful investment recommend
ations.  
Implementation Details

**Tools Used**  
Composio, CrewAI, SERP, Python

**Setup**

1. Navigate to the
 project directory.
2. Run the setup file.
3. Fill in the `.env` file with your secrets.
4. Run the Python script.
5
. Alternatively, run the IPython Notebook `investment_analyst.ipynb` in Jupyter for an interactive experience.

**Resu
lts**  
The system will populate your Notion page with comprehensive investment data and analysis.

**Repo**: [GitHub
 Repository](https://git.new/Invest)

Feel free to explore the project, give it a star if you find it useful, and let 
me know your thoughts or suggestions for improvements!
```
---

     
 
all -  [ Lightweight python DAG framework ](https://www.reddit.com/r/Python/comments/1e9wrve/lightweight_python_dag_framework/) , 2024-07-25-0911
```
**What my project does:**

[https://github.com/dagworks-inc/hamilton/](https://github.com/dagworks-inc/hamilton/) I've b
een working on this for a while.

If you can model your problem as a [directed acyclic graph (DAG)](https://en.wikipedia
.org/wiki/Directed_acyclic_graph) then you can use Hamilton; it just needs a python process to run, no system installati
on required (\`pip install sf-hamilton\`).

For the pythonistas, Hamilton does some  cute 'meta programming' by using th
e python functions to \_really\_ reduce boilerplate for defining a DAG. The below defines a DAG by the way the functions
 are named, and what the input arguments to the functions are, i.e. it's a 'declarative' framework.:

    #my_dag.py
   
 def A(external_input: int) -> int:
       return external_input + 1
    
    def B(A: int) -> float:
       '''B depend
s on A'''
       return A / 3
    
    def C(A: int, B: float) -> float:
       '''C depends on A & B'''
       return A
 ** 2 * B

Now you don't call the functions directly (well you can it is just a python module), that's where Hamilton he
lps orchestrate it:

    from hamilton import driver
    import my_dag # we import the above
    
    # build a 'driver'
 to run the DAG
    dr = (
       driver.Builder()
         .with_modules(my_dag)
        #.with_adapters(...) we have m
any you can add here. 
         .build()
    )
    
    # execute what you want, Hamilton will only walk the relevant pa
rts of the DAG for it.
    # again, you 'declare' what you want, and Hamilton will figure it out.
    dr.execute(['C'], 
inputs={'external_input': 10}) # all A, B, C executed; C returned
    dr.execute(['A'], inputs={'external_input': 10}) #
 just A executed; A returned
    dr.execute(['A', 'B'], inputs={'external_input': 10}) # A, B executed; A, B returned.
 
   
    # graphviz viz
    dr.display_all_functions('my_dag.png') # visualizes the graph.

Anyway I thought I would shar
e, since it's broadly applicable to anything where there is a DAG:

* web requests (Hamilton has [async support](https:/
/github.com/dagworks-inc/hamilton/tree/main/examples/async))
* data processing (e.g. [pyspark](https://github.com/DAGWor
ks-Inc/hamilton/tree/main/examples/spark))
* [machine learning ](https://github.com/DAGWorks-Inc/hamilton/tree/main/exam
ples/spark)
* [LLM workflows](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows)
* etc.

I also 
recently curated a bunch of [getting started issues](https://github.com/DAGWorks-Inc/hamilton/issues?q=is%3Aissue+is%3Ao
pen+label%3A%22good+first+issue%22) - so if you're looking for a project, come join.

**Target Audience**

This anyone d
oing python development where a DAG could be of use.

More specifically, Hamilton is built to be taken to production, so
 if you value one or more of:

* self-documenting readable code
* unit testing & integration testing
* data quality
* st
andardized code
* modular and maintainable codebases
* hooks for platform tools & execution
* want something that [can w
ork with Jupyter Notebooks](https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/jupyter_notebook_magic/example.i
pynb) & production.
* etc

Then Hamilton has all these in an accessible manner.

**Comparison**

|Project|Comparison to 
Hamilton|
|:-|:-|
|Langchain's LCEL|LCEL isn't general purpose & in my opinion unreadable.  See [https://hamilton.dagwor
ks.io/en/latest/code-comparisons/langchain/](https://hamilton.dagworks.io/en/latest/code-comparisons/langchain/) .|
|Air
flow / dagster / prefect / argo / etc|Hamilton doesn't replace these. These are 'macro orchestration' systems (they requ
ire DBs, etc), Hamilton is but a humble library and can actually be used with them! In fact it ensures **your code can r
emain decoupled & modular**, enabling reuse across pipelines, while also enabling one to no be heavily coupled to any ma
cro orchestrator.|
|Dask|Dask is a whole system. In fact [Hamilton integrates with Dask very nicely](https://github.com/
DAGWorks-Inc/hamilton/tree/main/examples/dask/hello_world) -- and can help you organize your dask code.|

If you have mo
re you want compared - leave a comment.

To finish, if you want to try it in your browser using pyodide @ [https://www.t
ryhamilton.dev/](https://www.tryhamilton.dev/) you can do that too!
```
---

     
 
all -  [ Beginners guide for GraphRAG ](https://www.reddit.com/r/LanguageTechnology/comments/1e9wdpa/beginners_guide_for_graphrag/) , 2024-07-25-0911
```
GraphRAG has been the talk of the town since Microsoft released the viral gitrepo on GraphRAG, which uses Knowledge Grap
hs for the RAG framework to talk to external resources compared to vector DBs as in the case of standard RAG. The below 
YouTube playlist covers the following tutorials to get started on GraphRAG

1. What is GraphRAG?

2. How GraphRAG works?


3. GraphRAG using LangChain

4. GraphRAG for CSV data

5. GraphRAG for JSON

6. Knowledge Graphs using LangChain

7. R
AG vs GraphRAG

[https://www.youtube.com/playlist?list=PLnH2pfPCPZsIaT48BT9zmLmkhYa\_R1PhN](https://www.youtube.com/play
list?list=PLnH2pfPCPZsIaT48BT9zmLmkhYa_R1PhN)
```
---

     
 
MachineLearning -  [ [D] Embedding generation in production? How are you doing it? ](https://www.reddit.com/r/MachineLearning/comments/1e7xt6k/d_embedding_generation_in_production_how_are_you/) , 2024-07-25-0911
```


For those building production RAG pipelines, how are you generating embeddings. More than which model, I'm interested 
in how your deploying it. Are you calling the openai/vertex API endpoint directly? Using langchain/llamaindex wrappers? 
Using vectordb  classes? Or some other way?
```
---

     
 
MachineLearning -  [ [D] Is Anyone Else Setting Up Real-Time Django Workers for their AI Application? What's the best way ](https://www.reddit.com/r/MachineLearning/comments/1e0qens/d_is_anyone_else_setting_up_realtime_django/) , 2024-07-25-0911
```
We completely underestimated this one tbh, thought it would be much more straight forward. But we've done it now and doc
umented how step by step [in this article series](https://medium.com/p/5828a1ea43a3).

A bit of context, we're building 
a mini free AI Agent that auto-generates manually customisable plots, so the user can basically style however they want.
 It needs to be cost effective and efficient, so we thought about how to do it and tested a couple other ways.

https://
preview.redd.it/cmly0a6bhwbd1.png?width=640&format=png&auto=webp&s=be1f5b2853e744adcaf8013e6d43b43f6be89617

We plan on 
releasing the project open source, so all feedback welcome! Is anyone else doing this and has any feedback? or do know o
f a better way to do it?
```
---

     
 
MachineLearning -  [ [P] Real Time AI Workers Web Application ](https://www.reddit.com/r/MachineLearning/comments/1dzryk9/p_real_time_ai_workers_web_application/) , 2024-07-25-0911
```
Hi everyone!

I've created a mini series on how to build a real time AI application using Django, LangChain and Celery.


Free knowledge - posting it in here for anyone working on something similar and had the same blockers I had when buildi
ng.

Let me know what you think on how I could potentially improve this architecture.

Part 1

[https://medium.com/towar
dsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79](https://medium.
com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79)

Part 
2

[https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5
828a1ea43a3](https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time
-django-5828a1ea43a3)

Part 3

[https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-red
is-docker-real-time-django-8e73c7b6b4c8](https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-cha
nnels-redis-docker-real-time-django-8e73c7b6b4c8)

Part 4

[https://medium.com/towardsdev/how-to-set-up-django-from-scra
tch-with-celery-channels-redis-docker-real-time-django-c090c300517a](https://medium.com/towardsdev/how-to-set-up-django-
from-scratch-with-celery-channels-redis-docker-real-time-django-c090c300517a)

Part 5  
[https://medium.com/@cubode/how-
to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-0845eb7e083c](https://medium.com/@cubod
e/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-0845eb7e083c)
```
---

     
 
deeplearning -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-07-25-0911
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
