 
all -  [ Reading data from multiple datatypes ](https://www.reddit.com/r/LangChain/comments/1cfkr6u/reading_data_from_multiple_datatypes/) , 2024-04-29-0910
```
I am working on a small project where i have pdf files and .md files. I am reading md files using TextLoader to separate
 on '#' and it works well. How do i read pdf files, should i read all of them using a common loader ? is there a way to 
do it separately?
```
---

     
 
all -  [ Using Langchain to create a writing/style guide ](https://www.reddit.com/r/LangChain/comments/1cfkcbj/using_langchain_to_create_a_writingstyle_guide/) , 2024-04-29-0910
```
I want to create a tool to rewrite text based on content and style guidelines (ex: use this word instead of this, readin
g level, etc.). Is there a way to do this easily with Langchain / ex give it some docs of all the vocabulary it should u
se and a list of style rules?
```
---

     
 
all -  [ Langchain with Azure OpenAI gpt4 ](https://i.redd.it/d9e3rud6daxc1.jpeg) , 2024-04-29-0910
```
Iâ€™ve been recently trying to get (title) working on a simple python file - just by following the docs - however no matte
r what YouTube video or documentation I follow, it seems I always get that the error shown in the attached photo. 

Iâ€™m 
confused what to do - there must definitely be a way to use langchain with gpt 4 Azure OpenAI. 

Thanks in advance!
```
---

     
 
all -  [ LLMs Or What Even Are Those? ](https://www.reddit.com/r/LangChain/comments/1cff5pa/llms_or_what_even_are_those/) , 2024-04-29-0910
```
**Large Language Models**, or LLMs, are advanced AI systems that enhance text prediction to an exceptional level â€” imagi
ne the autocorrect & text prediction on your phone, but far more sophisticated.

When you type 'I am going to the...', y
our phone might suggest words like 'store' or 'gym.', based on the words you wrote before. LLMs operate similarly, but o
n a much larger scale, using vast amounts of text to predict and generate language accurately.

**The Core Pillars of LL
Ms are:**

* **Transformer Models**Â - the backbone of most LLMs, these models process data by breaking down input text i
nto smaller parts (tokens) and analyzing the relationships between them. This helps the model understand and generate la
nguage based on the context provided.Just like our brain uses neurons to process and relay information, transformer mode
ls use tokens to process and generate language, making sense of the input based on context.
* **Training**Â - LLMs learn 
by consuming vast amounts of text data, from websites like Wikipedia to books and articles. This training allows them to
 understand language patterns and context, and, as a result, generate better text.Itâ€™s just like reading hundreds of boo
ks to enhance your knowledge and master a subject, we feed LLMs with text data from diverse sources like Wikipedia and v
arious books to help them learn, though with a small caveat â€” LLMs can do this anywhere from 100-1000 timesÂ *faster*Â tha
n us.
* **Fine-tuning**Â - after their initial training, LLMs can be fine-tuned with specific data sets to perform tasks 
like translation, content generation, or even coding.With fine-tuning, youâ€™re giving your little helper a specific role 
& legend to fill â€” for example, 'Sir Code-a-lotâ€, who, after his rigorous initial training, is now sharpening the specif
ic skills needed to slay the mighty dragons in the C++ Language.

And if you want to see how different your autocorrect 
& text prediction on your phone is from actual Large Language Models â€“ then hereâ€™s a cool visual showing the sheer scale
 of the various GPT LLMs Essentially, LLMs predict what comes next, depending on the context & your input. If youâ€™re a p
rogrammer and youâ€™re writing code in Python, and use an LLM-powered code editor, the model understands every line of cod
e youâ€™ve written and suggests the next one accurately!

**The History of LLMs & Transformers**

The evolution of LLMs (*
*Large Language Models**) began with the introduction of the Transformer model by Google at NeurIPS 2017.Â 

This model i
ntroduced a new approach called 'attention mechanisms' that improves how machines understand the context within text. Ba
sically, a Transformer allows the model to focus on different parts of the input data at different times, improving its 
ability to generate accurate and contextually appropriate responses.

This model led to significant developments such as
 BERT and GPT models. GPT models, starting from GPT-1 to the latest iterations like GPT-3.5 and GPT-4, have significantl
y advanced in capabilities, achieving tasks that range from simple text generation to complex decision-making and proble
m-solving tasks.

And you know whatâ€™s the best part about LLMs becoming mainstream?Â 

Nearly every SaaS company is lever
aging them by building apps to solve the problems we creators & entrepreneurs face daily â€“ responding to emails, schedul
ing meetings, finding time for family and leisure, data entry, everything you could imagine â€” thereâ€™s an LLM-based tool 
for it now.
```
---

     
 
all -  [ Parquet format to vectors ](https://www.reddit.com/r/LocalLLaMA/comments/1cfduko/parquet_format_to_vectors/) , 2024-04-29-0910
```
So, I have definitely struggled with trying to create embeddings locally for Parquet files. 
Do I really have to convert
 to CSV/Json. 

Always have trouble using FAISS locally, in most langchain based programs, may be an issue of me not und
erstanding paths to load and retrieve from. Which is very possible. 

Long Story short I have about 60 Parquet files (av
eraging about 150k rows each) 
I was wanting to test them with the new Snowflake embeddings models.


I mostly do low co
de//no code solutions to test things out, and am generally pretty comfortable in them. 
```
---

     
 
all -  [ Has langchain become mature for production environments? ](https://www.reddit.com/r/LangChain/comments/1cfbqf9/has_langchain_become_mature_for_production/) , 2024-04-29-0910
```

```
---

     
 
all -  [ Research topics in LLMs for a data scientist ](https://www.reddit.com/r/datascience/comments/1cfaga5/research_topics_in_llms_for_a_data_scientist/) , 2024-04-29-0910
```
Hi everyone,

In my experience, my company does a lot of work on LLMs and I can say with absolute certainty that those p
rojects are permutations and combinations of making an intelligent chatbot which can chat with your proprietary document
s, summarize information, build dashboards and so on. I've prototyped these RAG systems (nothing in production, thankful
ly) and am not enjoying building them. I also don't like the LLM framework wars (Langchain vs Llamaindex vs this and tha
t - although, Langchain sucks in my opinion).   


What I am interested in putting my data scientist / (fake) statistici
an hat back on and approach LLMs (and related topics) from a research perspective. What are the problems to solve in thi
s field? What are the pressing research questions? What are the topics that I can explore in my personal (or company) ti
me beyond RAG systems?

Finally, can anyone explain what the heck is agentic AI? Is it just a fancy buzzword for this se
ntence from Russell and Norvig's magnum opus AI book- ' A rational agent is one that acts so as to achieve the best outc
ome or, when there is uncertainty, the best expected outcome'.   


  


&#x200B;
```
---

     
 
all -  [ [0 YoE] Web Developer to Data Science. 4 months of applying, but ZERO interviews. ](https://www.reddit.com/r/EngineeringResumes/comments/1cfa81d/0_yoe_web_developer_to_data_science_4_months_of/) , 2024-04-29-0910
```
Hi,

I am a fullstack web developer with 3 years of experience in the field. Recently i wanted to switch to Data Science
 and so i completed my masters in Data Science in January.

I've been applying for jobs and doing self projects on the s
ide to make up for the lack of experience, but so far, I have not been successful in landing even a single interview.

I
t's really starting to afffect me mentally. I know that the market isn't so great right now.

But i want to get a job am
ongst all these hurdles.

I do not know if my resume is the problem or something else.  
It would be great if you could 
please help me with this and help me land atleast one interview.

&#x200B;

&#x200B;

https://preview.redd.it/4nk3kgu0x8
xc1.png?width=5100&format=png&auto=webp&s=69da8298e4bbac4091e89feb56a3fcac43e2b2d5
```
---

     
 
all -  [ How LangChain and ChatGPT plugins are getting attacked by this bug ](https://www.reddit.com/r/bugbounty/comments/1cf9wov/how_langchain_and_chatgpt_plugins_are_getting/) , 2024-04-29-0910
```
[https://journal.hexmos.com/insecure-output-handling/](https://journal.hexmos.com/insecure-output-handling/)
```
---

     
 
all -  [ How LangChain and ChatGPT plugins are getting attacked by this bug ](https://www.reddit.com/r/LangChain/comments/1cf9wl3/how_langchain_and_chatgpt_plugins_are_getting/) , 2024-04-29-0910
```
[https://medium.com/@sreedeep200/how-langchain-and-chatgpt-plugins-are-getting-attacked-by-this-bug-9a47807b66a3](https:
//medium.com/@sreedeep200/how-langchain-and-chatgpt-plugins-are-getting-attacked-by-this-bug-9a47807b66a3)
```
---

     
 
all -  [ How LangChain and ChatGPT plugins are getting attacked by this bug ](https://www.reddit.com/r/ChatGPT/comments/1cf9qnf/how_langchain_and_chatgpt_plugins_are_getting/) , 2024-04-29-0910
```
[https://journal.hexmos.com/insecure-output-handling](https://journal.hexmos.com/insecure-output-handling)
```
---

     
 
all -  [ Leveling up RAG ](https://www.reddit.com/r/LangChain/comments/1cf97bh/leveling_up_rag/) , 2024-04-29-0910
```
Hey guys, need advice on techniques that really elevate rag from naive to an advanced system. I've built a rag system th
at scrapes data from the internet and uses that as context. I've worked a bit on chunking strategy and worked extensivel
y on cleaning strategy for the scraped data, query expansion and rewriting, but haven't done much else. I don't think I 
can work on the metadata extraction aspect because I'm using local llms and using them for summaries and QA pairs of the
 entire scraped db would take too long to do in real time. Also since my systems Open Domain, would fine-tuning the embe
dding model be useful? Would really appreciate input on that. What other things do you think could be worked on (impress
ive flashy stuff lol)

I was thinking hybrid search but then I'm also hearing knowledge graphs are great? idk. Saw a pap
er that just came out last month about context-tuning for retrieval in rag - but can't find any implementations or disco
urse around that. Lot of ramble sorry but yeah basically what else can I do to really elevate my RAG system - so far I'm
 thinking better parsing - processing tables etc., self-rag seems really useful so maybe incorporate that?
```
---

     
 
all -  [ How LangChain and ChatGPT plugins are getting attacked by this bug ](https://www.reddit.com/r/developersIndia/comments/1cf8cgf/how_langchain_and_chatgpt_plugins_are_getting/) , 2024-04-29-0910
```
[https://journal.hexmos.com/insecure-output-handling](https://journal.hexmos.com/insecure-output-handling/)
```
---

     
 
all -  [ LangChain Wrapper for easy RAG Deployments ](https://www.reddit.com/r/LangChain/comments/1cf7q6y/langchain_wrapper_for_easy_rag_deployments/) , 2024-04-29-0910
```
Hey guys, I tested this app called talkdai/dialog on Github, and it allowed me to deploy a RAG with my customized conten
t in just some few minutes and a Docker-compose file.

It's totally based on langchain right now, and with a toml file w
ith my prompt and model settings, I was able to deploy it online using caddy and a simple PGVector instance.

Is there a
ny other application that does that?

Here is the link for the source code: [https://github.com/talkdai/dialog](https://
github.com/talkdai/dialog)
```
---

     
 
all -  [ Recommend me some courses for LLM ](https://www.reddit.com/r/Automate/comments/1cf5j96/recommend_me_some_courses_for_llm/) , 2024-04-29-0910
```

I recently tried to make a chatbot, and it was really frustrating to have chatgpt not work (idk why but it just couldn'
t answer langchain questions , maybe the training cutoff date) , the docs are not so well arranged... And even if I do s
omehow get the code to work, it does not perform very well bcz I don't know much in the first place, I have a theoretica
l understanding of ML, but idk what are the diff kind of chains, retrievers, agents... I just find it to be a lot of thi
ngs which are scattered all over the place


So, can someone pls recommend me a course on langchain which consolidates a
ll the different techniques (chains, agents, vectordb etc.) And goes a bit in depth for everything, like how does this c
hain work or the diff methods of querying to the vectordb... 
Also feel free to recommend courses other than langchain, 
it's just langchain is the only LLM framework I know... 

```
---

     
 
all -  [ Recommend me some courses for LLM ](https://www.reddit.com/r/MLQuestions/comments/1cf5egs/recommend_me_some_courses_for_llm/) , 2024-04-29-0910
```
I recently tried to make a chatbot, and it was really frustrating to have chatgpt not work (idk why but it just couldn't
 answer langchain questions , maybe the training cutoff date) , the docs are not so well arranged... And even if I do so
mehow get the code to work, it does not perform very well bcz I don't know much in the first place, I have a theoretical
 understanding of ML, but idk what are the diff kind of chains, retrievers, agents... I just find it to be a lot of thin
gs which are scattered all over the place


So, can someone pls recommend me a course on langchain which consolidates al
l the different techniques (chains, agents, vectordb etc.) And goes a bit in depth for everything, like how does this ch
ain work or the diff methods of querying to the vectordb... 
Also feel free to recommend courses other than langchain, i
t's just langchain is the only LLM framework I know... 

```
---

     
 
all -  [ What web scraper for web search agent?  ](https://www.reddit.com/r/LangChain/comments/1cf2dwc/what_web_scraper_for_web_search_agent/) , 2024-04-29-0910
```
Hi everyone... 

I build an advanced RAG pipeline, and that include an agent that should get data from web, opening link
s from web search results... Anyway, I've zero past experience with web scraping, and my html knowledge is really basic.
 I'm going mad trying to extract the main text from web pages without lot of noise from tag, headers and other UI elemen
ts. 
As temporary solution, I added an llm agent 'in the middle', using it to clean the scraped text... But that's slow,
 expensive (using cloud providers) and fondamentally inefficient. 

Someone can give me some tips/help? There is some li
brary, repo or framework that may help me? 

Any kind of replay will be really appreciate! 

Thanks in advance for your 
time. 
```
---

     
 
all -  [ Managing Slug Size on Heroku: Excluding Unnecessary Dependencies ](https://www.reddit.com/r/Heroku/comments/1cf1eaj/managing_slug_size_on_heroku_excluding/) , 2024-04-29-0910
```
I'm encountering a recurring issue with slug size limitations on Heroku, primarily driven by unnecessary dependencies of
 the Langchain library

Despite implementing aÂ `.slugignore`Â file, the slug is still >3GB, which is much too large.

Aft
er following [this Heroku support guide](https://help.heroku.com/KUFMEES1/my-slug-size-is-too-large-how-can-i-make-it-sm
aller), I identified the main culprits as the `torch`Â andÂ `scipy` dependencies of `langchain`.  
  
I can get it to work
 locally by running `pip freeze > requirements.txt` , excluding  `torch`Â andÂ `scipy`, from `requirements.txt`,  then run
ning `pip install -r requirements --no-deps`.

This installs all of the necessary libraries without `torch`Â andÂ `scipy`.


The problem is: **I cannot replicate this on Heroku.** i.e. I can't find a way to install `langchain` and its \*requir
ed\* dependencies while excluding non-essential dependencies for my use case, namely `torch`Â andÂ `scipy`.  
  
Here's wh
at I've tried so far:

1. **Initial Attempt:**
   * I modified my deployment process to includeÂ `release: pip install -r
 requirements --no-deps`Â in the Procfile after excludingÂ `torch`Â andÂ `scipy`Â from my dependencies.
   * However, this ap
proach did not prevent these libraries from being installed, as it seems the command in the Procfile is executed *in add
ition* to the standardÂ `pip install -r requirements.txt` (which I believe [this guide](https://devcenter.heroku.com/arti
cles/release-phase) confirms).
2. **Subsequent Strategy:**
   * I consolidated all necessary dependencies intoÂ `requirem
ents.txt`, excludingÂ `torch`Â andÂ `scipy`.
   * I then attempted to useÂ `release: pip install -r requirements-langchain.t
xt --no-deps`Â in my ` Procfile` for Langchain-specific dependencies (i.e. only `langchain` and its required dependencies
).
3. **Resulting Error:**
   * Post-implementation, the application failed to build, throwing aÂ `ModuleNotFoundError`Â f
orÂ `langchain_openai`. The logs indicated a transition from state up to crashed due to this error, detailed as follows: 
`ModuleNotFoundError: No module named 'langchain_openai'` (note: this was explicitly included in ` requirements-langchai
n.txt`)
4. **Dependency Management:**
   * Despite ensuring all Langchain dependencies were listed inÂ `requirements-lang
chain.txt`Â and seemingly installed via the Procfile command, runtime errors suggest these modules are not accessible dur
ing execution.

I suspect there might be a misunderstanding or misconfiguration with my use of theÂ `release:`Â command in
 my `Procfile`, affecting how dependencies are managed and recognized at runtime.  
  
I'd love some guidance on configu
ring my deployment to avoid the installation of superfluous large dependencies while ensuring all necessary libraries ar
e correctly recognized and accessible by the application.

Thank you very much for your help.
```
---

     
 
all -  [ Has anyone utilized agents for document summarization and information extraction?  ](https://www.reddit.com/r/LangChain/comments/1ceztk1/has_anyone_utilized_agents_for_document/) , 2024-04-29-0910
```
Specifically, legal documents.
```
---

     
 
all -  [ Use Langchain vs individual LLM API in an npm library ](https://www.reddit.com/r/LangChain/comments/1ceyzjy/use_langchain_vs_individual_llm_api_in_an_npm/) , 2024-04-29-0910
```
Hi I am writing a Nodejs library that uses LLM to process documents. I plan to support LLMs in OpenAI, Groq, Ollama. Is 
it a good practice to directly to use Langchain or Llama Index in my npm library and introduce it as a dependency?

Yes?
 (the code will be simpler and supporting multiple LLMs out of the box) today I do use Langchain in my bigger project th
at includes the code I want to split into this library.

Or shall I use separate LLM APIs like OpenAiâ€™s directly. Or may
be try Llama Index

Any feedback is welcome ðŸ™ 
```
---

     
 
all -  [ Does llama-index and langchain Knowledge Graph RAG actually depends on the LLMs capability to follow ](https://www.reddit.com/r/LocalLLaMA/comments/1ceyuww/does_llamaindex_and_langchain_knowledge_graph_rag/) , 2024-04-29-0910
```
I was trying to build a custom RAG pipeline to fetch contextual data from an already existing Knowledge Graph (NebulaGra
ph DB).
What i noticed from both llama-index and langchain packages was that they actually use the LLM to generate the g
raph cypher and then execute the cypher on the provided graph store.

Does this not limit the entire efficiency as if th
e LLM did not correctly follow the instruction and couldnâ€™t  generate the cypher correctly?

Found a way to generate ind
ex embeddings in neo4j and retrieve documents off similarity search, which does not work very efficiently in fetching al
l the connected information from the graph.

So the big question.
Is there a  better way to implement KG Based RAG?
```
---

     
 
all -  [ How to quickly build and deploy scalable enterprise-grade RAG applications? ](https://www.reddit.com/r/LanguageTechnology/comments/1cetz9h/how_to_quickly_build_and_deploy_scalable/) , 2024-04-29-0910
```
While RAG is undeniably impressive, the process of creating a functional application with it can be daunting. There's a 
significant amount to grasp regarding implementation and development practices, ranging from selecting the appropriate A
I models for the specific use case to organizing data effectively to obtain the desired insights. While tools like LangC
hain and LlamaIndex exist to simplify the prototype design process, there has yet to be an accessible, ready-to-use open
-source RAG template that incorporates best practices and offers modular support, allowing anyone to quickly and easily 
utilize it.  
TrueFoundry has recently introduced a new open-source framework called [Cognita](https://github.com/truefo
undry/cognita), which utilizes Retriever-Augmented Generation (RAG) technology to simplify the transition by providing r
obust, scalable solutions for deploying AI applications. AI development often begins in experimental environments such a
s Jupyter notebooks, which are useful for prototyping but not well-suited for production environments. However, Cognita 
aims to bridge this gap. Developed on top of Langchain and LlamaIndex, Cognita offers a structured and modular approach 
to AI application development. Each component of the RAG, from data handling to model deployment, is designed to be modu
lar, API-driven, and extendable.
```
---

     
 
all -  [ How to make LSTM for local LLMs? ](https://www.reddit.com/r/ChatGPT/comments/1cepxkd/how_to_make_lstm_for_local_llms/) , 2024-04-29-0910
```
LangChain example for PDF Q&A:
```
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter imp
ort RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_community.embeddings import 
GPT4AllEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import LlamaCpp

# Load website
 data
loader = WebBaseLoader('https://lilianweng.github.io/posts/2023-06-23-agent/')
data = loader.load()

# Split text 
into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_d
ocuments(data)

# Generate embeddings locally using GPT4All
gpt4all_embeddings = GPT4AllEmbeddings()
vectorstore = Chrom
a.from_documents(documents=docs, embedding=gpt4all_embeddings)

# Load GPT4All model for inference  
llm = LlamaCpp(
   
 model_path='/Users/yuki/Documents/Github/yuna-ai/lib/models/yuna/yuna-ai-v2-q6_k.gguf',
    temperature=0.4,
    max_ne
w_tokens=256,
    context_window=2048,
    verbose=True,
)
# Create retrieval QA chain
qa = RetrievalQA.from_chain_type(
llm=llm, chain_type='stuff', retriever=vectorstore.as_retriever())

# Ask a question 
query = 'What are the approaches t
o Task Decomposition?'
result = qa.invoke(query)
print(result)
```

Hello everyone. I have this example of Q&A on a larg
e text using RAG. But if I want just long term memory with the modelâ€™s answers instead of Q&A what should I use?

Exampl
e of usage: the chat history is 30K tokens, and model should remember that we went to the party even if it was in the be
ginning of the conversation. But also be talk directly, not just only Q&A

Note: no OpenAI, everything local and open-so
urce.
```
---

     
 
all -  [ Trigger a firebase function from another function ](https://www.reddit.com/r/Firebase/comments/1cepbft/trigger_a_firebase_function_from_another_function/) , 2024-04-29-0910
```
I'm trying to create a function trigerring another to make a chain but I don't understand how to do it inside. Here is m
y code:

    import * as admin from 'firebase-admin'
    import * as firebaseFunctions from 'firebase-functions'
    imp
ort * as OpenAI from 'openai'
    import * as logger from 'firebase-functions/logger'
    import mustache = require('mus
tache')
    import { ChatAnthropicMessages } from '@langchain/anthropic'
    import functions, { getFunctions } from 'fi
rebase/functions'
    import { getApp, getApps } from 'firebase/app'
    import { initializeApp } from 'firebase-admin'

    import { onMessagePublished } from 'firebase-functions/v2/pubsub'
    
    // Firebase Admin SDK to access Firestore
.
    admin.initializeApp()
    
    // Initialize Firebase for SSR
    const app = initializeApp()
    const db = admin
.firestore()
    /**
     * Create the story entry
     */
    export const createStoryReference = firebaseFunctions.htt
ps.onCall(
      async (data, context) => {
        const owner = context.auth?.uid
    
        const doc = await db.co
llection('stories').add({
          owner: owner,
          inputs: data,
        })
        const createTitle = functio
ns.httpsCallable(
          getFunctions(app),
          'createTitle'
        )
        createTitle({ id: doc.id })
   
     return doc.id
      }
    )

I think i'm using the wrong library. though I'm also lost with the imports...
```
---

     
 
all -  [ Agent tool to work with rest API ](https://www.reddit.com/r/LangChain/comments/1ceonep/agent_tool_to_work_with_rest_api/) , 2024-04-29-0910
```
Hi, I am trying to fire out how to create tool for agent to work with Simple rest API (build with Fast API, no auth). I 
am just learning and couldn't find practical implementation. I've read about using API chain but My api have 4 endpoints
. It's really basic one 
```
---

     
 
all -  [ LangChain client connection error ](https://www.reddit.com/r/LangChain/comments/1cennxi/langchain_client_connection_error/) , 2024-04-29-0910
```
I keep getting this error when using LangSmith:  
HTTPError: \[Errno 403 Client Error: Forbidden for url: https://api.sm
ith.langchain.com/datasets\] {'detail':'Forbidden'}

This was working fine just yesterday :(

    os.environ['LANGCHAIN_
TRACING_V2'] = 'true'
    os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
    os.environ['LANGCHAIN
_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')

I have accessed the api\_keys.

How do I fix this? Can someone please help?


Edit: I am also importing

    from langsmith import Client 
    client = Client()
```
---

     
 
all -  [ Microsoft Launches Tiny AI Model Phi-3
 ](https://www.reddit.com/r/LangChain/comments/1cemgtd/microsoft_launches_tiny_ai_model_phi3/) , 2024-04-29-0910
```
Microsoft announced its smallest AI model yet, Phi-3. This model, measuring just 3.8 billion parameters, was learned fro
m â€˜bedtime storiesâ€™ created by other LLMs. Thanks to innovations in learning, the company says this family outperforms t
he same and next-size models on a range of tests assessing language, programming, and math abilities.

https://preview.r
edd.it/dav6udi2m2xc1.jpg?width=2000&format=pjpg&auto=webp&s=cc3538181ff7ad3a69064991c7b0dff507eb7ee6

The new model is a
vailable in theÂ Microsoft Azure AI Model CatalogÂ and onÂ Hugging Face, as well asÂ Ollama, a lightweight framework for run
ning models on a local machine. Microsoft says it will also be available as anÂ NVIDIA NIMÂ microservice with a standard A
PI interface that can be deployed anywhere.
```
---

     
 
all -  [ Where to hire LLM engineers who know tools like LangChain? Most job board don't distinguish LLM engi ](https://www.reddit.com/r/LangChain/comments/1cejzmq/where_to_hire_llm_engineers_who_know_tools_like/) , 2024-04-29-0910
```
I'm looking for a part-time LLM engineer to build some AI agent workflows. It's remote.

Most job boards don't seem to h
ave this category yet. And the person I'd want wouldn't need to have tons of AI or software engineering experience anywa
y. They just need to be technical-enough, a fan of GenAI, and familiar with LLM tooling.

Any good ideas on where to fin
d them?
```
---

     
 
all -  [ [psa] slow JSON output using ollama and llama3? try this! ](https://www.reddit.com/r/ollama/comments/1cej8eu/psa_slow_json_output_using_ollama_and_llama3_try/) , 2024-04-29-0910
```
hey! are you experimenting with langchain or just `'format': 'json'`, but have noticed incredibly slow output compared t
o regular formatting? this is because ollama allows the model to output whitespace forever, and eventually cancels the r
equest when it repeats `\n` or a space too many times. this makes json pretty slow.

there is currently an [open pull re
quest](https://github.com/ollama/ollama/pull/3784) which solves this issue and makes json output speedy. you can use [th
is branch on their fork](https://github.com/hughescr/ollama/tree/bugfix/disallow-possibly-infinite-trailing-whitespace-i
n-json) in order to solve the issue yourself.
```
---

     
 
all -  [ Contribute to the Cognita RAG Framework ](https://www.reddit.com/r/truefoundry/comments/1cej0mu/contribute_to_the_cognita_rag_framework/) , 2024-04-29-0910
```
In recent weeks, numerous engineers have explored [Cognita](https://github.com/truefoundry/cognita), providing invaluabl
e insights and feedback. We deeply appreciate your input and encourage ongoing dialogue (share your thoughts in the comm
ents â€“ let's keep this â€˜open sourceâ€™).

While RAG is undoubtedly powerful, the process of building a functional applicat
ion with it can feel overwhelming. From selecting the right AI models to organizing data effectively, there's a lot to n
avigate. While tools like LangChain and LlamaIndex simplify prototyping, an accessible, ready-to-use open-source RAG tem
plate with modular support is still missing. That's where Cognita comes in.

Key benefits of Cognita:

1. Central reposi
tory for parsers, loaders, embedders, and retrievers. 2. User-friendly UI empowers non-technical users to upload documen
ts and engage in Q&A. 3. Fully API-driven for seamless integration with other systems.

We invite you to explore Cognita
 and share your feedback as we refine and expand its capabilities. Interested in contributing? Join the journey at [http
s://www.truefoundry.com/cognita-launch](https://www.truefoundry.com/cognita-launch).
```
---

     
 
all -  [ Sharing RAG enhanced documents ](https://www.reddit.com/r/LangChain/comments/1cefdw6/sharing_rag_enhanced_documents/) , 2024-04-29-0910
```
Are there any libraries that can allow me to create a shareable versions of rag documents using links. 

&#x200B;

I am 
looking to create a system that will allow me to share a document using links with an LLM trained using RAG. How would y
ou go about this?
```
---

     
 
all -  [ Capture case where LLM did not find any answer in context ](https://www.reddit.com/r/LangChain/comments/1ce9jnl/capture_case_where_llm_did_not_find_any_answer_in/) , 2024-04-29-0910
```
I have built a RAG application and I am getting back the source file from which the LLM answered a question.

My issue i
s that a document is always retrieved but the LLM might not give an answer based on that.

I would like to capture this 
case when I call the chain. 

Is that possible?
```
---

     
 
all -  [ Can you get back similarity scores from retrievers? ](https://www.reddit.com/r/LangChain/comments/1ce9fh1/can_you_get_back_similarity_scores_from_retrievers/) , 2024-04-29-0910
```
Is there a way to get back similarity scores from retrievers?

If not, do you know any reliable function that computes s
imilarity score between user's query and retrieved chunks?

My issue is that I am working with non-English documents and
 many custom similarity score computation functions don't work very accurately. 
```
---

     
 
all -  [ Diving into RAG with a Small Team ](https://www.reddit.com/r/LangChain/comments/1ce8z9h/diving_into_rag_with_a_small_team/) , 2024-04-29-0910
```
Hey everyone, our small engineering team is exploring RAG for querying our massive internal document system. It's exciti
ng, but also a little overwhelming with all the choices - LLMs, embedding models, vector databases, hyperparameters... y
ou name it!

Here's what we're thinking:

* Manually create a test set of 10-20 custom Q&As (should we allow multiple an
swer options?).
* Automate deployment of various combinations: different LLMs, hyperparameters, embedding models, etc.
*
 Compare the generated answers to our gold standard answers (thinking ROUGE score for evaluation).

Does this approach s
ound reasonable? Are there any tools or frameworks out there that can streamline this process for a small team like ours
? Any advice would be greatly appreciated!
```
---

     
 
all -  [ How to build an agent that goes back and forth into the vector db ](https://www.reddit.com/r/LangChain/comments/1ce8lzd/how_to_build_an_agent_that_goes_back_and_forth/) , 2024-04-29-0910
```
I have a complex documentation and multiple requirements. I ask a question about a requirement which itself has requirem
ents from the same document.  Kindly advice on what should I use and how do I build?
```
---

     
 
all -  [ ExllamaV2 has been integrated to langchain. ](https://www.reddit.com/r/LocalLLaMA/comments/1ce7pzi/exllamav2_has_been_integrated_to_langchain/) , 2024-04-29-0910
```

Since I was using exllamav2 a lot, I was wondering why it wasnâ€™t available in langchain.
So I added it there. For now y
ou can run inference directly through langchain like any other compatible LLM library. Some stuff are missing but I will
 add them later. 

The PR has been merged today and if you want to contribute you can check the code files here :  https
://github.com/langchain-ai/langchain/pull/17817
```
---

     
 
all -  [ Building an Anime Character Generator with LangChain and OpenAI ](https://www.reddit.com/r/LangChain/comments/1ce7m2u/building_an_anime_character_generator_with/) , 2024-04-29-0910
```
 [Learn](https://dly.to/emJTz7UM5hG) how to build an anime character generator using LangChain and OpenAI. No HTML or CS
S required, just use Streamlit to create a simple web interface. Activate the virtual environment, install the necessary
 libraries, and run the code. Get creative and generate unique anime character names with different themes, along with w
ise, dramatic, or humorous quotes. 
```
---

     
 
all -  [ Need Help with Llama 3 ](https://www.reddit.com/r/LangChain/comments/1ce742z/need_help_with_llama_3/) , 2024-04-29-0910
```
I am building a mock interview bot with langchain js and fireworks ai api.

but getting an continuous output like this i
n the response:

response <|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>

{'response': 'Welcome to the int
erview for the React Developer position! Can you please tell me a little about yourself and why you're interested in thi
s role?', 'feedback': null}<|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>

{'response': 'What experience d
o you have with React and its ecosystem, and can you give me an example of a project you've worked on that you're partic
ularly proud of?', 'feedback': null}<|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>

{'response': 'How do y
ou handle state management in React applications, and have you used any libraries like Redux or MobX in your previous pr
ojects?', 'feedback': null}<|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>

{'response': 'Can you explain t
he concept of a 'Higher-Order Component' in React and give an example of how you would use it in a real-world scenario?'
, 'feedback': null}<|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>

{'response': 'How do you optimize the p
erformance of a React application, and what tools or techniques have you used in the past to improve rendering efficienc
y?', 'feedback': null}<|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>

sometimes it is returning the code, 
Can you tell me how to get a single and correct response? 
```
---

     
 
all -  [ Rag without langchain  ](https://www.reddit.com/r/LocalLLaMA/comments/1ce6yct/rag_without_langchain/) , 2024-04-29-0910
```
I donâ€™t need over abstraction of langchain or tools like that, i just need one good code example that works for rag , an
d i can change part of  that code for my needs(different llm or vector db., etc..). I can salvage langchain or that kind
 of tools source  code to create what I described or if anyone has already done that and kind enough to share ?
```
---

     
 
all -  [ Looking for a coach (paid) ](https://www.reddit.com/r/LangChain/comments/1ce6ndb/looking_for_a_coach_paid/) , 2024-04-29-0910
```
Hi,

I am looking for someone who would be willing to coach me and help me get started in building a bot. Am on a Mac. I
s this something that someone would be willing to do?
```
---

     
 
MachineLearning -  [ [D] Self-optimizing deterministic LLM memory using dspy, neo4j and vector databases. Need your input ](https://www.reddit.com/r/MachineLearning/comments/1cakjaf/d_selfoptimizing_deterministic_llm_memory_using/) , 2024-04-29-0910
```
Hey there, Redditors!

I'm back with the latest installment on creating deterministic LLM memory.

If you've been follow
ing along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://topoteretes.github.io/cognee/blog
/2023/10/05/going-beyond-langchain--weaviate-and-towards-a-production-ready-modern-data-platform/)' trend and tackle the
 challenges of building robust LLM memory.

  
That's why we built cognee, a python library to process documents and bui
ld knowledge graphs on top of them.

After a few weeks of work, we integrated DSPy and extended cognee.

Here is brief o
verview of the logic: 

[Architecture overview](https://preview.redd.it/fcs3lifx53wc1.png?width=1380&format=png&auto=web
p&s=9316cba52147a5b764565b8438f3f143d8e7ac84)

We aim to understand:

1. Have you tried building knowledge graphs with o
ther tools before?

2. If so, what were the biggest obstacles?

3. How would you approach semantic linking of documents 
without knowledge graphs?

*Remember to give this post an upvote if you found it insightful!*  
*And also star our*Â [Git
hub repo](https://github.com/topoteretes/cognee)
```
---

     
 
MachineLearning -  [ [D] How to get the source documents from the retrieved context for RAG?  ](https://www.reddit.com/r/MachineLearning/comments/1bvoc1t/d_how_to_get_the_source_documents_from_the/) , 2024-04-29-0910
```
I'm not using Lanchain but only making API calls to an LLM service provider. The retriever is connected to a vector DB, 
and I would like to know what the LLM refers to WITHIN that retrieved context whenever it provides an answer, similar to
 how return_source_documents works in Langchain.

I'm using AzureOpenAI. I couldn't find much in their docs that related
 to returning the source documents. Any help will be greatly appreciated!

```
---

     
 
MachineLearning -  [ [P] RAG pipeline to query the ML Engineering Open Book ](https://www.reddit.com/r/MachineLearning/comments/1bu4wyx/p_rag_pipeline_to_query_the_ml_engineering_open/) , 2024-04-29-0910
```
I built a quick RAG implementation using Langchain to make it easy to query the [ML Engineering Open Book](https://githu
b.com/stas00/ml-engineering) by [Stas Bekman](https://twitter.com/StasBekman). The Multi-Vector retriever gave pretty go
od results and was quite easy to set up too. 

Github link - [https://github.com/shreyansh26/RAG-ML-Engg-Open-Book](http
s://github.com/shreyansh26/RAG-ML-Engg-Open-Book)

Hope this is useful for folks!
```
---

     
 
MachineLearning -  [ [Project] FinancialAdvisorGPT : LLM+RAG Boilerplate Project ](https://www.reddit.com/r/MachineLearning/comments/1btlpgd/project_financialadvisorgpt_llmrag_boilerplate/) , 2024-04-29-0910
```
FinancialAdvisorGPT is a boilerplate project designed for RAG (Retriever-Augmented Generation) and LLM (Large Language M
odel) applications in financial analysis. Built on a technology stack including MongoDB, MongoDB VectorDB, Chroma, FastA
PI, Langchain, and React submodule for UI, it offers a framework for developers to implement and customize RAG+LLM proje
cts. Leveraging parallelized data pipelines, FinancialAdvisorGPT processes and integrates various data sources such as s
tock data, news, SEC filings, and local PDFs.

Github project includes long documentation on how the project is structur
ed, how LLM+RAG works for specific task :Â [https://github.com/mburaksayici/FinancialAdvisorGPT](https://github.com/mbura
ksayici/FinancialAdvisorGPT)
```
---

     
 
MachineLearning -  [ [Project] Picachain, image search made simple. ](https://www.reddit.com/r/MachineLearning/comments/1bt7epv/project_picachain_image_search_made_simple/) , 2024-04-29-0910
```
I am working on creating something for image search, basically something like langchain for images. Probably add videos 
too.

Currently supports chromaDB. Working on pinecone and other vector dbs. [https://github.com/d1pankarmedhi/picachain
](https://github.com/d1pankarmedhi/picachain)

Will you use something like this? What else should I add? Feel free to ad
d your views.

Appreciate your feedback or any feature ideas :)

&#x200B;
```
---

     
 
deeplearning -  [ Seeking Advice: Solving Data Challenges with Large Language Models (LLMs) ](https://www.reddit.com/r/deeplearning/comments/1ca4nc1/seeking_advice_solving_data_challenges_with_large/) , 2024-04-29-0910
```
Hi all

I am presented with a problem that I need to solve using LLM to get the right data from text that has only \~20%
 structure to it. Here's a sample data

XXXXX

AA          BB

CCCC:  (optional DDDD)

C1......(A1) (B1)

C2......(A2) (
B2)

C3.....(A3) (B3)

I am required to anwer with either of these results from A1/B1 till A3/B3 pairs but in order to d
o that I need to go back and ask the user which one of the options C1 to C3 applies to him?

The above is not the most c
omplex structure, it increases in complexity from here so a lot of chatting with user is required to get to the right da
ta that will always exist in the chunk like above.

In the most simplist case the data structure will look like below

X
XXXX

AA          BB

CCCC: ......(A1) (B1)



How would you build a system like this? I am looking at multi-agent syste
ms with Langchain, what about prompt chaining?
```
---

     
 
deeplearning -  [ Share the Coolest Out of The Box LLM Applications That Made You Say 'Wow that was smart' ](https://www.reddit.com/r/deeplearning/comments/1c9e6dj/share_the_coolest_out_of_the_box_llm_applications/) , 2024-04-29-0910
```
Hi, I'm looking at some LLM applications today but apart from guys doing big rags with langchain I don't see too many us
es that are out of the box or that make me say 'wow that was smart to use an LLM here'. Have you seen any cool stuff usi
ng LLMs recently that made you say 'wow, that was smart'?
```
---

     
