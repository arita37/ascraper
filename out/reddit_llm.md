 
all -  [ Do you need Vectorized Data to perform RAG? ](https://www.reddit.com/r/Rag/comments/1f6skbm/do_you_need_vectorized_data_to_perform_rag/) , 2024-09-02-0912
```
I have some code for a religious jurisprudence assistant that uses langchain with the GPT-4o-Mini API that makes it so t
hat if the LLM detects religious content, the code activates a similarity search function after embedding the users quer
y and comparing it to a npy database of pre-embedded quotes. However, I started to notice quite soon that embedding wasn
't always accurate and that there were issues, especially with it jumping to words, sorta like semantic search. I origin
ally started with a csv file full of quotes, where sometimes they were split across lines or referenced one another (typ
ically chronologically). I don't believe the model (ada-002) was able to fully grasp the context, and therefore the embe
dding quality was skewed. I looked into the mteb leaderboard on huggingface, but for some reason I can never get those t
o work. Should I proceed to try to embed the data (maybe through somehow 'telling' the model how to do it and to place s
pecific weights) or is it possible to have plaintext RAG where I could just feed the llm the data and it would be able t
o respond properly?

  
I'd appreciate any and all help, thank you!
```
---

     
 
all -  [ Best opensource rag with ui ](/r/Rag/comments/1f6jua9/best_opensource_rag_with_ui/) , 2024-09-02-0912
```

```
---

     
 
all -  [ Help with web scraping ](https://www.reddit.com/r/LangChain/comments/1f6pyth/help_with_web_scraping/) , 2024-09-02-0912
```
Hi everyone, is there a tool that can help navigate websites using LLM? For instance, if I need to locate the news secti
on of a specific company, I could simply provide the homepage, and the tool would find the news page for me.
```
---

     
 
all -  [ GraphRAG with existing graphs ](https://www.reddit.com/r/LangChain/comments/1f6pvx9/graphrag_with_existing_graphs/) , 2024-09-02-0912
```
how GraphRAG works with existing graphs, meaning if I already have entities and relationships, how would I load this gra
ph in GraphRAG? Can you guys help me figure out this ?
```
---

     
 
all -  [ [OFFER] I will code you almost everything in Python, for an affordable price  ](https://www.reddit.com/r/slavelabour/comments/1f6ni9s/offer_i_will_code_you_almost_everything_in_python/) , 2024-09-02-0912
```
Hey there! I'm David,

Are you drowning in boring tasks or dreaming up with exciting projects? Let me bring some coding 
magic to the table with Python.

With over 4 years of experience, I've worked in a variety of projects, including automa
tion, chatbots, APIs, web development, web scraping, and more!

What can I do for you?

* Automate the boring stuff: Say
 goodbye to repetitive tasks!

* Build Chatbots: Want a digital buddy? I can make that happen.

* Build your web app: Ti
me to build the next big thing together.

* Grab data from websites: Web scraping made easy.

* Work with AI: Using cool
 stuff like Langchain or the Chat-GPT API.

If you believe you have something that I could do for you, please place a $b
id and send me the details of the project. As always, the price depends, but It's usually from 15$ to 20$ for simple or 
intermediate projects.
```
---

     
 
all -  [ ScrapegraphAI with chatgpt ](https://www.reddit.com/r/LangChain/comments/1f6myui/scrapegraphai_with_chatgpt/) , 2024-09-02-0912
```
Here’s what I’m trying to do: using Google sheets I want to give chatgpt a prompt, the prompt requires gpt to scrape a w
ebsite and answer questions related to the website/company for example, “browse the website and tell me what brands has 
this company worked with” 

The issue here is, web browsing is not available with chatgpt API - so I’m trying to use alt
ernatives like scrapegraphAI that will work alongside chatGPT, browse the website for me and then answer the prompt. 

I
’ve been testing scrapegraph AI but it’s a bit inconsistent and I’m not entirely sure if it’s fulfilling what I need. So
 my question is, is what im trying to do possible with scrapegraph ai and if not, what is a good alternative to do what 
I need - essentially use web browsing with chatgpt api 
```
---

     
 
all -  [ Allow dangerous code ](https://www.reddit.com/r/LangChain/comments/1f6luxb/allow_dangerous_code/) , 2024-09-02-0912
```
Looking for guidance on how to navigate around the allow_dangerous_code, I understand this is a massive security risk if
 deployed in production, what are the general guidelines and safety guards that’s worth considering if using this option
 in your agents, are there alternatives? 
```
---

     
 
all -  [ Roadmap for genai  ](https://www.reddit.com/r/LangChain/comments/1f6l8rl/roadmap_for_genai/) , 2024-09-02-0912
```
Can some one pls tell me a roadmap and someone whom I can follow on YouTube or a course to get a comprehensive guide to 
genai 
```
---

     
 
all -  [ What’s more important? Observability or Evaluations? ](https://www.reddit.com/r/LangChain/comments/1f6kl5z/whats_more_important_observability_or_evaluations/) , 2024-09-02-0912
```
I am wondering what’s more important when you are building apps using LLMs? I have realized having a good observability 
lets me understand what’s going on and generally eye ball and understand how well my app is doing or the model is genera
ting responses.

I am able to optimize and iterate based on this. Which brings to my question as to whether evals are re
ally needed? Or is it more relevant for more complicated workflows? What are your thoughts?
```
---

     
 
all -  [ RAG quality/accuracy metric exist? ](https://www.reddit.com/r/LangChain/comments/1f6i6fd/rag_qualityaccuracy_metric_exist/) , 2024-09-02-0912
```
Perhaps a bit early to the industry, but is there a metric that assess the quality of the vectors retrieved in the RAG p
aradigm?
```
---

     
 
all -  [ [Student] Software eng student looking for an internship aboard This is my resume any tips? ](https://www.reddit.com/r/EngineeringResumes/comments/1f6ebmv/student_software_eng_student_looking_for_an/) , 2024-09-02-0912
```
hello everyone ! im a software eng student who is looking for a graduation intrenship aboard starting in feb 2025 ... i 
have seen few resumes and success stories over here so im trying to make mine and want to take ur opinion , anything i s
hould change or add ? and what do you think overall ... any tip or suggestion would be appreciated cause i have been fee
ling overwhelmed by this seeking journey   
thanks in advance 

https://preview.redd.it/bfbquoq587md1.jpg?width=5100&for
mat=pjpg&auto=webp&s=5ff8f060289263b70baf5772a3d2eccdc71658f6

https://preview.redd.it/iwk5frz687md1.jpg?width=5100&form
at=pjpg&auto=webp&s=cb22f00d3157be43d870aba2d3080c432255af89


```
---

     
 
all -  [ Hierarchical Indices: Optimizing RAG Systems for Complex Information Retrieval ](https://medium.com/@nirdiamant21/hierarchical-indices-enhancing-rag-systems-43c06330c085?sk=d5f97cbece2f640da8746f8da5f95188) , 2024-09-02-0912
```
I've just published a comprehensive guide on implementing hierarchical indices in RAG systems. This technique significan
tly improves handling of complex queries and large datasets.
Key points covered:

Theoretical foundation of hierarchical
 indexing
Step-by-step implementation guide
Comparison with traditional flat indexing methods
Challenges and future rese
arch directions

I've also included code examples in my GitHub repo: https://github.com/NirDiamant/RAG_Techniques
Lookin
g forward to your thoughts and experiences with similar approaches!
```
---

     
 
all -  [ How to Handle XML Files with Langchain and GPT-4o? Need Help with XPath, Node Manipulation, and Larg ](https://www.reddit.com/r/learnmachinelearning/comments/1f6dvih/how_to_handle_xml_files_with_langchain_and_gpt4o/) , 2024-09-02-0912
```
Hey everyone,

I'm currently working on a project where I need to interact with dynamically changing XML files using Lan
gchain4j or Langchain with OpenAI's GPT-4o model. The XML files I'm dealing with are quite large (over 100MB each), and 
I'm not able to include them directly in the prompts. The XML schema (XSD) is also available, but it's split across mult
iple files and spans thousands of lines.

The XML files are already parsed into Java Objects, and here's what I'm trying
 to achieve:

* Write XPath queries to navigate the XML structure.
* Retrieve specific nodes from the XML.
* Edit nodes,
 such as modifying attributes.

I've attempted to use Retrieval-Augmented Generation (RAG) and Function Calling, but nei
ther approach has provided a satisfactory solution.

Given the constraints, how can I efficiently communicate with and m
anipulate these large, dynamic XML files using Langchain(or 4j) and GPT-4o? Any advice, examples, or guidance would be g
reatly appreciated!

Thanks in advance!
```
---

     
 
all -  [ Production Grade RAG Applications ](https://www.reddit.com/r/LangChain/comments/1f6ckpw/production_grade_rag_applications/) , 2024-09-02-0912
```
I'm interested in some production grade RAG applications that can be tried online or where there's a video of someone de
moing its capabilities. What are the best RAG applications out there?

The only two I know of so far are Perplexity (whi
ch I've tried, I like it) and CoCounsel (for which I've seen an impressive demo). In particular, is there a RAG applicat
ion to chat with or run analysis tasks for large code bases?
```
---

     
 
all -  [ I built a local chatbot for managing docs, wanna test it out? [DocPOI] ](https://www.reddit.com/r/LangChain/comments/1f6b5ki/i_built_a_local_chatbot_for_managing_docs_wanna/) , 2024-09-02-0912
```
https://preview.redd.it/hsjig26cb6md1.png?width=1497&format=png&auto=webp&s=7e243b9998df63972ab4a5d2b02334a5ef2fa78e

He
y everyone! I just put together a local chatbot that helps manage and retrieve your documents securely on your own machi
ne. It’s not super polished yet and also am not a pro yet, but I’m planning to improve it. If anyone’s interested in giv
ing it a spin and providing some feedback, I'd really appreciate it!

You can check it out here: [DocPOI on GitHub](http
s://github.com/Darthph0enix7/DocPOI_repo)

Feel free to hit me up with any issues, ideas, or just to chat! We’ve got a s
mall community growing on Discord too—come join us!
```
---

     
 
all -  [ [AskJS] How to learn Generative AI as a full stack developer? ](https://www.reddit.com/r/javascript/comments/1f6awbw/askjs_how_to_learn_generative_ai_as_a_full_stack/) , 2024-09-02-0912
```
I'm trying to make a roadmap for someone who is a full stack developer experienced in JS. They don't have a background i
n ML so I would love it if you all could share how you have gone about it.

Vector databases and libraries like langchai
n already have JS/TS support so all that they'll be able to integrate. And prompt engineering is also easy to get starte
d with. But what all did you learn from a model point of view?

I have a background in ML so I understand all the topics
 but I don't know what challenges does one face if they don't have the foundation, on the surface to me it seems like al
l they need to learn is prompt engineering, and RAG software architecture.
```
---

     
 
all -  [ How I can Crete Knowledge Using Google Gemini API  ](https://www.reddit.com/r/LangChain/comments/1f6apg8/how_i_can_crete_knowledge_using_google_gemini_api/) , 2024-09-02-0912
```
How I can create Knowledge graph using Gemini API and docker only
```
---

     
 
all -  [ Choosing frameworks for RAG ](https://www.reddit.com/r/Rag/comments/1f65o7g/choosing_frameworks_for_rag/) , 2024-09-02-0912
```
Hello,  
I'm curious how you all are deciding which framework to use for production RAG at scale. Are you using haystack
, llamaindex, langchain? Langchain is ubiquitous in the written media about RAG, but I can't tell if it's actually being
 used in the industry for larger scale (high queries per second).   
What about the canopy framework from Pinecone? The 
framework looks reasonable, but it comes from a vector database company--do you really want vector db lock-in when it's 
early to call what the best vector db technology/provider will be?  
How are you all thinking about this and how are you
 dealing with it for now?
```
---

     
 
all -  [ Ansible Playbook & Role Generator (Python + Langchain) ](https://www.reddit.com/r/ansible/comments/1f64xgc/ansible_playbook_role_generator_python_langchain/) , 2024-09-02-0912
```
I made a ansible playbook/role generation tool. Posted about it last week but don’t think anyone noticed. :/ I think it’
s really cool and can be very helpful if you use ansible a lot. Check it out ! 

[Ansible Beam Github](https://github.co
m/bluehatkeem/ansible-beam)

Also made a YouTube vid about it. [https://youtu.be/auYgSJF5dCU?si=q02ixF_CVm_UOeIB](https:
//youtu.be/auYgSJF5dCU?si=q02ixF_CVm_UOeIB)
```
---

     
 
all -  [ Building crowdsource Genrative AI Builder Community  ](https://www.reddit.com/r/comfyui/comments/1f5zz44/building_crowdsource_genrative_ai_builder/) , 2024-09-02-0912
```
Day9, 10 & 11: Building crowdsource GenAI Builder Community

- Released newsletter for this week (Community members only
)
- Explored finetuning in more detail and creating a ppt to take session on finetuning.
- Finished Analysing Google beg
inner GenAI course and enrolled for advanced GenAI.
- Revisiting my approach to get 1st 1000 Members for AIBuilder Commu
nity. (Currently 35 Members in the community)
- New Projects code added: Building a RAG system with Meta's Llama 3.1 70B
 model using ChromaDB as vector store and langchain.
- Updating newsletter source and planning to automate.

[Community 
Access](https://discord.com/invite/dKskAgbhYH) to give your feedback and if you have any suggestions or you want to cont
ribute to build AI Builder Community together. 
```
---

     
 
all -  [ Buulding AI/GenAI croudsourced community  ](https://www.reddit.com/r/aipromptprogramming/comments/1f5z20a/buulding_aigenai_croudsourced_community/) , 2024-09-02-0912
```
Day9, 10 & 11: Building crowdsource GenAI Builder Community

I realise that Posting here on reddit with the community so
metimes helps me to keep going.
- Released newsletter for this week (Community members only)
- Explored finetuning in mo
re detail and creating a ppt to take session on finetuning.
- Finished Analysing Google beginner GenAI course and enroll
ed for advanced GenAI.
- Revisiting my approach to get 1st 1000 Members for AIBuilder Community. (Currently 35 Members i
n the community)
- New Projects code added: Building a RAG system with Meta's Llama 3.1 70B model using ChromaDB as vect
or store and langchain.
- Updating newsletter source and planning to automate.

[Community Access](https://discord.com/i
nvite/dKskAgbhYH) to give your feedback and if you have any suggestions or you want to contribute to build AI Builder Co
mmunity together. 
```
---

     
 
all -  [ Building AI/GenAI croudsourced community  ](https://www.reddit.com/r/PROJECT_AI/comments/1f5yvqa/building_aigenai_croudsourced_community/) , 2024-09-02-0912
```
Day9, 10 & 11: Building crowdsource GenAI Builder Community

I realise that Posting here on reddit with the community so
metimes helps me to keep going.
- Released newsletter for this week (Community members only)
- Explored finetuning in mo
re detail and creating a ppt to take session on finetuning.
- Finished Analysing Google beginner GenAI course and enroll
ed for advanced GenAI.
- Revisiting my approach to invite 1st 1000 Members for AIBuilder Community. (Currently 35 Member
s in the community)
- New Projects code added: Building a RAG system with Meta's Llama 3.1 70B model using ChromaDB as v
ector store and langchain.
- Updating newsletter source and planning to automate.

[Community Access](https://discord.co
m/invite/dKskAgbhYH) to give your feedback and if you have any suggestions or you want to contribute to build AI Builder
 Community together. 
```
---

     
 
all -  [ What should I use to train a model usong discord messages ](https://www.reddit.com/r/LangChain/comments/1f5ys9s/what_should_i_use_to_train_a_model_usong_discord/) , 2024-09-02-0912
```
Hello everyone, I have a big database on messages. I tried vector databases but outputs werent so different. 
```
---

     
 
all -  [ NiceGUI 2.0 with updated dependencies and some breaking changes to streamline the API ](https://www.reddit.com/r/nicegui/comments/1f5vwru/nicegui_20_with_updated_dependencies_and_some/) , 2024-09-02-0912
```
# New features and enhancements, breaking changes and migration guide

This major release introduces several new feature
s and enhancements, as well as breaking changes. We always try to keep breaking changes to a minimum, guide you through 
the migration process using deprecation warnings, and provide migration instructions. Please read the following release 
notes carefully to understand the changes and adapt your code accordingly before upgrading.

* **Semantic versioning** N
iceGUI 2.0 starts to implement [semantic versioning](https://semver.org/), which means that we will follow the MAJOR.MIN
OR.PATCH versioning scheme. This release is a major version because it introduces breaking changes. We will increment th
e MAJOR version for breaking changes, the MINOR version for new features and enhancements, and the PATCH version for bug
 fixes.
* **Fix Quasar's layout rules for** `ui.card` **that remove children's borders and shadows**⚠️ **BREAKING:** Qua
sar's QCard, the foundation of NiceGUI's [`ui.card`](https://nicegui.io/documentation/card), usually comes without any p
adding and requires nested card sections wrapping the actual content. NiceGUI simplified the use of cards by adding padd
ing, flex layout and gaps automatically. But because a QCard also removes the outer-most borders and shadows of its chil
dren, this caused unexpected results in certain cases. NiceGUI 2.0 fixes the behavior of [`ui.card`](https://nicegui.io/
documentation/card) by disabling Quasar's respective CSS rules.
* **Improve the API of** [`ui.table`](https://nicegui.io
/documentation/table)⚠️ **BREAKING:** The API for adding and removing rows in a [`ui.table`](https://nicegui.io/document
ation/table) has been improved. Passing rows as multiple arguments has been deprecated. Now these methods expect lists o
f rows.The `column` argument for `ui.table` is optional now. If not provided, the columns are infered from the first row
.A new `update_from_pandas` method has been introduced to update rows and columns from a new dataframe.A new `column_def
aults` parameter has been introduced to allow specifying some properties for all columns at once.
* **Improve support fo
r drawing items in** [`ui.leaflet`](https://nicegui.io/documentation/leaflet)⚠️ **BREAKING:** The [`ui.leaflet`](https:/
/nicegui.io/documentation/leaflet) element used to remove drawn items and required the user code to add new layers to th
e map for visualization. Now such items remain visible by default. This new behavior can be disabled by passing `hide_dr
awn_items=True` to `ui.leaflet`.
* **Unify declaration of third-party dependencies**⚠️ **BREAKING:** This release deprec
ates the `libraries`, `extra_libraries` and `exposed_libraries` parameters for subclassing `ui.element`. It introduces a
 new `dependencies` parameter to be used instead. New examples ['Custom Vue Component'](https://github.com/zauberzeug/ni
cegui/tree/main/examples/custom_vue_component) and ['Signature Pad'](https://github.com/zauberzeug/nicegui/tree/main/exa
mples/signature_pad) demonstrate how to use NPM and this parameter for integrating custom components based on third-part
y JavaScript libraries.
* **Reserve bottom space in validation elements for error messages**⚠️ **BREAKING:** UI elements
 with input validation like [`ui.input`](https://nicegui.io/documentation/input) used to omit the bottom space for a pot
ential error message. This caused a layout jump when the first error occurred. This release fixes this issue be reservin
g the space by default whenever the `validation` argument and property is not `None`. You can disable this behavior usin
g the 'hide-bottom-space' prop.
* **Remove** [`ui.timer`](https://nicegui.io/documentation/timer) **objects from UI hier
archy after they are finished:** Especially one-shot timers are now removed from the UI hierarchy after their callback h
as been executed. This avoids a potential memory leak.
* **Disable FastAPI docs by default**⚠️ **BREAKING:** NiceGUI app
s used to automatically serve FastAPI docs at /docs, /redoc, and /openapi.json. This behavior has been disabled. You can
 enable it by passing `fastapi_docs=True` to `ui.run`. Furthermore, you can specify the individual routes by setting `co
re.app.docs_url`, `core.app.redoc_url`, and `core.app.openapi_url`.
* **Make** `client.ip` **available before socket con
nection is established**⚠️ **BREAKING:** The client's IP is now already available before the page built and is returned 
to the client. On the auto-index page the `client.ip` property is `None`. If you need to check if the socket connection 
is established, use `client.has_socket_connection` instead.
* **Remove and update deprecated APIs**⚠️ **BREAKING:** Seve
ral deprecated APIs have been removed. The remaining deprecations will show warnings including the version when they wil
l be removed. Please update your code accordingly.

# Documentation and examples

* Use newer langchain package

# Pytho
n Dependencies

* Bump ruff from 0.6.2 to 0.6.3
* Bump plotly from 5.23.0 to 5.24.0
* Bump FastAPI from 0.109.2 to 0.112
.2 and remove the upper bound

# JavaScript Dependencies

The following JavaScript dependencies have been updated to the
 latest versions (#3654 by u/falkoschindler):

* Vue: 3.3.6 → 3.4.38
* Quasar: 2.13.0 → 2.16.9
* TailwindCSS: 3.2.0 → 3.
4.10
* Socket.IO: 4.7.2 → 4.7.5
* ES Module Shims: 1.8.0 → 1.10.0
* AG Grid: 30.2.0 → 32.1.0
* CodeMirror: 6.0.1 (unchan
ged)
* ECharts: 5.4.3 → 5.5.1
* ECharts-GL: 2.0.9 (unchanged)
* Leaflet: 1.9.4 (unchanged)
* Leaflet-draw: 1.0.4 (unchan
ged)
* Mermaid: 10.5.1 → 11.0.2
* nippleJS: 0.10.1 → 0.10.2
* Plotly: 2.27.0 → 2.35.0
* three.js: 0.157.0 → 0.168.0
* tw
een.js: 21.0.0 → 25.0.0
* vanilla-jsoneditor: 0.18.10 → 0.23.8

Many thanks to all contributors and users who reported i
ssues and provided feedback. We hope you enjoy this new release!
```
---

     
 
all -  [ Text2SQL Wars Vannai v/s Langchain v/s Lamadaindex Bitconfused created his while considering a frame ](https://www.reddit.com/gallery/1f5v2ip) , 2024-09-02-0912
```
Hello Guys Bit confused please which framework to choose #text2sql
In Finance Domain for correct long SQLs on SQLServer 
DataBases more that 100+ 

Considerations international usecase
Minimal spendings 💰 
Mostly Opensourced as not Customer 
Facing Directly 

```
---

     
 
all -  [ Openperplex: Web Search API - Citations, Streaming, Multi-Language & More! ](https://www.reddit.com/r/LangChain/comments/1f5mkc8/openperplex_web_search_api_citations_streaming/) , 2024-09-02-0912
```
Hey fellow devs! 👋 I've been working on something I think you'll find pretty cool: Openperplex, a search API that's like
 the Swiss Army knife of web queries. Here's why I think it's worth checking out:

🚀 Features that set it apart:

* Full
 search with sources, citations, and relevant questions
* Simple search for quick answers
* Streaming search for real-ti
me updates
* Website content retrieval (text, markdown, and even screenshots!)
* URL-based querying

🌍 Flexibility:

* M
ulti-language support (EN, ES, IT, FR, DE, or auto-detect)
* Location-based results for more relevant info
* Customizabl
e date context

💻 Dev-friendly:

* Easy installation: `pip install --upgrade openperplex`
* Straightforward API with cle
ar documentation
* Custom error handling for smooth integration

🆓 Free tier:

* 500 requests per month on the house!

I
've made the API with fellow developers in mind, aiming for a balance of power and simplicity. Whether you're building a
 research tool, a content aggregator, or just need a robust search solution, Openperplex has got you covered.

Check out
 this quick example:

    from openperplex import Openperplex
    
    client = Openperplex('your_api_key')
    result =
 client.search(
        query='Latest AI developments',
        date_context='2023',
        location='us',
        resp
onse_language='en'
    )
    
    print(result['llm_response'])
    print('Sources:', result['sources'])
    print('Rele
vant Questions:', result['relevant_questions'])

I'd love to hear what you think or answer any questions. Has anyone wor
ked with similar APIs? How does this compare to your experiences?

[https://api.openperplex.com](https://api.openperplex
.com/)

🌟 Open Source : Openperplex is open source! Dive into the code, contribute, or just satisfy your curiosity:

👉 [
Check out the GitHub repo](https://github.com/YassKhazzan/openperplex_backend_os)

If Openperplex sparks your interest, 
don't forget to smash that ⭐ button on GitHub. It helps the project grow and lets me know you find it valuable!

(P.S. I
f you're interested in contributing or have feature requests, hit me up!)
```
---

     
 
all -  [ Set of documents for RAG showcase ](https://www.reddit.com/r/LangChain/comments/1f5ktin/set_of_documents_for_rag_showcase/) , 2024-09-02-0912
```
Is there any publicly available set of pdf documents which is suitable for a RAG showcase?
```
---

     
 
all -  [ Langchain stop reason = Token limit ](https://www.reddit.com/r/LocalLLaMA/comments/1f5kq3a/langchain_stop_reason_token_limit/) , 2024-09-02-0912
```
I have a 300 page PDF that I am trying to extract structured output through a pydantic model, however I am getting only 
half of the expected output because I am hitting the max tokens. In these cases, what is the best way to ensure I at lea
st get the object output?
```
---

     
 
all -  [ Flowise ](https://www.reddit.com/r/LangChain/comments/1f5cuar/flowise/) , 2024-09-02-0912
```
I hear about Flowise but it seems like it's not very popular. Their subreddit has questions 16 days old with no comment/
responses and only 453 members in the subreddit. YouTube has like 1 or 2 people that ever train on it.  


Does Flowise 
really have all those brands whose logos are on their page as actual users?  
  
Any insight would be appreciated. I'm b
asically trying to think through if I should invest in learning how to use it.
```
---

     
 
all -  [ What is the difference between organizing data with collections vs with meta tags? ](https://www.reddit.com/r/LangChain/comments/1f5661h/what_is_the_difference_between_organizing_data/) , 2024-09-02-0912
```
I'm using Chroma with huggingface, I'm trying to figure out ways to organize the data. I process multiple large PDFs, ea
ch for different topics. I was planning on adding meta tags for each document so I can filter questions so that they onl
y apply to a specific topic, but I'm wondering if I should add them to different collections instead. What would the mai
n differences be?
```
---

     
 
all -  [ Help: Managing multiple inputs ](https://www.reddit.com/r/LangChain/comments/1f55i46/help_managing_multiple_inputs/) , 2024-09-02-0912
```
Hi Everyone,

I'm developing an AI-powered resume analysis chatbot using LangChain. The process begins with the user pro
viding a job description and a resume. The application then analyzes the resume based on the job description and offers 
feedback.

After this initial analysis, the user can ask questions or queries related to the resume, and the chatbot sho
uld respond based on the earlier context.

My challenge is managing this input flow—specifically, handling the initial i
nputs (job description and resume) and then seamlessly transitioning to processing subsequent user queries. Note that I 
need to take the job description separately since it's the only input passed to the retriever.

Here’s the code snippet 
for the initial chain:

    chain = (
        {
            'context' : get_context,
            'job_description' : lam
bda inputs: inputs['job_description'],
            'resume' :  lambda inputs: inputs['resume']
        }
        | templ
ate
        | hf
        | StrOutputParser()
    )

Has anyone implemented a similar setup, or do you have suggestions o
n structuring the input handling for this scenario?

Thanks in advance for your help!
```
---

     
 
all -  [ How to work with Claude using ChatVertexAI ](https://www.reddit.com/r/LangChain/comments/1f54feh/how_to_work_with_claude_using_chatvertexai/) , 2024-09-02-0912
```
Hi everyone, I wanna use an llm via ChatVertexAI from Google Cloud. As you know there's lots of different LLM on Google 
Cloud but when I pass a model name in ChatVertexAI I only can Google's gemini models. I can't see the model names that f
rom a different provider such as Antropic, Mistral, Llama etc. How to use these models using ChatVertexAI library.
```
---

     
 
all -  [ Does anyone have experience using tools with image responses? ](https://www.reddit.com/r/LangChain/comments/1f4vbyo/does_anyone_have_experience_using_tools_with/) , 2024-09-02-0912
```
I have a tool that generates an image (in particular, it graphs some data).

I'm trying to create an agent that can call
 the tool and then further analyze the image. The idea is that the agent will have an easier time interpreting the data 
if it is visually represented (as opposed to a long string of numbers), same as a human.

If I make the graph myself and
 then pass it to the llm (I'm using ChatGPT 4o-mini) using the method [here](https://python.langchain.com/v0.2/docs/how_
to/multimodal_inputs/) (which uses a `HumanMessage` with the image represented by a byte64-string), ChatGPT works exactl
y as expected.

  
But if I create a tool that returns the same `HumanMessage`, bind that tool to the llm with `create_o
penai_tools_agent` and ask the llm to load the image itself, it is no longer able to interpret the image as anything mor
e than a string of bytes. I have looked through the docs without finding anything.

  
Does anyone have experience with 
something like this?

  
MWE:

    importimport matplotlib.pyplot as plt
    import io
    
    from langchain_core.tool
s import tool
    from langchain_openai import ChatOpenAI
    
    from langchain.agents import AgentExecutor, create_op
enai_tools_agent
    from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
    
    from langc
hain_core.messages import HumanMessage
    
    
    @tool
    def load_secret_image():
        'load a secret image tha
t I have prepared for you'
    
        #mwe: just plot a simple graph
        plt.plot([1,2,4])
    
        with io.By
tesIO() as buf:
            plt.savefig(buf, format='png')
            buf.seek(0)
            image_data = base64.b64en
code(buf.read()).decode('utf-8')
        
        return HumanMessage(
            content = [
            {'type': 'tex
t', 'text': ''},
            {
                'type': 'image_url', 
                'image_url': {'url': f'data:image/p
ng;base64,{image_data}'}
            }]
            )
    
    
    
    llm = ChatOpenAI(model='gpt-4o-mini')
    
    

    # it works when manually using the tool's output as input
    pure_message = load_secret_image({})
    response = l
lm.invoke([
        ('system', 'You are a helpful AI bot'),
        ('human', 'What is this image?'),
        pure_messa
ge
        ])
    
    print('--- Response when directly using the tool's output as input ---')
    print(response) #suc
cess
    
    
    # it doesn't work when using the tool as part of an agent run 
    prompt = ChatPromptTemplate.from_m
essages([
        ('system', 'You are a helpful AI bot'),
        ('human', '{input}'),
        MessagesPlaceholder('age
nt_scratchpad'),
    ]
    )
    agent = create_openai_tools_agent(llm, tools = [load_secret_image], prompt = prompt)
  
  executor = AgentExecutor(agent = agent, tools=[load_secret_image])
    
    
    #run model
    print('\n--- Response 
when using the tool as part of an agent run ---')
    response = executor.invoke({'input': 'What is this image?'})
    p
rint(response) #fail
    
    
     matplotlib.pyplot as plt
    import io
    
    from langchain_core.tools import too
l
    from langchain_openai import ChatOpenAI
    
    from langchain.agents import AgentExecutor, create_openai_tools_a
gent
    from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
    
    from langchain_core.me
ssages import HumanMessage
    
    
    @tool
    def load_secret_image():
        'load a secret image that I have pre
pared for you'
    
        #mwe: just plot a simple graph
        plt.plot([1,2,4])
    
        with io.BytesIO() as b
uf:
            plt.savefig(buf, format='png')
            buf.seek(0)
            image_data = base64.b64encode(buf.rea
d()).decode('utf-8')
        
        return HumanMessage(
            content = [
            {'type': 'text', 'text': 
''},
            {
                'type': 'image_url', 
                'image_url': {'url': f'data:image/png;base64,{i
mage_data}'}
            }]
            )
    
    
    
    llm = ChatOpenAI(model='gpt-4o-mini')
    
    
    # it wo
rks when manually using the tool's output as input
    pure_message = load_secret_image({})
    response = llm.invoke([

        ('system', 'You are a helpful AI bot'),
        ('human', 'What is this image?'),
        pure_message
        ]
)
    
    print('--- Response when directly using the tool's output as input ---')
    print(response) #success
    
  
  
    # it doesn't work when using the tool as part of an agent run 
    prompt = ChatPromptTemplate.from_messages([
  
      ('system', 'You are a helpful AI bot'),
        ('human', '{input}'),
        MessagesPlaceholder('agent_scratchpa
d'),
    ]
    )
    agent = create_openai_tools_agent(llm, tools = [load_secret_image], prompt = prompt)
    executor =
 AgentExecutor(agent = agent, tools=[load_secret_image])
    
    
    #run model
    print('\n--- Response when using t
he tool as part of an agent run ---')
    response = executor.invoke({'input': 'What is this image?'})
    print(respons
e) #fail
    
    
    



[Model outputs](https://preview.redd.it/6pmutq500tld1.png?width=1088&format=png&auto=webp&s=9
5bfe9099063c7824d7a0511cbc2121fcdeb027e)

  


  

```
---

     
 
all -  [ Function call Authentication  ](https://www.reddit.com/r/LangChain/comments/1f4uxpe/function_call_authentication/) , 2024-09-02-0912
```
I have a dashboard that displays client portfolio information. I'm trying to implement a chatbot using LangChain and the
 function calling method to retrieve this portfolio information through API calls. However, the challenge I'm facing is 
that all these APIs require a bearer token for authentication. I'm struggling to find a way to pass this token to the (f
unction calling) tools I'm using. Can anyone help me with this? 
```
---

     
 
all -  [ RAG Me Up - Easy RAG as a service platform ](https://www.reddit.com/r/Rag/comments/1f4uosk/rag_me_up_easy_rag_as_a_service_platform/) , 2024-09-02-0912
```
New to this subreddit but highly relevant so figured I'd post our repository for doing RAG: [https://github.com/AI-Comma
ndos/RAGMeUp](https://github.com/AI-Commandos/RAGMeUp)

Key features:

* Built on top of Langchain so you don't have to 
do it (trust me, worth it)
* Uses self-inflection to rewrite vague queries
* Integrates with OS LLMs, Azure, ChatGPT, Ge
mini, Ollama
* Instruct template and history bookkeeping handled for you
* Hybrid retrieval through Milvus and BM25 with
 reranking
* Corpus management through web UI to add/view/remove documents
* Provenance attribution metrics to see how m
uch documents attribute to the generated answer <-- this is unique, we're the only ones who have this right now

Best of
 all - you can run and configure it through a single .env file, no coding required.
```
---

     
 
all -  [ 🚀 Revolutionizing RAG: The Power of Re-ranking: ](https://medium.com/@nirdiamant21/relevance-revolution-how-re-ranking-transforms-rag-systems-0ffaa15f1047) , 2024-09-02-0912
```
Ever wondered how to take your Retrieval-Augmented Generation (RAG) system to the next level? Re-ranking is the game-cha
nger in information retrieval that's transforming how we deliver relevant content to users.

Key benefits:
- Enhanced re
levance in search results
- Improved handling of complex queries
- Boosted performance in RAG systems

Curious to learn 
more? Read a short but comprehensive Medium blog post I wrote about it:
```
---

     
 
all -  [ RAG Me Up now supports Ollama ](https://www.reddit.com/r/LangChain/comments/1f4thb0/rag_me_up_now_supports_ollama/) , 2024-09-02-0912
```
You can now run super fine-grained RAG out of the box on Ollama without writing a single line of code, check it out: [ht
tps://github.com/AI-Commandos/RAGMeUp](https://github.com/AI-Commandos/RAGMeUp)
```
---

     
 
all -  [ You can avoid wasting resources with Semantic Caching - a guide to reduce your app cost and latency ](https://www.reddit.com/r/ChatGPTCoding/comments/1f4rmjf/you_can_avoid_wasting_resources_with_semantic/) , 2024-09-02-0912
```
Hey everyone,

Today, I'd like to share a powerful technique to drastically cut costs and improve user experience in LLM
 applications: S**emantic Caching**.  
This method is particularly valuable for apps using OpenAI's API or similar langu
age models.

The Challenge with AI Chat Applications As AI chat apps scale to thousands of users, two significant issues
 emerge:

1. Exploding Costs: API calls can become expensive at scale.
2. Response Time: Repeated API calls for similar 
queries slow down the user experience.

**Semantic caching addresses both these challenges effectively.**

Understanding
 Semantic Caching Traditional caching stores exact key-value pairs, which isn't ideal for natural language queries. Sema
ntic caching, on the other hand, understands the meaning behind queries.

(🎥 I've created a YouTube video with a hands-o
n implementation if you're interested: [https://youtu.be/eXeY-HFxF1Y](https://youtu.be/eXeY-HFxF1Y) *)*

# How It Works:


1. Stores the essence of questions and their answers
2. Recognizes similar queries, even if worded differently
3. Reus
es stored responses for semantically similar questions

The result? Fewer API calls, lower costs, and faster response ti
mes.

Key Components of Semantic Caching

1. Embeddings: Vector representations capturing the semantics of sentences
2. 
Vector Databases: Store and retrieve these embeddings efficiently

The Process:

1. Calculate embeddings for new user qu
eries
2. Search the vector database for similar embeddings
3. If a close match is found, return the associated cached re
sponse
4. If no match, make an API call and cache the new result

Implementing Semantic Caching with GPT-Cache GPT-Cache
 is a user-friendly library that simplifies semantic caching implementation. It integrates with popular tools like LangC
hain and works seamlessly with OpenAI's API.

# Basic Implementation:

    from gptcache import cache
    from gptcache.
adapter import openai
    
    cache.init()
    cache.set_openai_key()

# Tradeoffs

Benefits of Semantic Caching

1. Co
st Reduction: Fewer API calls mean lower expenses
2. Improved Speed: Cached responses are delivered instantly
3. Scalabi
lity: Handle more users without proportional cost increase

Potential Pitfalls and Considerations

1. Time-Sensitive Que
ries: Be cautious with caching dynamic information
2. Storage Costs: While API costs decrease, storage needs may increas
e
3. Similarity Threshold: Careful tuning is needed to balance cache hits and relevance

# Conclusion

Conclusion Semant
ic caching is a game-changer for AI chat applications, offering significant cost savings and performance improvements.  

Implement it to can scale your AI applications more efficiently and provide a better user experience.

Happy hacking : 
)
```
---

     
 
all -  [ Help: Managing Chat History Efficiently ](https://www.reddit.com/r/LangChain/comments/1f4pkvl/help_managing_chat_history_efficiently/) , 2024-09-02-0912
```
Hi everyone,

I'm working on a project where I use a Vision-Language Model to perform detailed assessments on a series o
f uploaded images. The process involves analyzing up to 20 images in several steps. However, I've run into a significant
 challenge with managing input tokens.

The core issue is that with each interaction, I need to resend the entire histor
y of the conversation, including all the images, to ensure the model has the necessary context. This quickly leads to a 
high token cost and eventually runs into the context window limit.

I’m wondering if it's possible to add memory to the 
chat in LangChain to store and reuse the input tokens without needing to resend them every time. Ideally, I’d like to ma
intain the model's understanding of the images across multiple steps without re-uploading them, keeping the token usage 
efficient.

Does anyone have experience dealing with a similar use case? Are there strategies within LangChain to manage
 this more effectively? I’m open to advice, suggestions, or even a paid consultation to explore potential solutions.

Th
anks in advance for any help!
```
---

     
 
MachineLearning -  [ [P] using GPT4o with langchain/chroma for sports analysis  ](https://www.reddit.com/r/MachineLearning/comments/1enuzlp/p_using_gpt4o_with_langchainchroma_for_sports/) , 2024-09-02-0912
```
Hi all, I'm working on a side project that helps with sports analysis for historical games, which in turn will help with
 sports betting. Currently I've been only focused on MLB because I wanted to see how the use case would pan out.

My fir
st attempt at this was to use the openai endpoint and load all the relevant JSON objects and send a prompt along with th
em to GPT and see what I get back. Eventually, the context size was getting way too big and the problem I was running in
to was that it was expensive. Although, the prompts back were actually pretty decent and relevant to the data.

My secon
d attempt was to setup a RAG using Chroma/LangChain/GPT4o. I got it to work but the answers all seem very off and super 
vague. None of the data I have was shown in any of the prompts i asked, or any of the players that were playing in a gam
e were mentioned at all in the prompt back, plus it kept mentioning wrong games/teams whe asking it specific games. I’m 
assuming I might need to adjust the vector store a bit but not sure how I can do that with chroma.

My question is what 
might be the best way to setup some sort of process? My end result, I would like a response back using the historical da
ta I've provided to make assumptions on what a game could be like based off all the stats given, with some room for GPT 
to also make some inference as well.

I am a super new at this so it's been a learning process so far; please bear with 
me.
```
---

     
 
MachineLearning -  [ [R] [D] Langchain Evaluation with BeyondLLM
 ](https://www.reddit.com/r/MachineLearning/comments/1eki1fv/r_d_langchain_evaluation_with_beyondllm/) , 2024-09-02-0912
```
Hey everyone! Just came across a new feature of Beyond LLM that can evaluate Langchain RAG pipelines! It provides contex
t relevancy, answer relevancy, and groundedness. Check out the code snippet I’m sharing—perfect for testing your RAG pip
elines! For more info, be sure to check it out on GitHub [here](https://github.com/aiplanethub/beyondllm/blob/main/cookb
ook/evaluate_langchain_rag_pipeline_beyondllm.ipynb).

https://preview.redd.it/172m1y3dvsgd1.png?width=3972&format=png&a
uto=webp&s=63d5b0f41f0e46a58e7a2d5fb0d2bbc4384b3b1d


```
---

     
 
deeplearning -  [ Creating a project on NLP ](https://www.reddit.com/r/deeplearning/comments/1ey2e85/creating_a_project_on_nlp/) , 2024-09-02-0912
```
So me and my friend completed the ML and DL specialization by AndrewNg, and were just gonna get started on a project. We
 decided to make a academic assistant. So basically what this does is a user can upload a PDF,text file or any other sup
ported media and the can ask questions related to it's contents. The main objective being making learning quick given la
rger documents.

The pipeline we decided is pretty standard for such a project.

1. Split the text into chunks
2. Genera
te embeddings of the chunks
3. Store the chunks in a vector DB
4. Find the top K similar chunks to the query 
5. Retriev
e context and feed it into a LLM for an answer.

So I looked up for a library and framework to use and decided on langch
ain. We haven't decided on an LLM yet but want to run it locally so no OpenAI please. 

Since this is gonna be out first
 AI project confidence is low. I would really appreciate any heads up on the issues we may face, any suggestions on libr
aries,frameworks or models will be really helpful as well. 

Appreciate any resourceful comment 😊
```
---

     
 
deeplearning -  [ How To Build Your Retrieval Augmented Generation (RAG) Using Open-source Tools: LangChain, LLAMA 3,  ](https://www.reddit.com/r/deeplearning/comments/1emdotx/how_to_build_your_retrieval_augmented_generation/) , 2024-09-02-0912
```


TL;DR: RAG overcomes the limitations of LLMs by bringing in external sources of information as relevant context.  
  

At the end of the step-by-step tutorial, you will be able to give your favorite LLM (ChatGPT, LLAMA 3, Mixtral, Gemini, 
Claude, etc.) some documents, ask it a question and see it respond based on relevant context.  
  
This will be running 
locally, using open-source libraries. Zero API and tooling costs.

[Step-by-step Notebook with zero-cost RAG](https://co
decompass00.substack.com/p/build-open-source-rag-langchain-llm-llama-chroma)

![img](69v6kjfj3wgd1)


```
---

     
 
deeplearning -  [ Need help with creating CLI for 'non-programmers' (LLMs) ](https://www.reddit.com/r/deeplearning/comments/1elrfgm/need_help_with_creating_cli_for_nonprogrammers/) , 2024-09-02-0912
```
***TL;DR*** What is the best way to convert user input into sequence of commands and their corresponding parameters? Lik
e, imagine you are not a programmer and there is a console app with a CLI, but, well, you don't know the structure and t
he syntax of commands. And you don't want to know. YBut! You have a locally running instance of llama3.1 -- or whatever 
open LLM is out there now -- and you can ask it to create a CLI command for you. What would you do to accomplish that?


**Intro**

A little bit of context. I'm working on a project that targets scientists as end users. It has some UI using 
which it's possible to do all sort of things the lab workers would like to do. But recently the projects product owner d
ecided that it would be cool to have a small chat window that is accessable basically everywhere throughout the applicat
ion UI in which 'lives' a bot that can accept some input from a user and do what is requested. The pool of commands is f
inite and predefined.

**The issue**

So, putting details aside, the main issue to be solved is parsing user input (unst
ructured and possible incomplete data) to some structured form. In general, each and every user input should be transfor
med into a data structure that represents a sequence of commands with their parameters, for example:

User input: Please
, create X with param1 set to value1 and param2 equal to value2

Desired output:

    create_x --param1 value1 --param2 
value2

In this example, there is only one command, but in real life the request can represent a sequence of N commands,
 and they may depend on each other (sequence of execution does matter)

**What I've tried so far**

I have an 'experimen
t' environment: a python project with `ollama` and `langchain` installed. The main model I test is llama3.1-instruct wit
h 5bit quantization. (I'm sort of limited with hardware resourses, so XXB parameter models do not fit).

Up until now, I
've tried to achieve what I want with prompting in different forms, but in general I do the following:

1. As the very f
irst message in the chat, I create a 'system' one which explain what commands are there. The format is the following (I 
replaced original data not to expose the context more, so it's very generic): 

```xml
<scope>
    <models>
        <mod
el name='entityA'>
            <field name='uniqueId' type='string' description='unique identifier for entityA'/>
      
      <field name='label' type='string' description='label for entityA'/>
            <field name='category' type='enum'
 possible-value='alpha, beta, gamma, delta'/>
        </model>
        <model name='entityB'>
            <field name='u
niqueId' description='unique identifier for entityB'/>
            <field name='entityAIds' type='array' description='id
entifiers of entityAs associated with this entityB'/>
        </model>
    </models>
    <commands>
        <command nam
e='create_entityA' description='creates an instance of entityA'>
            <param name='uniqueId' type='string' descri
ption='unique identifier for entityA'/>
            <param name='label' type='string' description='label for entityA' re
quired='true'/>
            <param name='category' type='enum' possible-values='alpha, beta, gamma, delta'
             
      description='category of entityA (one value from the possible values list)' required='true'/>
        </command>
 
       <command name='remove_entityA' description='removes an instance of entityA by its unique identifier'>
           
 <param name='uniqueId' description='unique identifier of the entityA to be removed'
                   required='true'/
>
        </command>
        <command name='create_entityB'>
            <param name='label' description='label for enti
tyB'/>
        </command>
        <command name='link_entityAs_to_entityB'
                 description='associates inst
ances of entityA with a specific entityB based on the provided unique identifier of entityB'>
            <param name='u
niqueId' description='unique identifier of the entityB to which entityAs should be associated'
                   requir
ed='true'/>
            <param name='entityAIds'
                   description='an array of unique identifiers of entit
yAs to associate with the entityB'
                   type='array'
                   required='true'/>
        </comman
d>
        <command name='navigate' description='indicates that a user wants to go to a specific section of the platform
'>
            <param name='section' possible-values='entitiesA, entitiesB, configuration' required='true'/>
        </c
ommand>
        <command name='support' description='should be executed when a user seeks assistance on available functi
ons'/>
    </commands>
</scope>
```

So, now the model is provided with some context. Then, also in the 'system' message
 I:

* 'tell' the model that user input should be converted into a sequence of commands along with the corresponding par
ameters, all of this is described in the XML above
* describe the desired output format
* try to enforce some restrictio
n and cover edge cases

**The question part**

*Is this approach* ***viable***\*?\*

If yes, maybe there are some ***way
s to improve it***?

If not, *what would be* ***the alternative***?

So far I don't see how to apply fine tuning here

T
hank you in advance!
```
---

     
