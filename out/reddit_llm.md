 
all -  [ Roast My Resume. New Grad here, struggling to get any interview calls. ](https://www.reddit.com/r/resumes/comments/1dwzvzn/roast_my_resume_new_grad_here_struggling_to_get/) , 2024-07-07-0912
```
https://preview.redd.it/29ljlcvtsyad1.png?width=645&format=png&auto=webp&s=1462236c0e7fb714a8e6880426c6261a0cc2c00c


```
---

     
 
all -  [ Alternative to LangSmith for voice agents ](https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/) , 2024-07-07-0912
```
What observability platforms are people using for their voice agents? Have found the current solutions to be not useful 
for audio use cases (running conversation level evals, detecting latency & interruptions, audio playback connected to tr
aces, flagging call failures, etc). Have checked out LangSmith, Agentops, and a few others
```
---

     
 
all -  [ regulation about LLM/AI ](https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/) , 2024-07-07-0912
```
Hey there,

now with RAG technologies being accessible to anyone with some basic programming skills, people are scraping
 any source of content online. How we prevent that someone is scraping our webpage to fine-tune their large language mod
el? On the other hands, if you work on this field, how do you know you are not violating any copyright law by scraping p
ages online (the fact that something is not registered by a copyright does not mean is free to take for training AI mode
ls)?
```
---

     
 
all -  [ Managing Large Token Volumes with LangChain OpenAPI Agent ](https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/) , 2024-07-07-0912
```
Hi everyone, 
I‚Äôm exploring the use of LangChain OpenAPI Agent for a project and have encountered a challenge with handl
ing large amounts of tokens efficiently. 
Does anyone have experience or tips on managing this effectively? 
I‚Äôm looking
 for best practices or adjustments to improve performance without compromising the quality of interactions. 
Any advice 
or insights would be greatly appreciated!
```
---

     
 
all -  [ Creating library to apply 58 prompting techniques to your prompt. Join me? ](https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/) , 2024-07-07-0912
```
OpenAI, Microsoft, et al surveyed 58 prompting techniques in this paper:

[https://arxiv.org/pdf/2406.06608](https://arx
iv.org/pdf/2406.06608)

I‚Äôm creating a library to automatically apply these techniques to your prompt:

[https://github.
com/sarthakrastogi/quality-prompts](https://github.com/sarthakrastogi/quality-prompts)

Eg, one such technique is System
2Attention which filters the relevant context needed to answer the user‚Äôs query.

Just call .system2attention() on your 
prompt and it‚Äôs done.

Similarly, in few shot prompting, suppose you have a large set of example inputs and labels.

All
 you have to do is call the .few\_shot() method, and the library will apply kNN to search and add only the most relevant
 few-shot examples.

The prompt is dynamically customised at runtime according to the user‚Äôs message.

Let‚Äôs write quali
ty prompts!

If you'd like to contribute to the library please raise a PR!

Colab notebook to get started:

[https://col
ab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few\_shot\_prompt\_usage.ipynb](https://
colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb)
```
---

     
 
all -  [ Need help! Is it possible to solve engineering college assignments in subject like Engineering Chemi ](https://www.reddit.com/r/ChemicalEngineering/comments/1dwqf87/need_help_is_it_possible_to_solve_engineering/) , 2024-07-07-0912
```

```
---

     
 
all -  [ Extracting hindi text from pdf for a hindi RAG chatbot  ](https://www.reddit.com/r/developersIndia/comments/1dwp244/extracting_hindi_text_from_pdf_for_a_hindi_rag/) , 2024-07-07-0912
```
Hello fellow developers! I am in a conundrum where I have to extract hindi text from a pdf as I am working on a rag chat
bot that will answer queries based on hindi PDFs. To extract text my first attempt was to use PyMuPdfLoader from langcha
in but that wasn't very good at extracting the text. 

I then found some code on stack overflow which can be found over 
here: [extraction of hindi text 
](https://stackoverflow.com/questions/35917848/extracting-text-written-in-hindi-from-pd
f-in-python
)

But even that is adding more than one matra to just one character. Do you guys have any suggestions on ho
w i can solve this issue? Do you know of any libraries for how I van go about this? 
```
---

     
 
all -  [ [0 YOE] software engineer 3rd year resume help 0 O/A given. Can't even shortlist. ](https://www.reddit.com/r/resumes/comments/1dwnvta/0_yoe_software_engineer_3rd_year_resume_help_0_oa/) , 2024-07-07-0912
```
Hi, l'm an incoming 3rd year student and I really wants to find a good software engineering internship for next summer. 
I 'm located in India and this is my current resume for the internship applications

What can I improve how

What can I 
add

I'm thinking of adding certifications

1. Winning hackathon, business case competitions
2. Andrew ng machine learni
ng course

How can I attach them

how can I add in the resume

Which one's to add

https://preview.redd.it/o8nmkotv0wad1
.png?width=1080&format=png&auto=webp&s=2e8762a511f888b49ba31203c0b65531e80bb7f6
```
---

     
 
all -  [ Help with CSV RAG. ](https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/) , 2024-07-07-0912
```
I'm trying to develop an application that can perform statistical analysis of CSV files and generate plots. I've been tr
ying to do this with rag, but I've no IDEA how to split/load/embed the CSV files, I've done this before with PDFs. PLEAS
E HELP!!! 
```
---

     
 
all -  [ What is suppose to go into here? Langflow ](https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/) , 2024-07-07-0912
```
https://preview.redd.it/ty9hu9dz5vad1.png?width=900&format=png&auto=webp&s=2627d1ad0ee0bb3f888b9c583e96060447d55b77


```
---

     
 
all -  [ LangGraph state - Create a cyclic graph and watchdog a directory ](https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/) , 2024-07-07-0912
```
[https://youtu.be/DBXdE\_5Jces](https://youtu.be/DBXdE_5Jces)
```
---

     
 
all -  [ Langchain with personalized memory (or summarized conversational memory) ](https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/) , 2024-07-07-0912
```
I think currenlty the langchain implementations like chat-langchain supports conversational memory.  But the conversatio
n can sometimes be too long.

  
I am lookin for memory-summarization like this.  [https://www.youtube.com/watch?v=oPCKB
9MUP6c&t=81s](https://www.youtube.com/watch?v=oPCKB9MUP6c&t=81s)

to reduce tokens.  Is there any chatbot implementation
 like this on github ?
```
---

     
 
all -  [ LangChain JavaScript ‚Äì execute generated code ](https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/) , 2024-07-07-0912
```
Made this short LangChain.js example on how to improve AI math accuracy  by asking the LLM to  create and execute JavaSc
ript code. 

[https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/](https://www.js-craft.io/blog/la
ngchain-javascript-execute-generated-code/)
```
---

     
 
all -  [ trying to find teammate for google gemini developer competition ](https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/) , 2024-07-07-0912
```
[Join the Gemini API Developer Competition ¬†|¬† Google for Developers](https://ai.google.dev/competition) Here is the lin
k

I have been freelancing for 4+ years and have decent experience of python. need someone who is competitive, creative,
 and willing to sacrifice at least 4 hours a day
```
---

     
 
all -  [ Django AI Assistant - Open-source Lib Launch ](https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/) , 2024-07-07-0912
```
Hey folks, we‚Äôve just launched an open-source library called¬†Django AI Assistant, and we‚Äôd love your feedback!

What It 
Does:

* **Function/Tool Calling**: Simplifies complex AI implementations with easy-to-use Python classes
* **Retrieval-
Augmented Generation**: Enhance AI functionalities efficiently.
* **Full Django Integration**: AI can access databases, 
check permissions, send emails, manage media files, and call external APIs effortlessly.

How You Can Help:

1. Try It:¬†
[https://github.com/vintasoftware/django-ai-assistant/](https://github.com/vintasoftware/django-ai-assistant/)
2. ‚ñ∂Ô∏è¬†[Wa
tch the Demo](https://www.youtube.com/watch?v=bSJv4OIKLog&ab_channel=VintaSoftware)
3. üìñ¬†[Read the Docs](https://vintaso
ftware.github.io/django-ai-assistant/latest/get-started/)
4. Test It & Break Things: Integrate it, experiment, and see w
hat works (and what doesn‚Äôt).
5. Give Feedback: Drop your thoughts here or on our GitHub issues page.

Your input will h
elp us make this lib better for everyone. Thanks!
```
---

     
 
all -  [ YouTube comments feature ](https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/) , 2024-07-07-0912
```
YouTube has a new feature where it organizes comments by. It it possible to organize a list of chat by topic with langch
ain?
```
---

     
 
all -  [ How to get into ML/AI domain?  ](https://www.reddit.com/r/careerguidance/comments/1dw24nk/how_to_get_into_mlai_domain/) , 2024-07-07-0912
```
Hi, I'm a software developer with 5 years of experience. The languages and technologies that I have used at work are Per
l and Oracle SQL at backend, React.js and Typescript for front-end with most of the work at backend. I did not care abou
t the tech stack at first as the job was paying me well. However, now I'm stuck at applying to other companies and start
ed to upskill myself. 

I'm interested in Machine learning recently and completed ML, DL and AI courses in Udemy. I have
 also started learning about Gen AI using Langchain.

Colleagues at office are suggesting to do AWS certifications if pl
anning to stay in the same domain and PG or MS in Machine Learning to get into AI/ML domain.

On going through sites lik
e quora and reddit, many have suggested to improve the skills instead of spending lakhs on getting a degree or certifica
tion.
Can anyone suggest me how to improve my ML/AI skills and get a job in this domain? Is PG/MS needed? 
```
---

     
 
all -  [ Is there a way to save a RAG after it has read its documents? ](https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/) , 2024-07-07-0912
```
Potentially dumb question lol. Basically when I run my RAG, it takes a long time to process all the documents that it wi
ll then retrieve. Is there a way to just save off the model after it is done reading the documents so that when you run 
it again, it can skip that step? Similar to how a fine-tuned model would work? It doesn't really make sense in my head, 
but I haven't been able to find a concrete answer to this so I want to be sure.
```
---

     
 
all -  [ Deploy Hugging Face model in Sagemaker ](https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/) , 2024-07-07-0912
```
I want to deploy a Huggingface model in Sagemaker with a context size of around 25-32k. I am having trouble finding a su
itable model that performs well with this context size. The model's task will be to map raw data to a target framework. 

```
---

     
 
all -  [ Any good resource/guide about how to do RAG on a codebase? (e.g. Github repo) ](https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/) , 2024-07-07-0912
```
like title. Thanks in advance!
```
---

     
 
all -  [ Concurrent/parallel requests with vLLM ](https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/) , 2024-07-07-0912
```
My question might be a bit basic, but I‚Äôm new to all of this and eager to learn.

I have a basic setup where I initializ
e an LLM using vLLM with Langchain RAG and the Llama model (specifically, llama2-13b-chat-hf). Here‚Äôs what I do:

* I de
fine a system prompt and an instruction f
* I create an¬†`llm_chain`
* I then run the chain with¬†`llm_chain.run(text)`¬†, 
which works for a single input.

I have build an app with FastAPI. Previously I used asyncio method to handle multiple r
equest to llm, but with each new request it become slower in response. So I decide to use vLLM method, but I got a probl
em now how to provide parallel or concurrent requests to vLLM when I have dealing with dozen or more users. Is there a w
ay to call¬†`run`¬†in parallel for several inputs and receive valid results for each input?
```
---

     
 
all -  [ What is the best approach to achieve a better performant RAG? ](https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/) , 2024-07-07-0912
```
Hi!

I'm working on a RAG system for my company where we can use it to search through our internal wiki page.  
My syste
m is nearly in a releasable state and finds the correct information 90% of the times, and I'm happy about it, but I'm co
nstantly thinking, can I make it better?

I've made a custom scraper for our wiki, we're using an older version of Media
Wiki.  
The scraper I've made is basically extracting all sections out into its own 'document' and then sending it into 
qdrant vector database.  
That means that in the vector database, it doesn't have a full wiki page but rather a cut up v
ersion to make it easier for the search query to hit something right. But I feel like this is kinda wrong?

Whenever you
 send in your query to the backend, it'll then search for the 10 documents matching and then reranking with BAAI/bge-rer
anker-large. Then the context is being sent to Llama3:8b with your question in mind.  
This means that Llama3 will never
 get a fully contextual article, since the vectors are only smaller sections from the full page.

What could be done do 
make this better in the end? The one thing I see as an issue here, is that it will never know anything about the rest of
 the full page, but if it has the full page, it feels like Llama3 get overwhelmed by the data and then craps out.

We ha
ve  \~258 articles and that's resulting in about 1488 points in qdrant.
```
---

     
 
all -  [ Young Doctor looking for a mentor/Unpaid remote opportunities to gain experience in private equity ](https://www.reddit.com/r/private_equity/comments/1dvmcbo/young_doctor_looking_for_a_mentorunpaid_remote/) , 2024-07-07-0912
```
Hey everyone,

I'm an MBBS medical doctor in the UK interested in transitioning into private equity down the line. I'm f
lexible with moving countries in the future.

Currently I'm completing my training, however, have dedicated 20 hours a w
eek to exploring this space.  
Relevant knowledge: 

- Cert: Advanced Valuation and Strategy - M&A, Private Equity, and 
VC.

---> easy to learn: Did Mathematics Extension 2 in schooling (Stats, Vectors, Matrices, Diff + Integration, Perms &
 Combs etc.) However need more weekly practice with DCF to solidify the knowledge to make it second nature.

- Good skil
ls with MS Office \~ Excel scripts needs solidification.

- Know Langchain, RAG, Python and familiar with open source LL
M works.

Familiar with crunchbase but don't have enough capital to fund it for longer unless justified.

Does anyone kn
ow where I can find remote, part time experiences or even mentors in private equity?

Really keen on learning and will w
ork for it.
```
---

     
 
all -  [ Beginner here: found something confusing ](https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/) , 2024-07-07-0912
```
I've been playing around with GPT4All and langchain, for which there is a minimal demo here:

https://python.langchain.c
om/v0.2/docs/integrations/llms/gpt4all/

In this demo, they invoke the following:

```from langchain_core.callbacks impo
rt StreamingStdOutCallbackHandler```

From the API, it states that this only works with LLMs that support streaming. Acc
ording to the integrations page:

https://python.langchain.com/v0.2/docs/integrations/llms/

gpt4all does NOT support st
reaming. So I'm confused - what gives with this demo?
```
---

     
 
all -  [ Hybrid search with Postgres ](https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/) , 2024-07-07-0912
```
I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.

Anyone using 
Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?

Would love to hear your e
xperiences.
```
---

     
 
all -  [ Need honest feedback on resume (Final year B.Tech student) ](https://www.reddit.com/r/developersIndia/comments/1dvct1d/need_honest_feedback_on_resume_final_year_btech/) , 2024-07-07-0912
```
Hello! I'm a final year CS student from a tier-3 college, and am going to start applying to companies soon (both on and 
off campus). The hiring scene seems to be really terrifying right now, so I want to make sure I put my best foot forward
. I'm looking for brutal feedback on my resume, and how to stand out as an applicant. Thank you!

https://preview.redd.i
t/8w24zb48hjad1.jpg?width=2550&format=pjpg&auto=webp&s=8ad25c3a55ea1db97cdddb41c0b9e22eec842c6f
```
---

     
 
all -  [ Tool for Comparing Outputs of Multiple LLMs from Single Prompts ](https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/) , 2024-07-07-0912
```
I'm searching for a tool that allows users to compare outputs generated by several LLMs using just one prompt. While I u
nderstand that LangChain could potentially enable building such a solution locally, I'm curious if any existing products
 offer this functionality.

I'm weary of manually inputting the same prompt across different models like GPT, Claude, Ba
rd, and Perplexity to cross-reference answers and verify accuracy. Any recommendations or insights would be greatly appr
eciated!     
```
---

     
 
all -  [ Issue with Ollama and Pydantic  ](https://www.reddit.com/r/ollama/comments/1dv947k/issue_with_ollama_and_pydantic/) , 2024-07-07-0912
```
Hi guys,

Are you also facing issues when using Ollama with Pydantic? It seems that the response from the LLM is often n
ot put in the output format as requested by Pydantic. Hence, the subsequent code in our program throws an error.

In our
 tests, we used Codestral-22B and the following function to call the LLM inference. We also tested the same with the API
 from Mistral, and this works. **So it seems there is a problem in the interface between Ollama and Pydantic.**



Do yo
u have good experience with Ollama and Pydantic? Do you know how we can solve this?

Many thanks for your help!

Pseudo 
code (not functional, just to show which functions we call)

    from langchain_experimental.llms.ollama_functions impor
t OllamaFunctions
    
    llm = OllamaFunctions(base_url=Config.HOST_LLM, temperature=0, model='codestral:latest', form
at='json')
    
    llm = llm.with_structured_output(MyPydanticOutputClass)
    
    llm.invoke({...}) # pass data requi
red by the LLM to perform inference 

---- 

We are using Pydantic through the call **with\_structured\_output:**

**llm
.with\_structured\_output(MyPydanticOutputClass)**

So, sometimes there are some fields of the Pydantic object that are 
not parsed, so the output of the LLM is incomplete, for example:

***Got: 3 validation errors for CodeGenerationOutput**
*

***dev\_mode***

***field required (type=value\_error.missing)***

***rtos***

***field required (type=value\_error.m
issing)***

***is\_func\_empty***

And sometimes producing a more general error:

***raise ValueError(***  
***ValueErro
r: Failed to parse a response from codestral-latest output: {}***

---
```
---

     
 
all -  [ Design decision for an LLM app ](https://www.reddit.com/r/StackoverReddit/comments/1dv936n/design_decision_for_an_llm_app/) , 2024-07-07-0912
```


Langchain async for document loader and character splitter

Should I use Async or not? 

I am building a Fast api RAG 
app that uses LLMs to generate responses using langchain but I am confused if it needs async. 

The user flow goes somet
hing like this:
1. User provides a link/ links to a text/pdf document in the form of a url.  
2. Langchain document load
ers are used to load text or pdf from the remote public url.
3. Character splitter is used to split and chunk the docume
nts which is saved into a vector db. 
4. Langchain chain library is used to invoke LLMs via the asynchronous ainvoke(). 


My question is whether the document loading step via langchain and character splitting text via langchain  would need 
to be made async? Langchain doesn‚Äôt support async for the libraries I am using. Would I need to implement them myself? I
f so, what are some options to implement? 
```
---

     
 
all -  [ Passing Chat History to Langchain Tool Calling Agent ](https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/) , 2024-07-07-0912
```
I'm working with Langchain to build a tool that calls an agent. Currently, I'm passing the chat history as an input vari
able to the agent. However, I've encountered an issue where the agent doesn't always seem to utilize the history data to
 answer questions consistently. This is especially problematic when users have queries spaced out over 10‚Äì15 days.

Is t
here a more efficient way to ensure the agent consistently remembers all chat history and context over multiple sessions
? What approach or best practices should I follow to address this issue?

Thanks in advance for your guidance!
```
---

     
 
all -  [ Load LLM (Mixtral 8x22B) from Azure AI endpoint as Langchain Model ](https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/) , 2024-07-07-0912
```
Hi,

  
I set up Mixtral 8x22B on Azure AI/Machine Learning and now want to use it with Langchain. I have difficulties w
ith the format I am getting, e.g. a ChatOpenAI response looks like this:

    from langchain_openai import ChatOpenAI
  
  llmm = ChatOpenAI()
    llmm.invoke('Hallo')

  
`AIMessage(content='Hallo! Wie kann ich Ihnen helfen?', response_meta
data={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-3.5-turbo', 's
ystem_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='r')`

This is how it looks when I am loading M
ixtral 8x22B with AzureMLChatOnlineEndpoint:

    from langchain_community.chat_models.azureml_endpoint import AzureMLCh
atOnlineEndpoint
    
    from langchain_community.chat_models.azureml_endpoint import (
        AzureMLEndpointApiType,

        CustomOpenAIChatContentFormatter,
    )
    from langchain_core.messages import HumanMessage
    
    chat = Az
ureMLChatOnlineEndpoint(
        endpoint_url='...',
        endpoint_api_type=AzureMLEndpointApiType.dedicated,
       
 endpoint_api_key='...',
        content_formatter=CustomOpenAIChatContentFormatter(),
    )
    
    chat.invoke('Hallo
')

  
`BaseMessage(content='Hallo, ich bin ein deutscher Sprachassistent. Was kann ich f√ºr', type='assistant', id='run-
23')`



So with the Mixtral model the output seems to be truncated and also the format is different (BaseMessage vs. AI
Message). How can I change this to make it work just like an ChatOpenAI model?

  
In my application I want to easily sw
itch between these two models.

  
Thanks in advance!
```
---

     
 
all -  [ How do I add meta data to Pinecone documents, I want to maintain overlap between chunks so I don‚Äôt w ](https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/) , 2024-07-07-0912
```
I‚Äôm developing an app that creates a knowledge base based on transcripts of YouTube videos. And I need a way to have the
 LLM recognize where the transcript came from, I have the data I just don‚Äôt know how to implement it effectively 
```
---

     
 
all -  [ History Aware Agent in Langgraph ](https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/) , 2024-07-07-0912
```
Hi,

I built a CRAG application and now want to further improve it. As a fist step I would like to check if the given qu
estion is a followup question of a previous question or not. If it is, then I want to use my messages history to create 
a new question based on the context in the chat history and the actual question. This is similar to the 'history\_aware\
_retriever' from Langchain.

However I am not satisfied with the classification of my model, as it often returns 'False'
 even if it is a followup question. So is there maybe a more elegant way of doing this or would you improve my prompt?


Heres the function within my Langgraph app:

    class CheckFollowupQuestion(BaseModel):
        '''Bool Werte um zu bes
timmen, ob die Frage auf eine zuvor gestellte Frage aufbaut.'''
    
        score: str = Field(
            description
='Die Frage bezieht sich auf eine vorherige Antwort oder zuvor gestellte Frage, 'True' oder 'False''
        )

    def 
followup_question_classifier(state: AgentState):
        messages = state['messages'][-5:]
        print(f'MESSAGES (in 
follow up): {messages}')
        question = state['question']
        system = '''<s>[INST] You assess whether the user'
s question is a follow-up question or not. For this, you get questions from the chat history and assess whether the ques
tion builds on a question or answer from the chat history or not.\n
            Evaluate with 'True' if it is a typical 
follow-up question. Also evaluate with 'True' if it seems that the question refers to a previous answer. \n
            
Evaluate with 'False' if it is a normal question, or if the question has nothing to do with the chat history. Here is an
 example:\n\n
            
            Example of a 'False' evaluation:\n
            Question: 'How much does a kebab c
urrently cost?'\n
            Chat history: [HumanMessage(content='Hello, I am Max, who are you?', id='8'), HumanMessage
(content='What was my name?', id='7'), HumanMessage(content='Name exactly one advantage of Multicloud.', id='f')]\n
    
        Your evaluation: 'False'. Reason: The questions from the chat history have nothing to do with the question asked
.\n\n
            
            Example of a 'True' evaluation:
            Question: 'And how warm will it be there tomo
rrow?'
            Chat history: [HumanMessage(content='Where is Munich located?', id='8'), HumanMessage(content='Which 
dialect is spoken in Munich?', id='7')]
            Your evaluation: 'True'. Reason: The chat history is about the city 
of Munich. In the follow-up question, the user wants to know what the weather will be 'there' tomorrow. Since the previo
us discussion was about Munich, 'there' refers to the city of Munich.
            [/INST]'''
    
        grade_prompt =
 ChatPromptTemplate.from_messages(
            [
                ('system', system),
                (
                 
   'human',
                    'User's question: {question} \n\n Chat history: {chat_history}',
                ),
    
        ]
        )
        llm = ChatOpenAI()
        structured_llm = llm.with_structured_output(GradeQuestion)
      
  grader_llm = grade_prompt | structured_llm
        result = grader_llm.invoke({'question': question, 'chat_history': m
essages})
        state['is_followup_question'] = result.score
        return state
```
---

     
 
all -  [ Pedantic data parsing ](https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/) , 2024-07-07-0912
```
I was not able to find anything on the web, the cases for which the pedantic parsers fail to parse the data coming from 
LLMs. I tried looking under the hood working and say that they are using json parsing, if anyone has info about this ple
ase enlighten me.
```
---

     
 
all -  [ How to increase the inference speed of Map reduce chain in langchain ](https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/) , 2024-07-07-0912
```
I have a problem. It takes literally 30 mins for my map-reduce summarisation chain to produce it's final output. Current
 vRAM is 16GB. What should I do to increase the speed?

Model: Llama2
```
---

     
 
all -  [ Ai Ml. Engg ](https://www.reddit.com/r/ranchi/comments/1dv0nub/ai_ml_engg/) , 2024-07-07-0912
```
Hello everyone. I hope you all are doing good. Any engineer doing Ai ML work here from Ranchi? 
Just wanted to know the 
level of mathematics required. 
Lot of talented Indians, but I feel like we are lagging way behind China, in terms of in
novation and research. I really hope I am wrong here, and my thinking is wrong. Recently came across Langchain and curre
ntly exploring it. Have a great day ahead everyone. Cheers. 
```
---

     
 
all -  [ Is it possible to stream function calling with Gemini? ](https://www.reddit.com/r/GoogleGeminiAI/comments/1dv0kut/is_it_possible_to_stream_function_calling_with/) , 2024-07-07-0912
```
Function calling is supported by Gemini models, but my initial tests with the LangChain implementation seem like I can't
 stream the function call output. I don't actually want to make an API call or something, I just want to get a structure
d output from the language model, and I would like to stream this to my frontend. Does anyone have some experience with 
this? Is it maybe possible to stream function calling output via Google's own implementation, without using LangChain? I
f it is not currently possible, are there any news or rumors about it?
```
---

     
 
all -  [ Frontend Resume Help! ](https://i.redd.it/di5xwxpjvfad1.jpeg) , 2024-07-07-0912
```
Hi Everyone! I'm looking for reviews on my resume. I have been a frontend developer for around 3 years and I'm looking f
or work. Been getting some hits (but not too great), I'm trying to see if there's anything that can be improved to get m
ore callbacks.

Would love to get some feedback. 
Thanks!

```
---

     
 
MachineLearning -  [ [P] Seeking Feedback on My GenAI Job Fit Project - New to LangChain/LangGraph ](https://www.reddit.com/r/MachineLearning/comments/1dgns9p/p_seeking_feedback_on_my_genai_job_fit_project/) , 2024-07-07-0912
```
Hi all,

Soo, i have been working on a a projectcalled [GenAI Job Fit](https://github.com/DAVEinside/GenAI_Job_Fit). It'
s an AI-driven system designed to enhance job applications by providing tailored recommendations based on individual pro
files.

I'm relatively new to LangChain and LangGraph, and I've incorporated them into this project. I would greatly app
reciate it if you could check out the repository and provide any feedback or suggestions for improvement.

Your insights
 on how I can better implement LangChain/LangGraph, or any other aspect of the project, would be incredibly valuable. I'
m eager to learn and make this project as robust as possible.

Thank you in advance for your time and feedback!

Repo Li
nk : [https://github.com/DAVEinside/GenAI\_Job\_Fit](https://github.com/DAVEinside/GenAI_Job_Fit)
```
---

     
 
MachineLearning -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-07-07-0912
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
deeplearning -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-07-07-0912
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
 
deeplearning -  [ What is ReAct Prompting? the most important piece in agentic frameworks ](https://www.reddit.com/gallery/1djk4nk) , 2024-07-07-0912
```
‚ÄúWhat is ReAct Prompting? the most important piece in agentic frameworks‚Äù - A quick read from Mastering LLM (Large Langu
age Model) 'Coffee Break Concepts' Vol.6

This document deeps dive into the ReAct Prompting method and why it's importan
t:
1. Limitations of LLM
2. Why ReAct prompting matters?
3. How ReAct Works?
4. LangChain Implementation
5. Why Prompt w
ithin agentic frameworks Matters?

Comment below on which topic you want to understand next in this 'Coffee Break Concep
ts' series and we will include those topics in upcoming weeks.
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-07-07-0912
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
