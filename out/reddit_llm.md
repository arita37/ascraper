 
all -  [ Cost effective Frameworks for llm applications in real world scenarios. ](https://www.reddit.com/r/LLMDevs/comments/1baohzc/cost_effective_frameworks_for_llm_applications_in/) , 2024-03-10-0911
```
Langchain 
Autogen 

these are the ones that i have use to develop applications using gpt-4.
but langchain gets too comp
lex for multi agent tasks.
and autogen throws random errors and can get stuck in a loop racking up my credits.
Not to me
ntion using autogen has really been heavy on my pocket…

what are the best options out there which are cost effective as
 well as have simple workflows to implement?? 

I know Langraph, crewai… but it would be great  if someone has experimen
ted with to comment on it… 

lastly gpt-4 seems too costly 
what other models would you recommend instead not compromisi
ng much on performance….?
```
---

     
 
all -  [ Academic AI Help ](https://www.reddit.com/r/LangChain/comments/1balwx1/academic_ai_help/) , 2024-03-10-0911
```
Hi,

Hopefully somebody here will be able to help with a few questions I have about creating a LM.

In short, I'd like t
o create a LM that focuses on Japanese mythology and folklore.

Creating my own GPT was working brilliantly, until I hit
 two major limitations.

The first limitation is that there is an upload limit of 20 files.

The second limitation is th
at a GPT can't be publicly accessed without the user also having a paid subscription.

At present, I'm currently using a
n inexpensive HP Stream as a Linux server.

In an ideal world, this machine would host the AI and serve it on a localhos
t port.

I realise, however, that it probably lacks the necessary CPU/GPU computing power to do so.

I don't mind buildi
ng a custom machine for this project, but I don't know where to really start.

The AI just needs to pull data from a few
 hundred documents (various eBook formats/PDFs), and a dozen or so websites, before spitting out a coherent academic ans
wer.

If self-hosting isn't an option, then are there any alternative services which may be able to do this, even for a 
monthly fee? I have come across a few services that are similar, but they're not designed for people wanting to create a
 free public AI.

Thank you in advance for your replies!
```
---

     
 
all -  [ Recommendations for RAG Evaluation Framework Resources ](https://www.reddit.com/r/LangChain/comments/1baltvi/recommendations_for_rag_evaluation_framework/) , 2024-03-10-0911
```
Hello everyone!

I'm currently on the lookout for some solid resources to help me delve into RAG evaluation frameworks. 
Whether it's articles, guides, or even personal recommendations, I'm eager to expand my knowledge in this area. If you'v
e come across any valuable resources or have expertise in RAG evaluation, I'd greatly appreciate your insights and sugge
stions. Thanks in advance!

 
```
---

     
 
all -  [ Need help reducing execution time while using langchain llama2 7b quantised version in CPU ](https://www.reddit.com/r/LangChain/comments/1balq8c/need_help_reducing_execution_time_while_using/) , 2024-03-10-0911
```
Hi, I am new to generative AI and building a document based question answering module using langchain and Llama2 as the 
LLM. I am using sentence-transformers/all-MiniLM-L6-v2 for the embeddings. But I think the actual time is being taken in
 the Retrieval Chain. Per response takes around 1 min, and I have limited logical cores to use (max: 10) and when using 
multiprocessing each response takes even longer. Any ideas on how I can reduce the execution time? Without usage of GPU?

```
---

     
 
all -  [ LangChain + python+ oobabooga API = Chat with a PDF? ](https://www.reddit.com/r/LocalLLaMA/comments/1balmqo/langchain_python_oobabooga_api_chat_with_a_pdf/) , 2024-03-10-0911
```
I have oobabooga running on my server with the API exposed.

Text-generation-webui works great for text, but not at all 
intuitive if I want to upload a file (e.g. PDF) and then ask whatever model is loaded questions about it.

I can write p
ython code (and also some other languages for a web interface), I have read that using LangChain combined with the API t
hat is exposed by oobabooga make it possible to build something that can load a PDF, tokenize it and then send it to oob
abooga and make it possible for a loaded model to use the data (and eventually answer questions about it).

Now, how / w
here do I start?

I rather utilize the API that I already have working (from ooba) and just write the part for making th
e tokenized PFF available to it, also maybe eventually create a simple web UI for it etc.

I could not find any concrete
 example, as it seem to look like the most recent trend is just find some tool, install snd use it (but then those tools
 have their own way and dont serve my intended purpose)

Could anyone send me in the right direction other then document
ation which I already am aware of (and other than general syntax don’t share code examples)

Thanks in advanced 
```
---

     
 
all -  [ Rag over updating data ](https://www.reddit.com/r/LangChain/comments/1baky7p/rag_over_updating_data/) , 2024-03-10-0911
```
Have you ever had to ingest documents about a specific topic that is changing over time? 
i.e. : news about a specific f
act may be accurate today (we only know so much) but inaccurate tomorrow (we discover some new things).
How would you re
turn only the most recent facts?
```
---

     
 
all -  [ Rag application for text and images ](https://www.reddit.com/r/LangChain/comments/1bajono/rag_application_for_text_and_images/) , 2024-03-10-0911
```
I have a use case where i got 100's of documents. I have implemented a rag for answering question related to text but th
e problem is my requirement extends to images also. The documents contains steps for some process. These steps have some
 text and followed by some image. The application i am trying to implement should behave in a way that, if asked any que
stion about the process it should not only give me the steps but also the images corresponding to it. (have to maintain 
the order of the images)  


For ex:   


Step 1: \_\_\_ some text \_\_\_  
respective image for step 1  


Step 2: \_\_
\_ some text \_\_\_  
respective image for step 2  


and so on.   


How do you even do this, is it possible?  

```
---

     
 
all -  [ Chat with rag - further questions ](https://www.reddit.com/r/LangChain/comments/1bajhg8/chat_with_rag_further_questions/) , 2024-03-10-0911
```
I already have embedding db to perform rag.
I’m just struggling with follow up questions.

- so user asks a question
- r
ag search to get content, send to loom, return response
- user asks a follow up like: “what about option 2”
- but how th
e backend know what to do next?

- should I do another rag for this specific query “what about option 2”, that is basica
lly useless.
- or send the full conversation with the prior rag, and this additional message
```
---

     
 
all -  [ Langchain recursive summary VS agentic summary VS Opus/Yi 200k context ](https://www.reddit.com/r/LocalLLaMA/comments/1baezc4/langchain_recursive_summary_vs_agentic_summary_vs/) , 2024-03-10-0911
```
What’s the best way to summarise text these days?


Was thinking of recursive summary using Langchain, or something with
 agents, or just putting it all in the Claude Opus or Yi 200k context


As an additional side note has anyone tried Cohe
re’s summary API product? Seems slightly different to other methods 
```
---

     
 
all -  [ How to connect to web-based chatpgt, gemini or claude? ](https://www.reddit.com/r/LangChain/comments/1badxki/how_to_connect_to_webbased_chatpgt_gemini_or/) , 2024-03-10-0911
```
Hi all! I am pretty new to this langchain world. But it seems langchain requires you to have api key for all llm service
s. Right now I have chatgpt 4 subscription, is it possible to connect langchain to the web-based chatgpt without using a
pi key which will cause extra money? Thanks!
```
---

     
 
all -  [ How do you decide which RAG strategy is best? ](https://www.reddit.com/r/LangChain/comments/1baba9c/how_do_you_decide_which_rag_strategy_is_best/) , 2024-03-10-0911
```
I really liked this idea of evaluating different RAG strategies. This simple project is amazing and can be useful to the
 community here. You can have your custom data evaluate different RAG strategies and finally can see which one works bes
t. Try and let me know what you guys think: [https://www.ragarena.com/](https://www.ragarena.com/) 
```
---

     
 
all -  [ Add memory to our GenAI application ](https://k33g.hashnode.dev/add-memory-to-our-genai-application) , 2024-03-10-0911
```

```
---

     
 
all -  [ Need help ](https://www.reddit.com/r/LangChain/comments/1ba9kqk/need_help/) , 2024-03-10-0911
```
My company is asking me to build a chatbot for appointment scheduling for patient engagement using LLM. Does anyone have
 experience in it? 

Is it possible with RAG approach? As we don’t have data looking to synthesize it. Or do I need to f
ine tune it with synthesized data?

It would be really great if someone could help!

```
---

     
 
all -  [ Difference between as_retriever() and .similarity_search() ](https://www.reddit.com/r/LangChain/comments/1ba77pu/difference_between_as_retriever_and_similarity/) , 2024-03-10-0911
```
Hi all--

I'm using PGVector as my vector store, but this applies to any vector store class, I assume.

If I instantiate
d the vector store and passed it as a retriever vs using the vector store and calling similarity\_search() off the insta
ntiated store to get ***k*** documents, is there a big difference? 

I understand that I would have to include the retri
eved documents more manually when packaging the prompt if going with the similarity\_search() route, but let's set that 
aside. I am just curious about the accuracy and strength comparisons between the two methods, if there is any tangible d
ifference in terms of accuracy (of retrieved docs).
```
---

     
 
all -  [ How can I use python in Godot? ](https://www.reddit.com/r/godot/comments/1b9vc31/how_can_i_use_python_in_godot/) , 2024-03-10-0911
```
Hi, I'm learning LangChain, and I want to use it in Godot. But LangChain works in Python so I'd need a way to have Godot
 communicate with Python.

How do I do that?
```
---

     
 
all -  [ New and need a little guidance from the pros ](https://www.reddit.com/r/LangChain/comments/1b9tu75/new_and_need_a_little_guidance_from_the_pros/) , 2024-03-10-0911
```
I am new and want to tackle a project and get some experience. My goal is to use langchain and or langgraph to have agen
ts fill in a standardized template with information it knows or references online. Review it for accuracy and ensure the
 template was adhered to then save it. Any advice would be most helpful, thanks. 
```
---

     
 
all -  [ Langfuse NemoGuardrails integration ](https://www.reddit.com/r/LocalLLaMA/comments/1b9kmc7/langfuse_nemoguardrails_integration/) , 2024-03-10-0911
```
Hello, I use langfuse for observability. 
It works like a charm but whenever I enable Nemo guardrails on a langchain cha
in (or part of a chain) I lose the ability to monitor that particular part of the chain. 
Do any of you know of any ways
 to solve this?
When I use langsmith It all works without problems.

```
---

     
 
all -  [ Langserve for complex cases ](https://www.reddit.com/r/LangChain/comments/1b9k8wf/langserve_for_complex_cases/) , 2024-03-10-0911
```
Hello everyone. Langserve is a great tool but it works just for single chains. I have a more complicated code structure 
where I first extract the docs from the vector store with one function, then for each document I make an llm call. How c
ould I implement this in langserve??

Thanks

&#x200B;

https://preview.redd.it/13j7n760x2nc1.png?width=796&format=png&a
uto=webp&s=4c677dd13ac857c3abed8f89baa8d2c5bd55819d

https://preview.redd.it/c366b21hw2nc1.png?width=775&format=png&auto
=webp&s=8039203c10a69c1b0d6e8b84906ea39c6b6690c4

https://preview.redd.it/eu8v689lw2nc1.png?width=802&format=png&auto=we
bp&s=0838356f0f358265831198d3286b22019606142e

https://preview.redd.it/xh9ihb6nw2nc1.png?width=643&format=png&auto=webp&
s=624c30471ade2421f6b9cb6ff61ebfaaa8544502
```
---

     
 
all -  [ Multimodel with RAG (vectordb) ](https://www.reddit.com/r/LangChain/comments/1b9jhm0/multimodel_with_rag_vectordb/) , 2024-03-10-0911
```
I have created a platform with document upload and search using vector db, embedding models and llm (simple RAG pattern)
.
Now. I want to use gpt-4 multimodel concept in this. For example, user uploads their documents through RAG but at the 
same time, while asking question, they can upload one more document in model context, or may be an image, and ask questi
on. System should get answers from document uploded in model context as well as vector db.
How can i achieve that? Is it
 a even a real usecase?
```
---

     
 
all -  [ ReadableStream ](https://www.reddit.com/r/reactnative/comments/1b9jei4/readablestream/) , 2024-03-10-0911
```
I keep getting this error after implementing Langchain in my app, expo can no longer compile it:  
I would really apprec
iate if someone could give any insights or possible fixes.

 ERROR  ReferenceError: Property 'ReadableStream' doesn't ex
ist, js engine: hermes

 ERROR  Invariant Violation: 'main' has not been registered. This can happen if:

\* Metro (the 
local dev server) is run from the wrong folder. Check if Metro is running, stop it and restart it in the current project
.

\* A module failed to load due to an error and \`AppRegistry.registerComponent\` wasn't called., js engine: hermes
```
---

     
 
all -  [ PyPDFLoader ](https://www.reddit.com/r/LangChain/comments/1b9iphd/pypdfloader/) , 2024-03-10-0911
```
Some of my pdfs will be loaded with the page_content ='', why is that so?
```
---

     
 
all -  [ How do create a chatbot that uses RAG in the llama index? ](https://www.reddit.com/r/LangChain/comments/1b9ihfy/how_do_create_a_chatbot_that_uses_rag_in_the/) , 2024-03-10-0911
```
If ask a question other than what is in the document, will not receive an answer.

I want to create a chatbot with rag f
unction using llama index.

How can I implement it?
```
---

     
 
all -  [ Do LLama-based APIs collect or store my data? ](https://www.reddit.com/r/LLMDevs/comments/1b9bty1/do_llamabased_apis_collect_or_store_my_data/) , 2024-03-10-0911
```
I've seen a few posts basically saying no but I'm not sure if that's actually a fact. Do APIs or libraries like Langchai
n, transformers or others you can find on huggingface that use LLama collect or store my data as in my prompts, any file
s or documents etc.? There are hundreds of projects out there and I personally find it difficult to understand which one
s really send data to some external server and which don't. I'm trying to narrow down the options to a few widely-accept
ed and local models that for sure don't send any of my data to any external servers. 

Thanks
```
---

     
 
all -  [ How I Reduced Our Startup's LLM Costs by Almost 90% ](https://www.reddit.com/r/SaaS/comments/1b92w5o/how_i_reduced_our_startups_llm_costs_by_almost_90/) , 2024-03-10-0911
```
With AI apps popping up everywhere, it’s fair to think building one is both easy and cheap.

Unfortunately, you’d be *(m
ostly)* wrong. I know because I learned the hard way.

I’m not saying it’s hard per se, but as of this writing, gpt-4-tu
rbo costs $0.01/$0.03 per 1000 input/output tokens. This can quickly add up if you’re building a complex AI workflow.

Y
es, you could use less expensive, worse performing models, like GPT 3.5 or an open-source one like Llama, stuff everythi
ng into one API call with excellent prompt engineering, and hope for the best. But this probably won’t turn out that gre
at. This type of approach doesn’t really work in production—at least not yet with the current state of AI.

**It could g
ive you the right answer 90% or even 99% of the time.** But that one time it decides to go off the rails, it’s really fr
ustrating. As a developer and/or business, you know you must never break a user’s experience. It might be okay for a toy
 app or prototype but not for a production-grade application you charge for.

Imagine if Salesforce or any other establi
shed software company said its reliability was only one or two nines. That would be insane. No one would use it.

**But 
this is the state of most AI applications today. They’re unreliable.**

# AI isn’t a Universal Function

The non-determi
nistic nature of LLMs forces us to be more thoughtful about how we write our code. We should not just “hope” that an LLM
 will always correctly respond. We need to build redundancy and proper error handling. For some reason, many builders fo
rget everything they learned about software engineering and treat AI like some magical universal function that doesn’t f
ail.

**It’s not there yet.**

To fix this limitation, we must write code that only interacts with AI when absolutely ne
cessary—that is, when a system needs some sort of “human-level” analysis of unstructured data. Subsequently, whenever po
ssible, we must force the LLM to return references to information (i.e., a pointer) instead of the data itself.

**When 
I recognized these two things, I had to redesign the backend architecture of my personal software business completely.**


# Rearchitecting Jellypod

For context, I started an app called Jellypod. It enables users to subscribe to email newsl
etters and get a daily summary of the most important topics from all of them as a single podcast.

This seems pretty sim
ple on the outside—and the MVP honestly was. The app would just process each email individually, summarize it, convert i
t to speech, and stitch all the audio together, side-by-side, into a daily podcast.

The output was fine, but it needed 
to be better.

If two different newsletters discussed the same topic, the “podcast” would talk about it twice, not reali
zing we had already mentioned it. You could say, “Well, why don’t you just stuff all the newsletter content into one big
 LLM call to summarize everything?”

Well, that’s what I tried at first.

And it failed. **Miserably.**

Even with an ex
tremely detailed prompt using all the best practices, I couldn’t guarantee that the LLM would always detect the most imp
ortant topics, summarize everything, and consistently create an in-depth output. Also, the podcast always needed to be \
~10 minutes long.

So I went back to the drawing board. How can I make this system better? And yes, we’re getting to the
 cost reduction part - don’t worry!

# Defining the Requirements

Jellypod must be able to process any number of input d
ocuments (newsletters) and create an output that always includes the top ten most important topics across all those inpu
ts. If two subparts of any input are about the same topic, we should recognize that and merge the sections into one topi
c.

For example, if the Morning Brew has a section about US Elections and the Daily Brief also has a section on the curr
ent state of US Politics, they should be merged. I’ll skip over how I determined a similarity threshold (i.e., should tw
o topics be merged or remain separate).

# Exploding Costs

I built on top of a few different approaches outlined in pap
ers written by the LangChain community to semantic chunk and organize everything in a almost deterministic way.

**But t
his was INSANELY expensive.** The number of API calls grew at a rate of O(n log n), with n being the number of input chu
nks from all newsletters.

So, I had a dilemma. Do I keep this improved and more expensive architecture or throw it down
 the drain?

I decided to keep it and figure out how to reduce costs.

# Reducing Costs

That’s when I discovered a tool
 called OpenPipe that allows you to fine-tune open-source models almost too easily. It looked legit and was backed by YC
ombinator, so I gave it a try.

I swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my
 LLM API calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which
 I could use to fine-tune a cheaper open-source model.

After about a week of recording Jellypod’s LLM calls, I had abou
t 50,000 rows of data. And with a few clicks, I fine-tuned a Mistral 7B model for each LLM call.

I replaced GPT-4 with 
the new fine-tuned model.

**And it reduced the costs by 88%.**

The cost of inference dropped from $10 per 1M input tok
ens to $1.20. And cost per output token dropped from $30 to $1.60.

I was blown away. I could now run Jellypod’s new arc
hitecture for approximately the same cost as the MVP’s trivial approach. I even confirmed that the fine-tuned Mistral ou
tput quality was just as high as GPT-4 by a series of evals and in-app customer feedback.

By redesigning the system to 
only use AI for the smallest unit of work it’s actually needed for, I could confidently deploy a fine-tuned model as a d
rop-in replacement for GPT 4. And by prompting to return pointers to data instead of the data itself, I could ensure dat
a integrity while reducing the number of output tokens consumed.

# In Conclusion

If you’re considering building an AI 
application, I would encourage you to take a step back and think about your architecture’s output reliability and costs.
 What happens if the LLM doesn’t answer your prompt in the right way? Can you prompt the model to return data identifier
s instead of raw data? And, is it possible to swap GPT-4 with a cheaper, fine-tuned model?

I wish I had these insights 
when I started, but hey, you live and learn.

I hope you found at least some parts of this interesting! I thought there 
were enough learnings to share. Feel free to reach out if you’re curious about the details.
```
---

     
 
all -  [ How I Reduced Our LLM Costs by Over 85% ](https://www.reddit.com/r/Entrepreneur/comments/1b92suo/how_i_reduced_our_llm_costs_by_over_85/) , 2024-03-10-0911
```
With AI apps popping up everywhere, it’s fair to think building one is both easy and cheap.

Unfortunately, you’d be *(m
ostly)* wrong. I know because I'm building one. 

I’m not saying it’s hard per se, but as of this writing, gpt-4-turbo c
osts $0.01/$0.03 per 1000 input/output tokens. This can quickly add up if you’re building a complex AI workflow.

Yes, y
ou could use less expensive, worse performing models, like GPT 3.5 or an open-source one like Llama, stuff everything in
to one API call with excellent prompt engineering, and hope for the best. But this probably won’t turn out that great. T
his type of approach doesn’t really work in production—at least not yet with the current state of AI.

**It could give y
ou the right answer 90% or even 99% of the time.** But that one time it decides to go off the rails, it’s really frustra
ting. As a developer and/or business, you know you must never break a user’s experience. It might be okay for a toy app 
or prototype but not for a production-grade application you charge for.

Imagine if Salesforce or any other established 
software company said its reliability was only one or two nines. That would be insane. No one would use it.

**But this 
is the state of most AI applications today. They’re unreliable.**

# AI isn’t a Universal Function

The non-deterministi
c nature of LLMs forces us to be more thoughtful about how we write our code. We should not just “hope” that an LLM will
 always correctly respond. We need to build redundancy and proper error handling. For some reason, many builders forget 
everything they learned about software engineering and treat AI like some magical universal function that doesn’t fail.


**It’s not there yet.**

To fix this limitation, we must write code that only interacts with AI when absolutely necessa
ry—that is, when a system needs some sort of “human-level” analysis of unstructured data. Subsequently, whenever possibl
e, we must force the LLM to return references to information (i.e., a pointer) instead of the data itself.

**When I rec
ognized these two things, I had to redesign the backend architecture of my personal software business completely.**

# R
earchitecting Jellypod

For context, I started an app called Jellypod. It enables users to subscribe to email newsletter
s and get a daily summary of the most important topics from all of them as a single podcast.

This seems pretty simple o
n the outside—and the MVP honestly was. The app would just process each email individually, summarize it, convert it to 
speech, and stitch all the audio together, side-by-side, into a daily podcast.

The output was fine, but it needed to be
 better.

If two different newsletters discussed the same topic, the “podcast” would talk about it twice, not realizing 
we had already mentioned it. You could say, “Well, why don’t you just stuff all the newsletter content into one big LLM 
call to summarize everything?”

Well, that’s what I tried at first.

And it failed. **Miserably.**

Even with an extreme
ly detailed prompt using all the best practices, I couldn’t guarantee that the LLM would always detect the most importan
t topics, summarize everything, and consistently create an in-depth output. Also, the podcast always needed to be ~10 mi
nutes long.

So I went back to the drawing board. How can I make this system better? And yes, we’re getting to the cost 
reduction part - don’t worry!

# Defining the Requirements

Jellypod must be able to process any number of input documen
ts (newsletters) and create an output that always includes the top ten most important topics across all those inputs. If
 two subparts of any input are about the same topic, we should recognize that and merge the sections into one topic.

Fo
r example, if the Morning Brew has a section about US Elections and the Daily Brief also has a section on the current st
ate of US Politics, they should be merged. I’ll skip over how I determined a similarity threshold (i.e., should two topi
cs be merged or remain separate).

# Exploding Costs

I built on top of a few different approaches outlined in papers wr
itten by the LangChain community to semantic chunk and organize everything in a almost deterministic way.

**But this wa
s INSANELY expensive.** The number of API calls grew at a rate of O(n log n), with n being the number of input chunks fr
om all newsletters.

So, I had a dilemma. Do I keep this improved and more expensive architecture or throw it down the d
rain?

I decided to keep it and figure out how to reduce costs.

# Reducing Costs

That’s when I discovered a tool calle
d OpenPipe that allows you to fine-tune open-source models almost too easily. It looked legit and was backed by YCombina
tor, so I gave it a try.

I swapped out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM A
PI calls to OpenAI but recorded all inputs and outputs. This created unique datasets for each of my prompts, which I cou
ld use to fine-tune a cheaper open-source model.

After about a week of recording Jellypod’s LLM calls, I had about 50,0
00 rows of data. And with a few clicks, I fine-tuned a Mistral 7B model for each LLM call.

I replaced GPT-4 with the ne
w fine-tuned model.

**And it reduced the costs by 88%.**

The cost of inference dropped from $10 per 1M input tokens to
 $1.20. And cost per output token dropped from $30 to $1.60.

I was blown away. I could now run Jellypod’s new architect
ure for approximately the same cost as the MVP’s trivial approach. I even confirmed that the fine-tuned Mistral output q
uality was just as high as GPT-4 by a series of evals and in-app customer feedback.

By redesigning the system to only u
se AI for the smallest unit of work it’s actually needed for, I could confidently deploy a fine-tuned model as a drop-in
 replacement for GPT 4. And by prompting to return pointers to data instead of the data itself, I could ensure data inte
grity while reducing the number of output tokens consumed.

# In Conclusion

If you’re considering building an AI applic
ation, I would encourage you to take a step back and think about your architecture’s output reliability and costs. What 
happens if the LLM doesn’t answer your prompt in the right way? Can you prompt the model to return data identifiers inst
ead of raw data? And, is it possible to swap GPT-4 with a cheaper, fine-tuned model?

I wish I had these insights when I
 started, but hey, you live and learn.

I hope you found at least some parts of this interesting! I thought there were e
nough learnings to share. Feel free to reach out if you’re curious about the details.
```
---

     
 
all -  [ How I Reduced Our LLM Costs by Over 85% ](https://www.reddit.com/r/ArtificialInteligence/comments/1b92hlk/how_i_reduced_our_llm_costs_by_over_85/) , 2024-03-10-0911
```
With AI apps popping up everywhere, it’s fair to think building one is both easy and cheap.

Unfortunately, you’d be *(m
ostly)* wrong.

I’m not saying it’s hard per se, but as of this writing, gpt-4-turbo costs $0.01/$0.03 per 1000 input/ou
tput tokens. This can quickly add up if you’re building a complex AI workflow.

Yes, you could use less expensive, worse
 performing models, like GPT 3.5 or an open-source one like Llama, stuff everything into one API call with excellent pro
mpt engineering, and hope for the best. But this probably won’t turn out that great. This type of approach doesn’t reall
y work in production—at least not yet with the current state of AI.

**It could give you the right answer 90% or even 99
% of the time.** But that one time it decides to go off the rails, it’s really frustrating. As a developer and/or busine
ss, you know you must never break a user’s experience. It might be okay for a toy app or prototype but not for a product
ion-grade application you charge for.

Imagine if Salesforce or any other established software company said its reliabil
ity was only one or two nines. That would be insane. No one would use it.

**But this is the state of most AI applicatio
ns today. They’re unreliable.**

# AI isn’t a Universal Function

The non-deterministic nature of LLMs forces us to be m
ore thoughtful about how we write our code. We should not just “hope” that an LLM will always correctly respond. We need
 to build redundancy and proper error handling. For some reason, many builders forget everything they learned about soft
ware engineering and treat AI like some magical universal function that doesn’t fail.

**It’s not there yet.**

To fix t
his limitation, we must write code that only interacts with AI when absolutely necessary—that is, when a system needs so
me sort of “human-level” analysis of unstructured data. Subsequently, whenever possible, we must force the LLM to return
 references to information (i.e., a pointer) instead of the data itself.

**When I recognized these two things, I had to
 redesign the backend architecture of my personal software business completely.**

# Rearchitecting Jellypod

For contex
t, I started an app called Jellypod. It enables users to subscribe to email newsletters and get a daily summary of the m
ost important topics from all of them as a single podcast.

This seems pretty simple on the outside—and the MVP honestly
 was. The app would just process each email individually, summarize it, convert it to speech, and stitch all the audio t
ogether, side-by-side, into a daily podcast.

The output was fine, but it needed to be better.

If two different newslet
ters discussed the same topic, the “podcast” would talk about it twice, not realizing we had already mentioned it. You c
ould say, “Well, why don’t you just stuff all the newsletter content into one big LLM call to summarize everything?”

We
ll, that’s what I tried at first.

And it failed. **Miserably.**

Even with an extremely detailed prompt using all the b
est practices, I couldn’t guarantee that the LLM would always detect the most important topics, summarize everything, an
d consistently create an in-depth output. Also, the podcast always needed to be ~10 minutes long.

So I went back to the
 drawing board. How can I make this system better? And yes, we’re getting to the cost reduction part - don’t worry!

# D
efining the Requirements

Jellypod must be able to process any number of input documents (newsletters) and create an out
put that always includes the top ten most important topics across all those inputs. If two subparts of any input are abo
ut the same topic, we should recognize that and merge the sections into one topic.

For example, if the Morning Brew has
 a section about US Elections and the Daily Brief also has a section on the current state of US Politics, they should be
 merged. I’ll skip over how I determined a similarity threshold (i.e., should two topics be merged or remain separate).


# Exploding Costs

I built on top of a few different approaches outlined in papers written by the LangChain community t
o semantic chunk and organize everything in a almost deterministic way.

**But this was INSANELY expensive.** The number
 of API calls grew at a rate of O(n log n), with n being the number of input chunks from all newsletters.

So, I had a d
ilemma. Do I keep this improved and more expensive architecture or throw it down the drain?

I decided to keep it and fi
gure out how to reduce costs.

# Reducing Costs

That’s when I discovered a tool called OpenPipe that allows you to fine
-tune open-source models almost too easily. It looked legit and was backed by YCombinator, so I gave it a try.

I swappe
d out the OpenAI SDK with their SDK (a drop-in replacement), which passed all my LLM API calls to OpenAI but recorded al
l inputs and outputs. This created unique datasets for each of my prompts, which I could use to fine-tune a cheaper open
-source model.

After about a week of recording Jellypod’s LLM calls, I had about 50,000 rows of data. And with a few cl
icks, I fine-tuned a Mistral 7B model for each LLM call.

I replaced GPT-4 with the new fine-tuned model.

**And it redu
ced the costs by 88%.**

The cost of inference dropped from $10 per 1M input tokens to $1.20. And cost per output token 
dropped from $30 to $1.60.

I was blown away. I could now run Jellypod’s new architecture for approximately the same cos
t as the MVP’s trivial approach. I even confirmed that the fine-tuned Mistral output quality was just as high as GPT-4 b
y a series of evals and in-app customer feedback.

By redesigning the system to only use AI for the smallest unit of wor
k it’s actually needed for, I could confidently deploy a fine-tuned model as a drop-in replacement for GPT 4. And by pro
mpting to return pointers to data instead of the data itself, I could ensure data integrity while reducing the number of
 output tokens consumed.

# In Conclusion

If you’re considering building an AI application, I would encourage you to ta
ke a step back and think about your architecture’s output reliability and costs. What happens if the LLM doesn’t answer 
your prompt in the right way? Can you prompt the model to return data identifiers instead of raw data? And, is it possib
le to swap GPT-4 with a cheaper, fine-tuned model?

I wish I had these insights when I started, but hey, you live and le
arn.

I hope you found at least some parts of this interesting! I thought there were enough learnings to share. Feel fre
e to reach out if you’re curious about the details.
```
---

     
 
all -  [ [For Hire] Programmer/Web Developer/IT Consultant (Python, PHP, AI, etc.) ](https://www.reddit.com/r/forhire/comments/1b8zd7n/for_hire_programmerweb_developerit_consultant/) , 2024-03-10-0911
```
To get in contact, please message me, I don't use the chat thing and might miss you or reply very late. Then we can swit
ch to email/discord/telegram or whatever else. Apologies for starting with this, but many missed it when it was lower.


I'm a programmer/web developer with 14 years of professional experience. I am available for all sorts of programming and
 web development tasks.

I also offer consulting services. If you need something done, but don't know how exactly, I can
 help. I'm an excellent researcher and I communicate well. I will work with you to find the best solution for your probl
em.

My services include, but are not limited to:

* websites

* desktop applications

* AI integration (chatGPT API, la
ngchain, whatever else turns up)

* integration with APIs and other webservices

* all kinds of scripts

* task automati
on

* website optimization

* debugging

* plugins for existing software

* bots (Reddit, Telegram, etc)

* code audits


If you're looking for someone to take care of a variety of different tasks, I can offer continuous support.

My preferr
ed environment is Python with Django, but I work with anything Python or PHP based. I have no problem with learning new 
technologies that are needed for the project.

Rate is $50/h.

Portfolio:

https://bdabkowski.yum.pl

Satisfied customer
s:

https://www.reddit.com/r/testimonials/comments/2e8gqy/pos_uqui_need_a_backend_web_dev_look_no_further/

https://www.
reddit.com/r/testimonials/comments/7fsdze/pos_hiring_uqui_was_an_example_of_how_it_should/

https://www.reddit.com/r/tes
timonials/comments/80pu9l/pos_uqui_great_work_detailed_and_fast/

https://www.reddit.com/r/testimonials/comments/b0nx68/
uqui_is_a_hardworking_intelligent_honest_apps/

https://www.reddit.com/r/testimonials/comments/j3mz3p/uqui_is_a_great_we
b_development_consultant_with/

https://www.reddit.com/r/testimonials/comments/v40ay3/pos_uqui_is_a_great_backend_dev_to
_work_with/

Please note: I am not a designer. To make it clear, it means zero aesthetic sense.
```
---

     
 
all -  [ Github web loader + Pinecone Index fail due to UTF-16 encoding when trying to Upsert vector embeddin ](https://www.reddit.com/r/LangChain/comments/1b8xh9g/github_web_loader_pinecone_index_fail_due_to/) , 2024-03-10-0911
```
Hi folks, I'm putting together a simple Github RAG chatbot using \`GithubRepoLoader\` and Pinecone vector store. However
 when upserting the embeddings to Pinecone, the upsert process fails with the error: \`PineconeBadRequestError: Missing 
low surrogate.\`  
After a bit of research this seem to be due to how UTF-16 encodes the Unicode Characters outside the 
BMP (Basic Multilingual Plane) are represented using a pair of surrogate code points in UTF-16 encoding. The error messa
ge indicates that a high surrogate code point (D800–DBFF) was found without a corresponding low surrogate (DC00–DFFF) fo
llowing it.  
Now with that being said, my question is:   
Would it be feasible to use UTF-8 instead? Where would be a g
ood place (at what level of the stack) in the code would you make some changes to inject a parameter for a different (i.
e. UTF-8) encoding?  
TYIA
```
---

     
 
all -  [ After struggling with LangChain text splitters, I decided to make my own convenient service to chunk ](https://www.reddit.com/r/LangChain/comments/1b8trzs/after_struggling_with_langchain_text_splitters_i/) , 2024-03-10-0911
```
In my experience developing RAG-based applications with LangChain, I was surprised to find that there aren't any simple,
 reliable ways to chunk files. The default [Text Splitters](https://js.langchain.com/docs/modules/data_connection/docume
nt_transformers/) that LangChain offers employ a naive form of chunking that doesn't consider positioning data like sect
ions, subsections, paragraphs or tables. 

This led me to implement my own chunking service that includes deep positioni
ng data like page index and bounding box coordinates for every chunk.

You can try it out for free here (no account/api 
key required):

https://filechipper.com

Would any of you be interested in something like this? Let me know!
```
---

     
 
all -  [ Chatbot via Kendra/Bedrock/LangChain returning non-relevant documents ](https://www.reddit.com/r/LangChain/comments/1b8sex2/chatbot_via_kendrabedrocklangchain_returning/) , 2024-03-10-0911
```
I am building a RAG chatbot on internal corporate documents. My specific architecture is Amazon Kendra for document retr
ieval, Amazon Bedrock using foundation models (llama or cohere), and LangChain as the orchestrator. I have purposely ask
ed it a question that is not in the corporate documentation. The bot correctly returns that it doesn’t know, but it is s
till returning documents from Kendra. I’m using AmazonKendraRetriever as the retriever and ConversationalRetrievalChain 
as the chain. Trying to figure out how to not return any src documents when it’s an out of scope question. Any help is a
ppreciated!
```
---

     
 
all -  [ How To Build a Custom Chatbot Using LangChain With Examples ](https://www.reddit.com/r/Langchaindev/comments/1b8rz4q/how_to_build_a_custom_chatbot_using_langchain/) , 2024-03-10-0911
```
Hey everyone, I have written a new blog that explains how you can create a custom AI-powered chatbot using LangChain wit
h code examples.

At the end of this blog, I have also given a working chatbot, that has been developed using LangChain,
 OpenAI API, and Pinecone that you can use and test.

You can read it at [LangChain Chatbot](https://www.deligence.com/l
angchain-chatbot/)

Feedback appreciated!
```
---

     
 
all -  [ Can't make the chat to understand previous context ](https://www.reddit.com/r/LangChain/comments/1b8prsz/cant_make_the_chat_to_understand_previous_context/) , 2024-03-10-0911
```
Could someone kindly assist me with this issue?

[https://github.com/langchain-ai/langchain/discussions/18722](https://g
ithub.com/langchain-ai/langchain/discussions/18722)
```
---

     
 
all -  [ Doubts about choosing vet for storage ](https://www.reddit.com/r/LangChain/comments/1b8po5e/doubts_about_choosing_vet_for_storage/) , 2024-03-10-0911
```
Hi.  Just starting a new journey on this, and Need some clarification. I’m building a complex rag system for many differ
ent kind of documents. The way I understand between the many commercially available vector stores, some have different s
trengths and advantages depending on what king of data you retrieving. 
There is some good comparison between then in re
gards of kind of data and chunk sizes? Ro help on which to choose, or this difference is negligible and we can choose wh
atever is easier to implement?
```
---

     
 
all -  [ stop agent from generate new input. ](https://www.reddit.com/r/LangChain/comments/1b8p37d/stop_agent_from_generate_new_input/) , 2024-03-10-0911
```
How I can stop LLM Agent new input, I want just to stop the generation process and extract the first AI answer.  


&#x2
00B;

https://preview.redd.it/xz8t0cdpbvmc1.png?width=765&format=png&auto=webp&s=b249549b5be459acfbc4aed3b3181023fb2298d
d

https://preview.redd.it/fjsfyv4abvmc1.png?width=1102&format=png&auto=webp&s=0a5f8df8e756e7cfa6580d014bbcbf7270822af1
```
---

     
 
all -  [ Building a email responder with langchain? ](https://www.reddit.com/r/LangChain/comments/1b8mfdw/building_a_email_responder_with_langchain/) , 2024-03-10-0911
```
As the text suggests, can I build an application for creating responses for emails when i provide them with a email text
? Any advice or heads up in this direction would help me start with the project.

Thanks in advance!
```
---

     
 
all -  [ Auto GPT is hallucinating.How to make AutoGPT read json data and work onto it to generate test seque ](https://www.reddit.com/r/LangChain/comments/1b8mans/auto_gpt_is_hallucinatinghow_to_make_autogpt_read/) , 2024-03-10-0911
```
So I'm using an LLM = AzureChatOpenAI() and i have a .json data file that has  the content of various APIs in the form  
:
 '2': {
        'Method': 'POST',
        'Path': '',
        'FunctionName': '',
        'FunctionCode': '{\...\}',
 
       'Queries': []
    } 

I have a list of these and I'm using AzureOpenAI() as my embeddings model to create a vecto
r store db and retrieve. The I'm initialising my agent= AutoGPT with memory as vectorstore.as_retriever.

My end goal is
 to generate end to end flow of these APIs for testing purposes for the given api and original prompt is 
 agent.run('Wh
at are the different API sequences that are possible to test the end to end flow of the API for the given APIs. The diff
erent fields that are present in the json are path, method, queries,FunctionName and FunctionCode. You cannot ask for hu
man input.Start by using APIs wivh no prerequisitesand authentication') 

Also I've defined custom tool which 
@tool
def
 get_api_based_on_index(index: int) -> dict:
    '''Get API details based on the given index number'''
    if str(index)
 in data:
        return data[str(index)]  
    else:
        raise ValueError(f'Index {index} not found in the data.')


But right now my langchain agent is hallucinating and not able to get api values and fields and is only looping around 
3-4 APIs .

Can anyone look into this and help me such that I can get this agent to retrieve the json data as vectors an
d  go through all of that data and generate me test flow sequences of various APIs that is generally done in end to end 
software testing.
```
---

     
 
all -  [ What is the difference between llama index and Langchain? ](https://www.reddit.com/r/LangChain/comments/1b8iaac/what_is_the_difference_between_llama_index_and/) , 2024-03-10-0911
```
So far, I have implemented RAG using Langchain. In the case of Langchain’s RAG,

It was like this: 'Load document -> Tex
t split -> Chroma vector DB embedding -> llm.'



However, in the case of llama index, it looks like there is no vector 
DB embedding. Am I misunderstanding it?

And setting the embedding model doesn't seem to exist in the llama index.

I wo
uld appreciate it if you could explain. or git code please
```
---

     
 
all -  [ Userinterface for the API key?? The answer is : Poe ](https://www.reddit.com/r/ChatGPTPro/comments/1b8fkb7/userinterface_for_the_api_key_the_answer_is_poe/) , 2024-03-10-0911
```
Yeah, it is fairly simple to set up a custom chatbot on  with the server feature. This way, you  can use any chatbot  th
at you have an api for, and make poe the user interface. Im so happy i found this, going to have a lot of fun trying all
 the new models without having to pay a subscription im not going to use to the fullest.

Basicly, you write some python
 code, deploy it on modal website which offer 30 dollars of compute per month for free, and this will only require max 1
dollar since it isnt the one doing the inference. Then connect it to poe and you are up and running. The only problem is
, that the documentation is confusing as f, but i feel like i understand it completely now, and it is fairly simple. Fee
l free to ask.

  
Note: this way you can make infinite chatbots and even do langchain or very complicated agents and mu
ch more, much cheaper even. Consider making a model, that uses some advanced prompt tactic before it comes up with the a
nswer. Possible!, on the web, available on your phone, anywhere. 
```
---

     
 
all -  [ Career Direction for a heavily technical but well rounded SE ](https://www.reddit.com/r/salesengineers/comments/1b8e8k9/career_direction_for_a_heavily_technical_but_well/) , 2024-03-10-0911
```
Hoping to get some internet wisdom here. My career has taken me from M&A and management consulting, to running a family 
business in petro equipment remanufacturing, and in the past \~8 years in Data Engineering, Data Science, and setting up
 a SE department.    


Here’s a quick rundown of my career path:

* **Early Career:** Undergrad in finance, first job i
n management consulting (M&A), ran systems engineering and sales for a family business while I was at PwC concurrently.

* **Shift to Tech:** Sold the family business and knew I wanted to do something heavily technical. Moved to TN, did a bo
otcamp and got linked with a network of heavy hitters in the health tech scene.  Ran BI and DW for a health tech start u
p, exited, and was recruited to do big-data DE for another startup. Earned my MS in Data Science from Georgia Tech, init
ially transitioning from data engineering to data science.
* **Move to SE:** I built a few prototypes and did some R&D o
n the side which landed some big clients outside of our target industry (our PE overlords pegged it at $150m of addition
al TAM).  Also did a side project which ended up featured in the New York Times and a bunch of other health tech publica
tions.  That generated enough interest that the C-Suite asked me to setup a sales engineering department (easy move beca
use DS was moving way too slow for my taste).
* **Tech and Sales Intersection:** Automated a lot of our sales team's pro
cesses which sped them up and built a few prototypes and case studies to land $2M in ARR within my first 8 months in the
 formal SE role.
* **Skills and Interests:** I'm deep into Spark (Scala, PySpark, SparkSQL), Python, SQL, DW architectur
e (big and normal data environments), Azure Cloud infrastructure, Machine Learning, LLMs / Agents, and Tableau (regretta
bly). My W2 expertise is largely in healthcare economy data but I’ve also built a side-project with a FastAPI, langchain
 backend / retool front-end to support a buddy from college who wants to expand his crypto hedge fund. I'm technically v
ery well rounded.

**What I’m Looking For:**

* Insight into sales or sales engineering roles that need someone who’s no
t just technical but *really* technical. I love programming and building things but also have a track record of closing 
deals and building stuff to make my sales team faster.
* To connect with folks in tech-heavy SE roles to understand what
 their day-to-day and teams look like.

**Why I’m Exploring:**

* Growth concerns with my current company, especially gi
ven our target industry's slim margins.
* I’m the go-to engineer on the non-engineering side, often stretched thin acros
s tasks that don’t align with my role or support my commission goals.
* Despite significant contributions, especially on
 the consulting/research side, it’s not reflecting in my compensation.

Would love to hear your thoughts, experiences, o
r if you know of roles that might fit my blend of skills and passions I've got going on. 
```
---

     
 
all -  [ Curly braces in prompts again... ](https://www.reddit.com/r/LangChain/comments/1b8cpcz/curly_braces_in_prompts_again/) , 2024-03-10-0911
```
I saw the previous discussion about using double curly braces inside a template to avoid expansion in a prompt's format 
method.

However, I'm still having a problem with this, even with double curly-braces, when I use a \`FewShotPrompt\`.  
I have \*examples\* in the FSP that contain code. So I suspect what is going wrong is that the double curly-braces help 
when the examples are folded into the prompt, but then when the \*input\* is folded in, the double curly braces have bee
n stripped and I get an error.

This seems like a numbskull problem on my part, so even just a pointer to some tutorial 
about using a FewShotPrompt with code (surely there must be one?) would be great.
```
---

     
 
MachineLearning -  [ [D] : Scale PDF Q&A App to 10K Users with GPUs – <$250/Mo ](https://www.reddit.com/r/MachineLearning/comments/1b6jv56/d_scale_pdf_qa_app_to_10k_users_with_gpus_250mo/) , 2024-03-10-0911
```
Hello everyone,

Check out this step-by-step detailed tutorial on building and scaling a PDF Q&A Application using Pinec
one, Langchain and Inferless

&#x200B;

[Architecture](https://preview.redd.it/zfta52cbddmc1.png?width=1301&format=png&a
uto=webp&s=440399212d3feb03e861759a31602e2cde0dc7fb)

Alongside, the detailed quick deploy guide, it also includes cost 
analysis on how you can save upto 84% cost with an example of processing 3000 documents and nearly 10,000 queries every 
month, all while dramatically cutting your costs from $1800 ( AWS) to just $250 a month on Inferless.

Here is the tutor
ial - [https://cookbook.inferless.com/](https://cookbook.inferless.com/)

If you resonate, join the discussion on Hacker
news here - [https://news.ycombinator.com/item?id=39594588](https://news.ycombinator.com/item?id=39594588)
```
---

     
 
MachineLearning -  [ [D] What Is Your LLM Tech Stack in Production? ](https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/) , 2024-03-10-0911
```
Curious what everybody is using to implement LLM powered apps for production usage and your experience with these toolin
gs and advice. 

This is what I am using for some RAG prototypes I have been building for users in finance and capital m
arkets.

**Pre-processing\ETL:**
Unstructured.io + Spark, Airflow

**Embedding model:**
Cohere Embed v3
Previously using
 OpenAI Ada but Cohere has significantly better retrieval recall and precision for my use case. Also exploring other ope
n weights embedding models

**Vector Database:**
Elasticsearch previously but now using Pinecone

**LLM:**
Gone through 
quite a few including hosted and self-hosted options. Went with gpt4 early during prototyping then switched to gpt3.5-tu
rbo for more manageable costs and eventually open weights models. 

Now using a fine-tuned Llama2 70B model self hosted 
with vLLM 

**LLM Framework:**
Started with Langchain initially but found it cumbersome to extend as the app became more
 complex. Tried implementing it in LlamaIndex at some point just to learn and found it just as bad. Went back to Langcha
in and now I am in the midst of replacing it with my own logic

What is everyone else using?

Edit: correct model Llama2
 70B
```
---

     
 
MachineLearning -  [ [D] Graphs + vectordbs? Need your input: Cognee.ai . AI Data Pipelines for Real-World Production (Pa ](https://www.reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/) , 2024-03-10-0911
```
Hey there, Redditors!

I'm back with the latest installment on creating dependable AI data pipelines for real-world prod
uction.

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://top
oteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba40a
ab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines.

After a few months of work
, we integrated cognitive architecture with [keepi.ai](https://www.keepi.ai) 

We aim to explore with our demo:

**1. Co
ntext sanitization**  
The world of AI is fast-moving, and we've realized that the context is becoming a building block 
we refer to as a crucial part of future cognitive architecture.  
**2. Best Practices for AI Memory**  
In this rapidly 
evolving landscape, there are no established best practices. You'll need to make educated bets on tools and processes, k
nowing that things will change. We assume that having traditional data engineering practices + frameworks + classifiers 
and other AI solutions can solve a lot of standard hurdles  
**3. AI Frameworks**  
They are trying to do too much, too 
fast, too broad. We want to find a pattern and a correct layer of abstraction for the AI memory to fit new industry.  



&#x200B;

How does it work? 

The Github repo is l:

  


[How cognee works](https://preview.redd.it/yuiabmyihyjc1.png?
width=1633&format=png&auto=webp&s=4384c4441b615f72caf1e0591c5ab23aee735fab)

Github repo is [here](https://github.com/to
poteretes/cognee)

Next steps:  
I have questions for you:

1. Is context sanitization relevant for you?
2. How do you m
anage metadata? 
3. How do you prepare data for LLMs?
4. Are there any data enrichment steps you perform?

Check out the
 blog post:

[Link to part 4](https://topoteretes.notion.site/Going-beyond-Langchain-Weaviate-Level-4-towards-production
-fe90ff40e56e44c4a49f1492d360173c?pvs=4)

*Remember to give this post an upvote if you found it insightful!*  
*And also
 star our* [Github repo](https://github.com/topoteretes/cognee)
```
---

     
 
MachineLearning -  [ [D] AI projects Suggestions ](https://www.reddit.com/r/MachineLearning/comments/1aunkmw/d_ai_projects_suggestions/) , 2024-03-10-0911
```
Hi Everyone, I need a suggestion to create AI courses for students ( Hands-on AI projects). I am thinking about the late
st AI trends such as Langchain, RAG, and vector databases. In each project, there can be multiple tasks, and the main th
ing is each task should have an automated system in which we can verify whether students have done it correctly or not.


For example: Project with visualization cannot be automatically tested. 

For example: A project with visualization can
not be automatically tested. . em can verify if the length of the text is smaller we can verify that it is correct.
```
---

     
 
MachineLearning -  [ Whats in your RAG setup? [D] ](https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/) , 2024-03-10-0911
```
What frameworks and libraries are you using in your RAG? 

I'm most curious if  LangChain is as popular as it was?

Here
's mine at a high-level: 

*  langchain to use OpenAI for creating embeddings
* Pinecone for storing embedding
* langcha
in to load document splitters and characters splitters for chunking
* Mongo for conversations memory

&#x200B;
```
---

     
 
deeplearning -  [ [D] WebVoyager: Navigating Digital Cosmos with LangGraph & Multimodal Models ](https://www.reddit.com/r/deeplearning/comments/1altlca/d_webvoyager_navigating_digital_cosmos_with/) , 2024-03-10-0911
```
Embark on a journey through the digital cosmos with WebVoyager, a groundbreaking Large Multimodal Model (LMM) web agent 
designed to navigate the vastness of the online universe. In collaboration with Langchain, WebVoyager represents a parad
igm shift in autonomous web agents, seamlessly integrating visual and textual information to complete user instructions 
end-to-end by interacting with real-world websites.

Link: [https://medium.com/@andysingal/webvoyager-navigating-digital
-cosmos-with-langgraph-multimodal-models-dace64196c2f](https://medium.com/@andysingal/webvoyager-navigating-digital-cosm
os-with-langgraph-multimodal-models-dace64196c2f)
```
---

     
