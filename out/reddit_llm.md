 
all -  [ Can you use an agent in a Router Chain? ](https://www.reddit.com/r/LangChain/comments/16g9pme/can_you_use_an_agent_in_a_router_chain/) , 2023-09-12-0909
```
I'm trying to use an an agent in a router chain for a chatbot I'm developing. looking at the logs it looks like the rout
er selects the agent and passes it everything properly and the agent runs just fine. But at the parse step it fails beca
use there's no key 'text', which makes no sense to me. Any advice? Has anyone been able to do this successfully? 
```
---

     
 
all -  [ What are the hard and technical skills you need to be a Machine Learning/ Data Scientist ](https://www.reddit.com/r/PinoyProgrammer/comments/16g2cnn/what_are_the_hard_and_technical_skills_you_need/) , 2023-09-12-0909
```
# [Context]

May naka sticky na thread which can be found here [How to Become a data scientist:](https://www.reddit.com/
r/PinoyProgrammer/comments/l8zf7v/how_to_become_a_data_scientist/)

&#x200B;

https://preview.redd.it/vk4bhtw24onb1.png?
width=256&format=png&auto=webp&s=226e9825b57afda2599f66e99318298b111792c7

Eto yung mga tips nya

&#x200B;

>***educatio
nal background*** *- <blah>*  
>  
>***learn the fundamentals*** *- <blah>*  
>  
>***rack up relevant experience*** *- 
<blah>*  
>  
>***apply, apply, apply***  *- <blah>*  
>  
>*Data Science jobs for fresh grads are a rarity back then so
 I took a job as a software engineer just to rack up experience, constantly joining Kaggle competitions and studying abo
ut DS. Finally got a job as a Data Scientist after 2 years.*

Now, I'm **NOT** going to dispute what he has shared, but 
tingin ko, medyo vague yung tips and hindi ganun ka-tangible. Unfortunately, OP already deleted his account so no way fo
r him to update and add more info. In case you have a new account, pls message me.

So, naisip ko na dagdagan with somet
hing more tangible yung tips and advice nya. By sharing the hard and technical skills, the courses, MOOCS, and links tha
t I personally used and utilized.

\[Massive Open Online Courses\]

* [Statistics for Data Science and Business Analysis
](https://www.udemy.com/course/statistics-for-data-science-and-business-analysis/)\- costs less than Php 1000, Udemy als
o has regular discounts pa. One can finish the course in a few weeks to a few months. What is important is that you, OO 
IKAW, don't need to rush finishing this as this is one of the fundamental skills. Now if you're very good in stat, no ne
ed for this. I finished this course in a month during covid
* [Introduction to Computational Thinking and Data Science](
https://learning.edx.org/course/course-v1:MITx+6.00.2x+3T2020)\- I took this course in EDX, may assignments, lectures, a
nd exams. I finished this in like 2 months during the height of covid. This is an official course and has a certificate 
from the Massachusetts Institute of Technology.
* [DeepLearning.AI TensorFlow Developer Professional Certificate](https:
//www.coursera.org/professional-certificates/tensorflow-in-practice) \- I completed this in around 2 months during the t
ail-end of COVID, but I was already using tensorflow for more than a year. I haven't taken the official Google certifica
tion, but this was an amazing course. Intermediate to Advance knowledge of Python is a must.
* [TensorFlow: Advanced Tec
hniques Specialization](https://www.coursera.org/specializations/tensorflow-advanced-techniques)\- took this course imme
diately after i finished the course above, it took me around 2 months to finish. Marami akong natutunan na bagong techni
ques and approaches using Tensorflow.
* [Fine Tune BERT with Tensorflow](https://www.coursera.org/learn/fine-tune-bert-t
ensorflow/)**-** **Bidirectional Encoder Representations from Transformers (BERT),** one of the most important libraries
 for Natural Language Processing, released in 2018 by Google. During that time, it was State of the Art (SOTA) and becam
e the defacto standard library when working with NLP with a Deep Learning Library.
* [ChatGPT Prompt Engineering for Dev
elopers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\- You will learn how to u
se a large language model (LLM) to quickly build new and powerful applications

# [Youtube channels]

* [STATQUEST](http
s://www.youtube.com/@statquest)\- this guy explains very complex Statistics and Data science concepts and formulas in an
 excellent way complete with visuals, animations, and sample computations. Very valuable resource to help 'bake-in' the 
knowledge and concepts

# [Cloud Competencies and Certs]

* [Microsoft Azure Fundamentals](https://learn.microsoft.com/e
n-us/certifications/exams/az-900/)\- this is the entry cert for Azure Cloud, I started poking into Azure circa 2019, I t
ook this cert during the height of COVID. Took me around 2 months to review, personally run and setup  the GITHUB repos

   * I took a UDEMY course for the Fundamentals but unfortunately, wala na yung course sa Udemy. So here's a good altern
ative [https://www.udemy.com/course/az900-azure/](https://www.udemy.com/course/az900-azure/)
* [Designing and Implementi
ng a Data Science Solution on Azure](https://learn.microsoft.com/en-us/certifications/exams/dp-100/)\- I took this durin
g the height of COVID pandemic, I also downloaded the official GITHUB repo of Microsoft then studied for this cert for a
round 2 months.

# [Website Memberships]

* [Kaggle.com](https://Kaggle.com) \- unarguably the largest data science comm
unity today, also leading the democratization of AI/ Machine Learning/ Deep Learning. Sign up for membership then study 
the notebooks (aka kernels), participate in the forums, upload and create datasets, as well as join competitions. They h
ave a discord channel too which one can optionally join.
* [Medium.com](https://Medium.com) \- good source of articles
*
 [Stackoverflow.com](https://Stackoverflow.com) \- no need for explanation
* [Huggingface.co](https://Huggingface.com)\-
 Simple, safe way to store and distribute neural networks weights safely and quickly.

# [Python, Libraries, and others]


* **Python-** one of the best language for datascience, has lots of libraries and ecosystem is very much alive.
* [Adh
erence to PEP8 Standards](https://peps.python.org/pep-0008/)\- for writing beautiful Python code.
* [Creating python env
ironments with conda](https://stackoverflow.com/questions/48174935/conda-creating-a-virtual-environment) **-** for modul
arity and managing environments
* **SQL-** plain-ol' SQL, as long as you can write optimal SQL code, and you know how to
 join tables properly and know when to use LEFT vs INNER vs OUTER.
* [Numpy](https://numpy.org/) **-** you have to get c
omfortable working with numbers
* [Scikit-Learn](https://scikit-learn.org/) **-** scikit-learn is a free software machin
e learning library for the Python programming language. It features various classification, regression and clustering al
gorithms
* [Pandas](https://pandas.pydata.org/) **(or Polar/ Dask) -** you need to become very competent when massaging 
and aggregating data
* **Ordinary Least Squares (OLS) -** Simple linear regression
* [XGBOOST](https://xgboost.readthedo
cs.io/en/stable/)\- if you work with structured or tabular data, almost nothing beats XGBOOST
* [FAISS (Facebook AI Simi
larity Search)](https://github.com/facebookresearch/faiss)  library used to compute cosine-similarity among dense and sp
arse vectors/ embeddings.
* [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)**,**
 [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)**,** [UMAP](https://umap-learn.rea
dthedocs.io/en/latest/), etc- various dimenion-reduction libraries, know when to use when, and what.
* [KMeans](https://
scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)**,** [HDBScan](https://scikit-learn.org/stable/mo
dules/generated/sklearn.cluster.HDBSCAN.html)**, and other clustering algorithms-** for clustering
* [NLTK](https://www.
nltk.org/)**-** a suite of libraries and programs for symbolic and statistical natural language processing for English w
ritten in the Python programming language
* [BERT](https://huggingface.co/blog/bert-101)\- and BERT derivations (Roberta
, ALBERT, SBERT, etc)

# [Tensorflow vs Pytorch + Keras]

* Either library would be good, but based on what I'm reading 
nowadays, Pytorch seem to have the advantage. You wont get wrong with either as both Deep Learning Frameworks are very m
ature, well documented. I personally prefer Tensorflow, but if you can learn and be proficient with both, then much much
 better.

# [Practice]

* [Kaggle Datasets](https://www.kaggle.com/datasets) \- download datasets that pique your intere
sts from Kaggle
* [Kaggle Notebooks](https://www.kaggle.com/code) \- best way to learn is to find a working example, wit
h a corresponding dataset.

# [Data Visualization]

* [MATPLOTLIB](https://matplotlib.org/)\- comprehensive library for 
creating static, animated, and interactive visualizations in Python
* [Seaborn](https://seaborn.pydata.org/)**-** Python
 library for better visually pleasing charts and graphs
* [Tableau](https://www.tableau.com/en-gb/trial/tableau-software
) **vs** [PowerBI](https://powerbi.microsoft.com/en-us/downloads/)\- optional, but I chose POWERBI kasi yun ang pinoprov
ide ng company namin.
* **Excel- When** you talk to business people, this is one of the best and easiest ways to share d
ata and charts
* **Powerpoint-** yes, you will be presenting your findings to business, technical and everything in betw
een

# [Cutting Edge/ State Of The Art (SOTA)]

Eto ang mga cutting edge NGAYON, as I write this September 12, 2023.

* 
[OpenAI.com](https://OpenAI.com) \- ChatGPT, no need to explain
* [Meta AI's Llama](https://ai.meta.com/blog/large-langu
age-model-llama-meta-ai/)\- a state-of-the-art foundational [large language model](https://ai.facebook.com/blog/democrat
izing-access-to-large-scale-language-models-with-opt-175b/) designed to help researchers advance their work in this subf
ield of AI.
* [Langchain](https://python.langchain.com/)\- is a framework for developing applications powered by languag
e models.
* [https://www.youtube.com/@DataIndependent](https://www.youtube.com/@DataIndependent)
   * One of the BEST re
sources on how to weave and integrate Langchain + LLM (like Llama or ChatGPT) + your own data + Retrieval Augmented Gene
ration (RAG)

# [So you want to deploy these LLMs  on your local eh?]

* [https://huggingface.co/TheBloke/](https://hugg
ingface.co/TheBloke/)\- choose your quantized GGUF/ GGML/ GPTQ models
* [https://github.com/ggerganov/llama.cpp](https:/
/github.com/ggerganov/llama.cpp) \- Port of Facebook's LLaMA model in C/C++
* [https://github.com/oobabooga/text-generat
ion-webui](https://github.com/oobabooga/text-generation-webui)\- A Gradio web UI for Large Language Models. Supports tra
nsformers, GPTQ, llama.cpp (GGUF), Llama models.
* [https://github.com/turboderp/exllama](https://github.com/turboderp/e
xllama) \- A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights
.
* NOTE: I have a deep-learning PC and i tried all the deployments methods above

# [Nice to haves]

* **Data Pipeline 
Orchestration**\- if you have knowledge with something like [Azure Data Factory](https://azure.microsoft.com/en-us/produ
cts/data-factory/) or [Databricks](https://www.databricks.com/) to pull data from point A to B, then much better. Most c
ompanies nowadays are still in the early stages of data maturity, only the FAANG level companies have dedicated Data Eng
ineers to pull the data for you. Most of the time, like sa case ko, I also double down as the data engineer
* [Docker](h
ttps://www.docker.com/)**-** when deploying your models to production, you will most likely create images of your applic
ation with your model for containerization and will deploy it
* **Linux-** sometimes I double down as a DEVOPS person as
 well and do my own deployment of models with DOCKER in Azure, most (if not all) VMs and computes in the cloud are HEADL
ESS Linux meaning  no GUI. So you have to be somewhat proficient with Linux command like `sudo -rm -rf` , **ok dont do t
hat if ayaw mong magulpi ng mga teammates mo.** But, seriously, linux proficiency is a nice to have.
* [Spacy](https://s
pacy.io/)**-** *spaCy* is a free open-source library for Natural Language Processing in Python
* **Object Oriented Progr
amming-** arguably not a must, but when your goal is to actually deploy models to production, your code must be very mod
ular, easy to understand, and adheres to industry standards and patterns.
* [Flask](https://flask.palletsprojects.com/en
/2.3.x/)**/** [Streamlit](https://streamlit.io/)**-** for your application's web part
* [Doccano](https://github.com/doc
cano/doccano)**-** open source labelling and text annotation software.
* [Beautiful Soup](https://pypi.org/project/beaut
ifulsoup4/) **+** [Selenium](https://www.selenium.dev/)**-** for webscraping and automating it

# [Software]

* **Jupyte
r notebook/ lab**\- notebook for Python
* **Visual Studio Code-** good IDE from Microsoft
* **GIT-** for storing your co
de, cloning repos, etc

# [Other Important Concepts and Misc]

* **Descriptive and Inferential statistics**
* **Measures
 of Central Tendency and Dispersion**
* **Null and Alternative Hypothesis**
* **Different t-tests**
* **How to read p-va
lues**
* **Confusion Matrix and Type-1 and Type-2 errors**
* **Multilabel vs Multiclass**
* **Imputations**
* **Scaling 
vs. Normalization, different preprocessing techniques**
* **Outlier detection, standard deviation, IQR**
* **Classificat
ion Metrics**\- when to use what and how to read
   * Accuracy, Precision, Recall, F1-score, etc
* **Regression Metrics*
*
   * Mean Squared Error, Mean Absolute Error, Root Mean Squared Error, R-squared
* **Sparse vs. Dense Vectors**
* **Di
stance Measurements**
   * Euclidean Distance, Manhattan Distance, Cosine Similarity
* **Dimension Reduction**
* **Super
vised, Semi-supervised, and Unsupervised learning**
* **Word Embeddings**
* **Handling imbalanced data**
   * Smote, Cla
ssweights, Undersampling, oversmapling, synthetic data generation, etc
* **Data Leakage and how to identify and address 
them**
* **Hyperparameterization**
* **(Model) Weights and biases**
* **Activation functions in Deep Learning**
* **Mode
l Ensembling**
* **Encodings**
   * ascii, utf-8,  utf-16
* **Data types**
   * parquet, csv, json, xml, excel
* **Token
s, unigrams, bigrams, trigrams, ngrams**
* **Gradient Descent, Learning Rates, local and global minima**
   * Statquest 
is very good in explaining the math and I manually computed the derivatives by hand as exercise. Very good discussion an
d tutoral.
* **(And many many many, ..., many more)-** Ill leave it up to you to research these topics, but you will nat
urally bump into these concepts and terms as you study and go along.

# [Related Post]

* [How to become a data engineer
](https://www.reddit.com/r/PinoyProgrammer/comments/166usob/comment/jym6906/?utm_source=share&utm_medium=web2x&context=3
)

# [Notes and Advice]

* I went Azure with my cloud platform,  you can choose other cloud platforms like AWS and GCP
*
 I went Tensorflow with my Deep learning library, you can choose Pytorch here
* Nagkaroon na ng mga Bachelor of Science 
In Data Science na medyo recent lang ata na naoffer sa mga universities, I dont have visibility sa curriculum nila.
* Tw
o people with the same role 'DATA SCIENTIST' can actually be doing different things.
* I believe there are two main flav
ors of data scientists, the 'theory-inclined' na mga super henyo sa mga algorithms and jargon, and the 'implementation-i
nclined' na just utilizes the libraries to do the calculations, I am more of the latter.
* Sometimes the problem is comp
lex, sometimes it's not, you have to know which algorithm to choose. But before everything else, you have to know the pr
oblem at hand and kelangan mo maintindihan ang nuances and gain domain knowledge.
* Don't fall for those MASTER-DATA SCI
ENCE in 3 months snake-oil stuff, The field is fast evolving and no, you can't MASTER this field in 3 months.
* **I WONT
 SUGARCOAT**  but this is a very deep and technical field, if you do not have a knack for studying, burning the midnight
 oil, failing-miserably nang paulit-ulit, learning from your mistakes, and overcoming them, **then go back**. But if you
 love challenges and you have the grit, soldier on.
* Di mo maiiwasan na makipagusap with people from other countries an
d various levels (c-levels, managers, fellow developers, business people, etc), so polish your communication skills.
* Y
ou must be open-minded as there are countless ways to approach a problem, but you also have to know when to call someone
's BS.
* The list above is my personal journey and there are countless resources, even better ones that I've mentioned. 
So share 'em in the comments!
* I'm far from being an expert in Data Science, and I consider myself as a perpetual stude
nt who is still learning and studying.
* Keep your ego in check, there will always be someone better than you.
* Buy a n
otebook and a pen, jot down notes, solve equations by hand, never underestimate the hand-brain connection
* Enjoy and ce
lebrate the small wins

# [GOOD LUCK]
```
---

     
 
all -  [ H2OGPT not saving documents in database ](https://www.reddit.com/r/LocalLLaMA/comments/16g26qi/h2ogpt_not_saving_documents_in_database/) , 2023-09-12-0909
```
Hi, I'm new to this so I may be doing this wrong but on the Windows  11 version of H2OGPT I can upload my documents and 
create a collection in LangChain Mode-Path. However whenever I restart the documents disappear. 

This is a pain as I am
 trying to analyse my own documents. I've clicked on 'Update DB with new/changed files on disc'.

Also does it matter if
 I add documents before or after I load a model? I've tried it with no model and with a model loaded.

Speaking of which
 is it possible to set a default model using the one-click installer of H2OGPT?
```
---

     
 
all -  [ Langchain + Azure Formrecognizer: How to pass documents? ](https://www.reddit.com/r/LangChain/comments/16g20dc/langchain_azure_formrecognizer_how_to_pass/) , 2023-09-12-0909
```
I am following a tutorial that says you should call it like this:

‚Äòagent.run('what is the due date of the following inv
oice?'
    'data/Sample-Invoice-printable.png')‚Äò

But I cannot get it to run on a local file. There is an error message 
that the resource cannot be found.

In addition: What kind of formatting is it in the example with the two arguments not
 separated by a comma? I am confused.

I read that the file needs to be given as a URL, so I tried it with ‚Äúfile:///path
-to-file‚Äú, but it didn‚Äôt work either.
```
---

     
 
all -  [ Advice on fine-tuning ](https://www.reddit.com/r/LangChain/comments/16g18bn/advice_on_finetuning/) , 2023-09-12-0909
```
I'm very much a newbie (both with Langchain and Python), so please forgive any questions asked in ignorance.

I am curre
ntly trying to fine-tune ChatGPT on a CSV of data I have. The problem is the CSV is very large and contains fields with 
large paragraphs of text. The CSV has about 50k entries in it. It works well when I try to fine tune based on a small sa
mple of around 20 entries on the original CSV. But if I increase the number of entries to even around 100, I get an erro
r saying the rate limit has been reached.

I'm using a CSVLoader to load my document, and then indexing it with the Vect
orstoreIndexCreator. What can I do to be more efficient?

Thank you!
```
---

     
 
all -  [ Open-source end to end chatbot product and data collector ](https://www.reddit.com/r/LangChain/comments/16g04ls/opensource_end_to_end_chatbot_product_and_data/) , 2023-09-12-0909
```
Hey all, I was thinking of creating an open-source tool that merges search engine, chatbot, and data collection function
alities, enabling searches across platforms like Slack, GitHub, and Google Docs and aiding in the collection of conversa
tional data for LLM fine-tuning (for when incremental training of chatbots becomes a thing).

I've got a slidedeck for t
he project below, analysing similar products and possible gaps in the market, and ultimately attempting to find a way to
 alleviate duplication of efforts across companies. Maybe there's some langchain contributors in here that has an opinio
n on this. Let me know your thoughts, and if anyone would like to contribute.

[https://docs.google.com/presentation/d/1
orbE7mxIH1iegC0koDvK05KiUv225MbbzSoYWjcljd4/edit#slide=id.g1c276c7e915\_4\_5](https://docs.google.com/presentation/d/1or
bE7mxIH1iegC0koDvK05KiUv225MbbzSoYWjcljd4/edit#slide=id.g1c276c7e915_4_5)
```
---

     
 
all -  [ Chain Conversations for Context ](https://www.reddit.com/r/LangChain/comments/16fyrje/chain_conversations_for_context/) , 2023-09-12-0909
```
**Tl;Dr:**If you have an idea that you need assistance in its creation; try using a 'Master' chat, and use only refined 
and condensed information from 'sub-chats' as its input to ensure you get the most contextual reply. This method reduces
 the amount of prompt engineering required at various stages of the chat, ie; the beginning/ middle/ end. Responses are 
more contextual, and there is a perceived compounding affect within the original model context window where replies are 
very good even when compared against advanced prompting techniques.---

Background:I've been using ChatGPT everyday sinc
e its inception. Over the past year, I've envisioned an assistant language model of my own for fun. With the help of Ope
nAI I've been able to achieve this at an advanced level, and there's been a poignant observation that has helped me real
ly develop this application (more than just creating a model, but also training, advanced nlp, vectorstores, tokens, emb
eddings/ embeddings models... it was a lot, I promise):

The issue:Using ChatGPT, you have a limited context window. Reg
ardless of the size of this window (it's not infinite), you will begin to have degrading quality replies at some point. 
Even with advanced gradient descent techniques, from what I've observed, you still sacrifice context as the conversation
 is lengthened. This issue is poignant when developing large code-bases, as eventually the code slowly becomes less and 
less compatible on the first try, or the use of advanced prompt techniques for later in the conversation are required (a
nd not guaranteed). But this is universally true if the conversation you're having requires specific context throughout 
its history.

My observed solution to maintain context without prompt engineering:Structure your chats so that the infor
mation you abstract during your conversation can be pulled in a formatted way, and then inserted into a 'Master' chain. 
The idea is, if you have a large conceptual idea, it's likely that it will require branches of thought to be unified at 
some point in order to replicate the big-picture idea; ie: for what you want to see to actually happen. In that sense, y
ou need to structure your responses so that when one branch is completed- you can copy/paste that information in its ful
l context back into your master chat branch. The idea is to shove as much context as possible into one single chat. It's
 best to have a unified context repository. A single chat container which contains only information with as much rich co
ntext as possible. Use multiple other chats to refine your ideas, condense the logic, and create a dense conversation ra
ther than a long one where useless embeddings are stealing context later in the chat.

Why it works:In the practice of r
efining your idea, you allow the nlp model to curate its own definitions and language. By the time you've refined your i
dea, you've basically let the model use its own parameters to curate suitable verbose (its own language with underlying 
context) that enhances the quality of the response when used as the input in your Master chat. This is a simple concept,
 but imo one of the most important distinctions of why this method is important.

Simple Example:i. I want to create an 
application that converts my pdf to tokens and embeddings.ii. To do this, I might ask ChatGPT, 'how can i do this', and 
use its response as my individual chat windows. So, it might say, okay you need to transform the data to CSV, use nltk t
o tokenize the data, use openai for the embeddings, and then you might want to explore the data- is there anything furth
er you'd like to discuss.... etc. etc.'iii. so, now my chat windows become; a. transform pdf to csv, b. nltk for tokeniz
ing csv data, c. openai for embeddings, d. exploring embedding data. from there, I'd ask questions to the chat window un
til I felt comfortable about how that specific step is achieved.\*I constantly ask ChatGPT to think critically, think an
alytically, and provide your response in a step by step format where each individual step is in succession of the previo
us step and forms a logical pattern. *something like that*I. Master Chat: includes the answers to my questions from the 
individual sub-chats.II. I'm updating my Master-Chat as new information is presented that contributes to a condensed sol
ution. \*the Master-Chat is still very conversational, although now it's a bit more intentional with its results. imo th
is creates a compounding effect in which the more dense the information provided, the longer lasting the context as the 
conversation continues. The answers you receive within the first context window (4k/ 8k/ 16k/ 32k, wtv.) will be vastly 
superior. This is easier than prompt engineering, it's simply organizing the information in a better way.

Final Tip:You
 can further refine the context of your discussions with ChatGPT, by simply using the instructions in your settings.i. W
hat would you like ChatGPT to know about you to provide better responses? 'Hi G, my name is 'x', and I'm working on the 
following project: '.ii. How would you like ChatGPT to respond? 'Just be your normal, helpful, self. Thank you. I love y
ou.'

Disclaimer: I understand the underlying technology is more complicated than the suggested advice implies, and in p
ractice may operate differently, albeit come to the same conclusion imo. This is practical advice. I just see so much no
nsense about 'prompt-engineering' and the reality is, while important, it's not the end-all. And, you can use simpler pr
ompting techniques like asking the chat for step-step replies, which are providing contextually rich information that ca
n be used to create better conversations in new chats.

I thought this was interesting, especially if new Users are havi
ng trouble with the context window, plus all the talk about degrading replies. This is just one way to combat that.
```
---

     
 
all -  [ Problem with asymmetric search ](https://www.reddit.com/r/LangChain/comments/16fu2e1/problem_with_asymmetric_search/) , 2023-09-12-0909
```
I vectorized my content and saved it in the database to be used in a chatbot. However, I have a problem with the search.


As the user's question is small, he favors small vectors, which are generally not the best. However, if I transform th
e user's question into a larger question, it returns better results.

What can I do to improve this asymmetric search, c
onsidering that the user's question will generally be small?
```
---

     
 
all -  [ How to Configure User Access Tokens for Tools in Langchain Agent? ](https://www.reddit.com/r/LangChain/comments/16frolt/how_to_configure_user_access_tokens_for_tools_in/) , 2023-09-12-0909
```
I have two essential tools, Jira and Notion, both of which require user access tokens in the request header. I'm using t
hese tools through the Langchain Agent, and I need guidance on how to properly add the relevant access tokens to ensure 
seamless integration. Can you provide step-by-step instructions or best practices for configuring user access tokens in 
the Langchain Agent when we are using tools?
```
---

     
 
all -  [ Which are some good, lightweight models that can be used? ](https://www.reddit.com/r/LangChain/comments/16frbes/which_are_some_good_lightweight_models_that_can/) , 2023-09-12-0909
```
I am creating a Document based Query/Answer application on my corporate machine. It is unable to get local issuer certif
icate, thus I am unable to use most of the LLMs that have been provided since most of them download the model when calle
d. So essentially, no API calls allowed.

As of right now, I am using GPT4All local model and the sentence transformer a
ll-MiniLM-L6-v2 for embeddings. But this combination is way too slow when querying. Do you guys have any other suggestio
ns and combinations? When suggesting, please keep in mind the problem I mentioned lol. 

Thank you so much.
```
---

     
 
all -  [ How to edit specific sections of a document using an LLM? ](https://www.reddit.com/r/LangChain/comments/16fp7s4/how_to_edit_specific_sections_of_a_document_using/) , 2023-09-12-0909
```
Are  there any pipelines or perhaps a Langchain chain that would allow me to  use an LLM to identify and edit specific p
ortions/sections of a  document based on a query?

I  understand I can have the document indexed using an abrupt charact
er  split of a set number of characters, and edit the relevant chunk and  re-append to the document, however if the cont
ent that is to be edited  is spread across two chunks, I would end up having to regenerate both  those chunks and re-app
ending to the original document. However I don't  know how to include the context of the previous chunk to have a smooth
  continuation into the 2nd chunk.

Hence  my question, is there any implementation for this? Or a simpler, better  appr
oach that I am missing? Any resources or help is greatly  appreciated.
```
---

     
 
all -  [ A RAG bot can retrieves content on-demand! ](https://www.reddit.com/r/Langchaindev/comments/16fin8v/a_rag_bot_can_retrieves_content_ondemand/) , 2023-09-12-0909
```
A RAG bot can retrieves web/local content on demand! it uses [ActionWeaver](https://github.com/TengHu/ActionWeaver) to c
ombine and orchestrate llama index and langchain tools together for a new search experience.

Here is the [Github](https
://github.com/TengHu/Interactive-RAG) repo

[Interactive RAG Demo](https://www.loom.com/share/f3d7a8e80b3e47618d27730e01
eb4bca)
```
---

     
 
all -  [ ü¶ô LlamaIndex vs. LangChain ü¶ú ](https://www.gettingstarted.ai/langchain-vs-llamaindex-difference-and-which-one-to-choose/) , 2023-09-12-0909
```
Here‚Äôs my latest post about LlamaIndex and LangChain and which one would be better suited for a specific use case.

Plea
se send me your feedback!
```
---

     
 
all -  [ [D] Data Extraction using fine-tuned LLM? ](https://www.reddit.com/r/MachineLearning/comments/16fenlb/d_data_extraction_using_finetuned_llm/) , 2023-09-12-0909
```
Hey Reddit,

I'm working on a tool to pull data from highly irregular Excel files. I've gotten reasonable results which 
is extremely fast with standard Python coding, but it's far from perfect due to the lack of standardized templates. 

In
terestingly, when I tested ChatGPT-4 on a sample table, it did a decent job at data extraction. However, relying solely 
on GPT-4 has its downsides like token limits and slow processing speed (and data privacy issues). Plus, splitting the Ex
cel sheet to fit within these limits results in loss of context and data.

I'm considering fine-tuning a language model 
to post-process data that was in a Pandas DataFrame (perhaps converted to JSON). Has anyone had success with this approa
ch or have alternative recommendations? I've tried Langchain, but it wasn't helpful.

I have figured out to extract the 
relevant columns, but the post-processing part is where I am considering using an LLM which understands the domain and w
hat needs to be extracted based on the examples I feed it.

Looking forward to your thoughts! And would be happy to answ
er any additional questions.
```
---

     
 
all -  [ I built an AI Agent (BondAI) that actually works and has a friendly API for easy integration into ot ](https://www.reddit.com/r/ChatGPTCoding/comments/16fecm1/i_built_an_ai_agent_bondai_that_actually_works/) , 2023-09-12-0909
```
üì¢ I'm excited to introduce **BondAI**, an AI Agent framework and CLI, with a lightweight yet robust API making integrati
on into your own applications straightforward and easy.

**Repository:** [**https://github.com/krohling/bondai**](https:
//github.com/krohling/bondai)

# ‚ö°Ô∏èExamples

Here's an example of buying/selling Stocks with [Alpaca Markets](https://al
paca.markets/). I strongly recommend using Paper Trading btw!

    from bondai import Agent 
    from bondai.tools.alpac
a_markets import (
        CreateOrderTool, 
        GetAccountTool, 
        ListPositionsTool
    )
    
    task = (

        'I want you to sell off all of my existing positions.'
        'Then I want you to buy 10 shares of NVIDIA with 
a limit price of $456.'
    )
    
    Agent(tools=[   CreateOrderTool(),   GetAccountTool(),   ListPositionsTool() ]).r
un(task) 

[**Here's an example**](https://github.com/krohling/bondai/tree/main/examples/online-research) of BondAI doin
g online research and [**here's a home automation example**](https://github.com/krohling/bondai/tree/main/examples/home-
automation).

# üîç What is BondAI?

**BondAI** is a framework crafted for the smooth integration and customization of Con
versational AI Agents. Leveraging the power of OpenAI's [**function calling support**](https://openai.com/blog/function-
calling-and-other-api-updates), it sidesteps the hurdles often encountered in building a Conversational Agent, offering 
solutions such as:

* Memory management
* Error handling
* Integrated semantic search
* A rich array of pre-existing too
ls
* Ease of crafting custom tools

Moreover, it offers a **CLI interface** that promises an impressive command line age
nt experience, available to anyone with an OpenAI API Key!

# üèóÔ∏è Why build BondAI?

I am convinced that AI agents are go
ing to be an important architecture for the future of AI. Despite their phenomenal problem-solving abilities, the existi
ng tooling often fell short in performing simple tasks, and the frameworks appeared unnecessarily complicated. This spur
red the birth of **BondAI**, aiming to address these shortcomings and offer a more optimized environment for agent imple
mentations.

I am keen on hearing your feedback on **BondAI**'s functionality and any suggestions for improvements!

# üõ†
Ô∏è Installation & Usage

Get started with BondAI with a simple: pip install bondaiThe CLI tool offers a ready-to-use agen
t experience packed with several default tools. You can also integrate it with various tools such as Google Search, Alpa
ca Markets, and LangChain Tools to execute a myriad of tasks effectively. Detailed guides and examples for usage are ava
ilable in the README.

# üîß APIs and Custom Tools

The BondAI framework offers flexible APIs to build your agent and crea
te custom tools for a personalized experience. It follows a straightforward implementation approach, making the tool cre
ation process hassle-free for developers.

Examples of included Tools:

* Google and Duck Duck Go Search
* Semantic Sear
ch for Files and Websites
* Alpaca Markets
* Gmail Integration
* Easily import tools from LangChain!

# üêã Docker Contain
er

For a secure environment, especially while using tools with file system access, running **BondAI** within a docker c
ontainer is highly recommended. Follow the steps in the REAME to easily build and run the **BondAI** container.

üöÄ Join 
the mission; contribute to BondAI! And please share feedback/ideas in the comments!
```
---

     
 
all -  [ Editing specific sections of documents? ](https://www.reddit.com/r/LocalLLaMA/comments/16fdr8r/editing_specific_sections_of_documents/) , 2023-09-12-0909
```
Are there any pipelines or perhaps a Langchain chain that would allow me to use an LLM to identify and edit specific por
tions/sections of a document based on a query? 

I understand I can have the document indexed using an abrupt character 
split of a set number of characters, and edit the relevant chunk and re-append to the document, however if the content t
hat is to be edited is spread across two chunks, I would end up having to regenerate both those chunks and re-appending 
to the original document. However I don't know how to include the context of the previous chunk to have a smooth continu
ation into the 2nd chunk.

Hence my question, is there any implementation for this? Or a simpler, better approach that I
 am missing? Any resources or help is greatly appreciated. 
```
---

     
 
all -  [ Support for additional query parameters ](https://www.reddit.com/r/LangChain/comments/16fb2p5/support_for_additional_query_parameters/) , 2023-09-12-0909
```
Hi! I would like to set a range and/or limit/offset while doing vector queries, so that instead of fetching the top n re
sults I can specify that I want the 1st, 2nd, 3,rd result and so on.

This seems to be a supported function in several o
f the integrated vector DBs, see for example [Weaviate](https://weaviate.io/developers/weaviate/search/similarity#number
-of-results) and [Supabase](https://supabase.com/docs/reference/javascript/range). How do I modify my queries using Lang
chain to make use of these functions? Is it supported? I tried asking this question in the langchain github but didn't g
et any help. I am using the JS version of Langchain.
```
---

     
 
all -  [ A RAG bot can retrieves content on demand ](https://www.reddit.com/r/LangChain/comments/16fac58/a_rag_bot_can_retrieves_content_on_demand/) , 2023-09-12-0909
```
hey guys, I implemented A RAG bot can retrieves web/local content on demand, it uses [ActionWeaver](https://github.com/T
engHu/ActionWeaver) to orchestrate llama index and langchain tools to combine search and RAG.

[Github](https://github.c
om/TengHu/Interactive-RAG)

[Interactive RAG Demo](https://www.loom.com/share/f3d7a8e80b3e47618d27730e01eb4bca)
```
---

     
 
all -  [ A RAG bot can retrieves content on demand ](https://www.reddit.com/r/LlamaIndex/comments/16faasl/a_rag_bot_can_retrieves_content_on_demand/) , 2023-09-12-0909
```
hey guys, I implemented A RAG bot can retrieves web/local content on demand, it uses [ActionWeaver](https://github.com/T
engHu/ActionWeaver) to orchestrate llama index and langchain tools to combine search and RAG.

[Github](https://github.c
om/TengHu/Interactive-RAG)

[Interactive RAG Demo](https://www.loom.com/share/f3d7a8e80b3e47618d27730e01eb4bca)

üì∑
```
---

     
 
all -  [ Where is the error stack trace...? ](https://www.reddit.com/r/LangChain/comments/16f72wy/where_is_the_error_stack_trace/) , 2023-09-12-0909
```
I get the error below sometimes which would be fine if I could trace the problem and debug but I can't for the love of m
e figure out why the stack trace is hidden because this is all I see in my terminal 

[LLM run error](https://preview.re
dd.it/3ltiwnihxgnb1.png?width=1320&format=png&auto=webp&s=ccc62cf930f9d91c48022dae69c5fea0fcf11e04)

How do you go about
 this?
```
---

     
 
all -  [ Multiple models running on one system ](https://www.reddit.com/r/LocalLLaMA/comments/16f4177/multiple_models_running_on_one_system/) , 2023-09-12-0909
```
Is it possible to run multiple models at the same time on the same system and let them interact with each other on somet
hing like langchain?
```
---

     
 
all -  [ I made a KNOWLEDGE ASSISTANT. ](https://www.reddit.com/r/LangChain/comments/16f1i6b/i_made_a_knowledge_assistant/) , 2023-09-12-0909
```
I made a **KNOWLEDGE ASSISTANT**, which allows you to upload PDF, WORD, TXT, and CSV documents, vectorize and store the 
documents after splitting, and combine them with the OpenAI GPT model to ask questions and return answers. Visual parame
ter adjustment is provided at each step. Turning on DEBUG mode lets you see information such as similarity search, promp
t words, produced content, etc.

&#x200B;

https://reddit.com/link/16f1i6b/video/wuywix0qtfnb1/player

Please let me kno
w what you think.
```
---

     
 
all -  [ LLMChain timeout error with OpenAI ](https://www.reddit.com/r/LangChain/comments/16f0nkg/llmchain_timeout_error_with_openai/) , 2023-09-12-0909
```
I'm running a series of calls using a vary simple LLMChain and very often get a TimeoutError.The error crashes my server
 and doesn't retry automatically (I read somewhere that LangChain should handle the retries automatically).

Here's the 
full error I'm getting:

    Error in handler LangChainTracer, handleChainError: TimeoutError: The operation was aborted
 due to timeout
    file:///Users/[redacted...]/node_modules/langchain/dist/util/openai.js:6
            error = new Err
or(e.message);
                    ^
    
    Error [TimeoutError]: Request timed out.
        at wrapOpenAIClientError 
(file:///Users/[redacted...]/node_modules/langchain/dist/util/openai.js:6:17)
        at file:///Users/[redacted...]/nod
e_modules/langchain/dist/chat_models/openai.js:517:31
        at process.processTicksAndRejections (node:internal/proces
s/task_queues:95:5)
        at async RetryOperation._fn (/Users/[redacted...]/node_modules/p-retry/index.js:50:12) {
   
   attemptNumber: 1,
      retriesLeft: 6
    }
    
    Node.js v20.5.1

Looking into LangSmith I can see the timed out
 requests have waited for 600 seconds. That's a lot!

* Is there anything I can do so LangChain handles the retries?
* I
f not, what's the best way to handle the retries. I was looking into [handleChainError()](https://js.langchain.com/docs/
api/callbacks/classes/LangChainTracer#handlechainerror)
* Can I reduce the waiting time to way less than 600 seconds?
```
---

     
 
all -  [ JS/TS EXAMPLE REPOS? ](https://www.reddit.com/r/LangChain/comments/16eyp9z/jsts_example_repos/) , 2023-09-12-0909
```
Looking to learn how to build a self hosted langchain bot that will use scrapers to pull in data. Not super sure where t
o start. I‚Äôve gone through some docs but I‚Äôd love to hear some recommendations of stacks or read through some repos to l
earn how to build properly. 

Thanks. Xo
```
---

     
 
all -  [ Query Length in FAISS for Document Retrieval ](https://www.reddit.com/r/LangChain/comments/16ewert/query_length_in_faiss_for_document_retrieval/) , 2023-09-12-0909
```
Hello fellow LangChainers,

I'm leveraging FAISS for my vector store and I'm curious about the optimal query length to r
etrieve pertinent documents. My goal is to develop an app that succinctly summarizes business reports tailored to specif
ic roles within a company. For instance, imagine a Head of Finance who prefers a concise overview rather than reading an
 entire report.

To form my query, I've been incorporating the job responsibilities of the target persona. However, I'm 
concerned that my query might be too extensive, leading to less relevant results.   


**Query** (that goes into FAISS v
ia similarity\_search\_with\_score method):

    
Based on the target persona's responsibilities, extract insights from 
a company's annual report.
    Target Persona: Head of Finance, a key figure in the Executive Leadership Team (ELT), who
 emphasizes financial reporting, risk management, and compliance. They play a crucial role in guiding the company's fina
ncial direction, ensuring adherence to regulations, and building relationships with external entities.
    Key Responsib
ilities:
    Overseeing organizational financial reporting, encompassing monthly summaries, budget projections, long-ter
m financial plans, and mandatory reports.
    Offering robust financial guidance to the ELT for informed business decisi
ons.
    Presenting financial outcomes and insights to the Board.
    Central to shaping the company's strategy.
    Sup
ervising the company's tax matters, collaborating with external tax consultants when needed.
    Managing company cash f
low and implementing appropriate treasury controls.

**Given the above, any insights on how I can optimize my query for 
better results in FAISS would be greatly appreciated!**
```
---

     
 
all -  [ Why is Langchain MIT and not gplv3 licensed? ](https://www.reddit.com/r/LangChain/comments/16ewbfn/why_is_langchain_mit_and_not_gplv3_licensed/) , 2023-09-12-0909
```
Langchain seems to use multiple libraries like PuMyPDF which is licensed as gplv3.

With this in mind, how and why does 
langchain claim to be licensed under a MIT license? 

Trying not to get into legal troubles here. If someone could expla
in it would be very much appreciated.
```
---

     
 
all -  [ Token request wildly jumps randomly? ](https://www.reddit.com/r/LangChain/comments/16em51x/token_request_wildly_jumps_randomly/) , 2023-09-12-0909
```
This is driving me crazy.  I'm running the SQL Agent and gathing insights from a mysql database. 

It works like 75% of 
the time. the other times it says I'm using 8000+ tokens. 

It seems to mess up when I add a function into that script o
r change anything in the script...

agent\_executor = create\_sql\_agent(  
 llm=OpenAI(temperature=0),  
 toolkit=SQLDa
tabaseToolkit(db=db, llm=OpenAI(temperature=0)),  
 verbose=True,  
 agent\_type=AgentType.ZERO\_SHOT\_REACT\_DESCRIPTIO
N,  
 top\_k=10000  
)  


result = agent\_executor.run('In 'db', up to 100 rows, find unique URLs in the 'From' column,
 decipher company name, rank & list top 10')  

```
---

     
 
all -  [ I built an AI Agent (BondAI) that actually works and has a friendly API for easy integration into ot ](https://www.reddit.com/r/AutoGPT/comments/16eilta/i_built_an_ai_agent_bondai_that_actually_works/) , 2023-09-12-0909
```
üì¢ Hello AI agent enthusiasts, developers, and learners!

I'm thrilled to introduce you to **BondAI**, an AI Agent framew
ork and CLI, with a lightweight yet robust API making integration into your own applications straightforward and easy.


**Repository:** [**https://github.com/krohling/bondai**](https://github.com/krohling/bondai)

https://preview.redd.it/hr
i7x7e20bnb1.png?width=1456&format=png&auto=webp&s=a24e6bcca613e4d961bc36eedf9131dd0f034d2a

&#x200B;

## ‚ö°Ô∏èExamples

Her
e's an example of buying/selling Stocks with [Alpaca Markets](https://alpaca.markets/). I strongly recommend using Paper
 Trading btw!

    from bondai import Agent
    from bondai.tools.alpaca_markets import CreateOrderTool, GetAccountTool,
 ListPositionsTool
    
    task = '''I want you to sell off all of my existing positions.
    Then I want you to buy 10
 shares of NVIDIA with a limit price of $456.'''
    
    Agent(tools=[
      CreateOrderTool(),
      GetAccountTool(),

      ListPositionsTool()
    ]).run(task)

[**Here's an example**](https://github.com/krohling/bondai/tree/main/exampl
es/online-research) of BondAI doing online research and [**here's a home automation example**](https://github.com/krohli
ng/bondai/tree/main/examples/home-automation).

## üîç What is BondAI?

**BondAI** is a framework crafted for the smooth i
ntegration and customization of Conversational AI Agents. Leveraging the power of OpenAI's [**function calling support**
](https://openai.com/blog/function-calling-and-other-api-updates), it sidesteps the hurdles often encountered in buildin
g a Conversational Agent, offering solutions such as:

* Memory management
* Error handling
* Integrated semantic search

* A rich array of pre-existing tools
* Ease of crafting custom tools

Moreover, it offers a **CLI interface** that prom
ises an impressive command line agent experience, available to anyone with an OpenAI API Key!

## üèóÔ∏è Why build BondAI?


I am convinced that AI agents hold the future. Despite their phenomenal problem-solving abilities, the existing tooling 
often fell short in performing simple tasks, and the frameworks appeared unnecessarily complicated. This spurred the bir
th of **BondAI**, aiming to address these shortcomings and offer a more optimized environment for agent implementations.


I am keen on hearing your feedback on **BondAI**'s functionality and any suggestions for improvements!

## üõ†Ô∏è Installa
tion & Usage

Get started with BondAI with a simple: pip install bondai  
The CLI tool offers a ready-to-use agent exper
ience packed with several default tools. You can also integrate it with various tools such as Google Search, Alpaca Mark
ets, and LangChain Tools to execute a myriad of tasks effectively. Detailed guides and examples for usage are available 
in the README.

### üîß APIs and Custom Tools

The BondAI framework offers flexible APIs to build your agent and create cu
stom tools for a personalized experience. It follows a straightforward implementation approach, making the tool creation
 process hassle-free for developers.

Examples of included Tools:

* Google and Duck Duck Go Search
* Semantic Search fo
r Files and Websites
* Alpaca Markets
* Gmail Integration
* Easily import tools from LangChain!

### üêã Docker Container


For a secure environment, especially while using tools with file system access, running **BondAI** within a docker cont
ainer is highly recommended. Follow the steps in the REAME to easily build and run the **BondAI** container.

üöÄ Join the
 mission; contribute to BondAI! And please share feedback/ideas in the comments!
```
---

     
 
all -  [ Langchain conversation history with ChatGPT? ](https://www.reddit.com/r/ChatGPTPro/comments/16ehksj/langchain_conversation_history_with_chatgpt/) , 2023-09-12-0909
```
In Langchain there are many types of conversational memory that adds conversation history into the prompt. I get it's us
eful for pre-ChatGPT era, but is it still useful for ChatGPT? ChatGPT has conversation history in itself so wonder what'
s the point of adding conversation history in the system prompt (other that summary)
```
---

     
 
all -  [ Fine-Tuning Handler / Data Preparation using Langchain ](https://www.reddit.com/r/LangChain/comments/16efnh7/finetuning_handler_data_preparation_using/) , 2023-09-12-0909
```
Is there something similar in langchain to the finetune handler in llama\_index? [https://gpt-index.readthedocs.io/en/v0
.8.19/examples/finetuning/react\_agent/react\_agent\_finetune.html](https://gpt-index.readthedocs.io/en/v0.8.19/examples
/finetuning/react_agent/react_agent_finetune.html)

Basically, we want to log all calls (exact parameters/messages to Op
enai so that we can use that data to finetune gpt-3.5. Right now, we're not really sure how the calls are made to the ll
m behind the scenes. Does the chat-conversational-react-description break down the entire message we see in verbose true
 mode in different messages (the dictionary that openai expects) or are they sending it all in the 'system' message?

We
 want to know what 'role' certain sections of the input to the llm are so that we can prepare the data for fine-tuning a
ppropriately.

Any help with this would be appreciated!
```
---

     
 
all -  [ I launched NextJS project & don't know what to do next (feedback) ](https://www.reddit.com/r/nextjs/comments/16ee473/i_launched_nextjs_project_dont_know_what_to_do/) , 2023-09-12-0909
```
Hi there, this question is not going to be highly technical, but I'm rather seeking some guidance from more experienced 
developers regarding to career decisions.  


I started learning code roughly 6 months ago after I quit my COO role at s
tart up. Long story short today, I am able to launch a full stack application, I know how Client/Server Components work,
 I understand API, Tailwind CSS, Supabase, Auth, OpenAI, Langchain Functions, Streaming etc. etc. etc.  


This is the a
pp that roughly present my skills: [copycopter.ai](https://copycopter.ai) 

&#x200B;

**Now the problem is, I don't real
ly know what to do next.**  
My dream would be learn further and build more, so that at some point, I can make money fro
m my own things. For now, however, I struggle to find people interested in my SaaS, so I probably have some work to do i
n terms of 'go to market' and 'PMF'.  


I could look for a job as a developer, but the only thing I know is NextJS App 
Router and I feel like I'm not yet ready to get a reasonably paid job as a dev. On the other hand, I see so many people 
that create much worse products yet they openly talk about the readiness to get employed.  


I have a lot of experience
 in no code, process automation, Zappier, operations management etc. so I could bring good value to any startup.   


**
What would you do if you were me?** I'm kina struggling to find the right path now.  
**What to learn next? What kind of
 jobs should I seek for?**

&#x200B;

Cheers,  
K.
```
---

     
 
all -  [ ChatGPT API seems to be producing much worse results than the Web Version ](https://www.reddit.com/r/OpenAI/comments/16ec4gs/chatgpt_api_seems_to_be_producing_much_worse/) , 2023-09-12-0909
```
I am using gpt 3.5 turbo on both the API and the web version, yet on the API, I am not getting the results that I get on
 the web version, with no finetuning, langchain, or any other modifications. Is there something wrong? How can I fix thi
s?
```
---

     
 
all -  [ quantized Falcon-7b is too slow ](https://www.reddit.com/r/LangChain/comments/16ea97a/quantized_falcon7b_is_too_slow/) , 2023-09-12-0909
```
Hi, I use 4-bit quantized version of Falcon 7-b: [https://huggingface.co/TheBloke/falcon-7b-instruct-GGML](https://huggi
ngface.co/TheBloke/falcon-7b-instruct-GGML) on my 8 GB RAM laptop, but it took 100 seconds to respond on 'hello' message
. Anything beyond takes multiple minutes. Model's file size is only 4,06 GB. Any other 7 billion params quantized model 
such as LLama2 has same issues. Do I anything wrong since GGML is here to allow models to run on consumer hardware witho
ut GPU? Can I speed up the model? 

    from langchain import CTransformers
    
    llm = CTransformers(
        model=
'path to model',
        model_type='falcon',
        config={
            'max_new_tokens': 128,
            'temperatu
re': 0.01
        }
    )
```
---

     
 
all -  [ Confused about the results trying to QA a web source ](https://www.reddit.com/r/LangChain/comments/16e6n37/confused_about_the_results_trying_to_qa_a_web/) , 2023-09-12-0909
```
I've just started with LLMs and LangChain, but I really need some help to understand the key concepts around RAG. I've r
ead it reduces the risk of hallucinations using your data, so I've been trying to build a basic prototype myself. I've e
nded up copying the code from [https://python.langchain.com/docs/integrations/llms/ollama](https://python.langchain.com/
docs/integrations/llms/ollama), this is what I've got:

    llm = Ollama(
     base_url='http://localhost:11434',
     m
odel='llama2',
     callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
      )
    
    loader = WebB
aseLoader('https://lilianweng.github.io/posts/2023-06-23-agent/')
    data = loader.load()
    text_splitter = Recursive
CharacterTextSplitter(chunk_size=100, chunk_overlap=50)
    all_splits = text_splitter.split_documents(data)
    
    
 
   vectorstore = Chroma.from_documents(
     documents=all_splits, embedding=HuggingFaceEmbeddings()
     )
    
    fro
m langchain.prompts import PromptTemplate
    prompt_template = '''Use the following pieces of context to answer the que
stion at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Only use p
rovided context, don't try to generate an answer based on assumptions.
    
    {context}
    
    Question: {question}

    Helpful Answer:'''
    QA_CHAIN_PROMPT = PromptTemplate(
        template=prompt_template, input_variables=['context
', 'question']
    )
    
    chain_type_kwargs = {'prompt': QA_CHAIN_PROMPT}
    qa_chain = RetrievalQA.from_chain_type
(
            llm,
            retriever=vectorstore.as_retriever(),
            chain_type_kwargs={'prompt': QA_CHAIN_P
ROMPT}
    )
    
    question = 'What are the various approaches to Task Decomposition for AI Agents?'
    result = qa_
chain({'query': question})

After unsuccessful experiments I even use the original web source and the question from the 
example in the documentation. But the output is rather disappointing (lots of hallucinations, perhaps coming from pre-tr
aining datasets, no information coming from the linked web page): 

>Based on the provided context, there are several ap
proaches to task decomposition for AI agents. Here are some of them:  
>  
>  
>  
>1. LLM with simple prompting: As men
tioned in the context, the agent can use language models (LLMs) with simple prompts like 'Steps for XYZ.' or 'What are t
he steps for XYZ?' to break down large tasks into smaller subgoals.  
>  
>2. Goal-based decomposition: The agent can de
compose a task into smaller subgoals based on its goals. For example, if the goal is to make a cup of coffee, the subgoa
ls could be 'obtain coffee beans,' 'boil water,' 'use coffee filter,' etc.  
>  
>3. Workflow-based decomposition: The a
gent can decompose a task into smaller subgoals based on a workflow or a series of steps involved in completing the task
. For example, if the task is to organize a trip, the subgoals could be 'book flight tickets,' 'make hotel reservations,
' 'plan itinerary,' etc.  
>  
>4. Hybrid approach: The agent can use a combination of goal-based and workflow-based dec
omposition to break down large tasks into smaller subgoals. For example, the agent can use goals to determine the overal
l task and then break it down into smaller subgoals based on a workflow.  
>  
>5. Machine learning-based approaches: Th
ere are various machine learning-based approaches to task decomposition, such as using neural networks to learn patterns
 in data or using reinforcement learning to learn how to break down tasks into smaller subgoals.

&#x200B;

I'm lost at 
this point, I thought using retriever=vectorstore.as\_retriever(), should have enforced using the source and only it as 
the context? Is there a mistake in the code or am I misunderstanding how it should work? 
```
---

     
 
all -  [ Implementing a few-shot template using Langchain and Llama (keywords extraction) ](https://www.reddit.com/r/LanguageTechnology/comments/16e22c5/implementing_a_fewshot_template_using_langchain/) , 2023-09-12-0909
```
My goal is as follows: I have main keywords, e.g., 'data warehouse zoom meeting participants.' I want to input only a sp
ecific keyword, and the model would generate more keywords, e.g., 'zoom,' 'meeting,' 'meeting participants,' 'company me
eting,' and so on. 

Below, I'm sharing a link to Colab; maybe someone could take a look and suggest something: https://
colab.research.google.com/drive/1dLdCsr39uwj3S0OaDi1H4D38Tgj266hH?usp=sharing


This version won't work if you don't hav
e nvidia gpu.
```
---

     
 
all -  [ Implementing a few-shot template using Langchain and Llama (keywords extraction) ](https://www.reddit.com/r/LangChain/comments/16e201i/implementing_a_fewshot_template_using_langchain/) , 2023-09-12-0909
```
My goal is as follows: I have main keywords, e.g., 'data warehouse zoom meeting participants.' I want to input only a sp
ecific keyword, and the model would generate more keywords, e.g., 'zoom,' 'meeting,' 'meeting participants,' 'company me
eting,' and so on. My script looks like this:

    model_id = 'meta-llama/Llama-2-13b-chat-hf'
    device = f'cuda:{cuda
.current_device()}' if cuda.is_available() else 'cpu'; print(device)
    
    <here I quantize and load the model, I wil
l skip>
    
    
    from langchain import PromptTemplate,  LLMChain
    from langchain.prompts.few_shot import FewShot
PromptTemplate
    
    examples = [
      {
        'Text': 'data warehouse zoom meeting participants',
        'Keywor
ds': 
    '''
    - 'data warehouse zoom meeting participants'
    - 'data warehouse'
    - 'zoom meeting'
    - 'meetin
g participants'
    - 'meeting'
    - 'zoom'
    '''
      },
      {
        'Text': 'published monthly chart',
       
 'Keywords': 
    '''
    - 'published monthly chart'
    - 'monthly chart'
    - 'revenue'
    - 'charts'
    - 'publis
hed chart'
    '''
      }
    ]
    
    example_prompt = PromptTemplate(input_variables=['Text', 'Keywords'], template
='You are a professional assistant for extracting keywords from text fragments given to you. Text fragment for keywords 
extraction is: {Text}\n Extracted Keywords:{Keywords}')
    
    print(example_prompt.format(**examples[0]))
    
    fe
w_shot_prompt = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_prompt,
        suffix=
'You are a professional assistant for extracting keywords from text fragments given to you. Text fragment for keywords e
xtraction is: {input}',
        input_variables=['input']
    )
    
    print(few_shot_prompt.format(input='my dataset'
))

Print result:

You are a professional assistant for extracting keywords from text fragments given to you. Text fragm
ent for keywords extraction is: data warehouse zoom meeting participants  Extracted Keywords: - 'data warehouse zoom mee
ting participants' - 'data warehouse' - 'zoom meeting' - 'meeting participants' - 'meeting' - 'zoom'   You are a profess
ional assistant for extracting keywords from text fragments given to you. Text fragment for keywords extraction is: publ
ished monthly chart  Extracted Keywords: - 'published monthly chart' - 'monthly chart' - 'revenue' - 'charts' - 'publish
ed chart'   You are a professional assistant for extracting keywords from text fragments given to you. Text fragment for
 keywords extraction is: my dataset

&#x200B;

I don't really know what to do next :(
```
---

     
 
all -  [ Example using vector dB retriever for top k docs to set context of LLM prompt ](https://www.reddit.com/r/LangChain/comments/16e1lib/example_using_vector_db_retriever_for_top_k_docs/) , 2023-09-12-0909
```
Hi,

&#x200B;

I'd like to retrieve top k passages from a vector store and insert them into a prompt to set the context 
for LLM text generation. Tried search but not finding nice examples, grateful for any pointers, thanks! 
```
---

     
 
all -  [ Unemployed and not sure how to represent on resume; not getting much traction. ](https://www.reddit.com/r/resumes/comments/16dr2fe/unemployed_and_not_sure_how_to_represent_on/) , 2023-09-12-0909
```
Thanks in advance for any advice!  


I'm applying for A) product manager, and B) product marketing manager jobs. (Somet
imes 'Senior' level, sometimes not). I have generally worked in 'tech' over my previous recent roles, but my experience 
is kind of a grab bag.  


I'm definitely stretching my qualifications a bit to land a tech PM/PMM role. Worth noting th
at I've done a lot of projects involving AI recently because that's a particular area of interest and often where I'm ap
plying.  


Issues I know I have to overcome:  


* Weird / small companies on my resume. I've started a couple companie
s and my recent work experiences aren't at 'known' companies. 
* I come off too senior (?). Since I've founded a couple 
companies, I have things like 'CEO' on my resume. But of a 15 person company... Nonetheless, I've been told before that 
hiring managers were worried I wouldn't be 'satisfied with the level of responsibility in this role.'
* I'm currently un
employed. I tried to remedy this by (truthfully) saying I've been doing self employed work. However, I don't know if I'm
 describing this correctly or whether it's even helpful to have that on here.

My main questions are:

&#x200B;

1. Are 
there any noticeable things I might consider changing to address the issues outlined above? (Or other issues you perceiv
e?)
2. Should I have an Overview section at all? What about the self employed Experience?  


Thanks again!   


https:/
/preview.redd.it/vti98hceh4nb1.png?width=674&format=png&auto=webp&s=870c5a55a60369df890a331bc5f433ae0f44a021
```
---

     
 
all -  [ Input variable for Prompt Template won't work with retrieval QA chain ](https://www.reddit.com/r/LangChain/comments/16djn0n/input_variable_for_prompt_template_wont_work_with/) , 2023-09-12-0909
```
&#x200B;

Hi Everyone, I have the following prompt template which requires an input variable {userName}:

    const prom
ptTemplate = `Use the following pieces of context to answer the question of {userName} at the end. If you don't know the
 answer, just say that you don't know, don't try to make up an answer.
    {context}
    
    Question: {question}
    A
nswer:`;
    
    const prompt = PromptTemplate.fromTemplate(promptTemplate);
    
    const chain = RetrievalQAChain.fr
omLLM(
      new ChatOpenAI({ modelName: 'gpt-3.5-turbo' }),
      vectorStore.asRetriever(),
      {
        returnSour
ceDocuments: true,
        prompt,
      }
    


&#x200B;

    const result = await chain.call({
        query: input,

        userName: 'James Conway'
      });
    

Calling the chain as shown above and giving query and userName as argum
ents outputs an error 'Uncaught Error: Missing value for input userName'.   


Has anyone got an idea why this is happen
ing? Thank you
```
---

     
 
all -  [ Using a RAG Model for Semantic Search & Document Question and Answering ](https://www.reddit.com/r/datascience/comments/16divx4/using_a_rag_model_for_semantic_search_document/) , 2023-09-12-0909
```
I am unsure if this is the right sub to ask such a question I'll try my best to provide as much context of my issue.

I 
am currently a IT Researcher looking at using RAG Models for document question and answering. I am working with Multiple
 500+ Page documents filled with charts, numbers, and text. We use these documents to record and track information in sp
readsheets. The current workflow is using a small team to go through the documents manually looking for the information 
they need to record in their sheets. The documents are too big to use a Ctrl + F keyword search so I was looking at RAG 
Models to make searching through these documents more effective.

I have been using ThirdAI's Pocket LLM (open to try al
ternative or open source RAG Models) to try to test the effectiveness of such a strategy but I have had issues properly 
prompting/searching for what I want. I have no problem building my own RAG Model using langchain I am just wondering if 
this is the best route for such an issue. Data privacy is not a concern because the documents we will be using are publi
cly available information. Data accuracy however is pretty important so I was wondering if hallucination is a factor whe
n dealing with RAG models.

It's fine if embeddings are expensive. Automating this process is far more valuable. Thanks 
for any insight or feedback and I will be happy to clarify anything just ask.
```
---

     
 
all -  [ LangChain based chatbot in Teams ](https://www.reddit.com/r/LangChain/comments/16dgoaz/langchain_based_chatbot_in_teams/) , 2023-09-12-0909
```
Is there any way to develop a chatbot in Teams (as a Teams contact I mean), that would be programmed using LangChain?
An
y pointer?
```
---

     
 
all -  [ How to parse AgentExecutor (ReAct) feedback to Streamlit? ](https://www.reddit.com/r/LangChain/comments/16dbyxa/how_to_parse_agentexecutor_react_feedback_to/) , 2023-09-12-0909
```
Hey y'all,

I wanted to put my ReAct agent on Streamlit. Thank you for the recommendations, btw, that was exactly what I
 was looking for.

The problem that I have is that the agent pipes the feedback into the shell but not the screen. Docum
entation doesn't really help.

Here is my agent definition

    llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turb
o')
    tools = load_tools(['human', 'serpapi', 'llm-math'], llm=llm)
    
    prefix = '''Some agent definition.
      
          Ask appropriate questions. 
                You have access to the following tools:'''
    suffix = '''Begin!'

    
    {chat_history}
    Question: {input}
    {agent_scratchpad}'''
    
    output_parser = CustomOutputParser()
 
   prompt = ZeroShotAgent.create_prompt(
        tools,
        prefix=prefix,
        suffix=suffix,
        input_vari
ables=['input', 'chat_history', 'agent_scratchpad'],
    )
    memory = ConversationBufferMemory(memory_key='chat_histor
y')
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose
=True, output_parser=output_parser)
    agent_chain = AgentExecutor.from_agent_and_tools(
        agent=agent, tools=too
ls, verbose=True, memory=memory
    )

Here is the streamlit part

    if prompt := st.chat_input('What is up?'):
      
  st.session_state.messages.append({'role': 'user', 'content': prompt})
        with st.chat_message('user'):
          
  st.markdown(prompt)
    
        with st.chat_message('assistant'):
            message_placeholder = st.empty()
     
       full_response = ''
            for response in agent_chain.run(st.session_state.messages):
                full_r
esponse += response.choices[0].delta.get('content', '')
                message_placeholder.markdown(full_response + '‚ñå'
)
            message_placeholder.markdown(full_response)
        st.session_state.messages.append({'role': 'assistant',
 'content': full_response})

What I want is that if the app queries 'human' it should pop the question to the chat inter
face.

&#x200B;

&#x200B;

&#x200B;
```
---

     
 
all -  [ CSV_AGENT HELP ](https://www.reddit.com/r/LangChain/comments/16dbjqy/csv_agent_help/) , 2023-09-12-0909
```
I'm trying to build a CSV Agent that holds memory of the previous conversations. Since , csv\_agent() does not support m
emory at the moment , how should I go about it ?
```
---

     
 
all -  [ LLM for context answers ](https://www.reddit.com/r/LangChain/comments/16datg3/llm_for_context_answers/) , 2023-09-12-0909
```
Hey guys, 
I‚Äôm looking for a good LLM that can take an input from a context and answer straight questions and is not ope
nai. Any suggestions?

If it works well with langchain agents, I‚Äôd be over the moon :-)
```
---

     
 
all -  [ How to architect a Rails monolith + modules? ](https://www.reddit.com/r/rails/comments/16d9a86/how_to_architect_a_rails_monolith_modules/) , 2023-09-12-0909
```
I want to use some services that I think are better in other languages, such as Langchain in Python or using Go to encod
e video (examples). But I want to keep most of the code in Rails.

The problem is that when I look for material about th
is, it always falls into extremes. Either you just use monoliths or you use microservices with Kafka, API Gateway, Clean
 Architecture and a bunch of other things.

What is the most suitable architecture for something like what I've describe
d? Rails handling most of the code, but freedom to create modules or services in other languages and frameworks. I belie
ve my biggest curiosity is how to orchestrate the communication between them in a consistent and secure way.
```
---

     
 
all -  [ Retrieve from FAQ documents ](https://www.reddit.com/r/LangChain/comments/16d8jpv/retrieve_from_faq_documents/) , 2023-09-12-0909
```
How would you best split and embed FAQ documents to be able to retrieve from them in something like a `ConversationalRet
rievalChain`?

\- Would you always put a single Question and Answer pair into a single chunk? How do you handle cases wh
ere such a Q&A pair is bigger than your max chunk size?

\- or would you rather just embed the questions and search them
 for similarity and return the answer if a retrieved question scores high?

Are there any configs/retrievers/chains/apps
 floating around doing this?
```
---

     
 
all -  [ Gen AI Jobs - Freelance Marketplace ](https://www.reddit.com/r/mlops/comments/16d7ipo/gen_ai_jobs_freelance_marketplace/) , 2023-09-12-0909
```
 

Hi everyone!

As we've been monitoring the latest developments in generative AI, we've noticed that at least four typ
es of jobs have emerged: AI Artists (specializing in Midjourney, Stable Diffusion, ControlNet, A1111, etc), Video Artist
s (familiar with RunwayML Gen2, Pika Labs, Fulljourney, etc), Prompt Engineers/Consultants, and LLM Model Trainers.

We'
ve decided to create Gen AI Jobs - Freelance Marketplace, a platform solely dedicated to these roles and any future jobs
 that may arise in this exciting field. Our mission is to become the one-stop-shop for generative AI professionals.  
Ex
clusive Access: Be among the first to access a marketplace tailored to your unique skills.  
Diverse Opportunities: Work
 on projects that align with your expertise and interests.  
Community Building: Connect with like-minded professionals 
and potential clients.

Join Our Waitlist: We're still in beta, but you can secure a front-row seat to the future by joi
ning our waitlist at [http://genaijobs.co](http://genaijobs.co/)  
Complete Our Survey: Fill out our short survey to hel
p us tailor the platform to your needs.

We're Looking For Expertise with: [OpenAI](https://www.linkedin.com/company/ope
nai/), [Anthropic](https://www.linkedin.com/company/anthropicresearch/), [Stability AI](https://www.linkedin.com/company
/stability-ai/) [Pika Labs](https://www.linkedin.com/in/ACoAAEZTw2oBGyi3YZbK-hM3l-0H9RhA5L-1Ecc), [Midjourney](https://w
ww.linkedin.com/company/midjourney/), [LangChain](https://www.linkedin.com/company/langchain/), [TensorFlow User Group (
TFUG)](https://www.linkedin.com/company/tensorflow/), [Hugging Face](https://www.linkedin.com/company/huggingface/), [Gi
tHub](https://www.linkedin.com/company/github/) [Runway](https://www.linkedin.com/company/runwayml/), Leonardo AI, [Elev
enLabs](https://www.linkedin.com/company/elevenlabsio/), [NVIDIA](https://www.linkedin.com/company/nvidia/), [Microsoft 
Azure](https://www.linkedin.com/company/microsoft-azure/) [Amazon Web Services (AWS)](https://www.linkedin.com/company/a
mazon-web-services/), etc.

[http://genaijobs.co](http://genaijobs.co/)
```
---

     
 
all -  [ Gen AI Jobs - Freelance Marketplace ](https://www.reddit.com/r/LargeLanguageModels/comments/16d7htx/gen_ai_jobs_freelance_marketplace/) , 2023-09-12-0909
```
  Hi everyone!

As we've been monitoring the latest developments in generative AI, we've noticed that at least four type
s of jobs have emerged: AI Artists (specializing in Midjourney, Stable Diffusion, ControlNet, A1111, etc), Video Artists
 (familiar with RunwayML Gen2, Pika Labs, Fulljourney, etc), Prompt Engineers/Consultants, and LLM Model Trainers.  


W
e've decided to create Gen AI Jobs - Freelance Marketplace, a platform solely dedicated to these roles and any future jo
bs that may arise in this exciting field. Our mission is to become the one-stop-shop for generative AI professionals.  

Exclusive Access: Be among the first to access a marketplace tailored to your unique skills.  
Diverse Opportunities: Wo
rk on projects that align with your expertise and interests.  
Community Building: Connect with like-minded professional
s and potential clients.  


Join Our Waitlist: We're still in beta, but you can secure a front-row seat to the future b
y joining our waitlist at [http://genaijobs.co](http://genaijobs.co/)  
Complete Our Survey: Fill out our short survey t
o help us tailor the platform to your needs.  


We're Looking For Expertise with: [OpenAI](https://www.linkedin.com/com
pany/openai/), [Anthropic](https://www.linkedin.com/company/anthropicresearch/), [Stability AI](https://www.linkedin.com
/company/stability-ai/) [Pika Labs](https://www.linkedin.com/in/ACoAAEZTw2oBGyi3YZbK-hM3l-0H9RhA5L-1Ecc), [Midjourney](h
ttps://www.linkedin.com/company/midjourney/), [LangChain](https://www.linkedin.com/company/langchain/), [TensorFlow User
 Group (TFUG)](https://www.linkedin.com/company/tensorflow/), [Hugging Face](https://www.linkedin.com/company/huggingfac
e/), [GitHub](https://www.linkedin.com/company/github/) [Runway](https://www.linkedin.com/company/runwayml/), Leonardo A
I, [ElevenLabs](https://www.linkedin.com/company/elevenlabsio/), [NVIDIA](https://www.linkedin.com/company/nvidia/), [Mi
crosoft Azure](https://www.linkedin.com/company/microsoft-azure/) [Amazon Web Services (AWS)](https://www.linkedin.com/c
ompany/amazon-web-services/), etc.  


[http://genaijobs.co](http://genaijobs.co/) 
```
---

     
 
all -  [ Chains and Agents ](https://www.reddit.com/r/aiengineer/comments/16d7fh4/chains_and_agents/) , 2023-09-12-0909
```
I think there's a lot of confusion around AI agents today and it's mainly because of lack of definition and using the wr
ong terminology.

We've been talking to many companies who are claiming they're working on agents but when you look unde
r the hood, they are really just chains.

I just listened to the Latent Space pod with¬†Harrison Chase (Founder of Langch
ain) and I really liked how he thinks about chains vs agents.

Chains: sequence of tasks in a more rigid order, where yo
u have more control, more predictability.  
Agents: handling the edge-cases, the long-tail of things that can happen.

A
nd the most important thing is that it's not an OR question but an AND one: you can use them in the same application by 
starting with chains -> figuring our the edge-cases -> using agents to deal with them.

&#x200B;

https://preview.redd.i
t/xbjmtlt4j0nb1.png?width=3127&format=png&auto=webp&s=8a787028bfb20d5fa6d25baab9ed98b88ff44b1f
```
---

     
 
all -  [ [D] Chains and Agents ](https://www.reddit.com/r/MachineLearning/comments/16d7ee6/d_chains_and_agents/) , 2023-09-12-0909
```
I think there's a lot of confusion around AI agents today and it's mainly because of lack of definition and using the wr
ong terminology.

We've been talking to many companies who are claiming they're working on agents but when you look unde
r the hood, they are really just chains.

I just listened to the Latent Space pod with¬†Harrison Chase (Founder of Langch
ain) and I really liked how he thinks about chains vs agents.

Chains: sequence of tasks in a more rigid order, where yo
u have more control, more predictability.  
Agents: handling the edge-cases, the long-tail of things that can happen.

A
nd the most important thing is that it's not an OR question but an AND one: you can use them in the same application by 
starting with chains -> figuring our the edge-cases -> using agents to deal with them.

https://preview.redd.it/l59sc4sr
i0nb1.png?width=3127&format=png&auto=webp&s=1f3f8730c48687eaabf1f554deb181cf35b96036
```
---

     
 
MachineLearning -  [ [P] FalkorDB - a fast Graph Database - Knowledge Graph as RAG ](https://www.reddit.com/r/MachineLearning/comments/16cg6k7/p_falkordb_a_fast_graph_database_knowledge_graph/) , 2023-09-12-0909
```
We're building a fast low latency Graph Database called FalkorDB that will also support Vector search.  
It's based on R
edis and can be used both as a stand alone database or a module for existing Redis.  
It feels like that is going to be 
the most optimized way to serve Knowledge as RAG, would love to get your feedback.  
[https://github.com/FalkorDB/falkor
db](https://github.com/FalkorDB/falkordb)  


It already supports LlamIndex and Langchain:  
[https://python.langchain.c
om/docs/use\_cases/more/graph/graph\_falkordb\_qa](https://python.langchain.com/docs/use_cases/more/graph/graph_falkordb
_qa)  
[https://gpt-index.readthedocs.io/en/latest/examples/index\_structs/knowledge\_graph/FalkorDBGraphDemo.html](http
s://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/FalkorDBGraphDemo.html)

&#x200B;
```
---

     
 
MachineLearning -  [ [D] Is there anything LangChain can do better than using LLMs directly (either through a website or  ](https://www.reddit.com/r/MachineLearning/comments/165airj/d_is_there_anything_langchain_can_do_better_than/) , 2023-09-12-0909
```
I haven't used ChatGPT a lot or any other LLMs, I've been reading about  Langchain and its use cases, and I'm having tro
uble wrapping my head  around exactly what it does. From what I understand, its an alternative  interface for LLMs, allo
wing for easy switching between them, and makes  some work for specific use cases easier. If I wanted to write an app or
  script to interact with LLMs and do other tasks, how would LangChain be  better than just making API call(s) to an LLM
, getting back the result  as a string, and doing whatever with it?
```
---

     
 
MachineLearning -  [ Apache Airflow vs. LangChain and LlamaHub for LLM data pipeline [D] ](https://www.reddit.com/r/MachineLearning/comments/160lexg/apache_airflow_vs_langchain_and_llamahub_for_llm/) , 2023-09-12-0909
```
I‚Äôm looking for recommendations, suggestions, and/or good documentation that outlines which data pipeline would be best 
to ingest my private data (which will then be split into chunks/nodes for vector embeddings and so forth). Thank you in 
advance!
```
---

     
 
MachineLearning -  [ [P] LLM Apps Are Mostly Data Pipelines ](https://www.reddit.com/r/MachineLearning/comments/15z0muk/p_llm_apps_are_mostly_data_pipelines/) , 2023-09-12-0909
```
My colleague just wrote up an article on [LLM-based apps and how to use data engineering tools to help build them faster
](https://meltano.com/blog/llm-apps-are-mostly-data-pipelines/) that I found really insightful.

It contains a complete 
implementation

* with scraping context data from a docs website
* chunking it, getting embeddings via the openAI API
* 
loading it into pinecone
* and finally a simple Q&A interface with streamlit on top of it

**Here's a quick summary:**


* LangChain and LlamaIndex are great tools for quick exploration
* But aren't perfect for production-grade use
* I think
 we all know the 'LangChain is pointless' debate, but there's a lot of real meat to it, and Pat describes a few of them 
(a lot of LangChains extractors are super basic, 2-3 liners without retries etc.)
* LLM applications are all about movin
g data, extracting and enriching data (creating embeddings!) are the most expensive ones of those steps
* A bunch of dat
a engineering tools are out there that make these two steps much easier, versionable, robust, and reproducible.
* Meltan
o is one such tool and Pat implemented the above described pipeline with it

**FWIW**: The GitHub project that comes wit
h the post is super easy to run and super modular. I just tested it and was able to modify everything for my own applica
tion within 30 mins.
```
---

     
 
MachineLearning -  [ [P] pgml-chat: A command-line tool for deploying low-latency knowledge-based chatbots ](https://www.reddit.com/r/MachineLearning/comments/15t5nzl/p_pgmlchat_a_commandline_tool_for_deploying/) , 2023-09-12-0909
```
We've created an open source chat bot builder, on top of PostgresML. This tool makes it easy to ingest documents and set
 a system prompt for a chatbot with knowledge of your content. The innovation is in the simplicity and efficiency, rathe
r than the functionality.

PostgresML runs open source embedding models alongside pgvector in Postgres to implement chat
 bot prompt creation without any network calls, which makes it \~4x faster than competing architectures. It can also do 
text generation with that prompt (and no additional network hops) using any open source model from HuggingFace, but it a
lso integrates with the GPT-4 API if you'd like to use that instead. 

The full writeup including some benchmarks for co
mpeting architectures is here:  [https://postgresml.org/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-kno
wledge-based-chatbots-part-I](https://postgresml.org/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowle
dge-based-chatbots-part-I)

You can chat with a deployment that has access to our blogs and documentation content it in 
\[our Discord\]([https://discord.com/channels/1013868243036930099/1013868243536072868](https://discord.com/channels/1013
868243036930099/1013868243536072868)), where it answers questions addressed to @PgBot.

&#x200B;

* The source code for 
the bot builder and server is only a few hundred lines of Python [https://github.com/postgresml/postgresml/tree/master/p
gml-apps/pgml-chat#readme](https://github.com/postgresml/postgresml/tree/master/pgml-apps/pgml-chat#readme)
* The chat a
pp is so small, because it's delegates all the vector db and embedding generation options to our Python client SDK, whic
h is available for anyone to build other apps with: [https://pypi.org/project/pgml/](https://pypi.org/project/pgml/)
* T
he Python client SDK is so small, because it's just a wrapper around the Rust client SDK: [https://github.com/postgresml
/postgresml/tree/master/pgml-sdks/rust/pgml](https://github.com/postgresml/postgresml/tree/master/pgml-sdks/rust/pgml). 
Currently we also support JS/Typescript SDKs as well, all generated from the same safe and efficient underlying Rust imp
lementation, using some fancy Rust macros.
* The Rust client SDK is also pretty simple though, because it just delegates
 everything to the Postgres database extension, which is where everything is computed in a single GPU accelerated proces
s, without having to load any ML models, data, or dependencies on client apps, effectively eliminating all the typical M
L data<->model network hops. Which makes it faster, simpler and safer.

This lays out what we think a is a better approa
ch to AI application architecture compared to libraries like LangChain or LlamaIndex, that focus on glueing together dis
parate data stores, algorithms, models over the network.  

```
---

     
 
MachineLearning -  [ [P] My apprehension about LangChain and why you don‚Äôt need LangChain for building a RAG bot. ](https://www.reddit.com/r/MachineLearning/comments/15ry3z4/p_my_apprehension_about_langchain_and_why_you/) , 2023-09-12-0909
```
A lot of you might be giving me a mouthful just by reading the title of this blog. But to each their own, and probably y
ou might be just riding the hype train. Initially, I was quite fascinated by the work being done on LangChain and using 
it. And so I thought I would give it a try, but when I was installing it, I saw it downloading loads and loads of other 
libraries and most of which were not useful for what I was trying to build.

Checkout the entire blog post at [https://t
hevatsalsaglani.medium.com/why-you-dont-need-langchain-for-building-a-rag-bot-a1dfbc74b64f](https://thevatsalsaglani.med
ium.com/why-you-dont-need-langchain-for-building-a-rag-bot-a1dfbc74b64f)
```
---

     
 
deeplearning -  [ How to find 'custom' datasets for LLM ](https://www.reddit.com/r/deeplearning/comments/16bj3hg/how_to_find_custom_datasets_for_llm/) , 2023-09-12-0909
```
Hey folks,

I've been digging everywhere, including here, for LLMs and custom applications. So, I read many things, lear
ned from ppl here. Its time to try something. I will try implement Llama v2 - Langchain - Chroma combination. But also I
 want to upload a dataset so that I can try my model on that. 

I find some datasets big enough (for now, 2-5 gb is ok) 
however they are table-style. I want something more texty, I mean I could use 'American Stories' or 'Arxiv' however I be
lieve that they are already used by Llama to train. 

&#x200B;

Is there any suggestions or sources that you can provide
 ? Thanks!
```
---

     
 
deeplearning -  [ VectorDB Operations with Faiss (View, Add, Delete, Save, QnA and Similarity Search) via Langchain ](/r/LangChain/comments/15qm2ie/vectordb_operations_with_faiss_view_add_delete/) , 2023-09-12-0909
```

```
---

     
