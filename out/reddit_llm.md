 
all -  [ CrewAI vs AutoGen? ](https://www.reddit.com/r/AI_Agents/comments/1ar0sr8/crewai_vs_autogen/) , 2024-02-15-0910
```
Hello, I wanted to ask about your opinion for comparison between different multi-agent frameworks. I have been playing w
ith both Autogen and CrewAI (I haven't tested ChatDev or others) and I am curious which you find better for your use cas
e and why.

&#x200B;

From my experience:  
\- Crew AI is more accessible and easily gets you something cool, cuz it's b
uilt on the the top of Langchain  
\- Autogen has better default code execution capabilities, maybe is more difficult to
 set up? Not sure.  


Happy to discuss!

  

```
---

     
 
all -  [ [3.5 YoE] Software Engineer trying to land interviews ](https://www.reddit.com/r/EngineeringResumes/comments/1ar0a5s/35_yoe_software_engineer_trying_to_land_interviews/) , 2024-02-15-0910
```
Hi all, looking to see how I can optimize this resume to land Software Engineering interviews. Haven't had much luck wit
h hearing back from the companies I've applied to (50+), either templated rejections or no response. Just recently updat
ed this using the wiki, but it's largely the same as versions I've been recently submitting.

https://preview.redd.it/fd
uu5gg6qmic1.png?width=5100&format=png&auto=webp&s=fb513ef1e5f5a26b798f4102f4e0d09194eee715

\- Currently located in the 
northeast but open to positions across the US and potentially abroad if it's a good fit

\- My experience is at small-me
dium sized startups and am looking to get experience at a larger pre-IPO company, also plan to eventually apply to FAANG
 but don't want my resume tossed out from the get go

\- Recently laid off from most recent position due to reorg

\- US
 citizen

Wondering if there are specific bullets I can reword to sound more impactful. Open to stylistic feedback also 
regarding formatting, spacing etc. Also curious about my Skills/Education/Honors section and if any of that doesn't seem
 useful or necessary. 
```
---

     
 
all -  [ How to set up RAG with Dolphin-mixtral-8x7b on ollama? ](https://www.reddit.com/r/ollama/comments/1aqt9v6/how_to_set_up_rag_with_dolphinmixtral8x7b_on/) , 2024-02-15-0910
```
Hey I am new to all this AI stuff and I'm trying to do my first setup on linux. I've decided to run Dolphin-mixtral-8x7b
 on my machine but I would like if it could retrieve information from the internet or my documents but I have no Idea on
 how to achieve this. I've heard of LangChain but I can't find a n00b friendly tutorial for 0 knowledge people.   


Can
 someone help me with this?
```
---

     
 
all -  [ Vector similarity check ](https://www.reddit.com/r/LangChain/comments/1aqt4w4/vector_similarity_check/) , 2024-02-15-0910
```
So I need to compare the similarity between 2 sentences contextually. I used tfIdVectorizer from sklearn to vectorize bo
th sentences and then calculate the cosine distance between them. This works good until any of the sentence is not in En
glish. Then I changed the embedding model to text-embedding-ada-002 which I believe is multilingual. Somehow the cosine 
distance is always too near around 0.7 even though both sentences are whole different contextually. Any advice for this?

```
---

     
 
all -  [ Seeking Insights: Data Science Fresher Resume Up for Critique ](https://i.redd.it/i2lmjwffekic1.png) , 2024-02-15-0910
```

```
---

     
 
all -  [ LangServe deployment tutorial ](https://www.reddit.com/r/LangChain/comments/1aqnnbo/langserve_deployment_tutorial/) , 2024-02-15-0910
```
Hey everyone, check out how to deploy a LangChain app using LangServe in local in this tutorial: https://youtu.be/7-zdc0
08mNo?si=WKVnRswLPDXxPVU9
```
---

     
 
all -  [ Best model and strategy for offline AI from embedding MS SQL ](https://www.reddit.com/r/LangChain/comments/1aqjo93/best_model_and_strategy_for_offline_ai_from/) , 2024-02-15-0910
```
What are the best model to use for offline AI embedding MS SQL local server?
```
---

     
 
all -  [ Dockerize RAG App with local Mixtral GGUF model ](https://www.reddit.com/r/LangChain/comments/1aqjbdc/dockerize_rag_app_with_local_mixtral_gguf_model/) , 2024-02-15-0910
```
Hi,

I have built a RAG App with LlamaCpp, Streamlit and Langchain and now want to move the application from local devel
opment to production. Therefore I want to dockerize my application.

I am using a Mixtral-Instruct GGUF model (Q4\_K\_M)
 with the size of 26GB and saw that the limit for Docker containers is 10GBs. So how can I dockerize this app then? Also
 I want to include my Vectorstore and embeddings.

Would be helpful to any tips and best practices. I am new to deployin
g an app to production and don't have that much experience with Docker.
```
---

     
 
all -  [ How to persistently save a Parent Document Retriever? ](https://www.reddit.com/r/LangChain/comments/1aqijs9/how_to_persistently_save_a_parent_document/) , 2024-02-15-0910
```
Hi,

i want to try out storing smaller embeddings for search with larger chunks for context. It seems that the Parent Do
cument Retriever serves this purpose. However the LangChain Documentation as well as numerous tutorials on YouTube do no
t mention any way of a persistent implementation. They just use an In Memory approach for the docstore. I have lots of d
ocuments i want to embed, so i basically want to have a persistent data structure with my embeddings and the correspondi
ng text chunks for context.

I first tried out using LangChains LocalStorage instead of InMemory, however this throws an
 error. 

Is there an easy way to store my Parent document retriever for later use?

&#x200B;

Thank you for your help
```
---

     
 
all -  [ How to retrieve document custom_id using source file name? ](https://www.reddit.com/r/LangChain/comments/1aqhlwg/how_to_retrieve_document_custom_id_using_source/) , 2024-02-15-0910
```
Is there any method to retrieve a document's custom_id using other attributes like the file name in Pgvector
```
---

     
 
all -  [ Best German Embedding Model? ](https://www.reddit.com/r/LangChain/comments/1aqh168/best_german_embedding_model/) , 2024-02-15-0910
```
Hi,

I wanted to discuss which german open-source embedding models worked best for your use case? My use case would be a
 RAG app where more or less complex german docs are retrieved. 

I played around with several multilingual/german embedd
ings but I am not sure which one to use. The results between the differnt embedding models are quite big I tried:

\- ji
naai/jina-embeddings-v2-base-de (recently published)

\- intfloat/multilingual-e5-large

\- T-Systems-onsite/cross-en-de
-roberta-sentence-transformer

\- sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

\- aari1995/German\_Seman
tic\_STS\_V2

These are the results:

`sentence1 = 'Ich liebe Cola'`

`sentence2 = 'Ich hasse Pepsi'`

`from numpy.linal
g import normcos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))embeddings = model.encode([sentence1,sentence2])embeddin
gs1 = model1.encode([sentence1,sentence2])embeddings2 = model2.encode([sentence1,sentence2])embeddings3 = model3.encode(
[sentence1,sentence2])embeddings4 = model4.encode([sentence1,sentence2])print(f'Jina: {cos_sim(embeddings[0], embeddings
[1])}')print(f'Intfloat: {cos_sim(embeddings1[0], embeddings1[1])}')print(f'RoBERTa: {cos_sim(embeddings2[0], embeddings
2[1])}')print(f'Paraphrase: {cos_sim(embeddings3[0], embeddings3[1])}')print(f'German Semantic: {cos_sim(embeddings4[0],
 embeddings4[1])}')`

&#x200B;

https://preview.redd.it/dcziehfp3iic1.png?width=282&format=png&auto=webp&s=858aa47781726
a3e543beba43001b9017bd4e2df
```
---

     
 
all -  [ My debut book on Generative AI is out !! ](https://www.reddit.com/r/GPT/comments/1aqewqs/my_debut_book_on_generative_ai_is_out/) , 2024-02-15-0910
```
I am thrilled to announce the launch of my debut technical book, “***LangChain in your Pocket: Beginner’s Guide to Build
ing Generative AI Applications using LLMs***” which is available on Amazon in Kindle, PDF and Paperback formats (bestsel
ler on Amazon in)

https://preview.redd.it/3wgfk1vwghic1.png?width=785&format=png&auto=webp&s=c7e4b15abf8572d88a10f5d0cc
5b033eec9d142f

In this comprehensive guide, the readers will explore LangChain, a powerful Python/JavaScript framework 
designed for harnessing Generative AI. Through ***practical examples and hands-on*** exercises, you’ll gain the skills n
ecessary to develop a diverse range of AI applications, including Few-Shot Classification, Auto-SQL generators, Internet
-enabled GPT, Multi-Document RAG and more.

***Key Features:***\- Step-by-step code explanations with expected outputs f
or each solution.- No prerequisites: If you know Python, you’re ready to dive in.- Practical, hands-on guide with minima
l mathematical explanations.

***Amazon*** : [https://www.amazon.com/LangChain-your-Pocket-Generative-Applications-ebook
/dp/B0CTHQHT25](https://www.amazon.com/LangChain-your-Pocket-Generative-Applications-ebook/dp/B0CTHQHT25)

***About me:*
**

I'm a Senior Data Scientist at DBS Bank with about 5 years of experience in Data Science & AI. Additionally, I manag
e 'Data Science in your Pocket', a [Medium Publication](https://medium.com/@mehulgupta_7991) & [YouTube channel](https:/
/www.youtube.com/@datascienceinyourpocket/videos) with \~600 Data Science & AI tutorials and a cumulative million views 
till date. To know more, you can check [here](https://www.linkedin.com/in/mehulgupta7991/)
```
---

     
 
all -  [ My debut book on Generative AI is out ! ](https://www.reddit.com/r/Chatbots/comments/1aqepbm/my_debut_book_on_generative_ai_is_out/) , 2024-02-15-0910
```
I am thrilled to announce the launch of my debut technical book, “***LangChain in your Pocket: Beginner’s Guide to Build
ing Generative AI Applications using LLMs***” which is available on Amazon in Kindle, PDF and Paperback formats (bestsel
ler on [Amazon.in](https://Amazon.in))

&#x200B;

https://preview.redd.it/l3cebhfyehic1.png?width=785&format=png&auto=we
bp&s=deefed64ac7e8ec9d6e47f86c59d9c4fdaf0c8f6

In this comprehensive guide, the readers will explore LangChain, a powerf
ul Python/JavaScript framework designed for harnessing Generative AI. Through ***practical examples and hands-on*** exer
cises, you’ll gain the skills necessary to develop a diverse range of AI applications, including Few-Shot Classification
, Auto-SQL generators, Internet-enabled GPT, Multi-Document RAG and more.

***Key Features:***\- Step-by-step code expla
nations with expected outputs for each solution.- No prerequisites: If you know Python, you’re ready to dive in.- Practi
cal, hands-on guide with minimal mathematical explanations.

***Amazon*** : [https://www.amazon.com/LangChain-your-Pocke
t-Generative-Applications-ebook/dp/B0CTHQHT25](https://www.amazon.com/LangChain-your-Pocket-Generative-Applications-eboo
k/dp/B0CTHQHT25)

***About me:***

I'm a Senior Data Scientist at DBS Bank with about 5 years of experience in Data Scie
nce & AI. Additionally, I manage 'Data Science in your Pocket', a [Medium Publication](https://medium.com/@mehulgupta_79
91) & [YouTube channel](https://www.youtube.com/@datascienceinyourpocket/videos) with \~600 Data Science & AI tutorials 
and a cumulative million views till date. To know more, you can check [here](https://www.linkedin.com/in/mehulgupta7991/
)
```
---

     
 
all -  [ The agent doesn't know how to process custom tools with multiple parameters. ](https://www.reddit.com/r/LangChain/comments/1aqdo35/the_agent_doesnt_know_how_to_process_custom_tools/) , 2024-02-15-0910
```
I'm trying to use Cohere API to run an agent, but even though it was able to correctly fetch the necessary parameters fr
om the query, it can't be properly passed into the custom tool with multiple parameter. I see it is not formatting the p
arameters as required, what can I do to help it do as the formatting want?

The link goes to the colab notebook with an 
example.

[https://colab.research.google.com/drive/1lShUML0qKM59Ds7Wr8eo4Z\_Nx8dxJ8f9?usp=sharing](https://colab.researc
h.google.com/drive/1lShUML0qKM59Ds7Wr8eo4Z_Nx8dxJ8f9?usp=sharing)
```
---

     
 
all -  [ Not sure why langsmith hub doesn't work ](https://www.reddit.com/r/LangChain/comments/1aqaejb/not_sure_why_langsmith_hub_doesnt_work/) , 2024-02-15-0910
```
So I'm trying to use Langsmith Hub for my prompts. I've tested using:  
prompt = hub.pull('hwchase17/openai-tools-agent'
)  


My script work fine. Now I want to add my own system prompt, so I've forked the above, and edited the system promp
t.  


But when I run the script now, I get this error:  
KeyError: 'Input to ChatPromptTemplate is missing variables {'
chat\_history'}.  Expected: \['agent\_scratchpad', 'chat\_history', 'input'\] Received: \['input', 'intermediate\_steps'
, 'agent\_scratchpad'\]'  


On langsmith hub, I've added the 'chat\_history'. But when i pull, for some reason it doesn
't pull through.  


Any thoughts?
```
---

     
 
all -  [ What's the standard practice for setting initialization prompts and maintaining context when switchi ](https://www.reddit.com/r/OpenAIDev/comments/1aq79ab/whats_the_standard_practice_for_setting/) , 2024-02-15-0910
```
Hi I am trying to build a modularized LLM application using Langchain where within any individual conversation, the app 
can seamless switch between multiple LLMs to respond to the query, for example:

1. User: What's 1+ 1?
2. App (GPT-3.5):
 1+1 is 2
3. User: Concatenate the last name of the current president of the US with the answer from your last response

4. App (Gemini Ultra): Biden2

I have two technical questions that I hope this subreddit can help answer:

1. What's the
 standard practice for setting the initialization prompts or background prompts? For example I want to tell this App tha
t 'your name is Bob', and I want this App to continuously remember it's Bob regardless how long the conversation has got
ten or any switching between LLMs. Do I set this at the beginning of the conversation or before every single response?
2
. What's the standard practice for conversation memory management when there's switching of LLM involved within one conv
ersation? Do I store all the conversation history within a vector database and do a index search prior to any individual
 response?
```
---

     
 
all -  [ [D] What's the standard practice for setting initialization prompts and maintaining context when swi ](https://www.reddit.com/r/MachineLearning/comments/1aq78ao/d_whats_the_standard_practice_for_setting/) , 2024-02-15-0910
```
Hi I am trying to build a modularized LLM application using Langchain where within any individual conversation the app c
an seamless switch between multiple LLMs to respond to the query, for example:

1. User: What's 1+ 1?
2. App (GPT-3.5): 
1+1 is 2
3. User: Concatenate the last name of the current president of the US with the answer from your last response
4
. App (Gemini Ultra): Biden2

I have two technical questions that I hope this subreddit can help answer:

1. What's the 
standard practice for setting the initialization prompts or background prompts? For example I want to tell this App that
 'your name is Bob', and I want this App to continuously remember it's Bob regardless how long the conversation has gott
en or any switching between LLMs. Do I set this at the beginning of the conversation or before every single response?
2.
 What's the standard practice for conversation memory management when there's switching of LLM involved within one conve
rsation? Do I store all the conversation history within a vector database and do a index search prior to any individual 
response?
```
---

     
 
all -  [ What is AI Observability ](https://www.reddit.com/r/AIObservability/comments/1aq6hxm/what_is_ai_observability/) , 2024-02-15-0910
```
Let's say there was an example Q&A bot powered by a large language model created by a coffee company to enage their cust
omers more effectively. A user's question such as 'what's a latte?' kicks off a workflow that identifies relevant conten
t from a knowledgebase about coffee using an embedding model and passes this context to a large language which then summ
arizes the content in a brief natural response.

In an sample deployment, the workflow could be implemented as a python 
program coded using LangChain framework and the embedding & language models are obtained from Huggingface. Workflow is s
erved up using Azure ML and the models are served using Nvidia Triton inference server hosted on kubernetes service in A
zure.

AI observability would require capturing and correlating metrics from across the LLM app components and the AI in
fra services used to run these components to understand what impacts performance and how to best optimize it.
```
---

     
 
all -  [ What can we do about user spam? ](https://www.reddit.com/r/LangChain/comments/1aq5n46/what_can_we_do_about_user_spam/) , 2024-02-15-0910
```
I’m implementing an agent to hopefully trial with some of our current users via WhatsApp and realized that users could e
asily spam messages to get a response, thus unnecessarily increasing our costs by calling some api to return a model’s r
esponse.

What strategies could someone implement to to tackle this issue?
```
---

     
 
all -  [ Introductory courses that aren't video based? ](https://www.reddit.com/r/LangChain/comments/1aq2xzl/introductory_courses_that_arent_video_based/) , 2024-02-15-0910
```
Management at my customer service job has given the green light to work on continuing education classes when it's slow. 
I'm looking for some introductory classes I can take that are text based as I cannot watch videos during my shift. Does 
anyone know of some course options that are read along only with no video component?
```
---

     
 
all -  [ Is there a way to store Vectorstores? ](https://www.reddit.com/r/LangChain/comments/1aq2nvr/is_there_a_way_to_store_vectorstores/) , 2024-02-15-0910
```
Hi, I am building a vectorstore using `Chroma.from_documents` of textual data. Because my dataset is large, the number o
f chunks is high and hence it takes a long time to create the vector store. What is the best way to - a) parallelize it.
 and b) store it so I don't have to do it every time I run my notebook?
```
---

     
 
all -  [ Deployment template for running LangServe on AWS Fargate ](https://www.reddit.com/r/LangChain/comments/1aq19oe/deployment_template_for_running_langserve_on_aws/) , 2024-02-15-0910
```
A template to deploy your LangChain app running locally to the cloud. This architecture specifically packages LangServe 
into a Docker image, stores the image in ECR, and runs the container in AWS Fargate with an ALB in front.

[langserve\_r
eference\_architecture](https://preview.redd.it/flzjgv5wfeic1.png?width=6079&format=png&auto=webp&s=b5a5d4ab4330d435cdf1
4d43f1a3f5487a8577ff)

The template comes in Python, Typescript, Golang, C#, YAML. It uses OpenAI for the LLM.

Read the
 [blog post](https://www.pulumi.com/blog/easy-ai-apps-with-langserve-and-pulumi) to follow along with the two examples: 
a Gandalf chatbot and a Pinecone RAG app. Let me know if you have any questions or if you would like to see any other ki
nd of template.
```
---

     
 
all -  [ Is this possible? ](https://www.reddit.com/r/generativeAI/comments/1apz09f/is_this_possible/) , 2024-02-15-0910
```
Hi, newbie to GenAI here. 

I wanted to know if we can download open source GenAI models and use them in an isolated env
ironment by fine tuning them to retrieve needed data and answer the questions asked based on it. 

I've started to learn
 LangChain and ig we can do that using it. If the whole system needs to be on cloud, how do we  basically host this whol
e thing?

Will there be any security issues like some leakage points in the downloaded model itself such that it steals 
data from us?

I need to make a POC for clients like media or law firms. 

Any input or suggestions will be helpful and 
appreciated. Thanks.
```
---

     
 
all -  [ Using Vertex AI Model Garden with Langchain ](https://www.reddit.com/r/LangChain/comments/1apwigv/using_vertex_ai_model_garden_with_langchain/) , 2024-02-15-0910
```
I am trying to use Vertex AI Model Garden for inference with Langchain. I have successfully deployed Mistral to an endpo
int in Google Cloud and want to get inference with the class VertexAIModelGarden which is already implemented. However, 
it seems the output returned by the endpoint is not correctly parsed as it only contains a single character (see image).


I found a pull request on the langchain github repo that mentions this issue and there is a link to another repo where
 someone has implemented a temporary fix ( [https://github.com/shikanime/langchain-vertexai-extended](https://github.com
/shikanime/langchain-vertexai-extended) ). I tried using this code and it seems to work but I was wondering if anyone el
se had this issue and if this 'fix' will eventually be included in Langchain. Furthermore, the stream method is not yet 
supported in this fix so we can just use the invoke. 

&#x200B;

https://preview.redd.it/lekh5bkvgdic1.png?width=1290&fo
rmat=png&auto=webp&s=cc51b00326ae91a98b3b5f6ee958129d61f3192f
```
---

     
 
all -  [ GPT4ALL function calling ](https://www.reddit.com/r/LangChain/comments/1aputis/gpt4all_function_calling/) , 2024-02-15-0910
```
I know there is this support for function calling for models like [OpenAI](https://python.langchain.com/docs/modules/mod
el_io/chat/function_calling) but is this also supported in the GPTAALL?
```
---

     
 
all -  [ Preparing model for production ](https://www.reddit.com/r/LangChain/comments/1apu4ml/preparing_model_for_production/) , 2024-02-15-0910
```
Hi, I have wrote a chat model based on huggingface inference through langchain, and I would like to prepare it for produ
ction. 

My scope is kinda small as I am aiming to host it locally in WAMP/XAMP or AWS server to be used as demonstratio
n of the app.  

I was able to find few API explanations out there but it is quite unclear hence it is my very first imp
lementation in such context.

I tried Writing a Flash app and prepare few files one that has Flash api calla, the second
 one has the code structure for the app (starting with package importing and ending with response), and the last file ha
s all the functions that I am using in my second file that preprocess and handle data.

I'm left wondering on one questi
on is that the model and memory initialization and construction is in the second file, which leave me wondering how the 
memory of the chat model is going to persist when the user query it multiple queries within the same interaction.
```
---

     
 
all -  [ Please give tips can't get any internship ,sophomore at tier 3 ](https://i.redd.it/qnje7e0h1cic1.jpeg) , 2024-02-15-0910
```
Suggestions or any advice on resume are appreciated 🙂
```
---

     
 
all -  [ Ollama sequential behaviour ](https://www.reddit.com/r/LangChain/comments/1apq6ql/ollama_sequential_behaviour/) , 2024-02-15-0910
```
Hey,   
I was exploring the Ollama library to serve the mistral model, I came into a case where the model was serving a 
single request at a time. 

i.e. Scenario: Simultaneous requests from two or more machines were served by the model on a
 first come first serve basis. Expected behavior: Serve multiple requests simultaneously.  


If the expected behavior i
s possible, guide me through the process.
```
---

     
 
all -  [ How can I use LangChain to filter through files in a directory given some filters ](https://www.reddit.com/r/LangChain/comments/1apq229/how_can_i_use_langchain_to_filter_through_files/) , 2024-02-15-0910
```
Im trying to mass filter through a windows directory given filters such as date, file type, and other important keywords
, is there any python library or langchain system that can filter using some basic filters like these? I have tried indi
vidually going file by file and running a loader but it takes forever and is quite cost intensive.
```
---

     
 
all -  [ How to use Hugging Face models ? Specify resources. ](https://www.reddit.com/r/ArtificialInteligence/comments/1app63v/how_to_use_hugging_face_models_specify_resources/) , 2024-02-15-0910
```
Guys I'm totally a beginner, idk anything about hugging face models, langchain,and i don't know how to use Api's . Sugge
st me Some resources to learn How to make use of open source models form hugging face.i know some basic python.
```
---

     
 
all -  [ Got out of SAP. Now working with proper desired tech stack. ](https://www.reddit.com/r/developersIndia/comments/1apne2r/got_out_of_sap_now_working_with_proper_desired/) , 2024-02-15-0910
```
This post is a continuation of my [previous post](https://www.reddit.com/r/developersIndia/s/MrnXDtOU5W). The company wh
ich I interviewed called me back after 3 weeks of my interview for the HR round. Got selected. Took an early release fro
m the previous company and joined my new company in December. 

The new company is in my hometown. Colleagues are friend
ly. Work is interesting and challenging. Also, as a bonus, I get free food as lunch.

I am a little confused on what job
 role exactly I am working (I know the designation but that is a company issued designation). 

I am/will working/work i
n AI (researching, training, creating models if needed), generative AI, langchain, also, I am building backend in my cur
rent project fully in Django (I also made the frontend too for my learning purpose and there were no other person to do 
that since this project was assigned to me only do that I can learn on my own, but my team that I'm assigned to has a fr
ontend developer).

Can anyone tell me what my job role is so that I can properly mention that in my resume?


Thank you
 in advanced.
```
---

     
 
all -  [ How can I get the embedding of a document in langchain? ](https://www.reddit.com/r/LangChain/comments/1aph0v8/how_can_i_get_the_embedding_of_a_document_in/) , 2024-02-15-0910
```
I use the langchain Python lib to create a vector store and retrieve relevant documents given a user query. How can I ge
t the embedding of a document in the vector store?

E.g., in this code:

    import pprint
    from langchain_community.
vectorstores import FAISS
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain.docsto
re.document import Document
    
    model = 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'
    embeddings = HuggingF
aceEmbeddings(model_name = model)
    
    def main():
        doc1 = Document(page_content='The sky is blue.',    metad
ata={'document_id': '10'})
        doc2 = Document(page_content='The forest is green', metadata={'document_id': '62'})
 
       docs = []
        docs.append(doc1)
        docs.append(doc2)
    
        for doc in docs:
            doc.metad
ata['summary'] = 'hello'
    
        pprint.pprint(docs)
        db = FAISS.from_documents(docs, embeddings)
        db
.save_local('faiss_index')
        new_db = FAISS.load_local('faiss_index', embeddings)
    
        query = 'Which colo
r is the sky?'
        docs = new_db.similarity_search_with_score(query)
        print('Retrieved docs:', docs)
        
print('Metadata of the most relevant document:', docs[0][0].metadata)
    
    if __name__ == '__main__':
        main()




---

How can I get the embedding of documents `doc1` and `doc2`?


The code was tested with Python 3.11 with:

```
p
ip install langchain==0.1.1 langchain_openai==0.0.2.post1 sentence-transformers==2.2.2 langchain_community==0.0.13 faiss
-cpu==1.7.4
```
```
---

     
 
all -  [ Whats in your RAG setup? ](https://www.reddit.com/r/LocalLLaMA/comments/1apcqq4/whats_in_your_rag_setup/) , 2024-02-15-0910
```
What frameworks and libraries are you using in your RAG? 

I'm most curious if  LangChain is as popular as it was?

Here
's mine at a high-level: 

*  langchain to use OpenAI for creating embeddings
* Pinecone for storing embedding
* langcha
in to load document splitters and characters splitters for chunking
* Mongo for conversations memory

&#x200B;
```
---

     
 
all -  [ Whats your RAG setup? ](https://www.reddit.com/r/LLMDevs/comments/1apcmxt/whats_your_rag_setup/) , 2024-02-15-0910
```
What frameworks and libraries are you using in your RAG? 

I'm most curious if  LangChain is as popular as it was?

Here
's mine at a high-level: 

*  langchain to use OpenAI for creating embeddings
* Pinecone for storing embedding
* langcha
in to load document splitters and characters splitters for chunking
* Mongo for conversations memory

&#x200B;

&#x200B;

```
---

     
 
all -  [ LlamaIndex or LangChain with LlamaIndex for RAG with documents ](https://www.reddit.com/r/LocalLLaMA/comments/1ap7hrt/llamaindex_or_langchain_with_llamaindex_for_rag/) , 2024-02-15-0910
```
My goal is to create a chatbot to do Q&As with documents. Do I need to learn both LangChain and LlamaIndex? Or just Llam
aIndex would be sufficient? In other words, for my intended applications, what will I miss if I just use LlamaIndex with
out LangChain? Thanks.
```
---

     
 
all -  [ Just found ctransformers doesn't work with Tokenizer ](https://www.reddit.com/r/LocalLLaMA/comments/1ap5xgl/just_found_ctransformers_doesnt_work_with/) , 2024-02-15-0910
```
I was looking for how to get the tokenizer from a GGUF model and realized it (almost) doesn't actually work.

Came acros
s this documentation on ctransformers ([https://pypi.org/project/ctransformers/#langchain](https://pypi.org/project/ctra
nsformers/#langchain)):

 

>To use it with 🤗 Transformers, create model and tokenizer using:  
>  
>`from ctransformers
 import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained('marella/gpt-2-ggml', hf=True) 
tokenizer = AutoTokenizer.from_pretrained(model)` 

I thought: 'Sweet, that's pretty easy!' But when I run it, `AutoToke
nizer.from_pretrained(model)` throws NotImplementedError.

https://preview.redd.it/66ic68kbw6ic1.png?width=1660&format=p
ng&auto=webp&s=a7652d0e36fc43882ec98957183f2c9c1a93d27f

  
Further search led me to [https://github.com/marella/ctransf
ormers/issues/184](https://github.com/marella/ctransformers/issues/184), which is this exact issue raised on ctransforme
rs repo back in Dec 2023. I haven't tried this but according to this the only solution is to downgrade transformers lib 
to 4.33, which rips a lot of features off. Also it looks like the not implemented error has been there all the time. Not
 sure what caused that breakage.

Just want to share my findings since it's pretty disappointing to find that such basic
 function is not working :(
```
---

     
 
all -  [ Control token size of a tool ](https://www.reddit.com/r/LangChain/comments/1ap2rr3/control_token_size_of_a_tool/) , 2024-02-15-0910
```
I have an agent which uses a custom tool to search and provide an output, my problem. Is many times the output of the to
ol which is being passes to. The llm surpasses the token limit, is there away to chunk the output of the tool and gather
 the responses and send the whole responses to the user?
```
---

     
 
all -  [ LangChain playlist with 60 tutorials ](https://www.reddit.com/r/learnmachinelearning/comments/1ap1uy7/langchain_playlist_with_60_tutorials/) , 2024-02-15-0910
```
Hey everyone, check out this LangChain (generative AI framework) playlist comprising of 60 tutorials explaining everythi
ng from scratch
https://youtube.com/playlist?list=PLnH2pfPCPZsKJnAIPimrZaKwStQrLSNIQ&si=8bXhqED-NiVITZK9
```
---

     
 
all -  [ Agents vs Chains ](https://www.reddit.com/r/generativeAI/comments/1ap1hf2/agents_vs_chains/) , 2024-02-15-0910
```
Hey everyone, check out how Agents are different from chains in LangChain in this new tutorial: https://youtu.be/A3Gm6KP
xy4k
```
---

     
 
all -  [ AI document jury ](https://www.reddit.com/r/LangChain/comments/1ap0pbx/ai_document_jury/) , 2024-02-15-0910
```
I need to make an AI that can judge a document based on some criteria. The criteria can be like:
- The document explains
 the market segmentation efficiently 
- The document mentions how market works and explaining each type of markets.

The
 AI will judge the document based on above criteria and will give a score. 
This is just a topic for example and the cri
teria and the document can be for other topics.

I have created a chatbot using RAG but for this one I still don't have 
a clue for the approach. Mainly because with chatbot, I retrieve like 5 nearest chunks by cosine similarity while for th
is one, I don't think retrieving nearest chunks by cosine similarity to the question will return all the chunks for the 
document part the criteria needs to judge. And I can't just give the LLM all chunks from the document right? Any advice 
for this one? Any help would be really appreciated.
```
---

     
 
MachineLearning -  [ [D] What's the best current RAG setup that would work with a local LLM? ](https://www.reddit.com/r/MachineLearning/comments/1ag6bo7/d_whats_the_best_current_rag_setup_that_would/) , 2024-02-15-0910
```
I've tried things like langchain in the past (6-8 months ago) but they were cumbersome and didn't work as expected.

I  
need RAG to get data from various pdfs (long one, 150+ pages) - and i  need a setup that will allow me to add more and m
ore data sources.

I wanna run this locally, can get a 24gb video card (or 2x16gb ones) - so i can run using 33b or smal
ler models.

I  know things in the industry change every 2 weeks, so i'm hoping there's  an easy and efficient way of do
ing RAG (compared to 6 months ago)
```
---

     
 
MachineLearning -  [ [P]: Anukool: My job hunting assistant ](https://www.reddit.com/r/MachineLearning/comments/1adu3tw/p_anukool_my_job_hunting_assistant/) , 2024-02-15-0910
```
Hey Reddit, I've been applying for jobs and found that writing a cover letter for each position was tedious. I also delv
ed into LLM and Langchain, hoping to leverage them for a project to aid in my job hunting. So, I developed Anukool under
 the GPL license. While it's far from perfect, it has proven very useful to me, and I hope it benefits you as well. All 
I have to do is provide it with a pdf containing information about me such as my experience, skills, projects, etc and i
t will use this information along with job description to generate cover letter for me. Since I'm new to ML and LLM, any
 advice or feedback is greatly appreciated, and contributions are also welcome. I plan to utilize Llama-2 soon to furthe
r open-source the project.

Check out the GitHub link, and please star it if you find the project interesting: https://g
ithub.com/dakshesh14/anukool
```
---

     
 
MachineLearning -  [ New Data API for Astra [N] ](https://www.reddit.com/r/MachineLearning/comments/199uobn/new_data_api_for_astra_n/) , 2024-02-15-0910
```
I saw that DataStax/Astra DB [just released a new Data API to help with building production GenAI and RAG applications](
https://www.datastax.com/blog/general-availability-data-api-for-enhanced-developer-experience). This API makes the prove
n petabyte-scale of Apache Cassandra easy to use and available to any JavaScript, Python, or full-stack application deve
loper.

There will also be a joint webinar with LangChain available for registration here: [https://www.datastax.com/eve
nts/wikichat-build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel](https://www.datastax.com/events/wikichat-
build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel)
```
---

     
 
MachineLearning -  [ [D] While using function calling or tools on openai or langchain, does openai have access to the dat ](https://www.reddit.com/r/MachineLearning/comments/199t8be/d_while_using_function_calling_or_tools_on_openai/) , 2024-02-15-0910
```
I am working on a client project and I am using langchain's tools and agents. I want to know if the data is getting pass
ed to openai or is it just like that - Output of one function is being directly passed to the second function with the k
nowledge of openai.
```
---

     
 
MachineLearning -  [ [D] Code vs JSON output for LLM agents? Frameworks like LangChain rely on LLMs responding with JSON  ](https://www.reddit.com/r/MachineLearning/comments/197f416/d_code_vs_json_output_for_llm_agents_frameworks/) , 2024-02-15-0910
```
[CaP](https://arxiv.org/pdf/2209.07753.pdf), [Voyager](https://arxiv.org/pdf/2305.16291.pdf), [Octopus](https://arxiv.or
g/abs/2310.08588)

I work primarily with JSON based agents but code-as-policy agents seem to be extremely powerful. Here
 are some of the benefits and weaknesses I've seen

Pros of code

1. Less tool creation needed - The prebuilt math/file/
string/list manipulation abilities that come with code are enormous. In a JSON based agent, you would have to formally d
eclare each of these as a tool which you expose to the LLM and explain in your prompting, which is a lot of work and eat
s up a ton of the context window. 
2. Reduced number of transactions - The LLM can write scripts that invoke multiple to
ols and manipulate their results in ways that are difficult to do in a single transaction via JSON. For example, in one 
script, the model could search a DB 3 times, perform regex on the query results, convert them to integers, and add them 
up. Doing this in one step via JSON tool invocations is basically impossible. 
3. Less syntax errors - this might be tot
ally just vibe-based reasoning, but it really seems like LLMs have an easier time writing valid python than valid JSON, 
especially when you have lots of nested arguments in your methods.

Cons

1. Crazy risky - This is the obvious one. You 
have a machine executing random code. There are ways to mitigate this but still. I mean seriously we all learned not to 
use eval, so it is crazy to basically see research tending towards just running eval on the outputs of these models. 
2.
 Scripts with errors - Sometimes the model tries to get too fancy and writes complex programs that have bugs, resulting 
in many needed retries. 

Do any of you have thoughts or experience with these approaches in the wild? 

Is anybody awar
e of any experiments that compare these two approaches against each other? 

&#x200B;
```
---

     
 
deeplearning -  [ [D] WebVoyager: Navigating Digital Cosmos with LangGraph & Multimodal Models ](https://www.reddit.com/r/deeplearning/comments/1altlca/d_webvoyager_navigating_digital_cosmos_with/) , 2024-02-15-0910
```
Embark on a journey through the digital cosmos with WebVoyager, a groundbreaking Large Multimodal Model (LMM) web agent 
designed to navigate the vastness of the online universe. In collaboration with Langchain, WebVoyager represents a parad
igm shift in autonomous web agents, seamlessly integrating visual and textual information to complete user instructions 
end-to-end by interacting with real-world websites.

Link: [https://medium.com/@andysingal/webvoyager-navigating-digital
-cosmos-with-langgraph-multimodal-models-dace64196c2f](https://medium.com/@andysingal/webvoyager-navigating-digital-cosm
os-with-langgraph-multimodal-models-dace64196c2f)
```
---

     
 
deeplearning -  [ [D] Langchain Elevates with Step-Back Prompting using RAGatouille ](https://www.reddit.com/r/deeplearning/comments/1agtyeh/d_langchain_elevates_with_stepback_prompting/) , 2024-02-15-0910
```
In the dynamic realm of natural language processing, a revolutionary synergy has emerged between Langchain and Step-Back
 Prompting. This article delves into the transformative collaboration, exploring how Langchain’s cutting-edge platform i
ncorporates Step-Back Prompting to redefine language processing capabilities. Join us on a journey of innovation and dis
covery as we unravel the intricacies of this powerful integration. As we explore the uncharted territories of language m
odels, Step-Back Prompting stands as a beacon of progress, promising a journey of nuanced understanding and elevated per
formance in the world of Large Language Models. Welcome to the future of language processing, where inspiration and inno
vation converge in a symphony of words and ideas.

Link: https://medium.com/ai-advances/langchain-elevates-with-step-bac
k-prompting-using-ragatouille-b433e6f200ea
```
---

     
 
deeplearning -  [ Become an AI Developer (Free 9 Part Series) ](https://www.reddit.com/r/deeplearning/comments/1afgp2r/become_an_ai_developer_free_9_part_series/) , 2024-02-15-0910
```
Just sharing a free series I stumbled across on Linkedin - DataCamp's 9-part AI code-along series.

This specific sessio
n linked below is 'Building Chatbots with OpenAI API and Pinecone' but there are 8 others to have a look at and code alo
ng to.

*Start from basics to build on skills with GPT, Pinecone and LangChain to create a chatbot that answers question
s about research papers. Make use of retrieval augmented generation, and learn how to combine this with conversational m
emory to hold a conversation with the chatbot. Code Along on DataCamp Workspace:* [*https://www.datacamp.com/code-along/
building-chatbots-openai-api-pinecone*](https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone)

Find
 all of the sessions at: [https://www.datacamp.com/ai-code-alongs](https://www.datacamp.com/ai-code-alongs)
```
---

     
 
deeplearning -  [ DSPy Explained! ](https://www.reddit.com/r/deeplearning/comments/1adypks/dspy_explained/) , 2024-02-15-0910
```
DSPy is the next big advancement for AI and building applications with LLMs!

Pioneered by frameworks such as LangChain 
and LlamaIndex, we can build much more powerful systems by chaining together LLM calls! This means that the output of on
e call to an LLM is the input to the next, and so on. We can think of chains as programs, with each LLM call analogous t
o a function that takes text as input and produces text as output.

DSPy offers a new programming model, inspired by PyT
orch, that gives you a massive amount of control over these LLM programs. Further the Signature abstraction wraps prompt
s and structured input / outputs to clean up LLM program codebases.

DSPy then pairs the syntax with a super novel compi
ler that jointly optimizes the instructions for each component of an LLM program, as well as sourcing examples of the ta
sk.

Here is my review of the ideas in DSPy, covering the core concepts and walking through the introduction notebooks s
howing how to compile a simple retrieve-then-read RAG program, as well as a more advanced Multi-Hop RAG program where yo
u have 2 LLM components to be optimized with the DSPy compiler! I hope you find it useful!

https://www.youtube.com/watc
h?v=41EfOY0Ldkc
```
---

     
