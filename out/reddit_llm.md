 
all -  [ How could I just return the final answer from SQL Agent? ](https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/) , 2024-05-25-0910
```
Im planning to do an endpoint that given a user question it makes the underlying work to get the query and i would like 
to only receive the final answer as im going to show it on a Streamlit chat app. Any idea on how to extract only that?
```
---

     
 
all -  [ Gemini api embedding error with langchain please help ](https://www.reddit.com/r/developersIndia/comments/1czsbmf/gemini_api_embedding_error_with_langchain_please/) , 2024-05-25-0910
```
Does anyone know how to solve error from Gemini api with langchain for embedding. I'm using it to get context from an Ex
cel sheet but I'm facing this error: Deadline Exceeded or this one ValueError: Expected each embedding in the embeddings
 to be a list, got ['Repeated']
```
---

     
 
all -  [ Guide to build RAG ecosystem on Node Server ](https://denuwanhimangahettiarachchi.medium.com/build-gen-ai-llm-rag-api-ecosystem-on-a-node-server-99d4a04e50fa) , 2024-05-25-0910
```

```
---

     
 
all -  [ Fresher. Need advice on which role to take in my company ](https://www.reddit.com/r/developersIndia/comments/1czroy7/fresher_need_advice_on_which_role_to_take_in_my/) , 2024-05-25-0910
```
I'm currently an intern at my company. It's a finance company. I work for the tech dept. 

My current intern work is ver
y good and the team is also very supportive. I'm working on LangChain agents and langsmith. 

The problem is. The team I
'm working for does not have an opening. So my company wants me to either apply for a different role in a different team
. I don't find anything attractive. The only openings for freshers are either support or testing roles. 

In my current 
team there's an opening for the testing role but it's under the QE team that's helping my current team. I really liked t
he current work. I don't have any other offers in hand.

Should I join as a tester for the LangChain apis they are build
ing or should I just take a different role or try outside?
```
---

     
 
all -  [ Attempt to be Forward-looking on a New Project ](https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/) , 2024-05-25-0910
```
I'm new to LLMs, but I'm planning to build an application that answers technical questions about my API using a RAG syst
em based on my tech docs (e.g., 'how can I configure the API request to wait for the response and to retry if the reques
t times out?'). To summarize ...Goal: answer technical questions (found in my /docs/ subdirectory) so the user doesn't h
ave to search and dig for it. My Plan:  


1. Setup LangChain (are there any tips or tricks I should keep in mind?)
2. C
onnect to Weaviate (if there's a better VectorDB to use, I'd love recommendations & rationale)
3. Connect to ChatGPT 3.5
 (pls let me know if there's a better model to use)
4. Vectorize my /docs/ directory (I've never done this before, but i
t looks like I need a chunking strategy & embedding model)
5. Create & style a simple modal UI for the chatbot in Airtab
le (again, if there's a better/quicker way to do this, I'm all ears)

I also want to make it not simply a Q&A bot so tha
t if the answer returned is not valid to the user's use case, there is a feedback query used to improve the process, giv
ing the tool either more context or showing where the documentation should be expanded/refined.

Thanks in advance for a
ny guidance and/or pitfalls to avoid :-)
```
---

     
 
all -  [ What evaluation tools/methods do you use? ](https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/) , 2024-05-25-0910
```
Looking to understand what evaluation tools/methods people like and use the most?

[View Poll](https://www.reddit.com/po
ll/1czpsmq)
```
---

     
 
all -  [ What should I use to run LLM locally? ](https://www.reddit.com/r/LocalLLaMA/comments/1czny3r/what_should_i_use_to_run_llm_locally/) , 2024-05-25-0910
```
I want to run this artificial intelligence model locally:

    Meta-Llama-3-8B-Instruct.Q5_K_M.gguf

maybe langchain? İd
k

I would be very grateful if you can help me with the sources where I can access sample codes.

The framework or struc
ture I will use should be suitable for preparing APIs in google cloud. so no ollama.

* **Processor**: Intel Core i9-139
80HX
* **Graphics**: NVIDIA GeForce RTX 4070 (140W)
* **RAM**: 64GB DDR5
```
---

     
 
all -  [ Infosys Certified Applied Generative AI Professional Answers List ](https://www.reddit.com/r/infosysuntold/comments/1cznjs0/infosys_certified_applied_generative_ai/) , 2024-05-25-0910
```
Last week, I successfully completed the Infosys Certified Generative AI Professional - Intermediate exam, and I'm thrill
ed to share my experience and some insights with all of you!

📝 During the exam, I encountered a variety of questions th
at tested my knowledge of Contents Covered on Study Materials. I'd be happy to discuss some of the questions covered and
 share tips for preparing for the exam.

**Question ->** Which of the following are key features of LangChain?

Option 1
 -> Provides a standard interface for chain of prompts.
Option 2 -> Integrations with other NLP tools.
Option 3 -> Flexi
bility and Speed
Option 4 -> None of the Above
Correct Solution -> abc

**Question ->** Which of the following statement
s describes the LangChain Chains correctly ?

Option 1 -> LangChain Chains are specifically designed to use Agents.
Opti
on 2 -> LLMchain is an end-to-end wrapper around multiple individual components.
Option 3 -> LangChain Chains take care 
of storing embedded data and performing vector search too.
Option 4 -> LangChain provides the Chain interface to create 
'chained' applications, so as to enable a sequence of calls to components, which can include other chains.
Correct Solut
ion -> bd

**Question ->** What is data poisoning in with respect to Large Language Models(LLMs)?

Option 1 -> The encry
ption of data in LLM systems
Option 2 -> The accidental deletion of data in LLM system
Option 3 -> The unauthorized acce
ss to data in LLM systems
Option 4 -> The intentional manipulation or contamination of data in LLM systems
Correct Solut
ion -> d

**Question ->** What does 'data toxicity' refer to?

Option 1 -> The quality of data in terms of 
accuracy
Opt
ion 2 -> The presence of valuable information 
in a dataset
Option 3 -> Harmful or inappropriate content present 
in the
 data
Option 4 -> Data that is toxic to the 
environment
Correct Solution -> c

**Question ->** What is a key considerat
ion when 
evaluating the performance of large language models using test data?

Option 1 -> The volume of training data

Option 2 -> The number of data hosts
Option 3 -> Whether test data appears in the model's 
training data
Option 4 -> The
 type of input contamination
Correct Solution -> c

**Question ->** What cybersecurity risk is associated with deploying
 LLMs in applications?

Option 1 -> LLMs have no impact on cybersecurity
Option 2 -> LLMs are immune to adversarial atta
cks
Option 3 -> LLMs might be exploited to generate 
convincing phishing emails or malicious code
Option 4 -> LLMs can o
nly generate text on predefined topics
Correct Solution -> c

**Question ->** How is the Perspective API used for 
toxic
ity classification?

Option 1 -> It trains models using toxic content from various sources
Option 2 -> It assigns a toxi
city score to content 
based on its complexity
Option 3 -> It creates a list of slurs and profanity for 
content filteri
ng
Option 4 -> It gives a machine-learning-based 
score to evaluate content toxicity
Correct Solution -> d

**Question -
>** What distinguishes data poisoning 
from data toxicity?

Option 1 -> Data poisoning involves accidental 
introduction
 of harmful content, while data toxicity is a deliberate attack
Option 2 -> Data poisoning occurs when a model 
produces
 incorrect outcomes, while data toxicity affects data collection methods
Option 3 -> Data poisoning is intentional manip
ulation of training data, whereas data toxicity is about the presence of harmful content
Option 4 -> Data poisoning targ
ets text-based models, while data toxicity primarily affects image recognition models
Correct Solution -> c



If you ha
ve new questions Please feel free to add more questions in comments

for all questions latest pdf contact me @ prepfixad
min[Telegram] or visit https://infosys.prepflix.net

Also follow me on Whatsapp for infosys related updates
https://what
sapp.com/channel/0029VaFbeOvC6Zvd2rsGkb10
```
---

     
 
all -  [ Infosys Certified Generative AI Professional - Intermediate Answers List ](https://www.reddit.com/r/infosysuntold/comments/1czncwv/infosys_certified_generative_ai_professional/) , 2024-05-25-0910
```
Last week, I successfully completed the Infosys Certified Generative AI Professional - Intermediate exam, and I'm thrill
ed to share my experience and some insights with all of you!

📝 During the exam, I encountered a variety of questions th
at tested my knowledge of Contents Covered on Study Materials. I'd be happy to discuss some of the questions covered and
 share tips for preparing for the exam.

**Question ->** Which of the following are key features of LangChain?

Option 1
 -> Provides a standard interface for chain of prompts. Option 2 -> Integrations with other NLP tools. Option 3 -> Flexi
bility and Speed Option 4 -> None of the Above Correct Solution -> abc

**Question ->** Which of the following statement
s describes the LangChain Chains correctly ?

Option 1 -> LangChain Chains are specifically designed to use Agents. Opti
on 2 -> LLMchain is an end-to-end wrapper around multiple individual components. Option 3 -> LangChain Chains take care 
of storing embedded data and performing vector search too. Option 4 -> LangChain provides the Chain interface to create 
'chained' applications, so as to enable a sequence of calls to components, which can include other chains. Correct Solut
ion -> bd

**Question ->** What is data poisoning in with respect to Large Language Models(LLMs)?

Option 1 -> The encry
ption of data in LLM systems Option 2 -> The accidental deletion of data in LLM system Option 3 -> The unauthorized acce
ss to data in LLM systems Option 4 -> The intentional manipulation or contamination of data in LLM systems Correct Solut
ion -> d

**Question ->** What does 'data toxicity' refer to?

Option 1 -> The quality of data in terms of accuracy Opti
on 2 -> The presence of valuable information in a dataset Option 3 -> Harmful or inappropriate content present in the da
ta Option 4 -> Data that is toxic to the environment Correct Solution -> c

**Question ->** What is a key consideration 
when evaluating the performance of large language models using test data?

Option 1 -> The volume of training data Optio
n 2 -> The number of data hosts Option 3 -> Whether test data appears in the model's training data Option 4 -> The type 
of input contamination Correct Solution -> c

**Question ->** What cybersecurity risk is associated with deploying LLMs 
in applications?

Option 1 -> LLMs have no impact on cybersecurity Option 2 -> LLMs are immune to adversarial attacks Op
tion 3 -> LLMs might be exploited to generate convincing phishing emails or malicious code Option 4 -> LLMs can only gen
erate text on predefined topics Correct Solution -> c

**Question ->** How is the Perspective API used for toxicity clas
sification?

Option 1 -> It trains models using toxic content from various sources Option 2 -> It assigns a toxicity sco
re to content based on its complexity Option 3 -> It creates a list of slurs and profanity for content filtering Option 
4 -> It gives a machine-learning-based score to evaluate content toxicity Correct Solution -> d

**Question ->** What di
stinguishes data poisoning from data toxicity?

Option 1 -> Data poisoning involves accidental introduction of harmful c
ontent, while data toxicity is a deliberate attack Option 2 -> Data poisoning occurs when a model produces incorrect out
comes, while data toxicity affects data collection methods Option 3 -> Data poisoning is intentional manipulation of tra
ining data, whereas data toxicity is about the presence of harmful content Option 4 -> Data poisoning targets text-based
 models, while data toxicity primarily affects image recognition models Correct Solution -> c

If you have new questions
 Please feel free to add more questions in comments

for all questions latest pdf contact me @ prepfixadmin\[Telegram\] 
or visit [https://infosys.prepflix.net](https://infosys.prepflix.net)

Also follow me on Whatsapp for infosys related up
dates [https://whatsapp.com/channel/0029VaFbeOvC6Zvd2rsGkb10](https://whatsapp.com/channel/0029VaFbeOvC6Zvd2rsGkb10)
```
---

     
 
all -  [ I'm sure we can implement Function Calling ](https://www.reddit.com/r/Oobabooga/comments/1czmsf3/im_sure_we_can_implement_function_calling/) , 2024-05-25-0910
```
Hi,

I started a PoC some time ago, i try to implement function calling in textgen but I'm stuck because of the format o
f the json returned in response.

Here is my understanding of how function calling works and how I try to make it work i
n textgen:

* the model receives the list of function/tools in the dedicated block in the form of a json
* I inject this
 json with a small explanatory prompt: 'here are the available functions \[...\], use them if necessary. Answer with jso
n syntax '{'name': 'functionName', 'arguments': '{ 'arg1': 'value' } '}'
* then when the model follows the syntax and re
quests a function's result, I want to return the json which will be processed by the backend to evaluate the function an
d return the result.

My implementation almost works but the returned JSON seems to be converted into JSON again and the
 syntax is no longer correct, from what I understand I lost escaped quotes or I have too many quote, I don't know... (la
ngchain crash because of input format)

Here is the code used, I'm not a python developer and I'm not sure what I'm doin
g but I think the idea is there: [https://gist.github.com/gloic/cf3002fc247be7549f893bde69b0a038](https://gist.github.co
m/gloic/cf3002fc247be7549f893bde69b0a038) See lines \~215 for the prompt injection and lines \~390 for the return.

Is t
he approach correct ? Did someone already tried this ?
```
---

     
 
all -  [ # Supabase Auth with SSR: AI Integration and Chat Enhancements 🚀 ](https://www.reddit.com/r/nextjs/comments/1czi13m/supabase_auth_with_ssr_ai_integration_and_chat/) , 2024-05-25-0910
```
I've updated my project that combines Supabase's SSR authentication with Next.js 14 and Material-UI. This release focuse
s on AI integration, chat improvements, and user experience enhancements. The project is designed with simplicity in min
d, making it easy for beginners to understand and build upon.

## Core Technologies

- **Supabase Authentication**: Secu
re your app with Supabase's authentication system.
- **Next.js 14 with Server-Side Rendering**: Improve performance and 
SEO with Next.js 14's SSR.
- **Material-UI Styling**: Create a responsive UI using Material-UI components.

## AI-Powere
d Chat

- **Vercel AI Integration**: Enable intelligent chat interactions with Vercel's AI package.
- **OpenAI's ChatGPT
**: Engage in natural language conversations with ChatGPT.
- **Claude AI Opus**: Expand chat capabilities with Claude AI
 Opus.

## Chat Enhancements

- **Upstash Redis for Chat History**: Store and retrieve chat history efficiently with Ups
tash Redis.
- **Mobile-Friendly Chat Interface**: Enjoy a responsive chat interface on desktop and mobile.
- **Swipeable
 Chat List Drawers**: Navigate chats easily with swipeable drawers on mobile.
- **Abort Signal Handling**: Cancel ongoin
g chat requests gracefully.
- **Partial Chat Save**: Minimize data loss with a robust partial chat save mechanism.

## U
ser Experience Improvements

- **Modern Authentication Pages**: Intuitive design for sign-in, sign-up, and password rese
t pages.
- **React Server Components**: Optimize data fetching and rendering for faster load times.
- **Markdown Support
**: Enhanced readability with Perplexity and code highlighting.
- **Server Actions for Mutations**: Perform server-side 
mutations securely using server actions.

## Get Started

Explore the project and code at [SupabaseAuthWithSSR](https://
github.com/electriccodeguy/supabaseauthwithssr) on GitHub. Provide feedback, suggest improvements, or contribute.


##Co
ming next: 
**Implement a similar Chat functionality using the new ai/rsc from Vercel**: This includes usable chat histo
ry, tools and functions implementation with live response UI.


**EDIT**: In the latest release (v1.3.0), I've made some
 changes to streamline the AI integration and improve performance:

1. **Migrated from Langchain to Vercel AI SDK**: The
 Vercel AI SDK provides a more straightforward approach to integrating AI models, reducing complexity in the codebase. T
his change simplifies the code and makes it easier to maintain and extend.

2. **Removed Langchain Dependency**: As a re
sult of the migration to Vercel AI SDK, the Langchain dependency has been removed. This further simplifies the codebase 
and reduces unnecessary complexity. For reference, an example of the previous Langchain integration can be found in the 
`exampleWithLangchain.md` file in the Root folder.

3. **Memoized Message Component**: To optimize performance, especial
ly in scenarios with large amounts of messages and frequent updates (such as streaming), I've implemented memoization fo
r the `Message` component using `React.memo`. This prevents unnecessary re-renders of the component when its props remai
n unchanged, improving overall performance.
```
---

     
 
all -  [ AI Agent for monitoring Snowflake Costs! ](https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/) , 2024-05-25-0910
```
Hey folks! My team recently worked on building this bot to help orgs monitor and even forecast costs on the Snowflake Da
ta Warehouse. We used LangChain, Streamlit, Snowflake Arctic + Cortex and GPT 4 Turbo for this. 

We just open sourced t
his Agent and even wrote a guide on how to create one yourself, check it out here: [https://medium.com/snowflake/crystal
costs-building-an-ai-agent-for-cost-monitoring-on-snowflake-c9d49645f5c4](https://medium.com/snowflake/crystalcosts-buil
ding-an-ai-agent-for-cost-monitoring-on-snowflake-c9d49645f5c4)

Would love to get inputs on this! 
```
---

     
 
all -  [ # Supabase Auth with SSR: AI Integration and Chat Enhancements 🚀 ](https://www.reddit.com/r/Supabase/comments/1czeylu/supabase_auth_with_ssr_ai_integration_and_chat/) , 2024-05-25-0910
```
I've updated my project that combines Supabase's SSR authentication with Next.js 14 and Material-UI. This release focuse
s on AI integration, chat improvements, and user experience enhancements. The project is designed with simplicity in min
d, making it easy for beginners to understand and build upon.

## Core Technologies

- **Supabase Authentication**: Secu
re your app with Supabase's authentication system.
- **Next.js 14 with Server-Side Rendering**: Improve performance and 
SEO with Next.js 14's SSR.
- **Material-UI Styling**: Create a responsive UI using Material-UI components.

## AI-Powere
d Chat

- **Vercel AI Integration**: Enable intelligent chat interactions with Vercel's AI package.
- **OpenAI's ChatGPT
**: Engage in natural language conversations with ChatGPT.
- **Claude AI Opus**: Expand chat capabilities with Claude AI
 Opus.

## Chat Enhancements

- **Upstash Redis for Chat History**: Store and retrieve chat history efficiently with Ups
tash Redis.
- **Mobile-Friendly Chat Interface**: Enjoy a responsive chat interface on desktop and mobile.
- **Swipeable
 Chat List Drawers**: Navigate chats easily with swipeable drawers on mobile.
- **Abort Signal Handling**: Cancel ongoin
g chat requests gracefully.
- **Partial Chat Save**: Minimize data loss with a robust partial chat save mechanism.

## U
ser Experience Improvements

- **Modern Authentication Pages**: Intuitive design for sign-in, sign-up, and password rese
t pages.
- **React Server Components**: Optimize data fetching and rendering for faster load times.
- **Markdown Support
**: Enhanced readability with Perplexity and code highlighting.
- **Server Actions for Mutations**: Perform server-side 
mutations securely using server actions.

## Get Started

Explore the project and code at [SupabaseAuthWithSSR](https://
github.com/electriccodeguy/supabaseauthwithssr) on GitHub. Provide feedback, suggest improvements, or contribute.


**ED
IT**: In the latest release (v1.3.0), I've made some changes to streamline the AI integration and improve performance:


1. **Migrated from Langchain to Vercel AI SDK**: The Vercel AI SDK provides a more straightforward approach to integrati
ng AI models, reducing complexity in the codebase. This change simplifies the code and makes it easier to maintain and e
xtend.

2. **Removed Langchain Dependency**: As a result of the migration to Vercel AI SDK, the Langchain dependency has
 been removed. This further simplifies the codebase and reduces unnecessary complexity. For reference, an example of the
 previous Langchain integration can be found in the `exampleWithLangchain.md` file in the Root folder.

3. **Memoized Me
ssage Component**: To optimize performance, especially in scenarios with large amounts of messages and frequent updates 
(such as streaming), I've implemented memoization for the `Message` component using `React.memo`. This prevents unnecess
ary re-renders of the component when its props remain unchanged, improving overall performance.
```
---

     
 
all -  [ Internet search for ai agent only returning a short snippet  ](https://www.reddit.com/r/AI_Agents/comments/1czdxnh/internet_search_for_ai_agent_only_returning_a/) , 2024-05-25-0910
```
Hey I gave the ai agent which I made on crewai the ability to search internet using serper api but it is only giving a s
hort snippet while I want the full content from the websites , I think I might need a web scrapper like firecrawl but ho
w do I make a custom tool for that like do I tell the model to store the urls in a list but how can it store In a list a
nd can a tool made with langchain  work with crewai , plus if you can suggest a video that gives a tutorial for making t
ools for beginners that helped you in making tools
```
---

     
 
all -  [ LangGraph Essentials: Create Your First Graph with Ease! ](https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/) , 2024-05-25-0910
```
[https://youtu.be/gflsu\_6R\_8g](https://youtu.be/gflsu_6R_8g)
```
---

     
 
all -  [ How I got the DDGS, Exa, Serper, SerAPI searches working. ](https://www.reddit.com/r/crewai/comments/1czb1qz/how_i_got_the_ddgs_exa_serper_serapi_searches/) , 2024-05-25-0910
```
No one has been able to explain why the search tools DDGS, Exa, and SerpAPI never worked, and Serper rarely worked. Whil
e suggestions to change my LLM and bump my tokens were good, that only made things worse. I looked into it more, and thi
s is what I have now, which works. I am posting this in case others have the same problem.

I replaced all the CrewAI se
arch classes with longchain, like this...

    from crewai_tools import Tool
    
    gput('searcher','DDG') # gput,gget
 writes and reads sysem env variables, a 'dotenv' wrapper.
    
    search_name = 'Search'
    search_desc = 'useful for
 when you need to answer questions about current events'
    
    if gget('searcher') == 'EXA':
        print(f'Using Se
arch API: EXA')
        from src.news.lib.exa_search_tool import ExaSearchToolFull
        search_tool = Tool(name=searc
h_name, description=search_desc, func=ExaSearchToolFull._exa().search)
    
    if gget('searcher') == 'DDG':  # still g
et ratelimit error
        from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
        search = DuckDuc
kGoSearchAPIWrapper()
        search.region='us-en'
        search.safesearch='off'
        search.backend='html' # back
end: api, html, lite.
        # search.max_results=1 # even with only 1 query it fails with RateLimit :/
        search_
tool = Tool(name=search_name, description=search_desc, func=search.run, query='Red Heffer')
    
    if gget('searcher')
 == 'SER':
        print(f'Using Search API: SER')
        from langchain_community.utilities import GoogleSerperAPIWrap
per
        search = GoogleSerperAPIWrapper(params={'engine': 'bing','gl': 'us','hl': 'en'})
        search_tool = Tool(
name=search_name, description=search_desc, func=search.run)
    
    if gget('searcher') == 'SAP': # defaults to SAP
   
     print(f'Using Search API: SAP')
        print('>>>',gget('SERPAPI_API_KEY'))
        from langchain_community.utili
ties import SerpAPIWrapper
        search = SerpAPIWrapper(params={'engine': 'bing','gl': 'us','hl': 'en'})
        from
 crewai_tools import Tool
        search_tool = Tool(name=search_name, description=search_desc,func=search.run)

This wo
rks great for all but DDGS.  To get that to work, I commented out

                # if _is_500_in_url(str(resp.url)) or
 resp.status_code == 202:
                #     raise DuckDuckGoSearchException('Ratelimit')

in `site-packages/duckduck
go_search/duckduckgo_search_async.py` (around lines 50-60).

Just below  added a `sleep(1)` just in case the rate limit 
was actually real (which it appears not to be).

                if resp.status_code == 200:
                    import 
time
                    time.sleep(1)
                    return resp_content

And  added the parameter `search.backend
='html'`  (see the code above), which made a huge difference.  When is the default, `search.backend='api'` it kept repor
ting  `No good result on DuckDuckGo found`, but with `'html'` it found and returned content.

Here's the standard Exa cl
ass being used.

    from crewai_tools import tool
    from exa_py import Exa
    from src.news.lib.utils import gget # 
this is mine.  Can be replaced with os.environ['VAR']
    import dotenv
    dotenv.load_dotenv(dotenv_path='/src/crewai/
.env')
    class ExaSearchToolFull:
        @tool
        def search(query: str):
            '''Search for a webpage ba
sed on the query.'''
            return ExaSearchToolFull._exa().search(
                f'{query}', use_autoprompt=True
, num_results=3
            )
    
        @tool
        def find_similar(url: str):
            '''Search for webpages 
similar to a given URL.
            The url passed in should be a URL returned from `search`.
            '''
          
  return ExaSearchToolFull._exa().find_similar(url, num_results=3)
    
        @tool
        def get_contents(ids: str)
:
            '''Get the contents of a webpage.
            The ids must be passed in as a list, a list of ids returned 
from `search`.
            '''
    
            print('ids from param:', ids)
    
            ids = eval(ids)
         
   print('eval ids:', ids)
    
            contents = str(ExaSearchToolFull._exa().get_contents(ids))
            print
(contents)
            contents = contents.split('URL:')
            contents = [content[:1000] for content in contents]

            return '\n\n'.join(contents)
    
        def tools():
            return [
                ExaSearchToolFu
ll.search,
                ExaSearchToolFull.find_similar,
                ExaSearchToolFull.get_contents,
            ]

        def _exa():
            return Exa(api_key=gget('EXA_API_KEY'))
```
---

     
 
all -  [ Vector Embedding and RAG Platforms ](https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/) , 2024-05-25-0910
```
Hey all, looking to learn :)

What are some good robust platforms that I can use to vectorize various types of data and 
implement RAG to generate results from LLMs? I have a few projects I am working on each with their own types of data and
 models to use. Wondering if there are any all-in-one platforms you know of that would limit my time spent learning new 
technologies that will have to be updated as the methods progress. Thanks!
```
---

     
 
all -  [ Caching in LLM Apps ](https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/) , 2024-05-25-0910
```
Which is your favourite caching technique in LLM Applications. 
Is it in memory or something else.
Which caching integra
tion you like the most and why for a scalable and reliable application.
```
---

     
 
all -  [ ReAct Agent with 0.3 instruct ](https://www.reddit.com/r/MistralAI/comments/1cz2gbb/react_agent_with_03_instruct/) , 2024-05-25-0910
```
Hi upgraded my Langchain ReAct agent with the new 0.3 model that dropped yesterday and wow!

it's faster, more determini
stic, and tool calling is much better.

Haven't tried out the new functions yet since I have no idea how to implement th
is in my framework.

Any tips?

Also, if you have a good prompt to ensure that the agent visits a website is the link is
 provided would be great.
```
---

     
 
all -  [ Is there a better way to get this json into my vectordb? (ollama, chromadb, gp4allembeddings) ](https://www.reddit.com/r/LocalLLaMA/comments/1cz1e2f/is_there_a_better_way_to_get_this_json_into_my/) , 2024-05-25-0910
```
I've been working on a simple chatbot, it responds to inquiries in intercom and in telegram. It makes a database of info
rmation to pull from based on current support articles in Intercom. It worked pretty well with 150 articles, but as I've
 added more and its up to almost 400 it seems to completely miss easy questions now, and I feel like i'm not ingesting a
ll that information in the most efficient way possible.

 It uses Ollama/Llama3 for the model, i have a custom modelfile
 that  looks like:
> FROM llama3
> 
> PARAMETER temperature 0.3
> 
> SYSTEM You are a helpful AI assistant for the 'X' p
latform. Your role is to provide detailed, accurate answers to user questions based on the information in your knowledge
 base, with the goal of assisting users without requiring a human response when possible. If a question can have multipl
e answers depending on the situation, provide guidance on the different options. When giving instructions, be as specifi
c as possible. Never answer questions that are remotely off-topic. Just let them know you can’t help with that.

It uses
 gpt4allembeddings/langchain for embedding and chromadb for the database.

I have a pre-prompt implemented that reads li
ke:
> Answer the question based on the provided context. Do not include introductory phrases. If the question is unclear
 or unrelated to the context, simply state 'I apologize, I can't help with your query, let me get a team member to assis
t.' Do not provide additional explanations.

The json that intercom is providing looks like this:
>         [
>         
  {
>             'id': '123',
>             'type': 'article',
>             'workspace_id': '123',
>             'pare
nt_id': null,
>             'parent_type': null,
>             'parent_ids': [],
>             'title': 'Title',
>      
       'description': 'Description',
>             'body': 'Body',
>             'author_id': 123,
>             'state'
: 'draft',
>             'created_at': 171,
>             'updated_at': 171,
>             'url': null
>           },
> 
          {
>             'id': '234',
>             'type': 'article',
>             'workspace_id': '234',
>          
   'parent_id': null,
>             'parent_type': null,
>             'parent_ids': [],
>             'title': 'Title',

>             'description': 'Description',
>             'body': 'Body',
>             'author_id': 123,
>            
 'state': 'draft',
>             'created_at': 171,
>             'updated_at': 171,
>             'url': null
>        
   },
>     
Here is my script please dont judge i'm an absolute hobbyist and this is my first time trying to dive into 
AI stuff:

    import json
    import logging
    import os
    import aiohttp
    import asyncio
    from dotenv import
 load_dotenv
    from quart import Quart, jsonify, request
    from telethon import TelegramClient, events
    from tele
thon.errors import RPCError, ChatAdminRequiredError, ChannelPrivateError
    from telethon.tl.types import PeerChannel
 
   from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import GPT4AllEmbeddings

    from langchain_core.prompts import PromptTemplate
    from langchain_community.llms import Ollama
    from langchain
.callbacks.manager import CallbackManager
    from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHa
ndler
    from langchain.chains import RetrievalQA
    from langchain.docstore.document import Document
    from hyperco
rn.config import Config
    from hypercorn.asyncio import serve
    import time
    
    os.makedirs('logs', exist_ok=Tr
ue)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', handlers=[
        logging.FileHandl
er('logs/app.log'),
        logging.StreamHandler()
    ])
    
    start_time = time.time()
    
    # .env
    load_do
tenv()
    api_id = os.getenv('API_ID')
    api_hash = os.getenv('API_HASH')
    bot_token = os.getenv('BOT_TOKEN')
    
intercom_token = os.getenv('INTERCOM_TOKEN')
    chat_id = int(os.getenv('CHAT_ID'))
    qa_chain_prompt_template = os.g
etenv('QA_CHAIN_PROMPT_TEMPLATE')
    
    app = Quart(__name__)
    
    client = TelegramClient('logs/tg_chat', api_id
, api_hash)
    
    vectorstore = None
    qa_chain = None
    
    QA_CHAIN_PROMPT = PromptTemplate(
        input_var
iables=['context', 'question'],
        template=qa_chain_prompt_template,
    )
    
    llm = Ollama(model='llama3-tem
p03', callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
    
    async def fetch_all_pages():
      
  url = 'https://api.intercom.io/articles'
        headers = {
            'Authorization': f'Bearer {intercom_token}',

            'Accept': 'application/json'
        }
        all_documents = []
        all_data = []
        async with a
iohttp.ClientSession() as session:
            while url:
                async with session.get(url, headers=headers) a
s response:
                    if response.status != 200:
                        logging.error(f'Failed to fetch data:
 {response.status}')
                        break
                    data = await response.json()
                    
all_data.extend(data.get('data', []))
                    if 'data' in data and data['data']:
                        do
cuments = [Document(page_content=article['body']) for article in data['data'] if article['body'].strip()]
              
          all_documents.extend(documents)
                    url = data.get('pages', {}).get('next', None)
        with
 open('info.json', 'w') as f:
            json.dump(all_data, f, indent=2)
        logging.info(f'Total records received
: {len(all_data)}')
        return all_documents
    
    async def rebuild_vectorstore():
        global documents, vec
torstore, qa_chain
        try:
            documents = await fetch_all_pages()
            if documents:
              
  vectorstore = Chroma.from_documents(documents=documents, embedding=GPT4AllEmbeddings())
                logging.info('
Documents processed and vector store rebuilt.')
                qa_chain = RetrievalQA.from_chain_type(
                
    llm,
                    retriever=vectorstore.as_retriever(),
                    chain_type_kwargs={'prompt': QA_C
HAIN_PROMPT},
                )
            else:
                logging.error('No valid documents with non-empty body 
found.')
        except Exception as e:
            logging.error(f'Error rebuilding vector store: {str(e)}')
    
    a
sync def handle_query(query):
        if qa_chain is None:
            logging.error('QA chain is not initialized.')
   
         return {'response': 'Initialization error: Vector store not available. Check log for details.', 'time_taken': 0
}
    
        start_time = time.time()
        try:
            result = qa_chain.invoke(query)
        except Exceptio
n as e:
            logging.error(f'Error during query handling: {str(e)}')
            return {'response': 'An error oc
curred while processing the query.', 'time_taken': 0}
    
        end_time = time.time()
        time_taken = end_time 
- start_time
    
        logging.info(f'Query result: {result}')
    
        if isinstance(result, dict):
            
result = result.get('result', 'No result field found in response.')
        elif isinstance(result, str):
            re
sult = result.strip()
        else:
            result = str(result).strip()
    
        if not result:
            res
ult = 'I apologize, but I don't have enough information to provide a helpful answer.'
    
        return {'response': r
esult, 'time_taken': time_taken}
    
    @app.route('/intercom', methods=['POST'])
    async def intercom_handler():
  
      data = await request.get_json()
        query = data.get('body')
        if query:
            result = await hand
le_query(query)
            response = result['response']
            time_taken = result['time_taken']
            retu
rn jsonify({'response': response, 'time_taken': time_taken}), 200
        else:
            logging.error('No query prov
ided in the request')
            return jsonify({'error': 'No query provided'}), 400
    
    @app.route('/rebuild_vect
orstore', methods=['POST'])
    async def rebuild_vectorstore_handler():
        await rebuild_vectorstore()
        ret
urn jsonify({'message': 'Vector store rebuilt'}), 200
    
    @client.on(events.NewMessage(pattern=r'^\.x (.+)', func=l
ambda e: e.text.lower().startswith('.x ')))
    async def answer_query(event):
        query = event.pattern_match.group
(1)
        logging.info(f'Received query: {query}')
        result = await handle_query(query)
        response = resul
t['response']
        time_taken = result['time_taken']
        await event.respond(f'```{response}```\n**Time to genera
te: {time_taken:.2f} seconds**', parse_mode='Markdown')
    
    @client.on(events.NewMessage(pattern='/rebuild'))
    a
sync def rebuild_vectorstore_command(event):
        logging.info('Received /rebuild command. Rebuilding the vector stor
e...')
        await event.respond('Rebuilding database...')
        await rebuild_vectorstore()
        await event.res
pond('Database rebuilt.')
    
    async def run_server():
        global start_time
        config = Config()
        c
onfig.bind = ['0.0.0.0:5001']
        
        async def custom_serve():
            end_time = time.time()
            
time_to_boot = end_time - start_time
            await send_message(chat_id, f'<span style='color:red'>Bot Online, Time 
to boot: {time_to_boot:.2f} seconds</span>')
            await serve(app, config)
    
        await custom_serve()
    

    async def send_message(chat_id, message):
        try:
            entity = await client.get_entity(PeerChannel(cha
t_id))
            await client.send_message(entity, message, parse_mode='html')
        except ChatAdminRequiredError:

            logging.error(f'Failed to send message to {chat_id}: Bot lacks admin rights.')
        except ChannelPrivate
Error:
            logging.error(f'Failed to send message to {chat_id}: Channel is private.')
        except RPCError as
 e:
            logging.error(f'Failed to send message to {chat_id}: {str(e)}')
    
    async def start():
        try:

            await client.start(bot_token=bot_token)
            logging.info('Telegram client connected.')
            
await rebuild_vectorstore()
            await run_server()
        except Exception as e:
            logging.error(f'Er
ror occurred: {str(e)}. Retrying in 5 seconds...')
            await asyncio.sleep(5)
            await start()
    
   
 async def main():
        try:
            await start()
        except KeyboardInterrupt:
            logging.info('Sc
ript interrupted by user.')
            await send_message(chat_id, '<span style='color:red'>Shutting Down</span>')
    
    finally:
            logging.info('Shutting down...')
            await client.disconnect()
            logging.info
('Client disconnected.')
            pending = [task for task in asyncio.all_tasks() if not task.done() and task is not 
asyncio.current_task()]
            for task in pending:
                task.cancel()
            await asyncio.gather(
*pending, return_exceptions=True)
            loop.stop()
            loop.close()
            logging.info('Script stop
ped.')
    
    if __name__ == '__main__':
        try:
            loop = asyncio.get_event_loop()
            loop.run
_until_complete(main())
        except RuntimeError as e:
            logging.error(f'Runtime error: {str(e)}')
        
finally:
            if not loop.is_closed():
                loop.close()


I'm not sure if i am embedding the json cor
rectly, i thought it would be straightforward in json format but the bad outputs make me second guess whatever im doing,
 really open to whatever, would love to learn what im missing here
```
---

     
 
all -  [ why two different kinds of messages? ](https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/) , 2024-05-25-0910
```
langchain\_core.messages.human.HumanMessage

langchain.schema.messages.HumanMessage

I got unsupported HumanMessage erro
r when using langchain and found out two kinds of messages. Why?
```
---

     
 
all -  [ [11 YOE] Unable To Get Any Tech Interviews With This Resume, What Am I Doing Wrong? ](https://www.reddit.com/r/resumes/comments/1cyyw3b/11_yoe_unable_to_get_any_tech_interviews_with/) , 2024-05-25-0910
```
https://preview.redd.it/gt05zrf5r72d1.png?width=5100&format=png&auto=webp&s=0745fdbb4f38cd6b4aa6c0104bb949b857496d8a

ht
tps://preview.redd.it/n8c7z5i5r72d1.png?width=5100&format=png&auto=webp&s=71660c9f380f4eb752fc854cbf66d0b6a5082472

http
s://preview.redd.it/zhfi8uf5r72d1.png?width=5100&format=png&auto=webp&s=769140b50f0755620be916931ba43199240f576b

I am o
pen to roles in AI/ML, Backend Full stack, SWE and Product roles, but cant seem to get interview calls, what am I doing 
wrong? I have been suggested to include the exact tech work I did to avoid looking inexperienced, and hence ended up add
ing a lot of tech jargon, could it be that? Please suggest me fixes. What am I doing wrong?  

```
---

     
 
all -  [ [11 YOE] I have Tech and Tech management experience in startups, but cant get an interview. ](https://www.reddit.com/r/EngineeringResumes/comments/1cyyiyu/11_yoe_i_have_tech_and_tech_management_experience/) , 2024-05-25-0910
```
I have been looking for AI/ML, Backend Full stack, SWE and Product roles but cant seem to get interview calls, what am I
 doing wrong? I have been suggested to include the exact tech work I did to avoid looking inexperienced, and hence ended
 up adding a lot of tech jargon. Please suggest me fixes.

https://preview.redd.it/n5cvga0go72d1.png?width=5100&format=p
ng&auto=webp&s=ee684cb4d88cc4569ad8c81fa643a928e3d80e21

https://preview.redd.it/4qkfva0go72d1.png?width=5100&format=png
&auto=webp&s=9c99b53b0164cfa1577f0d69544cb3c4058535d2

https://preview.redd.it/f7fy850go72d1.png?width=5100&format=png&a
uto=webp&s=69e881cfc736d8856a65fe6160c5ee6f23821ba7


```
---

     
 
all -  [ ParentDocumentRetriever.add_document function with 'ids' parameter - can't fix an error ](https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/) , 2024-05-25-0910
```
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.retrievers import ParentDocumentRetriever
    f
rom langchain.schema import Document
    from langchain.storage import InMemoryStore
    from langchain.text_splitter im
port RecursiveCharacterTextSplitter
    from langchain.vectorstores.chroma import Chroma
    
    vectorstore = Chroma(

        collection_name='full_documents',
        embedding_function=OpenAIEmbeddings()
    )
    store = InMemoryStore(
)
    
    docs = [Document(page_content=txt, metadata={'id': id}) for txt, id in [('aaaaaa', 1), ('bbbbbb', 2)]]
    Pa
rentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key='id',
        parent_spli
tter=RecursiveCharacterTextSplitter(
            chunk_size = 2,
            chunk_overlap  = 0,
            length_func
tion = len,
            add_start_index = True,
        ),
        child_splitter=RecursiveCharacterTextSplitter(
      
      chunk_size = 1,
            chunk_overlap  = 0,
            length_function = len,
            add_start_index = T
rue,
        ),
    ).add_documents(docs,ids=[doc.metadata['id'] for doc in docs])from langchain.embeddings import OpenA
IEmbeddings
    from langchain.retrievers import ParentDocumentRetriever
    from langchain.schema import Document
    f
rom langchain.storage import InMemoryStore
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    fr
om langchain.vectorstores.chroma import Chroma
    
    vectorstore = Chroma(
        collection_name='full_documents',

        embedding_function=OpenAIEmbeddings()
    )
    store = InMemoryStore()
    
    docs = [Document(page_content=t
xt, metadata={'id': id}) for txt, id in [('aaaaaa', 1), ('bbbbbb', 2)]]
    ParentDocumentRetriever(
        vectorstore
=vectorstore,
        docstore=store,
        id_key='id',
        parent_splitter=RecursiveCharacterTextSplitter(
     
       chunk_size = 2,
            chunk_overlap  = 0,
            length_function = len,
            add_start_index = 
True,
        ),
        child_splitter=RecursiveCharacterTextSplitter(
            chunk_size = 1,
            chunk_ov
erlap  = 0,
            length_function = len,
            add_start_index = True,
        ),
    ).add_documents(docs,i
ds=[doc.metadata['id'] for doc in docs])

The error :

    ValueError: Got uneven list of documents and ids. If `ids` is
 provided, should be same length as `documents`.

     

  
The size of documents list and ids list are nevertheless equ
al, i don't understand this error  
  

```
---

     
 
all -  [ How can I properly use tools within a chain in LangGraph? ](https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/) , 2024-05-25-0910
```
Hey guys! I'm trying to develop a chatbot that offers video games recommendations based on user input.  
Problem is, I'm
 stuck at the chain which objective is to use Tavily API tool to search for video games' titles that fit the user's crit
eria.

Here's what I've tried:

    # Game Title Search
    prompt = PromptTemplate(
        template='''You are part of
 a chatbot that provides personalized video game recommendations based on user preferences. \n
        Your task is to s
earch for the top 5 video games that match the user query. \n
        Only return the titles of the games. \n\n
    
   
     User Query: {query}''',
        input_variables=['query'],
    )
    
    game_title_search = prompt | llm.bind_too
ls(tools)
    
    QUERY = '''What games are similar to Skyrim?'''
    
    result = game_title_search.invoke({'query': 
QUERY})
    print(result)

Problem is, when I print result it gives me this instead of the response that I'm expecting (
which are the video games' titles:

`content='' additional_kwargs={'tool_calls': [{'id': 'call_xJGybVhCtBAYGHyNkEE04U1c'
, 'function': {'arguments': '{'query':'games similar to Skyrim'}', 'name': 'tavily_search_results_json'}, 'type': 'funct
ion'}]} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 141, 'total_tokens': 162}, 'model_n
ame': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-c7c3309
4-2173-43d8-9e9a-319c80265f57-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'games similar to 
Skyrim'}, 'id': 'call_xJGybVhCtBAYGHyNkEE04U1c'}]`

How can I solve this and use the tools alongside the ChatModel and t
he PromptTemplate to achieve what I want?
```
---

     
 
all -  [ Does unifying the infrastructure code and the application code in a single interface contribute to t ](https://www.reddit.com/r/devops/comments/1cyru9j/does_unifying_the_infrastructure_code_and_the/) , 2024-05-25-0910
```
Hey everyone! I'm currently developing a tool called [Pluto](https://github.com/pluto-lang/pluto), which offers a unifie
d programming interface that enables developers to seamlessly integrate infrastructure code with application code. I bel
ieve this approach could revolutionize the way we develop cloud-native applications, making the process more efficient a
nd streamlined. I'm curious to hear your thoughts on this matter. Do you think this is a step in the right direction for
 cloud-native app development? Your input would be greatly appreciated!

For a real-world example of how Pluto can be ut
ilized, I recommend checking out this article: [How to Bridge the Last Mile in LangChain Application Development](https:
//pluto-lang.vercel.app/blogs/240515-develop-ai-app-in-new-paradigm). It demonstrates how Pluto can be used to simplify 
the development process of a LangChain app.

Thanks in advance for your feedback!
```
---

     
 
all -  [ Does unifying the infrastructure code and the application code in a single interface contribute to t ](https://www.reddit.com/r/u_Zheng_SJ/comments/1cyrrfh/does_unifying_the_infrastructure_code_and_the/) , 2024-05-25-0910
```
Hey everyone! I'm currently developing a tool called [Pluto](https://github.com/pluto-lang/pluto), which offers a unifie
d programming interface that enables developers to seamlessly integrate infrastructure code with application code. I bel
ieve this approach could revolutionize the way we develop cloud-native applications, making the process more efficient a
nd streamlined. I'm curious to hear your thoughts on this matter. Do you think this is a step in the right direction for
 cloud-native app development? Your input would be greatly appreciated!

For a real-world example of how Pluto can be ut
ilized, I recommend checking out this article: [How to Bridge the Last Mile in LangChain Application Development](https:
//blog.stackademic.com/how-to-bridge-the-last-mile-in-langchain-application-development-e4734ca07169). It demonstrates h
ow Pluto can be used to simplify the development process of a LangChain app.

Thanks in advance for your feedback!
```
---

     
 
all -  [ Parsing solutions for PDF ](https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/) , 2024-05-25-0910
```
Been struggling with parsing pdf with complex layout, table, imagines.

The option that I am testing is multi modal vect
or, based on unstructured library for pdf extraction. 

I recently discovered llamaparse proprietary solution. Excluding
 the facts that isn't open source and limited for commercial use. Would it perform better then the unstructured approach
 for parsing?


```
---

     
 
all -  [ What are some ways to enforce structured outputs from LLMs not in your control beyond basic promptin ](https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/) , 2024-05-25-0910
```
Hi!

I'm currently facing this issue of trying to get an XML out of a model and I use that XML structure to extract and 
format a document that I generate but, no matter how I prompt the model, or even, using different calls generate the ans
wer and to structure it into the required format, sometimes going through different stages of structuring (like first ju
st bullet points, then try to only put stuff into a basic XML format before going into nested.), it still sometimes gene
rate an answer that's not structured.

I included retries on those calls hoping that the model in its second generation 
would structure the output correctly but often this doesn't work.

I was wondering how the community handles this issue 
or if there are creative ways you stumbled upon that deal well with it.

I have seen in the past some libraries that for
ce the generation in some kind of way like the grammars from llama-cpp, or outlines. Maybe there was guidance as well. B
ut I don't think they work with LLMs from providers. I'm facing this problem with mistral-large.


```
---

     
 
all -  [ Need help and knowledge in deployment ](https://www.reddit.com/r/AWS_cloud/comments/1cyp7c6/need_help_and_knowledge_in_deployment/) , 2024-05-25-0910
```
Hi all,

New user of aws here.

I have a python script of an LLM model using bedrock, langchain libraries and streamlit 
for frontend along with the requirements.txt file. I have saved it jnto a repository in CodeCommit and I am aware of two
 different ways to deploy it.

1). The CI/CD pipeline format using the respective services CodeCommit, CodeBuild, CodeDe
ploy, CodePipeline etc. but the problem is it is more suitable for a node.js or proper website project with multiple fil
es instead of a single python script. I found the portion of creating an appspec.yml or buildspec.yml file very complex 
for a single python script and I was not able to find any tutorial on how to do it as well.

2). The 2nd method is to wr
ite some commands on the terminal of an amazon linux machine on the EC2 server instance, I have successfully deployed a 
model using these method on the provided public IP but the problem is if I commit changes in the repository, it does not
 reflect in the EC2 instance even after rebooting the instance. the only way to make the changes reflect is to terminate
 the instance and create a new one, which is very time-consuming.

I would like to know if anyone can guide me in using 
the first method for a single python script or can help in having the changes reflect in the ec2 server as that is what 
will make ec2 method of deployment a CI/CD method.
```
---

     
 
all -  [ Need help in deployment on AWS ](https://www.reddit.com/r/aws/comments/1cyp6ce/need_help_in_deployment_on_aws/) , 2024-05-25-0910
```
Hi all,

New user of aws here.

I have a python script of an LLM model using bedrock, langchain libraries and streamlit 
for frontend along with the requirements.txt file. I have saved it jnto a repository in CodeCommit and I am aware of two
 different ways to deploy it.

1). The CI/CD pipeline format using the respective services CodeCommit, CodeBuild, CodeDe
ploy, CodePipeline etc.  but the problem is it is more suitable for a node.js or proper website project with multiple fi
les instead of a single python script. I found the portion of creating an appspec.yml or buildspec.yml file very complex
 for a single python script and I was not able to find any tutorial on how to do it as well.

2).  The 2nd method is to 
write some commands on the terminal of an amazon linux machine on the EC2 server instance, I have successfully deployed 
a model using these method on the provided public IP but the problem is if I commit changes in the repository, it does n
ot reflect in the EC2 instance even after rebooting the instance. the only way to make the changes reflect is to termina
te the instance  and create a new one, which is very time-consuming.

I would like to know if anyone can guide me in usi
ng the first method for a single python script or can help in having the  changes reflect in the ec2 server as that is w
hat will make ec2 method of deployment a CI/CD method.
```
---

     
 
all -  [ How can I get the csv_agent to return the complete results from its Observation? ](https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/) , 2024-05-25-0910
```
I'm using create\_csv\_agent to get a csv parsing agent to analyze and return a list of items that meets the criteria. T
he agent handles the questions fine and I can see the correct results printed out in its Observations. However it doesn'
t include the list of items in the final output. How can I get around this?
```
---

     
 
all -  [ I'm new to this and I need help for my RAG ](https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/) , 2024-05-25-0910
```
Hey I am doing an internship and my boss asked me to build a RAG that can read financial documents (pdf) and create a LL
M that, with a query, answers based on these documents. I was using BGE as the embedding model and ollama with llama2 fo
r the LLM. My problem is that I was using google collab with the free GPU but once it reaches the limit, I can't keep cr
eating the embeddings. Is there any FREE solution for this? Thank you and sorry for my inexperience.
```
---

     
 
all -  [ For those struggling with API function calls ](https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/) , 2024-05-25-0910
```
What worked for me was to create small modular functions out of one big function with different parameters. I broke down
 my API for the bot to use into smaller, modular endpoints with maximum of two parameters each. 

I have been able to us
e gpt-3.5 to get satisfactory outputs without fails. 
```
---

     
 
all -  [ Help Needed: To find total number of results ?  ](https://www.reddit.com/r/LangChain/comments/1cylb81/help_needed_to_find_total_number_of_results/) , 2024-05-25-0910
```
Hi Guys,  
I am exploring LangChain, and stuck at one issue, Needed your help!!

I am trying to get total number of empt
y parking spots available in csv, but I see we can only define k value in retriever,

Is there a way to ignore k value a
nd give full matching result ?

Here is my code: [Langchain/apps/find\_parking/parking\_spots.ipynb at main · DastanIqba
l/Langchain · GitHub](https://github.com/DastanIqbal/Langchain/blob/main/apps/find_parking/parking_spots.ipynb)

Thanks
```
---

     
 
all -  [ Simple choice selection ](https://www.reddit.com/r/LangChain/comments/1cykpbu/simple_choice_selection/) , 2024-05-25-0910
```
Looking to return only a specific choice with langchain using an ollama model and couldn't get the langchoice example to
 work. 
For example, How would I classify a bank transaction description if the only possible classification choices to 
choose from are: taxes, transfer, or payment?
```
---

     
 
all -  [ Best stack for RAG? ](https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/) , 2024-05-25-0910
```
We’re building a RAG based application which works on internal documents. We’re experimenting with OpenAI for embedding 
models, Milvus (Zilliz cloud) for embedding storage and similarity search, Postgres for all other data and AWS for hosti
ng.

Our main priorities are:
- being fast to market
- above average performance
- costs that don’t scale exponentially 
with scale
- being scalable so we don’t have to refactor all of the code, if we achieve any scale
```
---

     
 
all -  [ What features do you want in the local AI systems? ](https://github.com/yukiarimo/yuna-ai/issues/91) , 2024-05-25-0910
```
Hello guys! I’m a creator of Yuna AI. I need some ideas and suggestions on what we can implement. Here’s our list:

# Yu
na AI Current Project Status:

## What's Working in Yuna:

- [x] User Auth System
- [x] Multiple Chat Histories
- [x] In
dividual Message Deleting
- [x] History Editing
- [x] Full History Management (Import/Export, Edit)
- [x] Custom Message
s
- [x] Audio Transcription
- [x] Video In-Audio Transcription
- [x] Web Search
- [x] Web Q&A (a.k.a LangChain)
- [x] Im
age Transcription and Image Q&A
- [x] Kanojo Character Customization
- [x] Prompt Customization
- [x] Kanojo and Prompt 
Template Export/Import System
- [x] Audio Calls with TTS feedback
- [x] Modes for native and fast inferences. Llama CPP 
+ LM Studio and Siri TTS + Coqui TTS
- [x] Basic Diffusion Single File Inference
- [x] Landing Page
- [x] Dark Mode
- [x
] Gesture Control (alpha)

## What's NOT Working in Yuna (but will be in the future):

- [ ] 2D/3D Taking Head Animation
s for Video Calls
- [ ] Full LangChain Support for PDF, Audio, and Video Q&A
- [ ] RP LSTM
- [ ] Kanojo Connect with Mul
tiple Kanojo in a Single Chat
- [ ] Himitsu Copilot
- [ ] Himitsu Copiloting System
- [ ] Himitsu Actions
- [ ] History 
Collections
- [ ] Advanced Web Search
- [ ] Saved Messages (Notes)
- [ ] Pseudo APIs
- [ ] Light Mode and Automatic Mode

- [ ] Browser Extension
- [ ] Server Config Saver Per User
- [ ] Explore News Tab
- [ ] WebGPU PWA Mobile WASM
- [ ] Mo
del WebUI Manager
- [ ] Offline Viewer
- [ ] YUI (Yuna's Unified UI)
- [ ] Yuna AI Creator Studio
- [ ] Naked Mode
- [ ]
 Training
- [ ] Additional Advanced Models for Art, Uta, and More
- [ ] Emotional Profile
- [ ] Settings
- [ ] Multiling
ual Chats
- [ ] LoRAs
- [ ] Publish Share Link

Feel free to share everything you can think of, even if it exists in any
 other project!
```
---

     
 
all -  [ Need Help Understanding Why My Language Model Chain Isn't Producing Results ](https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/) , 2024-05-25-0910
```
I'm working on a project that involves using a language model chain to process questions and generate responses. However
, I've encountered an issue where the chain seems to get stuck at the invocation stage without producing any results.

*
*Background:**

* I'm using a Python script that involves various components such as document loaders, embeddings, text 
splitters, vector stores, retrievers, prompts, parsers, and language models.
* The script is designed to load a PDF docu
ment, split it into chunks, add the chunks to a vector database, initialize a language model, and then retrieve relevant
 information based on input questions.

**Problem:**

* Despite setting up the chain correctly and providing a question 
to the system, it seems to get stuck at the invocation stage without producing any results.
* I've checked the logs, and
 everything seems to be initialized and processed correctly up to the invocation step.

**Code:**

    from langchain_co
mmunity.document_loaders import UnstructuredPDFLoader
    from langchain_community.document_loaders import OnlinePDFLoad
er
    from langchain_community.embeddings import OllamaEmbeddings
    from langchain_text_splitters import RecursiveCha
racterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain.prompts import ChatPromptT
emplate, PromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_community.chat_
models import ChatOllama
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.tracers im
port ConsoleCallbackHandler
    from langchain.retrievers.multi_query import MultiQueryRetriever
    import asyncio
    

    # Use raw string notation for the file path
    local_path = r'C:/Users/User/zven/WEF_The_Global_Cooperation_Barome
ter_2024.pdf'
    
    # Load local PDF file
    if local_path:
        try:
            loader = UnstructuredPDFLoader(
file_path=local_path)
            data = loader.load()
            print('PDF loaded successfully.')
        except Exce
ption as e:
            print(f'Error loading PDF: {e}')
            data = None
    else:
        print('Upload a PDF f
ile')
        data = None
    
    if data:
        # Split and chunk text
        text_splitter = RecursiveCharacterTex
tSplitter(chunk_size=7500, chunk_overlap=100)
        chunks = text_splitter.split_documents(data)
        print(f'Docum
ent split into {len(chunks)} chunks.')
        print(data[0].page_content)
        
        # Add to vector database
   
     try:
            vector_db = Chroma.from_documents(
                documents=chunks, 
                embedding=Ol
lamaEmbeddings(model='nomic-embed-text', show_progress=True),
                collection_name='local-rag'
            )

            print('Chunks added to vector database.')
        except Exception as e:
            print(f'Error adding ch
unks to vector database: {e}')
    
        # Initialize LLM from Ollama
        local_model = 'Mistral'
        try:
  
          llm = ChatOllama(model=local_model)
            print('LLM initialized successfully.')
        except Exceptio
n as e:
            print(f'Error initializing LLM: {e}')
    
        # Define query prompt template
        QUERY_PROM
PT = PromptTemplate(
            input_variables=['question'],
            template='''You are an AI language model assi
stant. Your task is to generate five
            different versions of the given user question to retrieve relevant docu
ments from
            a vector database. By generating multiple perspectives on the user question, your
            goa
l is to help the user overcome some of the limitations of the distance-based
            similarity search. Provide thes
e alternative questions separated by newlines.
            Original question: {question}'''
        )
    
        # Ini
tialize retriever
        try:
            retriever = MultiQueryRetriever.from_llm(
                vector_db.as_retrie
ver(), 
                llm,
                prompt=QUERY_PROMPT
            )
            print('Retriever initialized 
successfully.')
        except Exception as e:
            print(f'Error initializing retriever: {e}')
    
        # De
fine RAG prompt template
        template = '''Answer the question based ONLY on the following context:
        {context
}
        Question: {question}
        '''
        print('Template: ', template)
        prompt = ChatPromptTemplate.fro
m_template(template)
        print('Prompt: ', prompt)
    
        chain = (
            {'context': retriever, 'questi
on': RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        pri
nt(chain)
        # Print the chain setup
        print('Chain setup completed.')
    
    async def run_chain():
      
  try:
            print('Invoking chain...')
            # Invoke the chain with a question
            result = await 
chain.ainvoke(
                {'question': 'What are the 5 pillars of global cooperation?'},
                config={'c
allbacks': [ConsoleCallbackHandler()]}  # 30 seconds timeout
            )
            print('Chain invoked successfully
.')
            print('Result:', result)
            # Print the answer if it exists
            if 'answer' in result:

                print('Answer:', result['answer'])
        except asyncio.TimeoutError:
            print('Chain invocat
ion timed out.')
        except Exception as e:
            print(f'Error invoking chain: {e}')
    
    # Run the chain

    asyncio.run(run_chain())
    
    # Delete all collections in the db
    vector_db.delete_collection()
    

>

Out
put when I Run it:  
OllamaEmbeddings: 100%|████████████████████████████████████████████████████████████████| 11/11 \[05
:16<00:00, 28.77s/it\]

Chunks added to vector database.

LLM initialized successfully.

Retriever initialized successfu
lly.

Template:  Answer the question based ONLY on the following context:

{context}

Question: {question}

Prompt:  inp
ut\_variables=\['context', 'question'\] messages=\[HumanMessagePromptTemplate(prompt=PromptTemplate(input\_variables=\['
context', 'question'\], template='Answer the question based ONLY on the following context:\\n    {context}\\n    Questio
n: {question}\\n    '))\]

first={

context: MultiQueryRetriever(retriever=VectorStoreRetriever(tags=\['Chroma', 'Ollama
Embeddings'\], vectorstore=<langchain\_community.vectorstores.chroma.Chroma object at 0x0000016DB7450390>), llm\_chain=L
LMChain(prompt=PromptTemplate(input\_variables=\['question'\], template='You are an AI language model assistant. Your ta
sk is to generate five\\n        different versions of the given user question to retrieve relevant documents from\\n   
     a vector database. By generating multiple perspectives on the user question, your\\n        goal is to help the use
r overcome some of the limitations of the distance-based\\n        similarity search. Provide these alternative question
s separated by newlines.\\n        Original question: {question}'), llm=ChatOllama(model='Mistral'), output\_parser=Line
ListOutputParser())),

question: RunnablePassthrough()

} middle=\[ChatPromptTemplate(input\_variables=\['context', 'que
stion'\], messages=\[HumanMessagePromptTemplate(prompt=PromptTemplate(input\_variables=\['context', 'question'\], templa
te='Answer the question based ONLY on the following context:\\n    {context}\\n    Question: {question}\\n    '))\]), Ch
atOllama(model='Mistral')\] last=StrOutputParser()

Chain setup completed.

Invoking chain...

\[chain/start\] \[chain:R
unnableSequence\] Entering Chain run with input:

{

'question': 'What are the 5 pillars of global cooperation?'

}

\[c
hain/start\] \[chain:RunnableSequence > chain:RunnableParallel<context,question>\] Entering Chain run with input:

{

'q
uestion': 'What are the 5 pillars of global cooperation?'

}

\[chain/start\] \[chain:RunnableSequence > chain:RunnableP
arallel<context,question> > chain:RunnablePassthrough\] Entering Chain run with input:

{

'question': 'What are the 5 p
illars of global cooperation?'

}

\[chain/start\] \[chain:RunnableSequence > chain:RunnableParallel<context,question> >
 retriever:Retriever > chain:LLMChain\] Entering Chain run with input:

{

'question': {

'question': 'What are the 5 pi
llars of global cooperation?'

}

}\[chain/end\] \[chain:RunnableSequence > chain:RunnableParallel<context,question> > c
hain:RunnablePassthrough\] \[16ms\] Exiting Chain run with output:

{

'question': 'What are the 5 pillars of global coo
peration?'

}

\[llm/start\] \[chain:RunnableSequence > chain:RunnableParallel<context,question> > retriever:Retriever >
 chain:LLMChain > llm:ChatOllama\] Entering LLM run with input:

{

'prompts': \[

'Human: You are an AI language model 
assistant. Your task is to generate five\\n        different versions of the given user question to retrieve relevant do
cuments from\\n        a vector database. By generating multiple perspectives on the user question, your\\n        goal 
is to help the user overcome some of the limitations of the distance-based\\n        similarity search. Provide these al
ternative questions separated by newlines.\\n        Original question: {'question': 'What are the 5 pillars of global c
ooperation?'}'

\]

}
```
---

     
 
all -  [ Problems with json and enum parser ](https://www.reddit.com/r/LangChain/comments/1cyh8zq/problems_with_json_and_enum_parser/) , 2024-05-25-0910
```
Langchain's enum and json parser just dont work and I can't figure out why. For example, here is my code below:

https:/
/preview.redd.it/smzg0411z22d1.png?width=741&format=png&auto=webp&s=26b6ef00927afc4a1a9e6c8e6bf297dfea96d56f

Where pred
iction is an enum with increase, decrease or no change. When I try it, I get this error:

https://preview.redd.it/3lp1jv
g6z22d1.png?width=1451&format=png&auto=webp&s=b41bdf522394561581dc924abe167898aa19edeb

Which gives the correct answer a
s decreased, but not as an enum. The same happens when I try this with the json parser, it adds unneccessary text around
 the dictionary so langchain doesnt read the output as a dictionary. Is there a fix for this?
```
---

     
 
all -  [ A question regarding ](https://www.reddit.com/r/LangChain/comments/1cycid7/a_question_regarding/) , 2024-05-25-0910
```
I have a use case where I have bunch of notes for a college class and I want to generate flash cards for them. Now I kno
w RAG is used to fetch most closest file from database and answer based on that, however in my case, all the notes shoul
d be loaded. So would RAG be applicable in this case. I can also just load all data in but that would probably run out o
f tokens quickly
```
---

     
 
all -  [ ChatCompletionRequest in AgentExecutor ](https://www.reddit.com/r/LangChain/comments/1cy9zxh/chatcompletionrequest_in_agentexecutor/) , 2024-05-25-0910
```
I was checking out the function calling capability of the new Mistral model and was wondering how to integrate this into
 a ReAct agent flow that uses AgentExecutor. 

[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3](https://huggi
ngface.co/mistralai/Mistral-7B-Instruct-v0.3)

  
Anyone got any hints? 
```
---

     
 
MachineLearning -  [ [R] Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI ](https://www.reddit.com/r/MachineLearning/comments/1crwh0q/r_building_an_observable_arxiv_rag_chatbot_with/) , 2024-05-25-0910
```
Hey r/MachineLearning, I published a new article where I built an observable semantic research paper application.

This 
is an extensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most rele
vant PDF documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval
.
3. Enhancing the app with LLM observability features from Literal AI.

You can read the article here: [https://medium.
com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](h
ttps://medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9
c345fcd1cd8)

Code for the tutorial: [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tah
reemrasul/semantic_research_engine)


```
---

     
 
MachineLearning -  [ [P] LLMinator: A Llama.cpp + Gradio based opensource Chatbot to run llms locally(cpu/cuda) directly  ](https://www.reddit.com/r/MachineLearning/comments/1cpbgd1/p_llminator_a_llamacpp_gradio_based_opensource/) , 2024-05-25-0910
```
Hi I am currently working on a context-aware streaming chatbot based on Llama.cpp, Gradio, Langchain, Transformers. LLMi
nator can pull LLMs directly from HF & run them locally on cuda or cpu.

I am looking for recommendations & help from op
ensource community to grow this further.

**Github Repo:** [https://github.com/Aesthisia/LLMinator](https://github.com/A
esthisia/LLMinator)

**Goal:** To help developers with kickstarter code/tool to run LLMs.

https://preview.redd.it/fnzja
7rjwqzc1.png?width=1846&format=png&auto=webp&s=a62c43614d63e82156fef8722b986b051cc1795b

**Features:**

* Context-aware 
Chatbot.
* Inbuilt code syntax highlighting.
* Load any LLM repo directly from HuggingFace.
* Supports both CPU & Cuda m
odes.
* Load & Offload saved models.
* Command Line Args
* API Access(Soon to be available)

Any review or feedback is a
ppreciated.
```
---

     
