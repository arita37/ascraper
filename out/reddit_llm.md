 
all -  [ Resume Review Request for Data Analyst Position ](https://www.reddit.com/r/Resume/comments/1gakxrp/resume_review_request_for_data_analyst_position/) , 2024-10-24-0913
```
Hey everyone,

I recently started applying for jobs and want to make sure my resume is up to the mark. I’m not entirely 
sure if it effectively highlights my skills and experiences or if it’s aligned with what recruiters are looking for. Cou
ld I get some feedback or advice on improving it? Any tips or suggestions would be super helpful!

Thanks in advance!

h
ttps://preview.redd.it/actn0y45jkwd1.png?width=956&format=png&auto=webp&s=7901adf906591f788e15b3c6ddd47003414056b7


```
---

     
 
all -  [ Forget LangChain, CrewAI and AutoGen — Try This Framework and Never Look Back ](https://medium.com/generative-ai/forget-langchain-crewai-and-autogen-try-this-framework-and-never-look-back-e34e0b6c8068?sk=0e77bf707397ceb535981caab732f885) , 2024-10-24-0913
```

```
---

     
 
all -  [ AutoGen v0.2.37 released ](https://www.reddit.com/r/AutoGenAI/comments/1gah8nn/autogen_v0237_released/) , 2024-10-24-0913
```
[New release: v0.2.37](https://github.com/microsoft/autogen/releases/tag/v0.2.37)

# What's Changed

* Use trusted publi
sher for pypi release by [@jackgerrits](https://github.com/jackgerrits) in [\#3596](https://github.com/microsoft/autogen
/pull/3596)
* Fix typos in Cerebras doc by [@henrytwo](https://github.com/henrytwo) in [\#3590](https://github.com/micro
soft/autogen/pull/3590)
* Add blog post announcing the new architecture preview by [@jackgerrits](https://github.com/jac
kgerrits) in [\#3599](https://github.com/microsoft/autogen/pull/3599)
* Update PR link in blog post by [@jackgerrits](ht
tps://github.com/jackgerrits) in [\#3602](https://github.com/microsoft/autogen/pull/3602)
* Create CI to tag issues with
 needs triage by [@jackgerrits](https://github.com/jackgerrits) in [\#3605](https://github.com/microsoft/autogen/pull/36
05)
* Update issue templates by [@jackgerrits](https://github.com/jackgerrits) in [\#3610](https://github.com/microsoft/
autogen/pull/3610)
* Fix small typo in the docs by [@jknaudt21](https://github.com/jknaudt21) in [\#3650](https://github
.com/microsoft/autogen/pull/3650)
* Update 0.2 CI to target branch, remove merge queue by [@jackgerrits](https://github.
com/jackgerrits) in [\#3656](https://github.com/microsoft/autogen/pull/3656)
* Update BaseUrl of docusaurus site by [@ja
ckgerrits](https://github.com/jackgerrits) in [\#3658](https://github.com/microsoft/autogen/pull/3658)
* Add announcemen
t bar for 0.4 by [@jackgerrits](https://github.com/jackgerrits) in [\#3717](https://github.com/microsoft/autogen/pull/37
17)
* Update links on 0.2 website by [@jackgerrits](https://github.com/jackgerrits) in [\#3734](https://github.com/micro
soft/autogen/pull/3734)
* Function Calling Support for Gemini - Part 2 by [@luxzoli](https://github.com/luxzoli) in [\#3
726](https://github.com/microsoft/autogen/pull/3726)
* Fix [\#2643](https://github.com/microsoft/autogen/issues/2643) \-
 groupchat model registration by [@Matteo-Frattaroli](https://github.com/Matteo-Frattaroli) in [\#2696](https://github.c
om/microsoft/autogen/pull/2696)
* Added a demonstartion notebook featuring the usage of Langchain with AutoGen by [@Kiru
shikesh](https://github.com/Kirushikesh) in [\#3461](https://github.com/microsoft/autogen/pull/3461)
* Autobuild Functio
n calling by [@krishnashed](https://github.com/krishnashed) in [\#3238](https://github.com/microsoft/autogen/pull/3238)

* Update Docs to Point to 0.4 by [@victordibia](https://github.com/victordibia) in [\#3764](https://github.com/microsoft
/autogen/pull/3764)
* Notebook on web crawling by [@WilliamEspegren](https://github.com/WilliamEspegren) in [\#2720](htt
ps://github.com/microsoft/autogen/pull/2720)
* Fix link to v0.4 documentation by [@ekzhu](https://github.com/ekzhu) in [
\#3772](https://github.com/microsoft/autogen/pull/3772)
* Remove path filter for website testing in 0.2 by [@jackgerrits
](https://github.com/jackgerrits) in [\#3782](https://github.com/microsoft/autogen/pull/3782)
* Fix broken image URL in 
README by [@gagb](https://github.com/gagb) in [\#3776](https://github.com/microsoft/autogen/pull/3776)
* Clarify stable 
package name and version on home page by [@jackgerrits](https://github.com/jackgerrits) in [\#3775](https://github.com/m
icrosoft/autogen/pull/3775)
* Fix CTA button alignment in docs home page by [@victordibia](https://github.com/victordibi
a) in [\#3788](https://github.com/microsoft/autogen/pull/3788)
* K8s code executor by [@questcollector](https://github.c
om/questcollector) in [\#3419](https://github.com/microsoft/autogen/pull/3419)
* Add Couchbase Vector DB Example Noteboo
k and Minor Bug Fix by [@lokesh-couchbase](https://github.com/lokesh-couchbase) in [\#3804](https://github.com/microsoft
/autogen/pull/3804)
* Add Zep ecosystem doc and notebook by [@danielchalef](https://github.com/danielchalef) in [\#3681]
(https://github.com/microsoft/autogen/pull/3681)
* \[bug\] Changes Text Cache Default to None by [@WaelKarkoub](https://
github.com/WaelKarkoub) in [\#3872](https://github.com/microsoft/autogen/pull/3872)
* \[bug\] Validates If The Role Tool
 is Handled Correctly after Transforms by [@WaelKarkoub](https://github.com/WaelKarkoub) in [\#3875](https://github.com/
microsoft/autogen/pull/3875)
* \[CAP\] Abstraction of actor\_connector to go along with runtime factory and runtime abst
raction by [@rajan-chari](https://github.com/rajan-chari) in [\#3296](https://github.com/microsoft/autogen/pull/3296)

#
 New Contributors

* [@jknaudt21](https://github.com/jknaudt21) made their first contribution in [\#3650](https://github
.com/microsoft/autogen/pull/3650)
* [@Matteo-Frattaroli](https://github.com/Matteo-Frattaroli) made their first contribu
tion in [\#2696](https://github.com/microsoft/autogen/pull/2696)
* [@WilliamEspegren](https://github.com/WilliamEspegren
) made their first contribution in [\#2720](https://github.com/microsoft/autogen/pull/2720)
* [@questcollector](https://
github.com/questcollector) made their first contribution in [\#3419](https://github.com/microsoft/autogen/pull/3419)
* [
@danielchalef](https://github.com/danielchalef) made their first contribution in [\#3681](https://github.com/microsoft/a
utogen/pull/3681)

**Full Changelog**: [v0.2.36...v0.2.37](https://github.com/microsoft/autogen/compare/v0.2.36...v0.2.3
7)
```
---

     
 
all -  [ Best Approach to Building a Chatbot with Twitter Data Using LLMs (LLaMA 3.2)? ](https://www.reddit.com/r/datascienceproject/comments/1gafune/best_approach_to_building_a_chatbot_with_twitter/) , 2024-10-24-0913
```
**Hello everyone,**

I'm currently working on analyzing customer support inquiries from various insurance companies and 
generating questions from these tweets using LLaMA 3.2. The dataset includes both full conversation and tweet-level form
ats, containing customer support inquiries.

Now, I'm looking to take it a step further and build a chatbot that can:

1
. Answer customer queries based on the patterns found in the historical tweets. (Currently doing manually)
2. Utilize th
e questions I've already generated.
3. Learn from ongoing interactions with users to improve its responses over time.

G
iven the data I have and my experience working with LLMs, what would be the best way to approach building this chatbot? 
Here are a few specifics I'm curious about:

* What framework or tools (open-source or otherwise) would work well for th
is kind of chatbot development?
* How can I integrate LLaMA 3.2 (or another model, if recommended) to handle real-time q
uestion generation and answering?
* How should I structure the chatbot's learning process to continuously improve its re
sponses from new tweets or user interactions?

Any suggestions on architecture, training strategies,RAGs or frameworks (
like Rasa, Langchain, etc.) would be greatly appreciated. Thank you!


```
---

     
 
all -  [ Favorite langchain features? ](https://www.reddit.com/r/LangChain/comments/1gae0jp/favorite_langchain_features/) , 2024-10-24-0913
```
While there's some general langchain hate, I'd like to know what are your favorite things about langchain? Favorite feat
ures, what makes your life easier, etc.?
```
---

     
 
all -  [ What is your favorite vector database that runs purely in a Python process ](https://www.reddit.com/r/LangChain/comments/1gadyp3/what_is_your_favorite_vector_database_that_runs/) , 2024-10-24-0913
```
I'm building a 'chat with your videos' desktop application and would like to run a vector database purely in application
 code rather than running it in a stand-alone server.



I've done some research and found these:

* [Weaviate embedded]
(https://weaviate.io/developers/weaviate/installation/embedded)
* [FAISS](https://github.com/facebookresearch/faiss)
* [
Milvus Lite](https://milvus.io/docs/milvus_lite.md)

Any other suggestions? Which is your favorite and why?
```
---

     
 
all -  [ Looking for an intern in Paris ](https://www.reddit.com/r/nextjs/comments/1gaawdm/looking_for_an_intern_in_paris/) , 2024-10-24-0913
```
Hey guys,

We’re a team of 2 based in [Station F](https://stationf.co/) in Paris, and we’ve been working on our startup 
for the past 6 months.

We build a bunch of AI Agents to automate manual tasks on a specific issue related to HR. We are
 an AI-first company, and work with best-in-class tech stack : Next, Neon, Inngest, Clerk, Shadcn, Langchain. Our infra 
is serverless.

We’re looking to hire an intern full stack rockstar who want to work on cutting edge tech. We have so ma
ny interesting challenges ahead and a very ambitious roadmap, but our 4 arms are not enough.

Feel free to dm me with yo
ur GitHub profile link if your are in Paris or want to relocate.  
Start date : ASAP

Best
```
---

     
 
all -  [ External interaction with LangGraph ](https://www.reddit.com/r/LangChain/comments/1ga8guy/external_interaction_with_langgraph/) , 2024-10-24-0913
```
Hello everyone

I've built an agent using LangGraph and I need to be able to call specific code  within it from the outs
ide like an API endpoint.

I've seen in the docs, for LangChain there's LangServe, what about LangGraph? Can I achieve t
he same using LangGraph Cloud? 

Thanks in advance,

  
co-founder Shaareable Apps
```
---

     
 
all -  [ How exactly does LLMGraphTransformer work? ](https://www.reddit.com/r/LangChain/comments/1ga70vg/how_exactly_does_llmgraphtransformer_work/) , 2024-10-24-0913
```
I am working on implementing knowledge graphs for RAG. I tried experimenting Microsofts's GraphRAG. Now i want to do usi
ng Neo4j. How are documents indexed? and How are entities extracted. I found that they use LLM to extract entities, is t
here a way I can find that prompt??

And once entities are found out, how are duplicate entities handled? I really need 
help.
```
---

     
 
all -  [ Guys, fckk my resume ](https://i.redd.it/fsdhqomy4hwd1.jpeg) , 2024-10-24-0913
```
Im keep applying for data science intern role, as my skillset but got no reply dont know what im missing. Suggest some c
hanges and what other can i put, im willing to contribute in opensource but not sure about how to do with my current ski
llset

Thank you for the review
Those freaking ai generating mediacore results
```
---

     
 
all -  [ How do you extract time metadata from question? ](https://www.reddit.com/r/LangChain/comments/1ga67mh/how_do_you_extract_time_metadata_from_question/) , 2024-10-24-0913
```
I have a RAG system that works great.

The users wants to ask questions like 'what are the news of this month?', 'what i
s the winner of the championship 2024?', and whatever.

I though to put a chain BEFORE the retrieval, trying to extract 
'time metadata' from the question, like 'date\_from' and 'date\_to', and then apply these filters to the retrieval query
 based document metadata.

I came up with a prompt like: 'today is %Y-%m-%d' + 'extract time metadata from the question.
..bla bla'.

Is this a good approach? Is there anything better i can do?
```
---

     
 
all -  [ Image Extraction Issue with WMF Format on Linux - Need Help Converting to PNG for OCR ](https://www.reddit.com/r/Rag/comments/1ga62v7/image_extraction_issue_with_wmf_format_on_linux/) , 2024-10-24-0913
```
Hi everyone,  
I’m building a multimodal pipeline involving LLMs and OCR where my app processes PPT files, extracting te
xt and images from slides. The app works perfectly in my local Windows environment, but images are extracted in WMF form
at on an AWS Ubuntu instance. Unfortunately, Linux can’t handle this format natively, which is causing issues for prepro
cessing (OCR) and further multimodal analysis.

I’m looking for suggestions on efficiently converting WMF images to PNG 
on Linux before feeding them into the OCR model within the LLM-driven multimodal architecture. Has anyone come across a 
similar issue in a LocalLLM or LangChain setup? Do you have any recommendations for tools, libraries, or workflows to in
tegrate this step into the pipeline? I appreciate any help you can provide.
```
---

     
 
all -  [ Final Year, Ideally looking for ML related FTE. Would love feedback on this Resume. ](https://www.reddit.com/r/developersIndia/comments/1ga5t7d/final_year_ideally_looking_for_ml_related_fte/) , 2024-10-24-0913
```
https://preview.redd.it/5wvdmub1ygwd1.png?width=580&format=png&auto=webp&s=64008ad4c4c5969e2b910f2370b225c6c39a035c


```
---

     
 
all -  [ The Most Affordable Search API for Scale. ](https://www.reddit.com/r/LangChain/comments/1ga4y55/the_most_affordable_search_api_for_scale/) , 2024-10-24-0913
```
So i am planning to create an AI Application, would like to know what do you guys prefer the best API, for an applicatio
n, which has a small feature of gettting results from Internet.
But the issue is that most Search API's are expensive (i
n my opinion) for scale.

I would really appreciate your recommendations.

```
---

     
 
all -  [ Request support on Jinja chat template for LLama3.1 and Llama3.2  ](https://www.reddit.com/r/LangChain/comments/1ga3dw3/request_support_on_jinja_chat_template_for/) , 2024-10-24-0913
```
I am trying to use vllm to serve llama 3.1 or 3.2 based on its outputs, to test which, I require a Jinja chat template


I wrote one, but not sure whether it's right as I get gibberish symbols as output. I attach the Jinja template herewith.


`<|begin_of_text|>
{% for message in messages %}
<|start_header_id|>{{ message['role'] }}<|end_header_id|>
{{ message[
'content'] }}<|eot_id|>
{% endfor %}
{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}
<|start_head
er_id|>assistant<|end_header_id|>
{% endif %}`

Please modify if I am wrong . Thanks in advance 
```
---

     
 
all -  [ [Student] Not getting any internship calls, please help me out ](https://www.reddit.com/r/EngineeringResumes/comments/1ga2qdo/student_not_getting_any_internship_calls_please/) , 2024-10-24-0913
```
https://preview.redd.it/5des7e5qvfwd1.png?width=5100&format=png&auto=webp&s=ee8a4c2816f7b933d001d1c9133024ed21f95210

Hi
 all,

I'm currently based in the UK on a student visa and looking for software engineering roles at big tech (FAANG, et
c.) as I approach graduation in 2026. I had an interview with Google last year and performed well in all three coding ro
unds, but still got rejected. Since then, I haven't had much luck securing calls for interviews from FAANG companies.

A
 bit about my background:

* I'm currently doing a year-long internship in my home country.
* I have experience in web d
evelopment and AI (I’ve worked with TensorFlow and trained models).
* I'm also focusing on improving my technical skills
 and projects in preparation for future applications.

I’d really appreciate any feedback on my resume in terms of conte
nt, structure, and overall strength. Is there something that could be holding me back from getting more interview calls 
from big tech? Any advice on how to improve my chances would be helpful!

Thanks!
```
---

     
 
all -  [ Rag not able to search image with name. ](https://www.reddit.com/r/LangChain/comments/1ga1ptb/rag_not_able_to_search_image_with_name/) , 2024-10-24-0913
```
I have implemented a Multimodal Retrieval-Augmented Generation (RAG) application, utilizing models such as CLIP and BLIP
, as well as multimodal models like GPT-4 Vision. While I am successfully able to retrieve images based on their content
 and details, I am facing an issue when trying to retrieve or generate images based solely on their file names.

For exa
mple, if I have document with multiple cats nickname, their description and then their image and if I ask model for imag
e of cat by their nickname, the system is not able to return the correct image. I've attempted various approaches, inclu
ding different file formats like PDFs and documents, as well as integrating OCR (Optical Character Recognition) to extra
ct text. Despite these efforts, I am still unable to generate the images using just their names. Could you provide guida
nce on how to resolve this issue?

Edit: I am using chromadb vector database.

Here is how my document is Structured -
T
here is name and then description and then image of cat, again cat name then description and image and so on..

```
---

     
 
all -  [ [Langgraph] Passing instruction messages in the graph ](https://www.reddit.com/r/LangChain/comments/1ga0k5s/langgraph_passing_instruction_messages_in_the/) , 2024-10-24-0913
```
I'm building a complex graph where agent 1 analyzes the message from customer, chooses a strategy and informs the agent 
2 how to proceed. 

What I do today is convert the agent 1's message to Human message and pass it to agent 2. This works
 with just 2 agents but when I started adding agents 3 & 4, the message conversion seems to confuse the AI. 

Looking fo
r strategies where I want to define something like this:

<General System Prompt>

<Specific feedback from a supervisor>


<Message from the user + message history>. 

What's the right way to implement this in Langgraph
```
---

     
 
all -  [ Claude's Coding Is Fantastic Right Now!  ](https://www.reddit.com/r/ClaudeAI/comments/1g9ynpo/claudes_coding_is_fantastic_right_now/) , 2024-10-24-0913
```
Just got home a short time ago after work, and I'm going back through some old prompts for several of my codebases and i
t's just running through the problems like nothing.

This is amazing.

Can't wait to mess with the tools and langchain/l
anggraph functions more this weekend. 
```
---

     
 
all -  [ AI agent for B2B research workflow - can pick tools on his own :) Upgrading to Langgraph soon ](https://www.reddit.com/r/LangChain/comments/1g9xq0a/ai_agent_for_b2b_research_workflow_can_pick_tools/) , 2024-10-24-0913
```
I built an AI agent for B2B research. It's all done with Langchain so far. I am upgrading to Langgraph soon to ensure my
 AI agent can combine tools to build even better research workflows for my users (978 users so far).   
  
To get specif
ic data points, the AI has to create intermediary steps, leveraging existing tools and combining them. 

**E.g I want to
 find companies in financial trouble.** 

This datapoint does not exist. This is quite subjective. Ai will ask to clarif
y.

Then the AI agent has to objectivize this information and develop tangible evidence and signals from the internet to
 help the users. 

here are some of the steps the Ai agent would take: 

**Here are 3 things the AI could suggest**  
\-
-> Glassdoor reviews

\--> Press release

\--> Suppliers complaints for late payments

**Action items:** 

Scrape that i
nformation

Find the appropriate taxonomy 

Verify the quality 

Apply the taxonomy based on the intelligence gathered. 


Display the results. 

===

One thing I noticed function\_calling is NOT the way to go regarding tool selection. Espec
ially if we want the Ai agent to be aware of thousands of tools, datapoints, and sources. 

We are using context window 
for that. Better results. 

https://reddit.com/link/1g9xq0a/video/rk0gsrzchewd1/player


```
---

     
 
all -  [ 🚀 Senior Machine Learning Engineer Opportunity! ](https://www.reddit.com/r/mlops/comments/1g9uykp/senior_machine_learning_engineer_opportunity/) , 2024-10-24-0913
```
We are seeking a seasoned **Senior Machine Learning Engineer** to join our innovative team and drive cutting-edge AI pro
jects. If you have a passion for building scalable machine learning systems and want to work in a collaborative environm
ent, this could be your next career move!

**Required Hard Skills** 

* **4+ years** of ML engineering experience
* **Ba
chelor’s degree** in Computer Science or related
* Experience with **Python, ML libraries and AI/ML frameworks (PyTorch,
 HuggingFace, TensorFlow, etc.)** 
* Experience building **GenAI solutions using LLMs**, including frameworks like LangC
hain and LlamaIndex, prompt engineering, fine-tuning and serving models, and implementing common patterns like RAG and N
LQ 
* **Client-facing experience**
* **Familiarity with containerization and orchestration tools** 

Link to the full jo
b posting: [https://boards.greenhouse.io/lokainc/jobs/4067015007?gh\_src=ff064e7b7us](https://boards.greenhouse.io/lokai
nc/jobs/4067015007?gh_src=ff064e7b7us)
```
---

     
 
all -  [ Coding a GeoGuessr Auto AI Bot with vision LLMs ](https://www.reddit.com/r/geoguessr/comments/1g9uw7x/coding_a_geoguessr_auto_ai_bot_with_vision_llms/) , 2024-10-24-0913
```
Video Tutorial: Coding a GeoGuessr AI Bot using LangChain and different Vision LLMs (GPT-4o, Claude 3.5, and Gemini 1.5)
 [https://www.youtube.com/watch?v=OyDfr0xIhss](https://www.youtube.com/watch?v=OyDfr0xIhss)

Disclaimer: this is not all
owed games with Leaderboards, prizes or competitive against other people publicly! This video is only for educational pu
rposes and to experiment LLMs capabilities :)
```
---

     
 
all -  [ Chromadb ](https://www.reddit.com/r/vectordatabase/comments/1g9trrw/chromadb/) , 2024-10-24-0913
```
Having a verbal crash issue with chromadb under langchain. I am trying to put chunks to the db and it kills the kernel. 


Any idea how this a constant? 

Built all using .venv so there shouldn't be any package issues.



```
---

     
 
all -  [ Cv debutant junior en reconversion ](https://i.redd.it/is740ys26dwd1.jpeg) , 2024-10-24-0913
```
Bonjour les amis
J’ai posté mon cv il y’a 3 jours et suite aux conseilles des membres j’ai l’ai modifié ..
Vos avis svp 
? Merci d’avance 

```
---

     
 
all -  [ Chromadb ](https://www.reddit.com/r/LangChain/comments/1g9qt3p/chromadb/) , 2024-10-24-0913
```
Chroma always kills the kernel when trying to load a set of chunks into the newest established database. I haven't found
 a single work around. 
```
---

     
 
all -  [ [0 YoE] Struggling to Get Interview Callbacks for New Grad SWE Roles – Resume Feedback Needed ](https://www.reddit.com/r/EngineeringResumes/comments/1g9ojcr/0_yoe_struggling_to_get_interview_callbacks_for/) , 2024-10-24-0913
```
Hi everyone, I’m a recent grad (May 2024) currently looking for full-time, entry-level software engineering positions ac
ross pretty much any industry. I’m based in the northeast but have been applying to both remote and in-person roles nati
onwide. I’m  open to relocating.

Despite submitting applications for a few months now, I’ve only received a handful of 
OAs, which I’ve completed with perfect scores. However, I’ve been ghosted after this stage, and I guess the issue lies w
ith my resume–> when it reaches the hiring manager(?).

I’m looking for advice on improving the overall content quality 
of my resume. Does it accurately reflect my skills and work experience? Is there something about the structure or syntax
 that could be holding me back? Any feedback on how to make it stronger would be greatly appreciated, as I feel it’s the
 main reason I’m not getting interview opportunities.

There are no visa or citizenship complications in my search.

htt
ps://preview.redd.it/nhemkytyjcwd1.png?width=5100&format=png&auto=webp&s=c91b2d651943a73464c71890b1c3f136e097c37c



Tha
nks so much in advance for your help!
```
---

     
 
all -  [ Ollama stopped code from running? ](https://www.reddit.com/r/ollama/comments/1g9gi9p/ollama_stopped_code_from_running/) , 2024-10-24-0913
```
I have the following code, it is supposed to load a pdf file, turn it into text chunks, embedding these chunks and store
 them locally to the disk.

`# Import time module to track execution time`

`import time` 

`# Import document loader an
d text splitter`

`from langchain.document_loaders import PyPDFLoader`

`from langchain.text_splitter import RecursiveCh
aracterTextSplitter`

`# Import embedding model of Ollama`

`from langchain.embeddings import OllamaEmbeddings`

`# Impo
rt Chromadb for vector storage`

`from langchain.vectorstores.chroma import Chroma`



`# Define a text splitter functio
n along with splitting parameters`

`text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)`




`# STEP 1: Load the PDF and split it into chunks of text`

`start_time = time.time()  # Start the timer`

`loader = P
yPDFLoader('machinedesign-1-100.pdf')`

`docs = loader.load_and_split(text_splitter)  # or use: docs = loader.load(); sp
lits = text_splitter.split_documents(docs)`



`# STEP 2: Create an embedding tool`

`embeddings = OllamaEmbeddings(mode
l='llama3.1')`



`# STEP 3: Create embedding vectors and store them in the vector database`

`start_embedding_time = ti
me.time()  # Start the embedding timer`

`spice_db_Ollama = Chroma.from_documents(`

`docs,` 

`embedding=embeddings,` 


`persist_directory='MD_embeddings_Ollama'`

`)`

`embedding_time = time.time() - start_embedding_time`

`# Import time 
module to track execution time`



`# Total time taken for the entire process`

`total_time = time.time() - start_time`


`print(f'Total time for the process: {total_time:.2f} seconds')`

After successfully running the code, the embeddings a
re stored locally. However, 

`# Import time module to track execution time` and `print(f'Total time for the process: {t
otal_time:.2f} seconds')` are not printed. This leads me to wonder, if my program is somehow terminated before the end, 
and if the embeddings stored are complete. Could you please explain to me why this happens? I appreciate your help!
```
---

     
 
all -  [ Applied to many internships but couldn't convert them need a resume review  ](https://i.redd.it/sw1xu5goe9wd1.jpeg) , 2024-10-24-0913
```
I am Third year CSE student working primarily in ML(and its aggregates like DL and Gen AI) I have been applying for inte
rnships since the last two months of my second year and am already at the mid point of my third year.
Please review my r
esume and let me know if I can make it better 
```
---

     
 
all -  [ Need help in RAG using LLAMA for invoice extraction ](https://www.reddit.com/r/Rag/comments/1g9cdpm/need_help_in_rag_using_llama_for_invoice/) , 2024-10-24-0913
```
I'm currently invested on a project, where I'm planning to use RAG for extracting invoice from the pdf ,images, and some
 of the structured data, the process I'm using using right now is:



->Extraction of data (using PyMuPDF, PaddleOCR, an
d Extractors for structured data)



->Place the content and Write a prompt to retrieve from vectordb, (Langchain and Ch
romaDB is used)



->Used LLama to use the data from vectordb, to get a meaningful json data,



Problem is structure ke
ep on changing, Need Help!!. (Tried using instructor not fruitful, Im new to GenAi and RAG)
```
---

     
 
all -  [ Building a community/network around AI Agents ](https://www.reddit.com/r/LangChain/comments/1g9ag3l/building_a_communitynetwork_around_ai_agents/) , 2024-10-24-0913
```
Hey ya'll, I'm currently building a curated community for AI Accelerators who believe that AI Agents will put more power
 in the hands of individual creators and entrepreneurs.

We're going to have

- Constantly Updated News  
- Learning Res
ources  
- Hackathons and Investment Resources (getting your ideas funded)  
- AI Agent Marketplace (Trading post for AI
 Agent buyers and sellers)  
- Ongoing agent experiments that the community can get involved in  
- and much more as we 
grow

Me and my partner truly believe that AI will soon enable people to start enterprise level businesses on their own.
 Imagine you want to build a one-person software company run almost entirely by agents. We're not there yet but we're ge
tting closer, and we want to build platforms to make it insanely easy to build and manage these projects using AI Agents
.

If you're excited for AI Agents and what they will help us create, consider joining! This community will be active wi
thin the next week. 


```
---

     
 
all -  [ Shutting down a graph ](https://www.reddit.com/r/LangChain/comments/1g99yg2/shutting_down_a_graph/) , 2024-10-24-0913
```
I have an app that creates one graph for each campaign. Over a period of time, I expect to have 1000s of graphs to be cr
eated.   
Are there memory implications for this? Is there a proper way to shutdown graphs once their lifecycle is compl
eted?
```
---

     
 
all -  [ Organização para estudos ](https://www.reddit.com/r/brdev/comments/1g92g6c/organização_para_estudos/) , 2024-10-24-0913
```
Olá, espero que estejam todos bem. Minha dúvida é: como organizam seus estudos no geral?

Fui contratado para atuar em p
rojetos de IA como desenvolvedor Fullstack Jr em uma empresa bem legal, vai fazer dois meses que estou nela. As principa
is stacks que eu preciso saber é Python, FastAPI, SQLAlchemy, Vue e Nuxt, além de Git, AWS, Docker, etc. 
Tanto no traba
lho quanto por fora eu busco estudar essas tecnologias, mas também tento estudar libs de IA (Langchain, LangGraph, Llama
Index), além de algoritmos e estrutura de dados, ML/Federated Learning para o meu TCC, inglês... Caramba! Muito coisa! 


Eu imagino que muitos aqui passam/passaram por situações como essa, de ter que estudar várias coisas. Como vocês se org
anizam pra isso? Tem como fazer sem atrapalhar família, namoro, amigos, lazer e etc? 
```
---

     
 
all -  [ Advice on scaling project with backend LLMs for servers with multiple models and making better decis ](https://www.reddit.com/r/LocalLLaMA/comments/1g91mik/advice_on_scaling_project_with_backend_llms_for/) , 2024-10-24-0913
```
Hello,  
I am a beginner working on an AI project for commercial use, and I’m currently stuck. I’ve been using three mod
els: Faster-Whisper, LLaMA 3.1 8B, and Parler-TTS. I hooked them up using Hugging Face's `transformers` library. My main
 bottleneck right now is the LLaMA model for real-time responses. I initially used the [shenzhi-wang/Llama3-8B-Chinese-C
hat at v2](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat/tree/v2) model (v2 gguf), but it struggled with sp
ecific role-based JSON responses.

I switched to LLaMA 3.1 8B and 3.2 3B while using and testing all of these, and while
 they improved the accuracy of the responses, the speed dropped significantly. The response time is now around 3-4 secon
ds, whereas before it was closer to 0.5 seconds.

I’ve tried performance optimizations from [huggingface-llama-recipes/p
erformance\_optimization at main · huggingface/huggingface-llama-recipes](https://github.com/huggingface/huggingface-lla
ma-recipes/tree/main/performance_optimization), but haven't seen much improvement. I also encountered a strange error wh
en trying to compile with `torch` (specifically with `model.forward`) and I dont get any with just `model`.

Here’s my c
urrent setup:

* **Hardware**: NVIDIA 3090 on a Windows PC
* **Goal**: Achieve real-time responses within 1-2 seconds, u
sing all three models. For context, I start streaming audio and compute 2 seconds of audio in 3 sec but I could change m
odel I guess.

I’m considering the following options, but I’d love your advice on which approach to prioritize:

1. **Do
ckerize the setup and test on H100 cloud GPUs**, then optimize performance once it works on that infrastructure as I cou
ld use it in future for scaling.
2. **Check out the new gguf version of LLaMA 3.1** (like [shenzhi-wang/Llama3.1-8B-Chin
ese-Chat]()), since the previous gguf model was faster or some others.
3. **Revisit LLaMA-cpp** (not `cpp-python`, since
 I ran into dependency issues). I had a better performance with LLaMA-cpp, but the Docker setup confused me. I shifted t
o `transformers` for model management and download automation, but that has slowed things down. I’m considering whether 
to switch back or figure out llama-cpp-python.
4. **Experiment with other models like Qwen or Mistral** for specific rol
es (like role-playing), though I’m unsure if they’ll outperform LLaMA for my use case.
5. **Explore Ollama** for ease of
 use, but I feel LLaMA-cpp might still be better for performance, especially when planning to switch to vLLM or LangChai
n later (although that would require reworking the system, and I’m short on time).

What would you recommend as the fast
est way to prove these models can achieve real-time responses (around 1-2 seconds sequentially? Any suggestions on impro
ving the performance, especially in a production setting, would be appreciated as I need to figure out everything alone.

```
---

     
 
all -  [ langchain setup in vent ](https://www.reddit.com/r/LangChain/comments/1g8snq5/langchain_setup_in_vent/) , 2024-10-24-0913
```
I have been used to setting up a venv for every python project and wondering if anyone has done the same with langchain 
and any lLLM models like local llama  (free) AND OpenAI? 

I believe I should install llama on my machine and  only pyth
on packages (e.g., langchain ) can be installed in venv (via pip install).
```
---

     
 
all -  [ Efficient Web Crawling for Keeping Vector Databases Updated - Seeking Advice ](https://www.reddit.com/r/LangChain/comments/1g8scol/efficient_web_crawling_for_keeping_vector/) , 2024-10-24-0913
```
Hey folks,  
  
We're developing chatbots that answer questions based on domain-specific knowledge for our clients. Our 
current process involves:  
  
1. Crawling the client's website  
2. Uploading the content to a vector database  
3. Uti
lizing this database for AI-powered responses  
  
The challenge we're facing is keeping this information up-to-date. Ou
r clients want real-time accuracy, which theoretically means crawling their websites daily. However, we've encountered s
ome issues:  
  
1. Time: A single website can take several hours to crawl completely.  
2. Cost: We're using APIFY, whi
ch works well but gets expensive when run daily across multiple clients.  
  
We've done some research but haven't found
 a viable solution yet. I'm curious:  
  
- Is anyone facing similar challenges?  
- Has anyone solved this problem effi
ciently?  
- Are there any alternative approaches or tools we should consider?  
  
We're open to any suggestions or ins
ights from the community. Thanks in advance for your help!  
Hey folks,  
  
We're developing chatbots that answer quest
ions based on domain-specific knowledge for our clients. Our current process involves:  
  
1. Crawling the client's web
site  
2. Uploading the content to a vector database  
3. Utilizing this database for AI-powered responses  
  
The chal
lenge we're facing is keeping this information up-to-date. Our clients want real-time accuracy, which theoretically mean
s crawling their websites daily. However, we've encountered some issues:  
  
1. Time: A single website can take several
 hours to crawl completely.  
2. Cost: We're using APIFY, which works well but gets expensive when run daily across mult
iple clients.  
  
We've done some research but haven't found a viable solution yet. I'm curious:  
  
- Is anyone facin
g similar challenges?  
- Has anyone solved this problem efficiently?  
- Are there any alternative approaches or tools 
we should consider?  
  
We're open to any suggestions or insights from the community. Thanks in advance for your help! 
 

```
---

     
 
all -  [ LangChain and LangGraph: My Take and Some Questions ](https://www.reddit.com/r/LangChain/comments/1g8qvz4/langchain_and_langgraph_my_take_and_some_questions/) , 2024-10-24-0913
```
Hey folks, been messing around with LangChain and LangGraph lately. Thought I'd share my thoughts and see if anyone can 
help clear up some stuff.

The Good Stuff
- Loving the YouTube videos and tutorials. They've been a big help.
- Shout ou
t to Harrison Chase. Dude's commitment to making sense of all this LLM chaos is awesome. Appreciate the transparency too
.
- Loved seeing the Open Canvas codebase as well as the LangChain Chat project, learned so much studying them.

Where I
'm Stuck
1. LangGraph as a Platform: What exactly can I expect from it? Can I use it as my main database for chats and u
sers?

2. Keeping User Data Separate: What's the go-to method for this? Kinda crucial if I want to take this to producti
on.

3. Practical Stuff: Trying to do something simple - generate a thread title after the AI responds, then store it wi
th the thread in my database. Serializing BaseMessages works, but it breaks when I try to get them back. Any tips?

4. R
eal-World Use: Anyone actually running a production app on LangGraph? How's it holding up? Does it scale well?

What's Y
our Take?

If you've been hands-on with LangChain or LangGraph, especially in production, I'd love to hear from you. How
 are you handling data storage and keeping user stuff separate? Any pro tips for building solid, scalable apps with thes
e tools?
```
---

     
 
all -  [ C8工卡被拒后，我躺平在庇护所，白天在GitHub上做开源（langchain, llama index...）+写论文投arxiv，晚上躺在床上玩游戏...我后半辈子都这么过了嘛？还是等到川普上台后 ](https://www.reddit.com/r/iwanttorun/comments/1g8ojqf/c8工卡被拒后我躺平在庇护所白天在github上做开源langchain_llama/) , 2024-10-24-0913
```
远程给印度某大学远程授课
```
---

     
 
all -  [ Need help in Approach to Extracting and Chunking Tabular Data for RAG-Based Chatbot Retrieval ](https://www.reddit.com/r/LangChain/comments/1g8nshy/need_help_in_approach_to_extracting_and_chunking/) , 2024-10-24-0913
```
1.	I need to extract data from the tabular structures in the documents. What are the best available tools or packages fo
r this task?

2.	I’m seeking the most effective chunking method after extraction to optimize retrieval in a RAG setup. W
hat would be the best approach?

Any guidance would be greatly appreciated!
```
---

     
 
all -  [ Am I a bad job candidate? ](https://www.reddit.com/r/jobs/comments/1g8msv0/am_i_a_bad_job_candidate/) , 2024-10-24-0913
```
Hello, dear reddit.

For the most part I have been a Software Engineer (generally) for the last 5 years. I have been str
uggling in jobs with no planning whatsoever or even stability besides the one project that I have had the last 5 years a
s a personal colleague project in which I have already provided a first product to clients (this last one job is more li
ke a part time than anything). Currently I have been looking for a job because my most recent job the owner of the compa
ny has pretty much decided to close up the company for a few months, months in which I cannot been unemployed due to the
 current economy in which we are leaving and my responsabilities as a provider in my home.

As you may see in my resume.
 I'm really profficient on JS/TS technologies and Python alike and I'm someone very well disposed to work on new stuff d
ue to some academical knowledge on it like AI/ML with the usage of technologies like Langchain an so on and even if I ha
ve share my profile to several job positions that I may have that match to my profile I keep getting rejected with no fe
edback whatsoever leaving me completely clueless on what to do for the sake of improving my professional profile.

My in
tention on making this posts is pretty much ask for some councelling. I made you the 'anonymous' version of my personal 
Resume just for you to have a sneek peak on my profile and let you provide me your opinion to double check if ever happe
ns that my profile could be look a discartable one to any recruiter or even recruitment systems that maybe out there. I'
m up to receive from harsh critics to none just for the sake to know what I could improve of it ever happens that I'm ma
ybe looking in the wrong platforms or if I'm being targeted as non recommended candidate just for my country of origin.


Thank you beforehand and bless to you if you have at least look at my post.
```
---

     
 
MachineLearning -  [ [D] How are folks building conversational Retrieval Augmented Generation apps ](https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/) , 2024-10-24-0913
```
I've read through various resources such as:  
- [https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/](htt
ps://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/)  
- [https://python.langchain.com/docs/tutorials/qa\_cha
t\_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)  
- [https://langchain-ai.github.io/langgraph
/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) 
 
- [https://docs.llamaindex.ai/en/stable/module\_guides/deploying/chat\_engines/](https://docs.llamaindex.ai/en/stable/
module_guides/deploying/chat_engines/)  
- [https://huggingface.co/datasets/nvidia/ChatRAG-Bench](https://huggingface.co
/datasets/nvidia/ChatRAG-Bench) 

But these feel overly reductive, since they don't address complexities like:  
1) when
 to retrieve vs. just respond immediately to reduce latency  
2) rely on existing context previously retrieved in the co
nversation instead of retrieving again at the current turn  
3) partition LLM context between retrieved information and 
past conversation history.

I'm sure some teams already have good systems for this, would appreciate pointers!
```
---

     
 
MachineLearning -  [ Built a web agent which call fill Google forms based on the user details [P] ](https://www.reddit.com/r/MachineLearning/comments/1fozud5/built_a_web_agent_which_call_fill_google_forms/) , 2024-10-24-0913
```
GitHub repo : [https://github.com/shaRk-033/web-agent](https://github.com/shaRk-033/web-agent)

Tried to solve it using 
two approaches:

# 1: Basic Scraping and Filling

This is the straightforward approach. The agent scrapes the form’s HTM
L and uses fixed XPaths to find and fill in the required fields.

* It pulls the form’s HTML, locates the fields with se
t XPaths, and inputs the answers. It’s a direct and simple method.
* If the form changes or an element isn’t where it’s 
expected, the process can fail and may need manual adjustments.

[basic approach](https://preview.redd.it/5e8g4a1k4xqd1.
png?width=1055&format=png&auto=webp&s=d8e984e4feaee2f0453b08c8696768c40a2a5c20)

2. Using LangChain Agents and tool call
ing

* LangChain Agent**:** The agent handles everything by using the LLM’s reasoning to decide what to do next, includi
ng generating those tricky XPaths.
* Error Handling**:** If something goes wrong (like an element not found), the agent 
tries again with better XPaths until it gets the job done.

[using langchain agents](https://preview.redd.it/948i88pl4xq
d1.png?width=782&format=png&auto=webp&s=ed1e6c19efec9f4cbbbd6ab5a22558f221cf745f)

Any recommendations to improve this w
ould be welcome. Also, if anyone has ideas on building similar web agents to automate other tasks, it would be great to 
hear them. :)
```
---

     
