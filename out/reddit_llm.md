 
all -  [ what should use to bulid Saas for chating with static 50K document's , Chatgpt Api ? or Langchain ?  ](https://www.reddit.com/r/PromptEngineering/comments/18tvtrh/what_should_use_to_bulid_saas_for_chating_with/) , 2023-12-30-0909
```
hello folks   
i hav an idea and i want start to build it but before i have question based on the nature of the project 
and data   
what should use to bulid it ?   
when the data is static and its contains 50K document's  ,  
should i use C
hatgpt Api ?   
or Langchain ?   
or lamaindex ? 
```
---

     
 
all -  [ what should use to bulid Saas for chating with static 50K document's , Chatgpt Api ? or Langchain ?  ](https://www.reddit.com/r/LangChain/comments/18tvsop/what_should_use_to_bulid_saas_for_chating_with/) , 2023-12-30-0909
```
hello folks 

&#x200B;

i hav an idea and i want start to build it but before i have question based on the nature of the
 project and data 

&#x200B;

what should use to bulid it ? 

when the data is static and its contains 50K document's  ,


should i use Chatgpt Api ? 

or Langchain ? 

or lamaindex ? 
```
---

     
 
all -  [ Backend Info ](https://www.reddit.com/r/SillyTavernAI/comments/18trfh6/backend_info/) , 2023-12-30-0909
```
Hi, I wanted to ask if silly tavern uses langchain and MemGPT for its responses.
```
---

     
 
all -  [ Open Source for Large Langage Model ](https://www.reddit.com/r/LangChain/comments/18tqmjm/open_source_for_large_langage_model/) , 2023-12-30-0909
```
Hello community ü§ó

How do we ensure the utmost protection of sensitive data prior to processing with advanced Large Lang
uage Models like ChatGPT 4, Llama 2, or Mistral AI? üõ°Ô∏è
```
---

     
 
all -  [ Can't decide on infrastructure for my RAG ](https://www.reddit.com/r/LocalLLaMA/comments/18tppmv/cant_decide_on_infrastructure_for_my_rag/) , 2023-12-30-0909
```
Hi everyone,

I'm stuck deciding the infrastructure for my production RAG chat.I am trying to decide between using a Fas
tapi server in python, or try to create everything in the Nextjs project with Typescript.

My thoughts so far:

1. There
 is no ready-made Hybrid-search for Supabase in Python (but there is in JS)
2. The more advanced RAG features seem to be
 released in the Python version of Langchain first. like Cohere Reranking, Hyde, Query-expansion etc.
3. I already have 
the setup for the RAG in langserve, but I'm struggling working out how to keep a good chat history integrated against th
e nextjs frontend and the langserve server.
4. I'm leaning towards Supabase pgvector as my vector storage, since I feel 
it's more cost-effective and safe in terms of control (the RAG chat will include that the user can upload files, and mix
 a lot of different businesses on the same index in Pinecone doesn't seem like the best approach, maybe I'm wrong?)

&#x
200B;

Would appriciate some feedback so I can make the decision and move forwards.
```
---

     
 
all -  [ Upgraded Eleven Labs + OpenAi Chatbots & GPTs ](https://v.redd.it/9djfshlw499c1) , 2023-12-30-0909
```

```
---

     
 
all -  [ Can't decide on infrastructure for my RAG ](https://www.reddit.com/r/ArtificialInteligence/comments/18tpo4d/cant_decide_on_infrastructure_for_my_rag/) , 2023-12-30-0909
```
Hi everyone,  
I'm stuck deciding the infrastructure for my production RAG chat.  
I am trying to decide between using a
 Fastapi server in python, or try to create everything in the Nextjs project with Typescript.  
My thoughts so far:  
1)
 There is no ready-made Hybrid-search for Supabase in Python (but there is in JS)  
2) The more advanced RAG features se
em to be released in the Python version of Langchain first. like Cohere Reranking, Hyde, Query-expansion etc.  
3) I alr
eady have the setup for the RAG in langserve, but I'm struggling working out how to keep a good chat history integrated 
against the nextjs frontend and the langserve server.  
4) I'm leaning towards Supabase pgvector as my vector storage, s
ince I feel it's more cost-effective and safe in terms of control (the RAG chat will include that the user can upload fi
les, and mix a lot of different businesses on the same index in Pinecone doesn't seem like the best approach, maybe I'm 
wrong?)  
Would appriciate some feedback so I can make the decision and move forwards.
```
---

     
 
all -  [ Can't decide on infrastructure for my RAG ](https://www.reddit.com/r/LangChain/comments/18tpbcb/cant_decide_on_infrastructure_for_my_rag/) , 2023-12-30-0909
```
Hi everyone,

I'm stuck deciding the infrastructure for my production RAG chat.

I am tyrying to decide between using a 
Fastapi server (langserve) in python, or try to create everything in the Nextjs project with Typescript.

**My thoughts 
so far:**

1) There is no ready-made Hybrid-search for Supabase in Python (but there is in JS)

2) The more advanced RAG
 features seem to be released in the Python version of Langchain first. like Cohere Reranking, Hyde, Query-expansion etc
.

3) I already have the setup for the RAG in langserve, but I'm struggling working out how to keep a good chat history 
integrated against the nextjs frontend and the langserve server.

4) I leaning towards Supabase pgvector as my vector st
orage, since I feel it's more cost-effective and safe in terms of control (the RAG chat will include that the user can u
pload files, and mix a lot of different businesses on the same index in Pinecone doesn't seem like the best approach, ma
ybe I'm wrong?)

Would appriciate some feedback so I can make the decision and move forwards.
```
---

     
 
all -  [ Issues for ChatBot with Persona ](https://www.reddit.com/r/LocalLLaMA/comments/18tncqj/issues_for_chatbot_with_persona/) , 2023-12-30-0909
```
So , I'm a beginner in LLMs. I'm working on creating a chatbot that can mimic a person based on his information (the dat
a contains info like his likes , dislikes and relationships and others)

I tried creating a synthetic data using gpt wit
h query response pairs and finetuned using autotrain which didn't work out.

Now , I tried using langchain and gave a te
xt file with the person data and then also a  prompt template asked it to roleplay the person using the context in the d
ata. This works but it reverts back to question like 'what do u think about ai' , where it says as a ai language model ,
 it can't answer it.

I used WizardLM Llama model. I think issues with context window can come as well if the data is bi
g. 

How can I work on this idea? Is there any ways to improve upon the langchain idea or should I have used a different
 approach altogether?
```
---

     
 
all -  [ Seeking Advice: structuring and managing text data for a chatbot. ](https://www.reddit.com/r/LocalLLaMA/comments/18tmsz9/seeking_advice_structuring_and_managing_text_data/) , 2023-12-30-0909
```
 This is my first post on this subreddit, so hello everyone! ;)

I'm currently working on a locally hosted chatbot for a
n HR division to assist with their daily tasks. I'm using LLaMA 2, RAG, and LangChain as orchestrator. However, I'm faci
ng a challenge due to my limited experience in dealing with a large number of text documents‚Äîspecifically, how to effect
ively store and organize them and then integrate them into a vector database. Simply storing data on physical disks does
n't provide me with the necessary version control for my data (and maybe analytics?). Moreover, I'm keen on implementing
 version control for the vector database itself. Do any of you have tips on how commercial projects typically structure 
such initiatives and what tools they employ?

Thank you all in advance!
```
---

     
 
all -  [ Splitting string data in order to apply load_qa_chain ](https://www.reddit.com/r/LangChain/comments/18tkz5v/splitting_string_data_in_order_to_apply_load_qa/) , 2023-12-30-0909
```
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=80) 
    chunks = text_splitter.split_
text(PolicyScheduleRaw) 
    
    for chunk in chunks:
        print(chunk)
        print('-----------------------------
----------------------') 

 So I have the code above that splits the string PolicyScheduleRaw into chunks. This above st
ep completes successfully. However, when I then proceed to execute the below code, I get the error message:  `'str' obje
ct has no attribute 'page_content'`  


    llm = ChatOpenAI(temperature=0, openai_api_key='SECRET') question = 'Are my 
Bosch power tools covered?' chain = load_qa_chain(llm, chain_type='stuff')  result = chain.run(input_documents=chunks, m
odel='gpt-3.5-turbo-1106',question=question)

 If I change 

    split_text

to 

    create_documents

in the first cod
e block (I read somewhere that this could be the cause), then the chunk size of 1,000 no longer applies and for some rea
son the text is split per each character.   


How can I fix this?
```
---

     
 
all -  [ What is a intuitive way to get used to LCEL ](https://www.reddit.com/r/LangChain/comments/18tkbvr/what_is_a_intuitive_way_to_get_used_to_lcel/) , 2023-12-30-0909
```
I find the syntax weird (tbh just saw it for the first time). What is a intuitive way I apply this syntax, the documenta
tion is not super clear.
```
---

     
 
all -  [ Langchain citing sources when answers not from context (LCEL vs RetrievalQAWithSourcesChain) ](https://www.reddit.com/r/LangChain/comments/18thci6/langchain_citing_sources_when_answers_not_from/) , 2023-12-30-0909
```
I'm following the langchain example [here](https://python.langchain.com/docs/use_cases/question_answering/#adding-source
s) that is used to cite sources. It works great when the answer actually comes from the context.  But a simple query suc
h as 'hello' will answer with 'Hello!' with sources. So even irrelevant sources are returned.  Is there anyway to modify
 the LCEL provided by langchain to not return sources if it doesn't find an answer from them?

I've also tried **Retriev
alQAWithSourcesChain** and it works better when returning sources, but it's not returning any metadata - only the link. 
Additionally, the quality of the results vs LCEL (human evaluation) seems to fall short. The only way I know change the 
quality of the results is to use custom PromptTemplates, but still not sure how to get the metadata.
```
---

     
 
all -  [ [R][P] Autogen + Langchain Tools + Local LLM doesn't work. ](https://www.reddit.com/r/MachineLearning/comments/18tex1j/rp_autogen_langchain_tools_local_llm_doesnt_work/) , 2023-12-30-0909
```
Hey folks, 

So I'm playing around with the agent framework Autogen and I'm trying to create agents by providing it cust
om tools to use. These custom tools are defined in the langchain framework. Furthermore, I am using open source LLM mode
ls like Mistral, LLAMA, Mixtral etc.

In my experience, I have been unable to get the Autogen+LocalLLM framework to iden
tify the right tools to use given the prompt. However it does a fantastic job with the GPT model. 

Please note that my 
goal is for the agent to mandatorily use the tools provided and not come up with its own code. And the agent should figu
re out the right tool to use. 

I have been very explicit with my prompting, despite which I am unable to get this to wo
rk.

Any thoughts and suggestions? Please let me know ! Please share your experiences as well. Cheers !
```
---

     
 
all -  [ Retrieval augmented generation: Basics and production tips ](https://www.reddit.com/r/LangChain/comments/18te9m1/retrieval_augmented_generation_basics_and/) , 2023-12-30-0909
```
Published a blog post with explanation of RAGs and some techniques we have seen work in production for effective pipelin
es. Check it out at: [https://deepchecks.com/retrieval-augmented-generation-best-practices-and-use-cases/](https://deepc
hecks.com/retrieval-augmented-generation-best-practices-and-use-cases/) 
```
---

     
 
all -  [ Suggestions wanted: Local VectorDB without langchain ](https://www.reddit.com/r/vectordatabase/comments/18tbdui/suggestions_wanted_local_vectordb_without/) , 2023-12-30-0909
```
Im using ChromaDB python package currently and I don't use langchain.    
The limited documentation is really tough for 
me to work through and I feel as though I'm missing out on easier to use, more intuitive, or more feature rich vector da
tabase solutions that I can run on my local PC without docker. 
Pls help ü§ó
```
---

     
 
all -  [ Langchain or Train model on documents? ](https://www.reddit.com/r/learnmachinelearning/comments/18t5gi7/langchain_or_train_model_on_documents/) , 2023-12-30-0909
```
Hello im currently doing a personal project I want to do something similar to Grammarly but specific to Cookbooks. Lets 
say a user is writes down steps for a recipe and enters the text to the NLP model  


The model in turn will tell the us
er if there is anything that is not to standard based on the documents. Let say they were making a cake and didn't add e
gg whites the model would highlight this and suggest an improvement.  This might not be the best example but my main obj
ective is to get something similar to Grammarly but based on documents not sentence structure    


I know this can be d
one both ways but I'm not sure for my purposes would be best?  


Sorry if if this is not in the correct subreddit. Here
's what I mean by Langchain in case im using the wrong term [https://www.youtube.com/watch?v=aywZrzNaKjs](https://www.yo
utube.com/watch?v=aywZrzNaKjs)
```
---

     
 
all -  [ Do we really need LCEL? ](https://www.reddit.com/r/LangChain/comments/18t3jn9/do_we_really_need_lcel/) , 2023-12-30-0909
```
Comment down below what you guys think about it. I think there is a huge push from the Langchain core dev community to p
ush it forward but I'm not sure if the larger community really welcomes this. I may be wrong here, but just curious.

[V
iew Poll](https://www.reddit.com/poll/18t3jn9)
```
---

     
 
all -  [ ConversationAgent + Llama-2 70B 4b - outputparser error llm not outputting prefix ](https://www.reddit.com/r/LangChain/comments/18t2u4g/conversationagent_llama2_70b_4b_outputparser/) , 2023-12-30-0909
```
Hello everyone.

We are still getting to grips with langchain, but we were able to get a phind-34B llm outputting the pr
efix using prompt tempate, works very well with tools. Problem is that we want to finetune a llama-2 70B using qlora to 
a 4bit, and for the life of us, we cannot get the llm to output the prefix in its responses , for example, we can't get 
it to respond as AI;  so the output parser picks it up. We tried all kinds of stuff in the prompt template like 'ALWAYS 
respond as AI: for example 'AI: (your response here)' which seems to work really well with the phind 34b (TheBloke awq v
ersion). 

Can anyone give us a hint of how to get langchain ConversationChatAgent (for conversation and tool usage) to 
work with a llama-2 based llm? We've been at it for 4 days, and no luck. 

Thanks in advance.
```
---

     
 
all -  [ Resume review. Losing my hopes of being the cog in a new wheel. ](https://i.redd.it/zoc7w4lv529c1.jpeg) , 2023-12-30-0909
```
I took a 50% paycut from my past organization because the work life balance was brutal and a scandal had surfaced wiping
 out a lot of my previous team. New org is slower than a kid with autism.  As a result, I haven't received any payraise 
in 2 years now and I really want to switch. Current TC: 17lpa. Looking for >30 LPA fixed pay roles.  I don't have a huge
 risk appetite so I turn down startups for the most part. I have got some really shitty job offers like one company tryi
ng to hire me for HR Analytics.  I don't even mind working 70 hours a week. I keep getting rejected and don't even reach
 interview stage.  Everywhere I apply.  I'm doing a hybrid masters from an old IIT. Topped my class.  I want to leverage
 that too.  
.
Is my resume really keyword stuffed or bad? Is there something wrong here? Please help.
```
---

     
 
all -  [ [Need Help] Langchain agent not executing properly in Celery worker ](https://www.reddit.com/r/LangChain/comments/18sw41b/need_help_langchain_agent_not_executing_properly/) , 2023-12-30-0909
```
I am working on implementing LangChain Agents in my Python project I am running this project using docker compose. In my
 project I am using Celery worker and have multiple worker services which executes from the queue. The entire setup is w
orking as expected.

One of these workers in agent worker where I have configured LangChain Agent. I have created a func
tion where I am loading tools, initializing agent and passing agent input.

When I run this setup as a simple standalone
 python project, everything runs fine and I see proper agent execution and output. But in the production environment I a
m using Celery workers running as Docker container. The above setup is run as a Celery worker. The setup is quite simple
 and straightforward.

But when I run this setup in as Celery worker, I observe that it **loads the tool and initializes
 the agent properly**. It also enters in the AgentExecutor chain **but fails to execute the tool function using the para
meter returned in the LLM response**.   


I have added more details, code the logs in the Github issue but haven't got 
any proper response yet: [https://github.com/langchain-ai/langchain/issues/15220](https://github.com/langchain-ai/langch
ain/issues/15220)  


Any help is highly appreciated. Happy Holidays!
```
---

     
 
all -  [ need help improving context quality to make a code assistant ](https://www.reddit.com/r/LocalLLaMA/comments/18sv8sw/need_help_improving_context_quality_to_make_a/) , 2023-12-30-0909
```
I tried to make a coding assistant some time ago. It should read my repo and point out and suggest code quality improvem
ents, suggest alternate design patterns, point out where complexity is getting out of hand, or even suggest alternate va
riable names.

However, I noticed that when I ask a question like the included 'What is the class hierarchy', or even so
mething specific about a specific struct or a file, the database does NOT include that struct or file in the context sen
t to the model! Chroma was terrible, but there was an online vector store (was it Activeloop's Deeplake?) that was much 
better at retrieving code chunks related to my question. Now, of course, the code doesn't run anymore (maybe I didn't pi
n some libraries in requirements.txt) so I can't show you the Documents the db loaded, but trust me they were terrible.


A few questions:
1. is Chroma really bad and I should use another vector db or do I have to tune it to get it to work? 
After all it's just a database storing numbers... maybe I have to try something other than 'mmr'?
2. Is there a better w
ay of getting context to the model? Right now the best I've found is https://cursor.sh/ but I want something that just r
eads my repo once I make a git commit and suggests any blind spots or points of improvement.
3. Just looking at my code 
real quick - any avenues I could improve context for the model?  Are embeddings better than simply including the code in
 the context? Perhaps embeddings take up less context space?

```
parser = argparse.ArgumentParser(description='Process 
some integers.')
parser.add_argument('repo_path', type=str, help='path to the repository')
parser.add_argument('--model_
path', type=str, default='llama-2-7b-32k-instruct.Q5_K_M.gguf', help='path to the model')
parser.add_argument('--llama',
 action='store_true', help='use LlamaCpp instead of ChatGPT')

args = parser.parse_args()

repo_path = args.repo_path
mo
del_path = args.model_path

# Load
loader = GenericLoader.from_filesystem(
    repo_path,
    glob='**/*',
    suffixes=
['.js'],
    parser=LanguageParser(language=Language.JS, parser_threshold=500) # Currently, the supported languages for 
code parsing are Python and JavaScript. 
    # Source https://api.python.langchain.com/en/latest/document_loaders/langch
ain.document_loaders.parsers.language.language_parser.LanguageParser.html
)
documents = loader.load()
# print('# of docu
ments', len(documents))

go_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.JS, 
             
                                                  chunk_size=2000, 
                                                    
           chunk_overlap=200)
texts = go_splitter.split_documents(documents)
# print('# of split documents (texts)', len
(texts))

def chatgpt(texts):
    db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))
    retriev
er = db.as_retriever(
        search_type='mmr', # Also test 'similarity'
        search_kwargs={'k': 8},
    )

    llm
 = ChatOpenAI(model_name='gpt-4')
    return db, retriever, llm

if args.llama:
    db, retriever, llm = llama(texts)
el
se:
    db, retriever, llm = chatgpt(texts)

memory = ConversationSummaryMemory(llm=llm,memory_key='chat_history',return
_messages=True)
qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)

print('going to ask
 some questions now')
questions = [
    'What is the class hierarchy?',
]


while True:
    question = input('Ask a ques
tion: ')
    if not question:
        break
    documents = retriever.get_relevant_documents(question)
    pp(documents)

    result = qa(question)
    print(f'-> **Question**: {question} \n')
    print(f'**Answer**: {result['answer']} \n')

```
```
---

     
 
all -  [ NL2SQL query engines ](https://www.reddit.com/r/developersIndia/comments/18sss8h/nl2sql_query_engines/) , 2023-12-30-0909
```
I've be assigned a task to research about how we can incorporate the NL2SQL query engines to run queries over the databa
se using the LLMs. So far, I've seen about LangChain and LlamaIndex but haven't got any solid grip over this idea. If yo
u have any experience regarding this, please help me out with any kind of knowledge source you have. Thank you.
```
---

     
 
all -  [ Why is Tuna designed like it is? ](https://www.reddit.com/r/LangChain/comments/18srt00/why_is_tuna_designed_like_it_is/) , 2023-12-30-0909
```
I came across Tuna, a tool for generating Q&A data from plain text files: [https://blog.langchain.dev/introducing-tuna-a
-tool-for-rapidly-generating-synthetic-fine-tuning-datasets/](https://blog.langchain.dev/introducing-tuna-a-tool-for-rap
idly-generating-synthetic-fine-tuning-datasets/)

From what I understand, Tuna generates question-answer pairs for each 
individual section of text, rather than creating complex cross-topic question-answer pairs that encompass the entire doc
ument. There is the 'Multi chunk' feature where it can take up to 5 chunks maximum, but still not taking into account an
ywhere near the entire document or multiple documents.

My point is that research, e.g.: the LIMA paper, suggests that t
he model will perform best and converge more quickly when provided with complex, high-quality question-answer data. Howe
ver, is it possible that the model could converge just as well with less intricate data, provided that there is a suffic
ient amount of it?

I see that Andrew Gao developed this, I will try reach out directly to him.
```
---

     
 
all -  [ Hosting guidance in deployment of web service ](https://www.reddit.com/r/render_/comments/18sr8e2/hosting_guidance_in_deployment_of_web_service/) , 2023-12-30-0909
```
I have a very general question and would like to get any insight even if it does not solve the problem.

My code works j
ust fine on VSCODE editor, however the error I get during deployment is:  
ModuleNotFoundError: No module named 'langcha
in.memory'

Note that python, langchain library, code syntax everything is ok, but I think I need to specify something i
n the build commands or environment variable or any place else on the options given at the stage of initial deployment i
n Render platform.  
 
```
---

     
 
all -  [ RAG + Prompting ](https://www.reddit.com/r/LangChain/comments/18spba0/rag_prompting/) , 2023-12-30-0909
```
Hey everyone, just looking for some guidance here. I've built a RAG bot on pinecone but its.. not great. It sometimes an
swers questions incorrectly or repeats itself inappropriately. Thoughts? Thanks in advance
```
---

     
 
all -  [ Looking for a langchain alternative ](https://www.reddit.com/r/LangChain/comments/18skpot/looking_for_a_langchain_alternative/) , 2023-12-30-0909
```
Going to production with a custom RAG application using chromadb and multiple document types. Langchain has been working
 well but need to understand what other alternatives people have been using? We ingest a lot of documents for which lang
chain seemed to have good support. Other frameworks not so much.
```
---

     
 
all -  [ ML/Gen AI expert needed ](https://www.reddit.com/r/pune/comments/18sj4ma/mlgen_ai_expert_needed/) , 2023-12-30-0909
```
Ladies and gents,

I was a part of maritime angel fund a small VC fund making investments in pre seed and series A start
ups in the tech space of the Oil and gas/supply chain/logistics companies in London and Singapore.

I have recently move
d to Pune and will be here for around 4 months.

I need a ML/Gen AI engineer who has used langchain and understands how 
to get python apps developed from open source LLMs and calling some APIs.
```
---

     
 
all -  [ Langchain + pgvector, need help. ](https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/) , 2023-12-30-0909
```
I ingested a bunch of documents into a postgres table, where 1 column is a vector embedding (pgvector extension) and the
 rest of them are a bunch of metadata and 1 column of the original text.  


I want to use this as a retrieval source in
 a RAG application. I'm finding it hard to use Langchain's [pgvector](https://python.langchain.com/docs/integrations/vec
torstores/pgvector) feature - since I didn't create the table itself using langchain.  


Does anyone have examples or p
ointers to using a separately generated vector table with langchain's pgvector feature? Or is using a custom retriever m
y only path forward?
```
---

     
 
all -  [ Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't ](https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/) , 2023-12-30-0909
```
    from langchain.output_parsers import ResponseSchema, StructuredOutputParser
    from langchain_core.prompts import P
romptTemplate
    from langchain.memory import ConversationSummaryBufferMemory
    
    
    checker_tmpl = '''
    # Ta
sk Description:
    As a customer support representative, your role is to provide accurate and helpful responses to cust
omer inquiries. Use the provided context to understand the customer's issue and answer their questions directly. Avoid a
ny extraneous information or explanations that are not directly relevant to the customer's query.
    
    {format_instr
uctions}
    
    
    # Given Context:
    {context}
    
    
    #Chat History:\n\n{chat_history} \n\n
    
    # Cus
tomer's Question:
    {question}
    
    # Your Response:
    [Please type your answer here, ensuring it is concise, re
levant, and directly addresses the customer's question based on the given context.]
    
    '''
    
    
    checker_r
esponse_schemas = [
        ResponseSchema(
            name='requires_customer_support_contact',
            descriptio
n='Indicates whether the user needs to contact customer support. Set to True if the context does not sufficiently addres
s the user's issue and further assistance is needed. Set to False if the provided context is adequate to answer the user
's query.',
            type='boolean'
        ),
        ResponseSchema(
            name='contextual_answer',
        
    description='Provides a direct answer to the user's question, utilizing the given context. The response should be co
ncise, accurate, and specifically tailored to address the query based on the context provided.',
        ),
    ]
    
 
   
    
    check_output_parser = StructuredOutputParser.from_response_schemas(checker_response_schemas)
    
    resum
e_checker_prompt = PromptTemplate(
        template=checker_tmpl,
        input_variables=['chat_history','context', 'qu
estion'],
        partial_variables={
            'format_instructions': check_output_parser.get_format_instructions()
 
       }
    )
    
    openai = ChatOpenAI(model_name='gpt-4-0613')
    
    memory = ConversationSummaryBufferMemory(l
lm = openai, memory_key='chat_history')
    
    # check_prompt_str = resume_checker_prompt.format(chat_history = memory
.get_memory(), context=context, question = 'I am 80 yeard old guy suggest some best plans' )
    
    
    chat_chain = 
LLMChain(
        llm=openai,
        prompt=resume_checker_prompt,
        verbose=True,
        memory=memory,
    )


&#x200B;

https://preview.redd.it/lfitazcosx8c1.png?width=1615&format=png&auto=webp&s=5a4e30848189e74d03baa4fd4790a58d81
a86333
```
---

     
 
all -  [ Trying to get into a prestigious AI master's program and I'm wondering what projects would be impres ](https://www.reddit.com/r/learnmachinelearning/comments/18seo0l/trying_to_get_into_a_prestigious_ai_masters/) , 2023-12-30-0909
```
I have about 2-3 more months to bolster my resume before the deadline for my master's applications, and I am wondering w
hat projects would be impressive to the professors reviewing my application. Currently, I have a big project using LangC
hain that I did for a company and a project analyzing shooting data and making predictions. I feel like professors will 
automatically discount my application because I majored in Information Systems, not CS or math, so I want to add somethi
ng very impressive to make up for that.
```
---

     
 
all -  [ Why do the langchain docs feel so all over the place? ](https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/) , 2023-12-30-0909
```
Been using langchain for a bit but these docs just annoy the hell out of me now. Is there any possible refactor that cou
ld help maybe or it‚Äôs just too bloated now? Seems like 5 ways to do 1 thing at times.
```
---

     
 
all -  [ RAG with Langchain Huggingface ](https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/) , 2023-12-30-0909
```
I am trying to implement a chatbot where a user can chat about a document. I am trying to implement it where I read the 
document through Langcahin -> create embedding and store them (use a huggingface model for embedding) in Chroma -> answe
r question where I use a huggingface model for LLM. I tried to use a QuestionAnswering Huggingface model, but I got an e
rror. Then I looked at the github code of Langchain and it says they only support few model types which are text-generat
ion, text2text-generation and summarization. 
Has anyone been able to implement this?
```
---

     
 
all -  [ chatgpt4 api dont work for me ](https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/) , 2023-12-30-0909
```
guys i have problem with my chatgpt4 acc i never use the api before but when i call for useing it he gives me this error
 msg ' You exceeded your current quota, please check your plan and billing details. For more information on this error, 
read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors. ' even if i have already 18 dollas as cre
dit and the graph said i never use it how can i solve this problem and can someone give me apt key  i really need it cuz
 i need to use it in univ project 

thanks to all of you .
```
---

     
 
all -  [ Turing Technical call ](https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/) , 2023-12-30-0909
```
 

I just received a call from Turing for a job, but it mentions a very broad domain, including:

Communication Skills


Machine Learning and Deep Learning Models

Python and large Language Models (LLMs)

AI Problem Solving

My call is in on
e day, and I'm confused about which concepts I should revise and the people in the CC are mostly Non-technical.
```
---

     
 
all -  [ Processing complex word documents with equations. ](https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/) , 2023-12-30-0909
```
I have a project that requires to extract data from complex word documents.

Each document is composed of a few tables (
10 to 30). In each tables I might have :

* Text
* Mathematical equations
* Images (mostly math graphs).

My initial goa
l is to be able to process the text and equations, I'll leave the images for latter.

So far I haven't been able to retr
ieve the equations in the table. I have tried the following methods:

* langchain + docx2txt : 1 huge blob of text, not 
exploitable at all.
* lagnchain + unstructured in classical mode : same.
* langchain + unstructured + elements : I got a
 list of tables, which is nice, however in both the tables\[1\]\['kwargs'\]\['metadata'\]\['text\_as\_html'\] and tables
\[1\]\['kwargs'\]\['page\_content'\] the equations aren't shown.
* pandoc docs -> html : both the equations and images a
re represented as html **{=html}** and no content is shown.

I suppose that my next best solution is to use OCR like log
ic but I don't really like the idea. Do you have any suggestion ?

&#x200B;
```
---

     
 
all -  [ How to delete an agent? ](https://www.reddit.com/r/LangChain/comments/18s3qre/how_to_delete_an_agent/) , 2023-12-30-0909
```
Hi everyone, I am building a python app that creates a new agent for each user. For security reasons, I need to delete t
he agent completely after use. I have searched everywhere but didn't find a way to do it...
```
---

     
 
all -  [ Simple Local Models Answers Evaluation Colab example (with GPT4, no langchain etc.) ](https://www.reddit.com/r/LocalLLaMA/comments/18s0t6q/simple_local_models_answers_evaluation_colab/) , 2023-12-30-0909
```
Hi, beloved sub!

I have tried to find any ready-to-go simple examples of local model response evaluation with GPT4 but 
have seen only complicated implementations with heavy libraries like langchaing, etc. 

[Here](https://colab.research.go
ogle.com/drive/1sfxCoW2tJd6IJ_psSumGWzNwqEbtbu-4?usp=sharing), I implemented a simple Python LLM answers scoring Colab w
ith GPT4 API and OpenAI functions. You can adapt it to process multiple responses from the local LLM, which could be use
ful for comparing different models. 

It is a reliable approach that I first encountered in the paper '[TinyStories](htt
ps://arxiv.org/abs/2305.07759)' (where a tiny 2.5 **million** parameters model solves a Creative Writing task), and base
d on their approach, I made a new version that our team uses for Creative Writing task evaluation at [littlestory.io](ht
tps://littlestory.io) (kids bedtime stories generation, we have scored like that thousands of stories already).

All sco
res will be given in the valid JSON format. 

**How to use colab:**

1) Specify your text language and past text that yo
u want to score;

2) Specify your API key;

3) Update agent if needed; our mainly focuses on kids' safety and creative w
riting;

4) Batch process answers of your local LLM and save responses.

That's it. It's quite simple

&#x200B;

Previou
s tutorials:

\[[Tutorial](https://www.reddit.com/r/LocalLLaMA/comments/18dohlt/tutorial_use_real_books_wiki_pages_and_e
ven/)\] **Use real books, wiki pages, and even subtitles for roleplay with the RAG approach in Oobabooga WebUI + superbo
oga v2**

\[[Tutorial](https://www.reddit.com/r/LocalLLaMA/comments/17aswq4/tutorial_integrate_multimodal_llava_to_macs/
)\] **Integrate multimodal llava to Macs' right-click Finder menu for image captioning (or text parsing, etc) with llama
.cpp and Automator app**

\[[Tutorial](https://www.reddit.com/r/LocalLLaMA/comments/15snlv1/tutorial_simple_soft_unlock_
of_any_model_with_a/)\] **Simple Soft Unlock of any model with a negative prompt (no training, no fine-tuning, inference
 only fix)**

\[[Tutorial](https://www.reddit.com/r/LocalLLaMA/comments/13j3747/tutorial_a_simple_way_to_get_rid_of_as_a
n_ai/)\] **A simple way to get rid of '..as an AI language model...' answers from any model without finetuning the model
, with llama.cpp and --logit-bias flag**

**\[**[Tutorial](https://www.reddit.com/r/SteamDeck/comments/12k1d8h/manual_ho
w_to_install_large_language_model_vicuna/)**\] How to install Large Language Model Vicuna 7B + llama.ccp on Steam Deck**

```
---

     
 
all -  [ How does mapReduce work ](https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/) , 2023-12-30-0909
```
Hey guys,

im new to langchain and I'm wondering how the mapreduce function works.  


If I have a chunksize of 10.000 a
nd run the mapreduce function, how can it summarize the chunks with a size of 10.000? I thought the token limit is under
 10.000.  


And when i change the verbose bool to true, it just shows me that it summarize the content of the chunk (wh
ich is 10.000, so I thought it doenst work).  


Im trying to understand the logic but im so confused. 
```
---

     
 
all -  [ LLM taking too long time to respond ](https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/) , 2023-12-30-0909
```
The below code is taking more than 5 min to answer the question, any solution to this?  


llm = CTransformers(model='Th
eBloke/CodeLlama-7B-Instruct-GGUF'**,**  
 model\_file='codellama-7b-instruct.Q5\_K\_M.gguf'**,**  
 \# callbacks=\[Stre
amingStdOutCallbackHandler()\],  
 config={'max\_new\_tokens': **4096,**  
 'context\_length': **4000,**  
 'temperature
': **0.01**})  
agent = create\_csv\_agent(llm**,**  
 'August2023.csv'**,**  
 verbose=True**,**  
 agent\_type=AgentTy
pe.ZERO\_SHOT\_REACT\_DESCRIPTION**,**)

agent.run('how many rows are there?')
```
---

     
 
all -  [ Please review my resume!! ](https://www.reddit.com/r/resumes/comments/18rx3k8/please_review_my_resume/) , 2023-12-30-0909
```
I am a final year student applying for machine learning engineer roles.  
I have improved my resume after getting some a
dvice from this community.  
Please review my resume now

https://preview.redd.it/oswqhzup6t8c1.jpg?width=1275&format=pj
pg&auto=webp&s=3b227d779ab54f96694b2972cdb169044f4aa55d
```
---

     
 
all -  [ What is your strategy for doing inference over a large SQL dataset? ](https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/) , 2023-12-30-0909
```
I'm pretty new to Langchain, but I'm comfortable building RAG applications with 'static' data, such as PDFs, webpages, e
tc. I'm now trying out the SQL toolkit, but I have some questions.

Since you're still limited by the model's context wi
ndow size, it seems that only certain queries make sense. But what if you wanted to find trends or insights from a large
r amount of data, like store orders, or analytics data?

Would you do this in batches and then combine them? Would you v
ectorize the data first? Is it even feasible to do this currently?

I'm very interested to hear how you would approach t
his problem
```
---

     
 
all -  [ Seeking Advice for Building a School Handbook Chatbot Using OpenAI and Vector Databases ](https://www.reddit.com/r/ChatGPT/comments/18rnecd/seeking_advice_for_building_a_school_handbook/) , 2023-12-30-0909
```
 

Hello everyone,

I'm embarking on a project to create a chatbot for my school's handbook, aiming to make it a resourc
e for students to easily access information. As someone relatively new to AI, I'm seeking guidance on implementing this.


My current plan is to use OpenAI as the primary language learning model, focusing on affordability. I am considering i
ntegrating RAG (Retrieval-Augmented Generation) and LangChain for enhanced functionality. However, I'm quite perplexed a
bout choosing an appropriate vector database, as many options appear costly. The goal is to keep this system live and ac
cessible for student usage without breaking the bank.

I'm also looking into open-source embedding models to pair with t
he vector database. Pinecone has caught my attention, but its pricing seems steep for our budget.

Does anyone have reco
mmendations or tips on affordable yet effective tools and strategies for this project? Any insights on vector databases 
suitable for educational use, or ways to optimize cost without compromising quality, would be greatly appreciated.

Than
k you in advance for your help!

(I typed out my problem and had gpt4 fix up the format and wording dont bash me)
```
---

     
 
all -  [ [P] Seeking Advice for Building a School Handbook Chatbot Using OpenAI and Vector Databases ](https://www.reddit.com/r/MachineLearning/comments/18rndcp/p_seeking_advice_for_building_a_school_handbook/) , 2023-12-30-0909
```
Hello everyone,

I'm embarking on a project to create a chatbot for my school's handbook, aiming to make it a resource f
or students to easily access information. As someone relatively new to AI, I'm seeking guidance on implementing this.

M
y current plan is to use OpenAI as the primary language learning model, focusing on affordability. I am considering inte
grating RAG (Retrieval-Augmented Generation) and LangChain for enhanced functionality. However, I'm quite perplexed abou
t choosing an appropriate vector database, as many options appear costly. The goal is to keep this system live and acces
sible for student usage without breaking the bank.

I'm also looking into open-source embedding models to pair with the 
vector database. Pinecone has caught my attention, but its pricing seems steep for our budget.

Does anyone have recomme
ndations or tips on affordable yet effective tools and strategies for this project? Any insights on vector databases sui
table for educational use, or ways to optimize cost without compromising quality, would be greatly appreciated.

Thank y
ou in advance for your help!

(I typed out my problem and had gpt4 fix up the format and wording dont bash me)
```
---

     
 
all -  [ How to name an offline LLM? ](https://www.reddit.com/r/LocalLLaMA/comments/18rlqbw/how_to_name_an_offline_llm/) , 2023-12-30-0909
```
This is probably a really easy answer. Don‚Äôt laugh - I‚Äôm still learning.

I‚Äôm using on offline model with RAG. The data 
is stored in a Chroma DB. It‚Äôs doing its job, answering questions on the data it‚Äôs been training on (pdf books on data e
ngineering, governance and strategy). I‚Äôve named it ‚ÄòEva‚Äô. But how do I get it to respond to that? If I ask it for its n
ame, how do I get it to call itself Eva?

Right now the question gets passed into langchain RetrievalQA with chain_type=
‚Äòstuff‚Äô. 

Do I need to feed it a md file into the Database to ‚Äòtell it about itself‚Äô, or do I pass additional context i
nto the RetrievalQA process on every query?
```
---

     
 
MachineLearning -  [ [D] github repositories for ai web search agents ](https://www.reddit.com/r/MachineLearning/comments/18dhtm4/d_github_repositories_for_ai_web_search_agents/) , 2023-12-30-0909
```
Do you know of any github repositories that either help with building a web search ai agent or that has a good one?

git
hub repositories that I saw so far but have not yet tried :

- langchain (the WebResearchRetriever and weblangchain for 
example (have not tried either) )
- autogpt
- gpt-researcher

[Edit: changed researchgpt to gpt-researcher]
```
---

     
 
MachineLearning -  [ [P] flex-prompt: a flexible prompt rendering engine that ensures you'll never exceed your LLM's cont ](https://www.reddit.com/r/MachineLearning/comments/18d581q/p_flexprompt_a_flexible_prompt_rendering_engine/) , 2023-12-30-0909
```
When working with LLMs, I frequently experience *token agony*.

[Error: This model's maximum context length is 4097 but 
you are trying to push in all of War and Peace, you imbecile](https://preview.redd.it/nldj0qva4s4c1.png?width=1348&forma
t=png&auto=webp&s=b16af79d83f329db1b77b32ed621f0138d7cc04d)

Perhaps you've experienced it too! The issue is particularl
y pronounced with retrieval augmented pipelines, since you have potentially quite a large set of documents which you cou
ld perhaps include in the prompt if only you knew how big it could be.

I got tired of hacking around this headache, so 
I wrote `flex-prompt` to address it. I wish I didn't have to. Perhaps someone can point me to a better solution! But I c
ouldn't find one, so alas, here it is.

`flex-prompt` provides a basic layout and component model to help you describe h
ow you want the pieces of your prompt to grow and shrink and a token-aware renderer which renders your prompt to fit you
r model's window.

[Github](https://github.com/queerviolet/flex-prompt), [Intro to flex prompt colab](https://colab.rese
arch.google.com/github/queerviolet/flex-prompt/blob/main/doc/intro_to_flex_prompt.ipynb)

# Quick examples

You can just
 `render(Flex(...))`, and flex prompt will fit the prompt into the context window, and tell you how many tokens are left
 over for the response:

    from flex_prompt import render, Flex, Expect
    rendered = render(
        Flex([
        
  'Given the text, answer the question.',
          '--Text--',
          WAR_AND_PEACE,
          '--End Text--',
     
     'Question: What's the title of this text?',
          'Answer:', Expect()
        ], join='\n'),
        model='tex
t-davinci-002')
    
    # rendered.output is the string to send to the model
    # rendered.max_response_tokens is how 
many tokens you can
    #   request in response without exceeding the model's context window
    print(rendered.output, 
rendered.max_response_tokens)

More typically, you'll want to define a prompt which takes parameters. To do this, you ca
n create a class (probably a dataclass) which derives `Flexed`:

    from flex_prompt import Flexed, Expect
    from dat
aclasses import dataclass
    
    @dataclass
    class Ask(Flexed):
      text: str
      question: str
      answer: s
tr | Expect = Expect()
      instruct: str = 'Given a text, answer the question.'
    
      flex_join = '\n' # yielded 
items will be joined by newlines
      def content(self, _ctx):
        if self.instruct:
          yield 'Given the tex
t, answer the question.'
          yield ''
        yield '-- Begin Text --'
        # note: we're using `Flex` here jus
t to attach a flex_weight
        # to the text, telling the renderer we'd like more space for the
        # text than a
nything else.
        yield Flex([self.text], flex_weight=2)
        yield '-- End Text --'
        yield 'Question: ', 
self.question
        yield 'Answer: ', self.answer

The renderer works much as you might expect. You can \`yield\` anyt
hing which you can pass to the top-level render function, including other components, creating a whole tree.

Note that 
the component above can be used to render both the actual prompt and examples. Examples simply have an `answer`. This is
 useful for experimenting with different ways of structuring a prompt while ensuring that all the examples we present to
 the LLM are in the same format.

# LangChain and Haystack Integrations

Flex prompt doesn't really care how you execute
 your prompt. For convenience, `render(model=)` does accept both LangChain and Haystack models:

    ask_tolstoy = Ask(t
ext=WAR_AND_PEACE, question='Who wrote this?')
    
    # Using LangChain
    from langchain.llms import OpenAI
    lc_l
lm = OpenAI()
    rendering = render(ask_tolstoy, model=lc_llm)
    print(lc_llm(rendering.output, max_tokens=rendering.
max_response_tokens))
    
    
    # Using Haystack
    from haystack.nodes import PromptModel
    
    hs_llm = Prompt
Model(model_name_or_path='text-davinci-002', api_key=os.environ['OPENAI_API_KEY'])
    rendering = render(ask_tolstoy, m
odel=hs_llm)
    print(hs_llm.invoke(rendering.output, max_tokens=rendering.max_response_tokens))
    

# Is it worth it
?

As models grow larger and larger context windows, I've asked myself whether this is worth it. Won't context sizes eve
ntually big enough to put in everything we might want without worry?

One response: 'everything I might want' is a very,
 very big set, plausibly bigger than any window size we're going to see soon.

Another: being able to do this kind of to
ken accounting is useful even if we don't completely fill context windows. For example, we might be able to augment our 
prompt with examples, documents, and tips. How much space should we allocate to each? The answer might well be model-dep
endent. How do we figure it out?

Flex prompt's output, a `Rendering` object, actually holds the entire component tree. 
You can look through the object to see how many tokens were allocated to each child. This is currently very manual, but 
it does provide the bedrock infrastructure to e.g. run tests to discover the optimal balance of augmented data for a giv
en prompt and model.

Additionally, the right admixture (and for that matter, the right *phrasing*) may well be model-de
pendent. Flex prompt currently provides only very limited model-specific rendering (you can look at [`ctx.target`](https
://ctx.target), but it doesn't tell you much), but there's no reason that can't be significantly improved. At the extrem
e limit is prompt *erasure*, where we fine-tune a model to require no or minimal instructions/examples for a given set o
f prompts. Flex prompt can enable transitions like this with no changes to the pipelines themselves: you'd still use the
 same prompt components, they'd just render differently if the target is a fine-tuned model vs. a generic one.

# Status
 & Future Work

Flex prompt is very much in early development. I would love to hear if and how people find it useful, an
d would love input and contributions!

Some things I'd like to tackle in the future:

* **Rendering message lists.** Fle
x prompt currently only renders strings, though it's set up to be able to render any type of output. Message histories b
asically grow without bound, so supporting this seems like a no-brainer.
* **Pagination**. If your rendering overflows (
as above, where we're trying to stuff *the entirety of war and peace* into a prompt), flex prompt will clip the offendin
g pieces to fit. But there's currently no way to get 'the next page'. But the `Rendering` actually retains enough inform
ation to do this! It would be great to be able to call `render(...).pages()` to get the sequence of prompts as we 'scrol
l' whatever has overflowed. This is medium-hanging fruit‚Äîa little tricky because we do have to descend the tree of rende
rings to find the exact one(s) which overflowed and then update only those.
* **Token accounting.** As mentioned above, 
you can currently grovel around in `Rendering` and look at the pieces of the prompt. This would be more useful if it wer
e a little easier, e.g. if you could use `rendering[Examples]` to find all the parts rendered by the `Examples` componen
t, or `rendering['advice']` to find all the parts which are tagged (somehow) as 'advice'. The use case here is prompt op
timization: discovering the optimal number or percentage of tokens to allot to each thing we might want to drop into the
 prompt.
* **More integrations.** Currently, flex prompt only supports OpenAI models. You can register your own target f
inders, but it would be great to have more support out of the box. This is mostly a matter of digging around and finding
 the tokenizers and window sizes for common models, and then writing the appropriate target finders. Contributions very 
welcome!
* **Model tuning.** As mentioned above, the rendering context could provide a mechanism for fetching model-spec
ific parameters. The basic idea is that `ctx[param]` will evaluate `param` against the context, and then we can define s
ome parameter types which load their model-specific values from *gestures vaguely* somewhere.

Thanks for reading!

* [F
lex prompt Github](https://github.com/queerviolet/flex-prompt)
* [Intro to flex prompt colab](https://colab.research.goo
gle.com/github/queerviolet/flex-prompt/blob/main/doc/intro_to_flex_prompt.ipynb)
* [My website](https://ashi.io). *shame
less plug: I have a lot of engineering experience and a bit of machine learning experience and* [*I am currently looking
 for a job*](https://ashi.io/resume.pdf)
```
---

     
 
MachineLearning -  [ [D] Working on RAG? You should be evaluating its performance and we've built a way to do that. ](https://www.reddit.com/r/MachineLearning/comments/18ciet5/d_working_on_rag_you_should_be_evaluating_its/) , 2023-12-30-0909
```
Check out our new open-source tool, Tonic Validate: [https://www.tonic.ai/validate](https://www.tonic.ai/validate)  


W
e've also been using the tool to evaluate different RAG tools out there. The latest post on LangChain vs Haystack is ava
ilable here:  [https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-langchain-vs-haystack](
https://www.tonic.ai/blog/rag-evaluation-series-validating-the-rag-performance-of-langchain-vs-haystack)

&#x200B;

Let 
us know what you think and if you're working on a RAG project, we'd love to hear about it! How are you measuring your RA
G system performance?
```
---

     
 
deeplearning -  [ Unlocking the Power of Language: How LangChain Transforms Data Analysis and More ](https://www.reddit.com/r/deeplearning/comments/18l788y/unlocking_the_power_of_language_how_langchain/) , 2023-12-30-0909
```
Language models have revolutionized natural language processing (NLP), yet they grapple with limitations that impede the
ir full potential. Enter LangChain, a pioneering framework that transcends these constraints, fostering innovative langu
age-based applications. To comprehend LangChain‚Äôs significance, we must first grasp the limitations plaguing large langu
age models (LLMs).

link: [https://medium.com/ai-advances/unlocking-the-power-of-language-how-langchain-transforms-data-
analysis-and-more-3c4f327d520d](https://medium.com/ai-advances/unlocking-the-power-of-language-how-langchain-transforms-
data-analysis-and-more-3c4f327d520d) 
```
---

     
 
deeplearning -  [ [D] Mastering Chain Composition with LangChain Expression Language (LCEL) ](https://www.reddit.com/r/deeplearning/comments/18i0wot/d_mastering_chain_composition_with_langchain/) , 2023-12-30-0909
```
In the intricate landscape of modern software development, orchestrating complex sequences of actions seamlessly poses a
 significant challenge. Enter LangChain Expression Language (LCEL), a groundbreaking declarative approach designed to re
volutionize the composition of chains within software architecture.

Link: [https://medium.com/ai-artistry/mastering-cha
in-composition-with-langchain-expression-language-lcel-2d5041fb0cbd](https://medium.com/ai-artistry/mastering-chain-comp
osition-with-langchain-expression-language-lcel-2d5041fb0cbd)  
```
---

     
