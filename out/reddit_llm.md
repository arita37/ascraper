 
all -  [ AI agent that acts as an expert in robotics (a LangChain application) ](/r/LangChain/comments/186acth/ai_agent_that_acts_as_an_expert_in_robotics_a/) , 2023-11-29-0910
```

```
---

     
 
all -  [ AI agent that acts as an expert in robotics (a LangChain application) ](https://www.reddit.com/r/LangChain/comments/186acth/ai_agent_that_acts_as_an_expert_in_robotics_a/) , 2023-11-29-0910
```
I would like to introduce you to  [ROScribe](https://github.com/RoboCoachTechnologies/ROScribe): an AI-native robot inte
gration solution that generates the entire robot software based on the description provided through natural language. RO
Scribe uses GPT and LangChain under the hood.

I am pleased to announce that we made a new release that supports a major
 feature which comes very helpful in robot integration.

**Training ROScribe on ROS index**

We trained [ROScribe](https
://github.com/RoboCoachTechnologies/ROScribe) on all open source repositories and ROS packages listed on ROS index. Unde
r the hood, we load all documents and metadata associated with all repositories listed on ROS index into a vector databa
se and use RAG (retrieval augmented generation) technique to access them. Using this method, we essentially teach the LL
M (gpt3.5 in our default setting) everything on ROS Index to make it an AI agent expert in robotics.  
ROScribe is train
ed on all ROS versions (ROS & ROS 2) and all distributions.

**Use ROScribe as a robotics expert**

With this release yo
u can use [ROScribe](https://github.com/RoboCoachTechnologies/ROScribe) as your personal robotics consultant. You can as
k him any technical question within robotics domain and have him show you the options you have within ROS index to build
 your robot. You can ask him to show you examples and demos of a particular solution, or help you install and run any of
 the ROS packages available in ROS index.  
Here is a [demo](https://www.youtube.com/watch?v=3b5FyZvlkxI) that shows ROS
cribe helping a robotics engineer to find a multilayer grid mapping solution and shows him how to install it.

To run RO
Scribe for this specific feature use: roscribe-rag in your command line.

You can find more info on our [github](https:/
/github.com/RoboCoachTechnologies/ROScribe) and its wiki page.

**New in this release**

Here are what’s new in release 
v0.0.4:

Knowledge extraction:

* Scripts for automatic extraction of ROS package documentation given your choice of ROS
 version
* Build a vector database over ROS Index

Retrieval augmented generation (RAG) capabilities for ROScribe:

* No
w ROScribe has access to the most recent open-source ROS repositories that can be found on ROS Index
* ROScribe can be c
alled as an AI agent that assists you with finding the relevant ROS packages for your project
* Use roscribe-rag to run 
the RAG agent

Creating a wiki page for documentation to keep the readme file short.

**Future roadmap**  
As of now, th
e entire code is generated by the LLM, meaning that the RAG feature (explained above) is currently a stand-alone feature
 and isn’t fully integrated into the main solution. We are working on a fully-integrated solution that retrieves the hum
an-written (open source) ROS packages whenever possible (from ROS index or elsewhere), and only generates code when ther
e is no better code available. This feature will be part of our next release.  
We also plan to give ROScribe a web-base
d GUI.

Please checkout our [github](https://github.com/RoboCoachTechnologies/ROScribe) and let us know what you think.
```
---

     
 
all -  [ Zapier NLA help ](https://www.reddit.com/r/LangChain/comments/1867cb2/zapier_nla_help/) , 2023-11-29-0910
```
I'm trying to use zapier natural language actions with langchain agents. However, it says that it's deprecaated on the l
angchain website [https://python.langchain.com/docs/integrations/tools/zapier](https://python.langchain.com/docs/integra
tions/tools/zapier)  How do I connect my agent to zapier now? There's no updates on either side regarding this deprecati
on.
```
---

     
 
all -  [ Can't run Falcon 7b locally ](https://www.reddit.com/r/Langchaindev/comments/1866nmp/cant_run_falcon_7b_locally/) , 2023-11-29-0910
```
Hello, I'm about 2 minutes from an aneurysm trying to get this running locally.

Here's my code:

    from langchain.pro
mpts import PromptTemplate
    from langchain.chains import LLMChain
    from langchain.llms.huggingface_pipeline import
 HuggingFacePipeline
    from transformers import AutoTokenizer, pipeline
    
    import torch
    
    model = 'tiiuae
/falcon-7b-instruct'
    
    tokenizer = AutoTokenizer.from_pretrained(model, offload_folder='./offload_dir')
    
    
pipeline = pipeline(
        'text-generation',
        model=model,
        tokenizer=tokenizer,
        torch_dtype=to
rch.bfloat16,
        device_map='auto',
        max_length=200,
        do_sample=True,
        top_k=10,
        num_r
eturn_sequences=1,
        eos_token_id=tokenizer.eos_token_id
    )

And the error output:

    Traceback (most recent 
call last): File '/home/friend/Documents/falcon7bLangChain/main.py', line 12, in <module> pipeline = pipeline( File '/ho
me/friend/Documents/falcon7bLangChain/venv/lib/python3.10/site-packages/transformers/pipelines/init.py', line 870, in pi
peline framework, model = infer_framework_load_model( File '/home/friend/Documents/falcon7bLangChain/venv/lib/python3.10
/site-packages/transformers/pipelines/base.py', line 282, in infer_framework_load_model raise ValueError( ValueError: Co
uld not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeli
ng_auto.AutoModelForCausalLM'>, <class 'transformers.models.falcon.modeling_falcon.FalconForCausalLM'>). See the origina
l errors:
    while loading with AutoModelForCausalLM, an error is thrown:
    Traceback (most recent call last):
      
File '/home/friend/Documents/falcon7bLangChain/venv/lib/python3.10/site-packages/transformers/pipelines/base.py', line 2
69, in infer_framework_load_model
        model = model_class.from_pretrained(model, **kwargs)
      File '/home/friend/
Documents/falcon7bLangChain/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py', line 566, in fr
om_pretrained
        return model_class.from_pretrained(
      File '/home/friend/Documents/falcon7bLangChain/venv/lib/
python3.10/site-packages/transformers/modeling_utils.py', line 3480, in from_pretrained
        ) = cls._load_pretrained
_model(
      File '/home/friend/Documents/falcon7bLangChain/venv/lib/python3.10/site-packages/transformers/modeling_uti
ls.py', line 3601, in _load_pretrained_model
        raise ValueError(
    ValueError: The current `device_map` had weig
hts offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` 
installed if the model you are using offers the weights in this format.
    
    while loading with FalconForCausalLM, a
n error is thrown:
    Traceback (most recent call last):
      File '/home/friend/Documents/falcon7bLangChain/venv/lib/
python3.10/site-packages/transformers/pipelines/base.py', line 269, in infer_framework_load_model
        model = model_
class.from_pretrained(model, **kwargs)
      File '/home/friend/Documents/falcon7bLangChain/venv/lib/python3.10/site-pac
kages/transformers/modeling_utils.py', line 3480, in from_pretrained
        ) = cls._load_pretrained_model(
      File 
'/home/friend/Documents/falcon7bLangChain/venv/lib/python3.10/site-packages/transformers/modeling_utils.py', line 3601, 
in _load_pretrained_model
        raise ValueError(
    ValueError: The current `device_map` had weights offloaded to th
e disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the mo
del you are using offers the weights in this format.

I have specified an `offload_folder`, called `offload_dir` in the 
same directory as the script. `safetensors` is installed, and the `transformers` package is the newest available version
. I'm completely lost on what the issue could be, any ideas?
```
---

     
 
all -  [ Claude2 on Bedrock/OpenGPTs ](https://www.reddit.com/r/LangChain/comments/1866fo7/claude2_on_bedrockopengpts/) , 2023-11-29-0910
```
Has anyone had any success at all using Claudev2 on Amazon Bedrock with LangChain's OpenGPTs library?
```
---

     
 
all -  [ Passing metadata through llm ](https://www.reddit.com/r/LangChain/comments/18658nq/passing_metadata_through_llm/) , 2023-11-29-0910
```
I am currently trying to pass survey data through an llm with the following structure. I cant use OpenAI because of the 
proprietary data. 

&#x200B;

(page\_content = question number, metadata = {'Company': 'Company Name', 'Question': 'Ques
tion Text', 'Response': 'Response text'})

&#x200B;

current when i pass this through the llm using the HuggingFaceEmbed
dings and FAISS and the following structure:

&#x200B;

from langchain.chains.question\_answering import load\_qa\_chain
#load q and a  chainchain = load\_qa\_chain(llm = hf\_pipeline, chain\_type = 'stuff')query = ('Query text')            
        

\#run the chainresponse = chain.run(input\_documents = documents,question = query,return\_full\_text = True,ve
rbose = True)similarity\_score = db.similarity\_search(query)

print(resposne)

&#x200B;

it will only return the questi
on number back to me. How do I incorporate the metadata so that the LLM knows to categorize the company and understand t
he question/response?

&#x200B;

&#x200B;
```
---

     
 
all -  [ Langchain SelfQueryRetriever generated JSON to pass to vectorstore never right ](https://www.reddit.com/r/LangChain/comments/1863l9b/langchain_selfqueryretriever_generated_json_to/) , 2023-11-29-0910
```
I've created a Vdb with Chroma and am trying to use MPT-30b as the LLM to query the content and metadata in the db. Util
izing SelfQueryRetriever the first step being performed is a json is created to both query and filter the documents in t
he db. It supposed to look something like the following....

    '''json
    {
    'query':'blahblahblah',
    'filter':
'and(eq('Field1':'value1'),eq('Field2':'value2'))'
    }

For some reaosn my model never creates this correctly. The que
ry parts usually fine but the filter part ALWAYS gets messed up. Typically I've noticed its whn there are multiple that 
need to be and'd together. One mistake it makes is putting an and between them instead out outside the parenthesis. I've
 checked the [base.py](https://base.py) file with the prompt for this formatting and its inline with all the other versi
ons ive seen.

I've been messing with the LLM settings (temp, top\_k, etc) for days trying to get this to work. 

Is it 
possible MPT-30b isnt smart enough to correctly create this format?

Has anyone else seen a model not be able to follow 
these instructions and create the correct json for SelfQueryRetriever?

&#x200B;
```
---

     
 
all -  [ Extracting the generated SQL from create_sql_agent ](https://www.reddit.com/r/LangChain/comments/1862vd2/extracting_the_generated_sql_from_create_sql_agent/) , 2023-11-29-0910
```
Hi all, I'm sure I'm just not understanding something simple, but I've looked around lots of examples, and haven't found
 anything to address my question yet...

I've created a sql\_agent with a half-dozen 'tools' that does a pretty nice job
 of generating sql against a database with about 100 tables... I'm getting pretty satifsfied with my end results. Howeve
r, in addition to returning the 'final answer,' I'd like to also extract the generated SQL in case the user wishes to re
view it (I have a web interface for asking questions, getting answers).  

I can see the SQL code in intermediate steps 
if I have 'verbose' turned on, but I'm at a loss as to where that SQL statement is retrievable. 

Using langsmith i can 
see the SQL as output from the sql query checker tool, or input to the sql db query tool... but I'm not clear on how I c
an access that from outside the chain....

Any thoughts would be most appreciated!

&#x200B;
```
---

     
 
all -  [ [D] Utilizing Multimodal LLM for Extracting Tables and Images LangChain+LlamaIndex’s Role in Semi-St ](https://www.reddit.com/r/deeplearning/comments/185vd56/d_utilizing_multimodal_llm_for_extracting_tables/) , 2023-11-29-0910
```
In the domain of document analysis, the convergence of text, tables, and images presents formidable challenges for conve
ntional RAG (Retrieval Augmented Generation) methodologies. This complexity is further compounded within semi-structured
 data, notably in the extraction of tables from PDFs. Enter LangChain, a pioneering tool adept at navigating these intri
cate landscapes. Augmenting its capabilities is LlamaIndex, integrating Multi-Modal Retrieval Augmented Generation (RAG)
 techniques. Together, LangChain and LlamaIndex stand poised to revolutionize the handling and extraction of diverse con
tent types, promising a breakthrough in unraveling insights from varied data formats.

Link in the comment
```
---

     
 
all -  [ How good your GPTs are at reading your uploaded files so far? Not working well in my case. ](https://www.reddit.com/r/ChatGPT/comments/185roau/how_good_your_gpts_are_at_reading_your_uploaded/) , 2023-11-29-0910
```
I uploaded 9 txt files, each with 1.4 million tokens. They are ll text content i collect like a big collection of funny/
horror stories. I made the gpt as personal library and want it to return relevant stories according to my request.  I ma
de request composed of key tags, source or author  that i remembered and I want the GPTs to tell me the plots of the rel
evant story. 

I've disabled web browsing and using its base knowledge, so I get tailored response solely from my files.
 

However, it usually takes more than 3mins for it to search and reture me with sth like 'sorry i cannot find relevant 
piece ...' while I know there must be certain piece there.

Is there anyone has similar issue so far? How good is GPTs g
ood at reading text based files？ It uses langchain or what？   
```
---

     
 
all -  [ xls, csv, or db data chatbot success ](https://www.reddit.com/r/LangChain/comments/185m6l2/xls_csv_or_db_data_chatbot_success/) , 2023-11-29-0910
```
Has anyone successfully written a xls csv or db chatbot WITHOUT using openAI. I have been unsuccessful trying multiple m
ethods. If you’ve had success what methods did you use ??
```
---

     
 
all -  [ Spawning agents to search the Internet and answer questions ](https://www.reddit.com/r/ChatGPTCoding/comments/185igle/spawning_agents_to_search_the_internet_and_answer/) , 2023-11-29-0910
```
I've finally got some spare time and wanted to try out GPT development. I remember watching a couple months ago what I t
hink was AutoGPT spawning agents that google searched their way to their goal.

I wanted to replicate that for a pet pro
ject but since OpenAI isn't allowing people to upgrade their accounts it seems that I'm forced to rely on a custom imple
mentation using ollama or LocalAI, for instance. Langchain seem super powerful was well but I'm not exactly sure how to 
use it right now.

Going back to the problem at hand there are two phases I need to address:

\- Spawning agents to sear
ch the Internet for data

\- Interpreting said data

The latter seems straightforward enough (as mentioned above) but I 
am not sure what I should use regarding the former.

Could anyone point me in the right direction? There's so much infor
mation and so many projects that it becomes confusing. Thanks in advance
```
---

     
 
all -  [ Help with the error: No parameter named 'allowed_tools' ](https://www.reddit.com/r/LangChain/comments/185i5zw/help_with_the_error_no_parameter_named_allowed/) , 2023-11-29-0910
```
I am working creating an agent with the code:

    agent = LLMSingleActionAgent(
      llm_chain=llm_chain, 
      outpu
t_parser=output_parser,
      stop=['\nObservación:'], 
      allowed_tools=tool_names
    )

That is working on my firs
t project, but on a different project I get:

    No parameter named 'allowed_tools'

What am I doing wrong? Help, pleas
e.
```
---

     
 
all -  [ Can someone explain async vs. non-async streaming to me? ](https://www.reddit.com/r/LangChain/comments/185fcte/can_someone_explain_async_vs_nonasync_streaming/) , 2023-11-29-0910
```
Super dumb question, but despite reading the documentation, I'm a bit lost on the distinction here.

I want to stream ou
t a Bedrock response token-by-token using FastAPI, creating a 'slowly typing' effect similar to what ChatGPT does.

Per 
the docs:

>Streaming support defaults to returning an Iterator(or AsyncIteratorin the case of async streaming) of a sin
gle value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-toke
n streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator o
f tokens can work for any of our ChatModel integrations.

Bedrock does not support async streaming.  Reading the above, 
it sounds like I can only get the 'final' response from Bedrock, which tracks if I use `FinalStreamingStdOutCallbackHand
ler`\--everything comes back as one response.

However, `StreamingStdOutCallbackHandler` does sort of seem to process to
ken-by-token, like I want--the problem is it spits out to stdout, whereas I want to return it in my API call.

&#x200B;


So, two questions:

1. Am I fundamentally misunderstanding the difference between async and non-async streaming, and I 
cannot get token-by-token with Bedrock?
2. If I can get token-by-token, what exactly am I doing wrong?   I can yield eac
h word (not token) in the response, but it just shows up in one block anyway as far as FastAPI is concerned.
```
---

     
 
all -  [ [R] LLMs for structured data? ](https://www.reddit.com/r/MachineLearning/comments/185ei6v/r_llms_for_structured_data/) , 2023-11-29-0910
```
I've been trying to work with structured data in language models, and it's proving to be quite challenging. I'm confiden
t that with Langchain, I should be able to solve the problem, but I'm not entirely sure which path to take among all the
 options the library offers.

My issue is as follows: I have data in the form of dictionaries regarding a series of prod
ucts, for example, laptops. The data looks like this:

{Identifier 1: X, Identifier 2: Y, Value name: Z}

(Several succe
ssive dictionaries like this.)

I want to use this series of dictionaries as context, then feed a different dictionary i
nto the Language Model, and have it tell me if the 'Value name' makes sense given Identifiers 1 and 2. An example would 
be Identifier 1: laptops, Identifier 2: brand, Value name: Lenovo. In this case, it should return affirmative since Leno
vo makes sense as a brand. However, if I input 'oranges,' it should return negative.

Any ideas on which library I could
 use to tackle this problem?
```
---

     
 
all -  [ Pros and Cons of relying on the new OpenAI Assistants and Knowledge Retrieval APIs ](https://www.reddit.com/r/LangChain/comments/185cdot/pros_and_cons_of_relying_on_the_new_openai/) , 2023-11-29-0910
```
I've been exploring this new OpenAI update [**in detail**](https://pashpashpash.substack.com/p/navigating-the-new-openai
-update)**,** and my conclusion is that while simplifying your tech stack by directly using OpenAI's Assistants/Retrieva
l APIs might be tempting, it may actually be a net negative for your project. Here are the pros and cons:

# Using OpenA
I Assistants for managing user conversations

Pros:

* **Simplicity:** reduces the complexity significantly. Devs spend 
less time on scalability and maintenance.  No need to manage user conversations – OpenAI's got that covered.
* **Privacy
 and Security:** Since conversations expire at the end of the session, privacy concerns take a back seat. OpenAI is resp
onsible for storing user conversations in a compliant manner.
* **Deep functionality, out-of-the-box:** Knowledge Retrie
val, Code Interpreter, and Function Calling are super helpful and work out of the box with the Assistants API.

Cons:

*
 **No ability to save/export history of messages:** Can't review or audit user conversations. Users starting on a new de
vice lose their previous chats (unless you store them yourself).
* **Limited control over context:** You can’t cherry-pi
ck which part of the conversation to focus on.
   * If the conversation is too long, OpenAI will naively throw out older
 messages in favor of newer ones.
* **Session Amnesia:** Once a session wraps up, so does the context. This can be a hur
dle if you need long-term memory or follow-up across multiple sessions.

# Using OpenAI Knowledge Retrieval

**Pros:**


* **Simplicity**: Less work and maintenance for your developers – no need to build your own RAG pipeline.
* **Data Priva
cy is now OpenAI’s responsibility**: OpenAI ensures compliance and safety for uploaded content, so you don't have to wor
ry about it.

**Cons:**

* **Limited Customization**: OpenAI’s retrieval process won't bend to suit your specific data n
eeds
   * *Example*: you won’t be able to pinpoint the exact page where chunks of retrieved text originate in their sour
ce documents, let alone extract detailed positional data within PDFs.
   * This will force you to implement hacky workar
ounds like searching documents using the chunk text (if you’ve worked with PDFs before you’ll understand what kind of he
adache this can be).
* **No access to underlying vector embeddings**: This makes key features like comprehensive summari
zation of documents (via K-means [extraction of meaning clusters](https://www.reddit.com/r/LangChain/comments/165xmzx/iv
e_been_exploring_the_best_way_to_summarize)) impossible.
* **No control over which chunks are selected**
   * *Example*:
 Imagine a user asks for details about a specific event in a historical document. You might want the AI to focus on a pa
rticular section that thoroughly covers this event. However, OpenAI might choose a different section that only briefly m
entions it, potentially overlooking the depth of information you know exists elsewhere in the document.
* **OpenAI stora
ge costs are** ***insane***: $0.20/GB/assistant/day
   * *Example*: Let's say you’re building an essay-writing co-pilot.
 An average student uploads 250mb worth of PDF references for each essay they write.
   * In a single semester, a studen
t will write 10 essays. So, in a year, an average user will write 20 essays, and upload 5GB of references.
   * **In thi
s example, the cost for storage for 10,000 active users will be between**
      * **$800/day** (if you limit users to on
ly being able to write one essay at a time)
      * **$10,000/day** (if users can write up to 10 essays at the same time
)
* **File Size Ceiling**: Each file has a 512 MB cap, which is a constraint you wouldn’t face with a custom pipeline.


# Bottom Line 

My overall impression is if you're trying to build any serious business with a key RAG component, **you'
re going to need a custom, future-proof, RAG pipeline** that gives you exactly what you need.

[Retrieval Service 'middl
e layer'](https://preview.redd.it/8shcsig27y2c1.png?width=1400&format=png&auto=webp&s=f19a5db901914ffd0e0db8953ca27dbf46
3f178f)

I discuss these pros and cons, as well as this concept of a Retrieval Service 'middle layer' in detail in my la
test Substack article here: [https://pashpashpash.substack.com/p/navigating-the-new-openai-update](https://pashpashpash.
substack.com/p/navigating-the-new-openai-update)

**The TL;DR** is that using these new APIs offers a quick entry point,
 allowing you to rapidly deploy and scale your applications. However, this comes with trade-offs in terms of customizati
on and control. Investing in a custom solution, especially a robust middle layer, provides a foundation for greater flex
ibility, deeper integration, and the ability to stay agile in a rapidly advancing technological landscape.

What do you 
guys think about all of this? I'm sure that over time, OpenAI will improve these APIs with more customization and featur
es. But even then, having a middle layer wrapper around these APIs that provides reliable, consistent results remains in
dispensable for any serious project. I'd love to hear your thoughts on this though.
```
---

     
 
all -  [ Pass variable tools using langserve ](https://www.reddit.com/r/LangChain/comments/185bo1k/pass_variable_tools_using_langserve/) , 2023-11-29-0910
```
Hello, we have a langserver setup with a conversstiona agent chain that uses tools as well. Working perfect. We need to 
pass a user authentication token to the tool functions because the tools access customer services. So we only want tools
 to make external api calls with authentication tokens. We can only figure out where input_string gets passed to the too
l function we create (so the tool can connect to an external api) but we need to send a token via langserve, not to the 
llm, but if the user query requieres a tool usage, have the tool be able to get the authentication token sent along with
 the query  (not as part of the string, but as a separte variable). What file should we modify to have langchain send tw
o variables to the tool funtion, not just the input_string?
```
---

     
 
all -  [ LLaMA 2 7b exclusively for summarization ](https://www.reddit.com/r/LocalLLaMA/comments/185bd6e/llama_2_7b_exclusively_for_summarization/) , 2023-11-29-0910
```
Hi all,

I am running a LLaMA 2 7b model on a AWS Sagemaker instance. I need the model just for providing me summaries o
f long documents, and i am using Langchain to do a map reduce on the data and get a summary of it.

I want to know if th
ere's a better way to do this or if you could share your personal experiences on summarizing efficiently.

I am not gett
ing good results since the summarization includes too much information.

Thanks in advance.
```
---

     
 
all -  [ [For Hire] Programmer/Web Developer/IT Consultant (Python, PHP, AI, etc.) ](https://www.reddit.com/r/forhire/comments/185b5h6/for_hire_programmerweb_developerit_consultant/) , 2023-11-29-0910
```
To get in contact, please **message** me, I **don't** use the chat thing and might miss you or reply very late. Then we 
can switch to email/discord/telegram or whatever else. Apologies for starting with this, but many missed it when it was 
lower.

I'm a programmer/web developer with 12 years of professional experience. I am available for all sorts of program
ming and web development tasks.

I also offer consulting services. If you need something done, but don't know how exactl
y, I can help. I'm an excellent researcher and I communicate well. I will work with you to find the best solution for yo
ur problem.

My services include, but are not limited to:

* websites

* desktop applications

* AI integration (chatGPT
 API, langchain, whatever else turns up)

* integration with APIs and other webservices

* all kinds of scripts

* task 
automation

* website optimization

* debugging

* plugins for existing software

* bots (Reddit, Telegram, etc)

If you
're looking for someone to take care of a variety of different tasks, I can offer continuous support.

My preferred envi
ronment is Python with Django, but I work with anything Python or PHP based, including Wordpress. I also do frontend stu
ff with JavaScript, jQuery, AJAX. I also have no problem with learning new technologies that are needed for the project.


Rate is $50/h. Can also do fixed price by project, but only if the project/milestone is well-defined.

Satisfied custo
mers:

https://www.reddit.com/r/testimonials/comments/2e8gqy/pos_uqui_need_a_backend_web_dev_look_no_further/

https://w
ww.reddit.com/r/testimonials/comments/7fsdze/pos_hiring_uqui_was_an_example_of_how_it_should/

https://www.reddit.com/r/
testimonials/comments/80pu9l/pos_uqui_great_work_detailed_and_fast/

https://www.reddit.com/r/testimonials/comments/b0nx
68/uqui_is_a_hardworking_intelligent_honest_apps/

https://www.reddit.com/r/testimonials/comments/j3mz3p/uqui_is_a_great
_web_development_consultant_with/

https://www.reddit.com/r/testimonials/comments/v40ay3/pos_uqui_is_a_great_backend_dev
_to_work_with/

Some examples of sites I worked on: http://bdabkowski.yum.pl/

Please note: I am **not** a designer.
```
---

     
 
all -  [ Host a reliable LLM on-prem for knowledge base consultation, which to choose from? ](https://www.reddit.com/r/learnmachinelearning/comments/1856xt9/host_a_reliable_llm_onprem_for_knowledge_base/) , 2023-11-29-0910
```
Hey all, I was tasked to find a suitable LLM that would be able to be run on-prem as this requires certain levels of sec
urity.

The obvious first thought went to GPT models, but then I need this to be absolutely precise as if reading word p
er word out of the documents it will be trained on. The idea is still to re-use a pre-trained model, as I have nowhere n
ear enough material to train a new one, but then using LangChain and other libraries there are ways to host this entirel
y on my own. Then the model will be further trained on the documents I will give it (mainly PDF files and HTML stuff).


I thought BERT would also be a good choice here, do you have any other suggestions?

&#x200B;
```
---

     
 
all -  [ Pandas agent ](https://www.reddit.com/r/LangChain/comments/1856sc5/pandas_agent/) , 2023-11-29-0910
```
Hi everyone, i am trying to use 'create\_pandas\_dataframe\_agent' using a local model 'Llama2-7b', for sometimes i get 
a right answer and sometimes not, i suspect because i have to  provide some additional context. I modified the current p
rompt template but it doesn't work as expected. Any ideas?

`agent_executor.agent.llm_chain.prompt.template=TEMPLATE_PAN
DAS (new prompt info)`
```
---

     
 
all -  [ [Machine Learning] [D] Formation d'un modèle pour les appels de fonction ](https://www.reddit.com/r/redditenfrancais/comments/1855is0/machine_learning_d_formation_dun_modèle_pour_les/) , 2023-11-29-0910
```
Serait-il possible de former ou de définir un petit modèle (1-3b) qui est le seul but est d'effectuer des appels de fonc
tion? Semblable à la façon dont nous avons de minuscules modèles comme Replit-V2-3B qui sont super capables à des choses
 spécifiques comme le code automatique du code.


Je sais que c'est ainsi que l'appel de fonction a mis en œuvre OpenAI 
par le réglage fin GPT-3.5 / 4, mais je pense qu'un modèle de base directement formé pour comprendre et exceller aux app
els de fonction (similaire à Gorilla for API)

Je pense que ce serait une 'colle' parfaite pour les plus grandes applica
tions LLM - éviter le besoin d'outils externes comme Langchain / Quidance / etc ...

Traduit et reposté à partir de la p
ublication https://www.reddit.com/15n1j52
```
---

     
 
all -  [ RAG Reranking ](https://www.reddit.com/r/LangChain/comments/185583g/rag_reranking/) , 2023-11-29-0910
```
Hi,

I developed a RAG model with Langchain and also implemented Advanced Methods like ParentDocumentRetriever, Ensemble
Retriever etc. 

I am always hearing that Reranking generally improves RAG applications. But how can you do Reranking pr
operly in Langchain? I saw some tutorials with the Cohere API, but I don't want to use an API, I want my data and model 
to be stored locally and private. So what alternatives are there for reranking?

&#x200B;

Thanks & Regards
```
---

     
 
all -  [ table extraction from pdf ](https://www.reddit.com/r/LocalLLaMA/comments/1854d06/table_extraction_from_pdf/) , 2023-11-29-0910
```
anyone knows some robust open source library for extracting tables from pdf , even ocr library is fine 

P.S- i have alr
eady tried tabula ,camelot , ing2table, unstructured.io and most of the document loader in langchain , none of them are 
even 95% robust
```
---

     
 
all -  [ Best approaches for chunking data in RAG ](https://www.reddit.com/r/LangChain/comments/18544mh/best_approaches_for_chunking_data_in_rag/) , 2023-11-29-0910
```
Hi everyone,

My team and I are doing a livestream on Wednesday covering 'Chunking Best Practices for RAG' if you're int
erested in joining. Our previous livestreams seem they have been helpful for the community here so thought I would share
 this one too. We're covering both concepts and code.

Specific topics include:

* Naive Chunking 
* Structural Chunkers
 
* Summarization
* Extraction 
* and Multi-Modal Chunking  
* and more

The livestream is this Wednesday at 11:30AM ET 
/ 4:30 PM UK. You can join here: [https://www.linkedin.com/events/chunkingbestpracticesforragappl7130973382363271168](ht
tps://www.linkedin.com/events/chunkingbestpracticesforragappl7130973382363271168)
```
---

     
 
all -  [ Best receiver to find small statements in large context. ](https://www.reddit.com/r/LangChain/comments/1853wwi/best_receiver_to_find_small_statements_in_large/) , 2023-11-29-0910
```
Hello everyone,

I am currently trying to build a RAG model for medical applications. I used OpenAI embeddings and store
d my PDF file into a chroma db, but now I am faced with a choice of 30 different receivers.

My question is now: What do
 I need to know to choose the best receiver. My base document is very long (500 pages of expert medical knowledge) and I
 want to extract information from it using as much context as possible, while getting back as many details as possible. 


Is there maybe an approach to find a suitable receiver or is it just a thing of trial and error?

Thank you
```
---

     
 
all -  [ LangChain Blog ](https://www.reddit.com/r/LangChain/comments/1851y95/langchain_blog/) , 2023-11-29-0910
```
Hey guys I wrote a new blog on LangChain for my website. Kindly proofread it and let me know what should I add/remove an
d an overall review from users point of view would be much appreciated!

[What is LangChain? AI App Development Framewor
k Explained](https://www.deligence.com/what_is_langchain_ai_app_development_framework_explained/)
```
---

     
 
all -  [ Can we do Q&A on different types of data using single Agent? ](https://www.reddit.com/r/LangChain/comments/1850r68/can_we_do_qa_on_different_types_of_data_using/) , 2023-11-29-0910
```
Hi All, I am a beginner with LangChain. I have the following use case:

1. Use a Single Agent for Q&A on multiple differ
ent data. For example, I have 3 types of data CSV files, Text files, and pdfs. I want to create an Agent that can figure
 out which type of data is required to answer the given question and use a tool to fetch the data from the corresponding
 data and provide the answer.

Can it be achieved? Any suggestions?
```
---

     
 
all -  [ How to use LCEL with OpenAI vision API? ](https://www.reddit.com/r/LangChain/comments/184yvjr/how_to_use_lcel_with_openai_vision_api/) , 2023-11-29-0910
```
Hey everyone!

I need some help adapting this example from the cookbook to work in LCEL.

    chat = ChatOpenAI(model='g
pt-4-vision-preview', max_tokens=256)
    chat.invoke(
        [
            HumanMessage(
                content=[
   
                 {'type': 'text', 'text': 'What is this image showing'},
                    {
                        '
type': 'image_url',
                        'image_url': {
                            'url': 'https://raw.githubusercon
tent.com/langchain-ai/langchain/master/docs/static/img/langchain_stack.png',
                            'detail': 'auto
',
                        },
                    },
                ]
            )
        ]
    )
    

&#x200B;

Thi
s is my code, where I'm trying to pass url to the template:

    prompt = ChatPromptTemplate.from_messages([
        Sys
temMessage(
            content=(
                'You are a helpful business analyst who specializes in analysing diagr
ams.'
            )
        ),
        HumanMessage(
            content=[
                {'type': 'text', 'text': 'Wha
t do you see on this image?'},
                {
                    'type': 'image_url',
                    'image_url
': {
                        'url': '{url}', # what should go here to make it work?
                        'detail': 'a
uto',
                    },
                },
            ]
        )]
    )
    
    model = ChatOpenAI(model='gpt-4-
vision-preview', max_tokens=256)
    
    chain = prompt | model
    chain.invoke({'url': 'https://raw.githubusercontent
.com/langchain-ai/langchain/master/docs/static/img/langchain_stack.png'})
    

How do I modify a prompt or chain to be 
able to pass an URL?

My main goal is to batch-call a chain to process a list of URLs, like I do with other chains:

   
 chain.batch(urls)

&#x200B;
```
---

     
 
all -  [ POI detections and positioning ](https://www.reddit.com/r/LangChain/comments/184yivc/poi_detections_and_positioning/) , 2023-11-29-0910
```
Hello, I'm currently trying to implement an AI agent that does the following:

From an arbitrary number of pictures and 
their localization info (lat/long) identifies points of interest (e.g. restaurants, monuments ...) and places them on an
 existing map. Basically the agent should accept images as inputs, identify relevant places and match them on a map. 

1
. is langchain suitable to build such system?
2. how would you structure such agent?
3. any useful resources? how can I 
help a LLM understand how to use a map?  


Thanks
```
---

     
 
all -  [ Is using a vector store to query a small amount of embeddings 'too much'? ](https://www.reddit.com/r/LangChain/comments/184q80n/is_using_a_vector_store_to_query_a_small_amount/) , 2023-11-29-0910
```
Hello, I'm pretty new to language models and I am attempting to build an app that from one document a chatbot can receiv
e questions about the content of the document and answer them.

These documents are relatively small but too big to give
 a conversational LLM as context. What I have thought until now is that I should use RAG by generating embeddings from p
ieces of the document and store them into a vector store like Pinecone to pick the most relevant pieces of text to give 
the conversational LLM as context to start the chat.

The thing is, I'm wondering if using, for example, a Pinecone inde
x partitioned per document using namespaces would be too 'overkill' to query the most relevant pieces of the document si
nce it would usually query over a small amount of embeddings (I estimate around 100 per document on average).

When I re
ad about vector stores in forums, it seemed to me (maybe it's a wrong appreciation) that people usually use them on real
ly big amounts of data, so I wondered if maybe there is another method to query the most relevant pieces of a document t
hat is more fit for my use case?
```
---

     
 
all -  [ use some python package for python agent ](https://www.reddit.com/r/LangChain/comments/184lsxt/use_some_python_package_for_python_agent/) , 2023-11-29-0910
```
Hi, i want to use langchain python agent and openai, to use some python packages. e.g. scanpy, and some python data obje
ct like adata.

    agent_executor = create_python_agent(
    llm=openai_llm,
    tool=PythonAstREPLTool(locals={'adata'
: adata, 'scanpy': sc} ),
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    )

&#x200B;

but g
ot error message like this one

>However, since I cannot import external libraries like scanpy in this environment or ac
cess external data objects like \`adata\`, I cannot execute this task directly.

My question is how can I use package li
ke scanpy with python agent, is that even possible?
```
---

     
 
all -  [ AutoGen v0.2.0 released ](https://www.reddit.com/r/AutoGenAI/comments/184ka6x/autogen_v020_released/) , 2023-11-29-0910
```
[New release: v0.2.0](https://github.com/microsoft/autogen/releases/tag/v0.2.0)  

This is a major release since v0.1.1,
 containing 13 minor releases (from v0.1.1 to v0.1.14) and 6 pre-releases (v0.2.0b1 to v0.2.0b6).

## Highlights since v
0.1.1

### Breaking changes

* Switching to openai v1. Please read the [migration guide](https://microsoft.github.io/aut
ogen/docs/Installation/#migration-guide-to-v02).

### New Features and Enhancements:

* **GPT Assistants Support**: Inte
gration of GPTAssistantAgent leveraging OpenAI Assistant API for conversational capabilities and state management. [http
s://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-
assistants)
* **Group Chat Enhancements**: Richer speaker selector options and robustness improvements.
* Enhanced retri
eve chat, such as rich text format and customized vector db or embedding function in RAG. [https://microsoft.github.io/a
utogen/blog/2023/10/18/RetrieveChat](https://microsoft.github.io/autogen/blog/2023/10/18/RetrieveChat)
* **AgentEval Fra
mework**: Introduction of AgentEval for assessing task utility in LLM-powered applications. [https://microsoft.github.io
/autogen/blog/2023/11/20/AgentEval](https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval)
* **CompressibleAgent
 for Long Conversations**: Handling long conversations with an experimental CompressibleAgent. [https://github.com/micro
soft/autogen/blob/main/notebook/agentchat\_compression.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/ag
entchat_compression.ipynb)
* Streaming Support: Added experimental streaming capabilities.
* Async Execution and Human I
nput Handling: Enhanced async function execution and better handling of human input.
* **Large Multimodal Models (GPT-4V
) Support**: Enhanced AgentChat capabilities with Large Multimodal Models integration. [https://microsoft.github.io/auto
gen/blog/2023/11/06/LMM-Agent](https://microsoft.github.io/autogen/blog/2023/11/06/LMM-Agent)
* **TeachableAgent**: Intr
oduction of TeachableAgent for persistent user teachings across chat boundaries. [https://microsoft.github.io/autogen/bl
og/2023/10/26/TeachableAgent](https://microsoft.github.io/autogen/blog/2023/10/26/TeachableAgent)
* Enhanced Documentati
on and Developer Tools: Comprehensive updates to README, FAQs, and developer tools.
* **AutoGen Assistant**: Deployment 
of a sample web application for practical demonstration. [https://github.com/microsoft/autogen/tree/main/samples/apps/au
togen-assistant](https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-assistant)
* Demonstration of Agent
s using Langchain Tools: [https://github.com/microsoft/autogen/blob/main/notebook/agentchat\_langchain.ipynb](https://gi
thub.com/microsoft/autogen/blob/main/notebook/agentchat_langchain.ipynb).

### Fixes and Improvements:

* Bug Fixes: Res
olved issues in caching, filter checking, and other minor bugs.
* Documentation Enhancements: Multiple improvements in d
ocumentation, including migration guides and useful tips.
* Model Compatibility and Stability: Enhancements in model com
patibility and overall stability of the package.
* Code Execution: more robust code detection/match/extraction, bug fixe
s for windows and sample code improvements, shell language switch in Docker
* Improved Codebase Reliability: Updates and
 bug fixes for better codebase reliability.

Thanks to the 80 contributors along this 2-month journey!

## Full Changelo
g: [v0.1.1...v0.2.0](https://github.com/microsoft/autogen/compare/v0.1.1...v0.2.0)
```
---

     
 
all -  [ No memory... ](https://www.reddit.com/r/LangChain/comments/184j4sa/no_memory/) , 2023-11-29-0910
```
I'm new to Langchain, I've tried this code, adapted from [https://medium.com/aimonks/chatgpt-clone-with-streamlit-and-la
ngchain-e0d4fa78e33d](https://medium.com/aimonks/chatgpt-clone-with-streamlit-and-langchain-e0d4fa78e33d), but it doesn'
t seem to send the memory of the conversation to the model:

    import streamlit as st
    from langchain.chat_models i
mport ChatOpenAI
    from langchain.chains import LLMChain
    from langchain.prompts import PromptTemplate
    from dec
ouple import config
    from langchain.memory import ConversationBufferWindowMemory
    
    prompt = PromptTemplate(
  
      input_variables=['chat_history', 'question'],
        template='''You are a very kindl and friendly AI assistant. 
You are
        currently having a conversation with a human. Answer the questions
        in a kind and friendly tone w
ith some sense of humor.
        
        chat_history: {chat_history},
        Human: {question}
        AI:'''
    )
 
   
    
    llm = ChatOpenAI(temperature = 0.7 , model = 'gpt-3.5-turbo')
    memory = ConversationBufferWindowMemory(m
emory_key='chat_history', k=14)
    llm_chain = LLMChain(
        llm=llm,
        memory=memory,
        prompt=prompt

    )
    
    
    st.set_page_config(
        page_title='ChatGPT Clone',
        page_icon='🤖',
        layout='wide'

    )
    
    
    st.title('ChatGPT Clone')
    
    
    # check for messages in session and create if not exists
  
  if 'messages' not in st.session_state.keys():
        st.session_state.messages = [
            {'role': 'assistant', 
'content': 'Hello there, am ChatGPT clone'}
        ]
    
    
    # Display all messages
    for message in st.session
_state.messages:
        with st.chat_message(message['role']):
            st.write(message['content'])
    
    
    u
ser_prompt = st.chat_input()
    
    if user_prompt is not None:
        st.session_state.messages.append({'role': 'use
r', 'content': user_prompt})
        with st.chat_message('user'):
            st.write(user_prompt)
    
    
    if st
.session_state.messages[-1]['role'] != 'assistant':
        with st.chat_message('assistant'):
            with st.spinn
er('Loading...'):
                ai_response = llm_chain.predict(question=user_prompt)
                st.write(ai_resp
onse)
        new_ai_message = {'role': 'assistant', 'content': ai_response}
        st.session_state.messages.append(ne
w_ai_message)

for exemple : 

Hello there, am ChatGPT clone  


give me 3 subject to speak so that i can choose  


Oh,
 the possibilities are endless! But let's narrow it down to a trio of tantalizing topics:  


1. **The Great Outdoors**:
 Whether you're a fan of scaling  mountains, surfing waves, or just enjoying a peaceful picnic in the  park, there's alw
ays an adventure waiting under the open sky. Plus, who  doesn't love a good story about a squirrel with overly ambitious
 acorn  storage plans?  

2. **Technological Triumphs and Tribulations**: From the  latest smartphone shenanigans to rob
ots that might be plotting to take  over our jobs or just vacuum our floors, technology is a never-ending  source of fas
cination and face-palms. It's like watching a sci-fi movie,  but you're in it!  

3. **Culinary Quests**: Share your mos
t epic kitchen  victories, your most hilarious cooking fails, or that one time you  discovered that chocolate and chili 
is a match made in flavor heaven.  Food brings everyone together, especially when there's a chance to say,  'I made this
, and I'm only slightly sure it's edible.'  


Choose your adventure, my human friend, and let's dive into a delightful 
discussion!  


i take 3  


Oh,  taking three, are we? I hope it's something good like cookies from the  jar, or maybe 
you're scooping up a trio of wins in your favorite game!  How can I assist you further with your triumphant trio? 😄  



  


do you remember what we talked about just before ?  


Oh,  you've caught me! If I had a penny for every conversati
on I've had, I'd  be a very wealthy AI, but alas, I don't have the ability to remember  past interactions. It's like eve
ry new chat is a clean slate for me. So,  what would you like to chat about today? Let's make some new memories,  even i
f they're just temporary!
```
---

     
 
all -  [ Using LLM's to help with software licensing ](https://www.reddit.com/r/sysadmin/comments/184ix3n/using_llms_to_help_with_software_licensing/) , 2023-11-29-0910
```
It's not exactly a hot take to suggest that software licensing sucks.

Has anybody had success using LLM's to answer spe
cific licensing questions? ChatGPT is incredibly adept at things like coding, as it's clearly been trained on millions o
f pages of relevant code syntax. 

However, software licensing often requires intense knowledge of product terms which c
an be absurdly complex and lengthy (looking at you Microsoft) ...

[This article](https://www.itamspot.com/make-chatgpt-
a-microsoft-licensing-expert/) describes using chatgpt's customized GPTs in order to train it on Microsoft licensing que
stions.

I've tried ChatGPT 3.5 & 4. They give more generic answers, and sometimes give specifics if it uses Bing to bro
wse. Things like  ['ChatPDF'](https://www.chatpdf.com/)  \- give some success but it seems to have issues citing specifi
c text. [LangChain](https://www.freecodecamp.org/news/langchain-how-to-create-custom-knowledge-chatbots/) also could be 
promising but requires technical knowledge.

**Are there any LLM solutions that you are using with a degree of success f
or licensing help?**
```
---

     
 
all -  [ Pydentic in prompt engineering ](https://www.reddit.com/r/LangChain/comments/184hdjz/pydentic_in_prompt_engineering/) , 2023-11-29-0910
```
Hello guys,

I am new to this field and exploring. 

I came accross pydentic library in python. but i dont understand ho
w can i use that library for prompt engineering?

in my understanding prompt engineering is text to text response. 

you
 give instructions to LLM using structured prompt(which is in english language) and it gives you response considering th
ose prompts. and pydentic we use instead of dataclasses for typeins and validations and for parsing json. 

than how can
 i use it for prompting
```
---

     
 
all -  [ Lanchain Agent stopping chain mid-process ](https://www.reddit.com/r/LangChain/comments/184hcra/lanchain_agent_stopping_chain_midprocess/) , 2023-11-29-0910
```
If someone knows what can cause this and whats the solution, plz help, i can't find anything similar online :/

[Idk why
 but it's doing this every time, so the \\'final response\\' is just something like \\'Thought: \[...\] }{Action\\' inst
ead of the actual final response](https://preview.redd.it/cpbwll8nkq2c1.png?width=476&format=png&auto=webp&s=c517a3e68d0
645d43257a3c47906afe58e38c47d)
```
---

     
 
all -  [ Pinecone vs Astra DB ](https://www.reddit.com/r/LangChain/comments/184gfet/pinecone_vs_astra_db/) , 2023-11-29-0910
```
I'm trying to make a user profile database where each user's history with the LLM and information around the user gets e
mbedded into a vector storage. The user's profile in the vector db will be associated with their user id.  


Which one 
is better for this, Pinecone or Astra DB?

EDIT: Also looking into using Firebase real time DB alongside pinecone
```
---

     
 
all -  [ Langchain Integration by n8n.io or Flowise.ai? ](https://www.reddit.com/r/LangChain/comments/184efvj/langchain_integration_by_n8nio_or_flowiseai/) , 2023-11-29-0910
```
What would you choose on over another? And why?

How these low-code applications can help enhance workflows for develope
rs?

&#x200B;

&#x200B;
```
---

     
 
all -  [ A new way of interacting with Hacker News ](https://www.reddit.com/r/ChatGPTCoding/comments/184bgda/a_new_way_of_interacting_with_hacker_news/) , 2023-11-29-0910
```
I was tired of scrolling down in HackerNews looking for stories that were of interest to me. So I created a ChatBot usin
g langchain and streamlit to 'ask' Hacker News for specific stories about a topic.

&#x200B;

Code: [https://github.com/
neural-maze/talking\_with\_hn](https://github.com/neural-maze/talking_with_hn)

App: [https://newsnerdhackerbot.streamli
t.app/](https://newsnerdhackerbot.streamlit.app/)

&#x200B;

https://reddit.com/link/184bgda/video/rxszs2nj8p2c1/player


&#x200B;
```
---

     
 
all -  [ pdf tables extraction ](https://www.reddit.com/r/LangChain/comments/184b06y/pdf_tables_extraction/) , 2023-11-29-0910
```
hy, trying to perfectly parse table from pdf , but not getting accurate result . and feed it into llm for QA .

P.S - i 
have tried tabula camelot and also many ocr tools such as paddleocr, unstructured, img2table .
also tried with adobe api
 which is 100% accurate , but i dont want to use any api 

any ideas ?
```
---

     
 
all -  [ import { LlamaCpp } from 'langchain/llms/llama_cpp causing 'illegal hardware instruction node index. ](https://www.reddit.com/r/LangChain/comments/1849ltt/import_llamacpp_from_langchainllmsllama_cpp/) , 2023-11-29-0910
```
`console.log('huh');`  
`import { LlamaCpp } from 'langchain/llms/llama_cpp';`  
`const llamaPath = './llama.bin';`  
`c
onst question = 'Where do Llamas come from?';`  
`const model = new LlamaCpp({ modelPath: llamaPath });`  
`console.log(
\`You: ${question}\`);`  
`const response = await model.call(question);`  
`console.log(\`AI : ${response}\`);`  
`{`  

 `'name': 'hmmm',`  
 `'version': '1.0.0',`  
 `'description': '',`  
 `'main': 'index.js',`  
 `'type': 'module',`  
 `
'scripts': {`  
 `'test': 'echo \'Error: no test specified\' && exit 1'`  
  `},`  
 `'author': '',`  
 `'license': 'ISC
',`  
 `'dependencies': {`  
 `'langchain': '^0.0.197-rc.1',`  
 `'node-llama-cpp': '^2.8.0'`  
  `}`  
`}`  


I am try
ing to run Llama on LangChain.js. But I keep getting \`zsh: illegal hardware instruction\`. I deleted everything except 
\``import { LlamaCpp } from 'langchain/llms/llama_cpp';\` and I still get the same error. Do you know how can I fix this
?`

&#x200B;
```
---

     
 
all -  [ Local Rag/embedding clarifications ](https://www.reddit.com/r/LocalLLaMA/comments/1849lrh/local_ragembedding_clarifications/) , 2023-11-29-0910
```
Hi all, I posted originally to langchain sub but didn’t get any response yet, could anyone give some pointers, thanks.


Basic workflow for questioning data locally?

Hi all,

I’m using lang chain js, and most examples I find are using openA
I but I’m using llama.
I managed to get a simple text file embedded and can ask basic questions, but most of the time th
e model just spits out the prompt.

I’m using just cpu at the moment so it’s very slow but that’s ok. I’m experimenting 
with loading txt files, csv files etc but clearly it’s not going well, I can ask some very simple question but most of t
he time it fails.

My understanding is;

1. Load model 
2. Load data and chunk (csv file for example. I chunk usually wi
th something like 200 and by separators /n
3. Load embedding (I’m supposed to load llama gguf model right? The same one 
as in step 1? As a parameter in llamaCppEmbeddings)
4. Vector store in memory 
5. Create chain and ask question
6. Conso
le log answer

Is this concept correct and do you have any tips to help me get better results.

Thank you
```
---

     
 
MachineLearning -  [ [P] A new way of interacting with Hacker News ](https://www.reddit.com/r/MachineLearning/comments/183n6h7/p_a_new_way_of_interacting_with_hacker_news/) , 2023-11-29-0910
```
Hi all!

A couple of days ago, when I was scrolling down Hacker News, exploring news about OpenAI and the latest specula
tion about Q\*, it occurred to me to create a ChatBot that would allow me to interact with Hacker News directly, in a co
nversation.

Using streamlit, langchain and openai functions I managed to create a first version of this chat (I still h
ave to add RAG for news analysis and test other types of LLMs). 

Here is an example, what do you think?

Code: [https:/
/github.com/neural-maze/talking\_with\_hn](https://github.com/neural-maze/talking_with_hn)

App: [https://newsnerdhacker
bot.streamlit.app/](https://newsnerdhackerbot.streamlit.app/)

&#x200B;

https://i.redd.it/rtpof7biqi2c1.gif
```
---

     
 
MachineLearning -  [ [Discussion] Is it possible to built a Multi-LLM Assistant? ](https://www.reddit.com/r/MachineLearning/comments/182uuwp/discussion_is_it_possible_to_built_a_multillm/) , 2023-11-29-0910
```
 

  
For example with the following structure:

* System = GPT-4 Turbo + Llama2 +3rd LLM (!)+ Google or Bing API for we
bsearch + Langchain + any vectorDB + Document upload + longterm Memory + …

Idee behind it is to get more accurate, upda
ted (websearch) and specialized system or even let the LLms discuss your prompt before completion! Question is also, how
 shall the interaction of multiple LLMs in a system be organzied (Algorithm, Python Library …)? And what kind of Interac
tion can/should this be? Master-slave or Multi-Master system?
```
---

     
 
MachineLearning -  [ [D] Made some promises. Time to learn how to conduct very large scale pdf doc analysis. ](https://www.reddit.com/r/MachineLearning/comments/181gzek/d_made_some_promises_time_to_learn_how_to_conduct/) , 2023-11-29-0910
```
I have about a half million pdfs I need to summarize. Very wide range of types: invoices, diagrams, contracts, emails, l
etters, pictures, schedules, notices, data sheets, manuals, more. 

Which is... woof. Something else. I've been trying f
or many hours now to figure out a service/combination thereof that can get me there, but I'm seriously struggling. The *
ideal* solution would be to throw the pdfs in and have it return a csv with dates and summaries, maybe parsed out email 
heading info.

I'm currently running these pdfs through Acrobat OCR now, which its own special hell.

I've tried myriad 
local and webhosted solutions. The BEST results in what is almost the perfect system for this I found on https://docalys
is.com/. Good text results, works in batches, BUT I can only upload a single document at a time. They have a service to 
do batch processing and so I'm waiting to hear from them now. I imagine at the scale I need it's expensive.

I also got 
this solution working: https://github.com/mayooear/gpt4-pdf-chatbot-langchain. Seemed solid, I was able to upload a thou
sand pdfs in a single go, but it would keep returning information from only 2-3 documents. Upload 5? Results for 2-3. Up
load a thousand? Results for 2-3. My uneducated guess is that it's hitting the OpenAI API token limit, but maybe not?

I
 know it's possible, just not whether it's feasible for an end user.
```
---

     
 
MachineLearning -  [ Google PaLM Error [D] ](https://www.reddit.com/r/MachineLearning/comments/17y7arb/google_palm_error_d/) , 2023-11-29-0910
```
Google PaLM Error

Using LangChain and Google PaLM, in sequential chain concept getting following error,

ChatGooglePalm
Error: ChatResponse must have atleast one candidate

Please help!
```
---

     
 
MachineLearning -  [ [D] System Design question for LangChain ](https://www.reddit.com/r/MachineLearning/comments/17x545j/d_system_design_question_for_langchain/) , 2023-11-29-0910
```
Hi

Just to prepare the system design question for LangChain. Is there a resource that can walk me through the high leve
l pipeline? I know there are a bunch of resources that dive into detail implementation. But that's not I want. I want hi
gh level conceptual walk-through. 
```
---

     
 
MachineLearning -  [ [P] GPT vs. StarCraft ](https://www.reddit.com/r/MachineLearning/comments/17ro6el/p_gpt_vs_starcraft/) , 2023-11-29-0910
```
This is the first in a series of webcasts covering the development and experimentation of using GPT algorithms, LangChai
n and Python to control the high-level strategy of a StarCraft II bot. I’ll be running through the basics of the impleme
ntation, discussing the use of prompts and prompt engineering, and demonstrating the implementation in action.

[https:/
/youtu.be/E3Sj2L6ZnXA](https://youtu.be/E3Sj2L6ZnXA)
```
---

     
 
MachineLearning -  [ [D] Is this close enough to be usable? Need your inputs: Automated RAG testing tool. AI Data Pipelin ](https://www.reddit.com/r/MachineLearning/comments/17kkbm0/d_is_this_close_enough_to_be_usable_need_your/) , 2023-11-29-0910
```
Hey there, Redditors! 

I'm back with the latest installment on creating dependable AI data pipelines for real-world pro
duction. 

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://t
opoteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba4
0aab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines. 

With 18 months of hands
-on experience and many user interviews, I realized that with the probabilistic nature of systems, we need better\_testi
ng.gpt:

  
**1. As you build you should test**  
The world of AI is a fast-moving one, and we've realized that just wor
king on systems is not an optimal design choice. By the time your product ships, it might already be using outdated tech
nology. So, what's the lesson here? Embrace change, test along, but be prepared to switch pace.  
**2. No Best Practices
 Yet for RAGs**  
In this rapidly evolving landscape, there are no established best practices. You'll need to make educa
ted bets on tools and processes, knowing that things will change. With the RAG testing tool, I tried allowing for testin
g many potential parameter combinations **automatically**  
**3. Testing Frameworks**  
If your generative AI product do
esn't have users giving feedback, then you are building in isolation. I used [Deepeval](https://github.com/confident-ai/
deepeval) to generate test sets, and they will soon support synthetic test set generation  
**4. Infographics only go so
 far**  
AI researchers and data scientists, while brilliant, end up in a loop of pursuing Twitter promotional content. 
New ways are promoted via new content pieces, but ideally, we need something above simple tracing but less than full-fle
dged analytics. To do this, I stored test outputs in Postgres and created a Superset instance to visualize the results  

**5. Bridging the Gap between VectorDBs**  
There's a noticeable number of Vector DBs. To ensure smooth product develop
ment, we need to be able to switch to best best-performing one, especially since user interviews signal that they might 
start deteriorating after loading 50 million rows

&#x200B;

Github repo is [here](https://topoteretes.notion.site/Going
-beyond-Langchain-Weaviate-Level-3-towards-production-e62946c272bf412584b12fbbf92d35b0?pvs=4)  


Next steps:  
I have q
uestions for you: 

1. What variables do you change when building RAGs?
2. What is the set of strategies I should add to
 the solution? (parent-son etc.)
3. How can I improve it in general? 
4. Is anyone  interested in a leaderboard for best
 parameter configs?

Check out the blog post:

[Link to part 3](https://topoteretes.notion.site/Going-beyond-Langchain-W
eaviate-Level-3-towards-production-e62946c272bf412584b12fbbf92d35b0?pvs=4)

  
*Remember to give this post an upvote if 
you found it insightful!*  
*And also star our* [*Github repo*](https://github.com/topoteretes/PromethAI-Memory)
```
---

     
 
MachineLearning -  [ [D] Relevance Extraction in RAG Pipelines ](https://www.reddit.com/r/MachineLearning/comments/17k6iha/d_relevance_extraction_in_rag_pipelines/) , 2023-11-29-0910
```
I came across this interesting problem in RAG, what I call **Relevance Extraction**.

After retrieving relevant document
s (or chunks), these chunks are often large and may contain several portions **irrelevant** to the query at hand. Stuffi
ng the entire chunk into an LLM prompt impacts token-cost as well as response accuracy (distracting the LLM with irrelev
ant text), and and can also cause bumping into context-length limits.

So a critical step in most pipelines is **Relevan
ce Extraction**: use the LLM to extract **verbatim** only the portions relevant to the query. This is known by other nam
es, e.g. LangChain calls it Contextual Compression, and the RECOMP paper calls it Extractive Compression [https://twitte
r.com/manelferreira\_/status/1713214439715938528](https://twitter.com/manelferreira_/status/1713214439715938528)

Thinki
ng about how best to do this, I realized it is **highly inefficient** to simply ask the LLM to 'parrot' out relevant por
tions of the text: this is obviously slow, and also consumes valuable token generation space and can cause you to bump i
nto context-length limits (and of course is expensive, e.g. for gpt4 we know generation is 6c/1k tokens vs input cost of
 3c/1k tokens).

I realized the best way (or at least a good way) to do this is to **number** the sentences and have the
 LLM simply spit out the relevant sentence **numbers.** Langroid's unique Multi-Agent + function-calling architecture al
lows an elegant implementation of this, in the RelevanceExtractorAgent ([https://github.com/langroid/langroid/blob/main/
langroid/agent/special/relevance\_extractor\_agent.py](https://github.com/langroid/langroid/blob/main/langroid/agent/spe
cial/relevance_extractor_agent.py)).  The agent annotates the docs with sentence numbers, and instructs the LLM to pick 
out the **sentence-numbers** relevant to the query, rather than whole sentences using a function-call (SegmentExtractToo
l [https://github.com/langroid/langroid/blob/main/langroid/agent/tools/segment\_extract\_tool.py](https://github.com/lan
groid/langroid/blob/main/langroid/agent/tools/segment_extract_tool.py)), and the agent's function-handler interprets thi
s message and strips out the indicated sentences by their numbers. To extract from a set of passages, langroid automatic
ally does this async + concurrently so latencies in practice are much, much lower than the sentence-parroting approach.


\[FD -- I am the lead dev of Langroid - [https://github.com/langroid/langroid](https://github.com/langroid/langroid))


I thought this **numbering** idea is a fairly obvious idea in theory, so I looked at LangChain's equivalent `LLMChainExt
ractor` (they call this Contextual Compression [https://python.langchain.com/docs/modules/data\_connection/retrievers/co
ntextual\_compression?ref=blog.langchain.dev](https://python.langchain.com/docs/modules/data_connection/retrievers/conte
xtual_compression?ref=blog.langchain.dev)) and was surprised to see it is the simple '**parrot**' method, i.e. the LLM w
rites out whole sentences verbatim from its input. I thought it would be interesting to compare Langroid vs LangChain, y
ou can see it in this Colab: [https://colab.research.google.com/drive/1RDPCR2xNuBffcmpUuPIXYDRG3SXIJC5F](https://colab.r
esearch.google.com/drive/1RDPCR2xNuBffcmpUuPIXYDRG3SXIJC5F)

On the specific example in the notebook, the Langroid **num
bering** approach is 22x faster and 36% cheaper (with gpt4) than LangChain's **parrot** method (I promise this name is *
not* inspired by their logo :). See table below.

&#x200B;

[Relevance Extraction: Langroid vs LangChain](https://previe
w.redd.it/1m7u6ulq8fxb1.png?width=1108&format=png&auto=webp&s=d2f35cf5db07e2e699baa54b274ffa60833e924a)

&#x200B;

I won
der if anyone had thoughts on relevance extraction, or other approaches. At the very least, I hope langroid's implementa
tion is useful to you -- you can use the `DocChatAgent.get_verbatim_extracts()` ([https://github.com/langroid/langroid/b
lob/main/langroid/agent/special/doc\_chat\_agent.py#L804](https://github.com/langroid/langroid/blob/main/langroid/agent/
special/doc_chat_agent.py#L804)) as part of your pipeline, regardless of whether you are using Langroid for your entire 
system or not.

&#x200B;
```
---

     
