 
all -  [ Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway.  ](https://pathway.com/developers/templates/multimodal-rag) , 2024-07-05-0911
```

```
---

     
 
all -  [ Beginner here: found something confusing ](https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/) , 2024-07-05-0911
```
I've been playing around with GPT4All and langchain, for which there is a minimal demo here:

https://python.langchain.c
om/v0.2/docs/integrations/llms/gpt4all/

In this demo, they invoke the following:

```from langchain_core.callbacks impo
rt StreamingStdOutCallbackHandler```

From the API, it states that this only works with LLMs that support streaming. Acc
ording to the integrations page:

https://python.langchain.com/v0.2/docs/integrations/llms/

gpt4all does NOT support st
reaming. So I'm confused - what gives with this demo?
```
---

     
 
all -  [ Hybrid search with Postgres ](https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/) , 2024-07-05-0911
```
I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.

Anyone using 
Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?

Would love to hear your e
xperiences.
```
---

     
 
all -  [ Need honest feedback on resume (Final year B.Tech student) ](https://www.reddit.com/r/developersIndia/comments/1dvct1d/need_honest_feedback_on_resume_final_year_btech/) , 2024-07-05-0911
```
Hello! I'm a final year CS student from a tier-3 college, and am going to start applying to companies soon (both on and 
off campus). The hiring scene seems to be really terrifying right now, so I want to make sure I put my best foot forward
. I'm looking for brutal feedback on my resume, and how to stand out as an applicant. Thank you!

https://preview.redd.i
t/8w24zb48hjad1.jpg?width=2550&format=pjpg&auto=webp&s=8ad25c3a55ea1db97cdddb41c0b9e22eec842c6f
```
---

     
 
all -  [ Tool for Comparing Outputs of Multiple LLMs from Single Prompts ](https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/) , 2024-07-05-0911
```
I'm searching for a tool that allows users to compare outputs generated by several LLMs using just one prompt. While I u
nderstand that LangChain could potentially enable building such a solution locally, I'm curious if any existing products
 offer this functionality.

I'm weary of manually inputting the same prompt across different models like GPT, Claude, Ba
rd, and Perplexity to cross-reference answers and verify accuracy. Any recommendations or insights would be greatly appr
eciated!     
```
---

     
 
all -  [ Issue with Ollama and Pydantic  ](https://www.reddit.com/r/ollama/comments/1dv947k/issue_with_ollama_and_pydantic/) , 2024-07-05-0911
```
Hi guys,

Are you also facing issues when using Ollama with Pydantic? It seems that the response from the LLM is often n
ot put in the output format as requested by Pydantic. Hence, the subsequent code in our program throws an error.

In our
 tests, we used Codestral-22B and the following function to call the LLM inference. We also tested the same with the API
 from Mistral, and this works. **So it seems there is a problem in the interface between Ollama and Pydantic.**



Do yo
u have good experience with Ollama and Pydantic? Do you know how we can solve this?

Many thanks for your help!

Pseudo 
code (not functional, just to show which functions we call)

    from langchain_experimental.llms.ollama_functions impor
t OllamaFunctions
    
    llm = OllamaFunctions(base_url=Config.HOST_LLM, temperature=0, model='codestral:latest', form
at='json')
    
    llm = llm.with_structured_output(MyPydanticOutputClass)
    
    llm.invoke({...}) # pass data requi
red by the LLM to perform inference 

---- 

We are using Pydantic through the call **with\_structured\_output:**

**llm
.with\_structured\_output(MyPydanticOutputClass)**

So, sometimes there are some fields of the Pydantic object that are 
not parsed, so the output of the LLM is incomplete, for example:

***Got: 3 validation errors for CodeGenerationOutput**
*

***dev\_mode***

***field required (type=value\_error.missing)***

***rtos***

***field required (type=value\_error.m
issing)***

***is\_func\_empty***

And sometimes producing a more general error:

***raise ValueError(***  
***ValueErro
r: Failed to parse a response from codestral-latest output: {}***

---
```
---

     
 
all -  [ Design decision for an LLM app ](https://www.reddit.com/r/StackoverReddit/comments/1dv936n/design_decision_for_an_llm_app/) , 2024-07-05-0911
```


Langchain async for document loader and character splitter

Should I use Async or not? 

I am building a Fast api RAG 
app that uses LLMs to generate responses using langchain but I am confused if it needs async. 

The user flow goes somet
hing like this:
1. User provides a link/ links to a text/pdf document in the form of a url.  
2. Langchain document load
ers are used to load text or pdf from the remote public url.
3. Character splitter is used to split and chunk the docume
nts which is saved into a vector db. 
4. Langchain chain library is used to invoke LLMs via the asynchronous ainvoke(). 


My question is whether the document loading step via langchain and character splitting text via langchain  would need 
to be made async? Langchain doesn’t support async for the libraries I am using. Would I need to implement them myself? I
f so, what are some options to implement? 
```
---

     
 
all -  [ Passing Chat History to Langchain Tool Calling Agent ](https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/) , 2024-07-05-0911
```
I'm working with Langchain to build a tool that calls an agent. Currently, I'm passing the chat history as an input vari
able to the agent. However, I've encountered an issue where the agent doesn't always seem to utilize the history data to
 answer questions consistently. This is especially problematic when users have queries spaced out over 10–15 days.

Is t
here a more efficient way to ensure the agent consistently remembers all chat history and context over multiple sessions
? What approach or best practices should I follow to address this issue?

Thanks in advance for your guidance!
```
---

     
 
all -  [ Load LLM (Mixtral 8x22B) from Azure AI endpoint as Langchain Model ](https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/) , 2024-07-05-0911
```
Hi,

  
I set up Mixtral 8x22B on Azure AI/Machine Learning and now want to use it with Langchain. I have difficulties w
ith the format I am getting, e.g. a ChatOpenAI response looks like this:

    from langchain_openai import ChatOpenAI
  
  llmm = ChatOpenAI()
    llmm.invoke('Hallo')

  
`AIMessage(content='Hallo! Wie kann ich Ihnen helfen?', response_meta
data={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-3.5-turbo', 's
ystem_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='r')`

This is how it looks when I am loading M
ixtral 8x22B with AzureMLChatOnlineEndpoint:

    from langchain_community.chat_models.azureml_endpoint import AzureMLCh
atOnlineEndpoint
    
    from langchain_community.chat_models.azureml_endpoint import (
        AzureMLEndpointApiType,

        CustomOpenAIChatContentFormatter,
    )
    from langchain_core.messages import HumanMessage
    
    chat = Az
ureMLChatOnlineEndpoint(
        endpoint_url='...',
        endpoint_api_type=AzureMLEndpointApiType.dedicated,
       
 endpoint_api_key='...',
        content_formatter=CustomOpenAIChatContentFormatter(),
    )
    
    chat.invoke('Hallo
')

  
`BaseMessage(content='Hallo, ich bin ein deutscher Sprachassistent. Was kann ich für', type='assistant', id='run-
23')`



So with the Mixtral model the output seems to be truncated and also the format is different (BaseMessage vs. AI
Message). How can I change this to make it work just like an ChatOpenAI model?

  
In my application I want to easily sw
itch between these two models.

  
Thanks in advance!
```
---

     
 
all -  [ How do I add meta data to Pinecone documents, I want to maintain overlap between chunks so I don’t w ](https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/) , 2024-07-05-0911
```
I’m developing an app that creates a knowledge base based on transcripts of YouTube videos. And I need a way to have the
 LLM recognize where the transcript came from, I have the data I just don’t know how to implement it effectively 
```
---

     
 
all -  [ History Aware Agent in Langgraph ](https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/) , 2024-07-05-0911
```
Hi,

I built a CRAG application and now want to further improve it. As a fist step I would like to check if the given qu
estion is a followup question of a previous question or not. If it is, then I want to use my messages history to create 
a new question based on the context in the chat history and the actual question. This is similar to the 'history\_aware\
_retriever' from Langchain.

However I am not satisfied with the classification of my model, as it often returns 'False'
 even if it is a followup question. So is there maybe a more elegant way of doing this or would you improve my prompt?


Heres the function within my Langgraph app:

    class CheckFollowupQuestion(BaseModel):
        '''Bool Werte um zu bes
timmen, ob die Frage auf eine zuvor gestellte Frage aufbaut.'''
    
        score: str = Field(
            description
='Die Frage bezieht sich auf eine vorherige Antwort oder zuvor gestellte Frage, 'True' oder 'False''
        )

    def 
followup_question_classifier(state: AgentState):
        messages = state['messages'][-5:]
        print(f'MESSAGES (in 
follow up): {messages}')
        question = state['question']
        system = '''<s>[INST] You assess whether the user'
s question is a follow-up question or not. For this, you get questions from the chat history and assess whether the ques
tion builds on a question or answer from the chat history or not.\n
            Evaluate with 'True' if it is a typical 
follow-up question. Also evaluate with 'True' if it seems that the question refers to a previous answer. \n
            
Evaluate with 'False' if it is a normal question, or if the question has nothing to do with the chat history. Here is an
 example:\n\n
            
            Example of a 'False' evaluation:\n
            Question: 'How much does a kebab c
urrently cost?'\n
            Chat history: [HumanMessage(content='Hello, I am Max, who are you?', id='8'), HumanMessage
(content='What was my name?', id='7'), HumanMessage(content='Name exactly one advantage of Multicloud.', id='f')]\n
    
        Your evaluation: 'False'. Reason: The questions from the chat history have nothing to do with the question asked
.\n\n
            
            Example of a 'True' evaluation:
            Question: 'And how warm will it be there tomo
rrow?'
            Chat history: [HumanMessage(content='Where is Munich located?', id='8'), HumanMessage(content='Which 
dialect is spoken in Munich?', id='7')]
            Your evaluation: 'True'. Reason: The chat history is about the city 
of Munich. In the follow-up question, the user wants to know what the weather will be 'there' tomorrow. Since the previo
us discussion was about Munich, 'there' refers to the city of Munich.
            [/INST]'''
    
        grade_prompt =
 ChatPromptTemplate.from_messages(
            [
                ('system', system),
                (
                 
   'human',
                    'User's question: {question} \n\n Chat history: {chat_history}',
                ),
    
        ]
        )
        llm = ChatOpenAI()
        structured_llm = llm.with_structured_output(GradeQuestion)
      
  grader_llm = grade_prompt | structured_llm
        result = grader_llm.invoke({'question': question, 'chat_history': m
essages})
        state['is_followup_question'] = result.score
        return state
```
---

     
 
all -  [ Pedantic data parsing ](https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/) , 2024-07-05-0911
```
I was not able to find anything on the web, the cases for which the pedantic parsers fail to parse the data coming from 
LLMs. I tried looking under the hood working and say that they are using json parsing, if anyone has info about this ple
ase enlighten me.
```
---

     
 
all -  [ How to increase the inference speed of Map reduce chain in langchain ](https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/) , 2024-07-05-0911
```
I have a problem. It takes literally 30 mins for my map-reduce summarisation chain to produce it's final output. Current
 vRAM is 16GB. What should I do to increase the speed?

Model: Llama2
```
---

     
 
all -  [ Ai Ml. Engg ](https://www.reddit.com/r/ranchi/comments/1dv0nub/ai_ml_engg/) , 2024-07-05-0911
```
Hello everyone. I hope you all are doing good. Any engineer doing Ai ML work here from Ranchi? 
Just wanted to know the 
level of mathematics required. 
Lot of talented Indians, but I feel like we are lagging way behind China, in terms of in
novation and research. I really hope I am wrong here, and my thinking is wrong. Recently came across Langchain and curre
ntly exploring it. Have a great day ahead everyone. Cheers. 
```
---

     
 
all -  [ Is it possible to stream function calling with Gemini? ](https://www.reddit.com/r/GoogleGeminiAI/comments/1dv0kut/is_it_possible_to_stream_function_calling_with/) , 2024-07-05-0911
```
Function calling is supported by Gemini models, but my initial tests with the LangChain implementation seem like I can't
 stream the function call output. I don't actually want to make an API call or something, I just want to get a structure
d output from the language model, and I would like to stream this to my frontend. Does anyone have some experience with 
this? Is it maybe possible to stream function calling output via Google's own implementation, without using LangChain? I
f it is not currently possible, are there any news or rumors about it?
```
---

     
 
all -  [ Frontend Resume Help! ](https://i.redd.it/di5xwxpjvfad1.jpeg) , 2024-07-05-0911
```
Hi Everyone! I'm looking for reviews on my resume. I have been a frontend developer for around 3 years and I'm looking f
or work. Been getting some hits (but not too great), I'm trying to see if there's anything that can be improved to get m
ore callbacks.

Would love to get some feedback. 
Thanks!

```
---

     
 
all -  [ How to incorporate a knowledge graph in RAG ](https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/) , 2024-07-05-0911
```

Hi there,

I am currently working on building a chatbot for internal use in my company. I have developed a relatively c
lassic RAG framework and now want to include a knowledge graph. I have read several papers on this topic, but I am confu
sed about how to actually 'activate' a graph in the RAG flow.

So far, I have found different approaches based on extrac
ting entities and relationships from chunks to generate community summaries of the related entities. This process occurs
 in the offline stage. At least, that is what I have tried to do. I am wondering how to activate this correctly. Current
ly, I match entities from the input with summaries as the flow runs, but I have the impression that others use the graph
 aspect differently, possibly using a function to inject relevant context into the LLM.

Can you help me understand this
 better?
```
---

     
 
all -  [ Ollama Function Calling Example ](https://www.reddit.com/r/ollama/comments/1duxz59/ollama_function_calling_example/) , 2024-07-05-0911
```
Just putting this out there for anyone interested in setting  up simple function calls using Ollama.

[https://github.co
m/BassAzayda/ollama-function-calling](https://github.com/BassAzayda/ollama-function-calling)



  
EDIT:  
This is using
 Langchain Experimental library, also recently discovered another nice light library called 'llm-axe'  
[https://github.
com/emirsahin1/llm-axe](https://github.com/emirsahin1/llm-axe)  
Might want to give this a a try too!
```
---

     
 
all -  [ LangFlow/LangChain to production ](https://www.reddit.com/r/LangChain/comments/1duug75/langflowlangchain_to_production/) , 2024-07-05-0911
```
Hello,

I'veen using LangFlow/LangChain to test a few concepts to create my RAG and works flawless.

Now my question may
be too easy or too complex, how to deliver to production?  
I saw the 'code snippets' for each component, but i can't fi
gure out to deliver directly to production without the GUI interface of LangFlow.

Here is a draft from my project:

htt
ps://preview.redd.it/iukb1l8fjead1.png?width=2532&format=png&auto=webp&s=1356a500acda75770e04aec93a89a5c2d4acdd1c

  



Any help will be really appreciated.

Thx
```
---

     
 
all -  [ How to build a reusable chain that takes the output of another chain as a keyword argument for the t ](https://www.reddit.com/r/LangChain/comments/1dursrw/how_to_build_a_reusable_chain_that_takes_the/) , 2024-07-05-0911
```
I'm trying to build reusable chains and I'm having issues trying to set it up so it takes the output of the previous cha
in as one of the template format arguments.

In this example, I have a chain that generates a title based on a topic and
 a chain that translates the title into a different language. The topic and the language I want to provide them when I i
nvoke the final chain.

If my 2nd chain only has one single input for the template (because I hardcode the language), ev
erything works perfectly:

    # Title Chain
    title_template = PromptTemplate.from_template('Generate a title for a b
log post about the following topic: {topic}')
    generate_title = title_template | llm | StrOutputParser()
    
    # T
ranslate Chain
    translate_template = PromptTemplate.from_template('Translate the following text to spanish: {text}')

    translate_chain = translate_template | llm | StrOutputParser()
    
    # Combine and run
    combined_chain = gener
ate_title | translate_chain
    result = combined_chain.invoke({'topic': 'The benefits of exercise'})

But if I want to 
be able to provide the language when I invoke the final chain then, I dont know how to pass the output to the template d
ictionary. See the 'HOW\_TO\_PASS\_OUTPUT\_OF\_PREVIOUS\_CHAIN' in the code

    # Title Chain
    title_template = Prom
ptTemplate.from_template('Generate a title for a blog post about the following topic: {topic}')
    generate_title = tit
le_template | llm | StrOutputParser()
    
    # Translate Chain
    translate_template = PromptTemplate.from_template('
Translate the following text to {language}: {text}')
    translate_chain = {'language': itemgetter('language'), 'text': 
HOW_TO_PASS_OUTPUT_OF_PREVIOUS_CHAIN?} | translate_template | llm | StrOutputParser()
    
    # Combine and run
    com
bined_chain = generate_title | translate_chain
    result = combined_chain.invoke({'topic': 'The benefits of exercise', 
'language': 'Spanish'})

  
I also tried to use a lambda function but the itemgetter doesn't seem to work:

    translat
e_chain = (
        lambda text: {'language': itemgetter('language'), 'text': text}
        | translate_template
       
 | llm
        | StrOutputParser()
    )

  
If anyone knows the answer I would be much appreciated.
```
---

     
 
all -  [ Read and write to Google sheets ](https://www.reddit.com/r/LangChain/comments/1dursic/read_and_write_to_google_sheets/) , 2024-07-05-0911
```
I’m not a programmer, only got basic understanding of the concepts. I’m wanting to do the following (I’ll probably get s
omeone from Upwork to do it) read and write to the company daily operations Google sheet using natural language. Users c
an type requests and the agent will write and read the document. I’m thinking I’ll have python scripts to do actions lik
e insert new work orders with all formulas in place.

My concern is, what happens if someone edits the GS manually. The 
agent will not know exactly which row a certain data is if I want to update it. Or can the agent read first to ensure th
e data is in the target position and then update it? 
```
---

     
 
all -  [ Retrieve a single document ](https://www.reddit.com/r/LangChain/comments/1duoub8/retrieve_a_single_document/) , 2024-07-05-0911
```
I've created a RAG with document retriever and everything works fine. 

I want to add a feature: the user can specify a 
document to apply search to. 
The question the user ask is something related to the single document, OR something like '
resume it', 'translate it', whatever. 

What I though was use parent document retriever and inject the whole document (d
on't think about tokens limit at the moment) in the context. Is theoretically possible but my question is: how can I det
ect a 'manipulation' question (resume, translate, etc)?

In these cases I want to use parent document (or whatever strat
egy), and in other cases the default strategy. 

Should I put some ML model between question and retriever? I want to av
oid using AI tools to not consume another open Ai call to detect what retriever I should use. 

Thanks 
```
---

     
 
all -  [ Best way to summarize a book? ](https://www.reddit.com/r/LocalLLaMA/comments/1dulrgg/best_way_to_summarize_a_book/) , 2024-07-05-0911
```
So, I have a collection of ebooks on my PC, and I want to summarize them.

I see there are many methods for doing this, 
but what was the best for you? I thought of summarizing chunks of it and adding them up in a final summary, but this can
 get buggy with LLM's due to the lack of context, and passing them through Claude or Gemini 2 is too expensive.

Word cl
ouds, vector databases, reducemap like in langchain, etc., etc.

Any other ideas of how to approach this?

My goal is to
 create a summary of each book then vectorize that and create a semantic search engine for my books. Maybe I could make 
an online version if it's good enough.

Thanks.

--

EDIT: Using gemini-1.5-flash seems good enough, however using pro, 
chat-gpt or claude yields even better results, however not scalable too much. Will chet other free alternatives.

Thanks
 a lot for your help.

What I'm doing is summarizing the book in chapters, sending them to 1.5-flash which is free. Then
 for the next chapter sending the previous summary + the chapter, and so on. This yields a consistent result with good m
odels, but not perfect yet.
```
---

     
 
all -  [ Unable to use Langchain with LiteLLM proxy ](https://www.reddit.com/r/LangChain/comments/1dukp60/unable_to_use_langchain_with_litellm_proxy/) , 2024-07-05-0911
```
The LiteLLMProxy needs a special authentication header (see [LiteLLM docs](https://litellm.vercel.app/docs/proxy/user_ke
ys)) which needs to go into a special HTTP header field:

    curl --location 'http://litellm/chat/completions' \
      
     -H 'Content-Type: application/json' \ 
           -H 'Authorization: Bearer <redacted>' ...

 As far as I can tell,
 it's impossible to get `langchain ` 
 to stuff this authorization header into its requests, meaning that `langchain` 
 
cannot be used with `LiteLLM` proxy atm.

My co-workers and I looked deep into the code to try to find a solution, but t
here are so many layers from a model's `invoke` method to anything that makes an HTTP request that we were defeated. Or 
perhaps I'm just missing something?
```
---

     
 
all -  [ Could use some advice on an approach to bring back a similarity search result from a vector store an ](https://www.reddit.com/r/LangChain/comments/1duhvrq/could_use_some_advice_on_an_approach_to_bring/) , 2024-07-05-0911
```
I am creating a fairly straightforward streaming chat thing.  I need to supplement the final llm result of my pretty sim
ple rag chain with a link that I get from a separate vector store search.  Literally just need to tack it on at the end 
of the result.  Does this fit any of the component types in LangChain?  Or should I just do it completely outside of the
 chain?  Any thoughts are appreciated.
```
---

     
 
all -  [ My app works fine in dev but I get type errors on Vercel.  ](https://www.reddit.com/r/nextjs/comments/1duhusj/my_app_works_fine_in_dev_but_i_get_type_errors_on/) , 2024-07-05-0911
```
So I'm making a personal app for fun using the template form vercel: [https://vercel.com/guides/nextjs-langchain-vercel-
ai](https://vercel.com/guides/nextjs-langchain-vercel-ai)

Everything works as it should in dev. It builds successfully 
on Vercel but I get these errors when I try to use the chatbot agent: 

    TypeError: Z.inputVariables is not iterable

        at (node_modules/@langchain/langgraph/node_modules/@langchain/core/dist/prompts/chat.js:689:0)
        at (node_
modules/@langchain/langgraph/dist/prebuilt/react_agent_executor.js:95:41)
        at (node_modules/@langchain/langgraph/
dist/prebuilt/react_agent_executor.js:35:0)
        at (app/api/chat/agents/route.ts:81:35)
        at (node_modules/nex
t/dist/esm/server/future/route-modules/app-route/module.js:195:0)
        at (node_modules/next/dist/esm/server/future/r
oute-modules/app-route/module.js:124:0)
        at (node_modules/next/dist/esm/server/future/route-modules/app-route/mod
ule.js:257:0)
        at (node_modules/next/dist/esm/server/web/edge-route-module-wrapper.js:81:0)
        at (node_modu
les/next/dist/esm/server/web/adapter.js:158:0)

It looks like the error is from the langchain module and not my code? I'
m completely lost at what to do and would really appreciate any help. 

Thank you. 
```
---

     
 
all -  [ RAG for CSV but with Sample Question and Answers? ](https://www.reddit.com/r/LangChain/comments/1duhla8/rag_for_csv_but_with_sample_question_and_answers/) , 2024-07-05-0911
```
I'm trying to build an RAG that answers questions about a dataset that I have using the create_pandas_dataframe_agent. I
 also have a long list of sample questions and answers that I want the RAG to imitate but not exactly copy. These questi
ons contain some domain knowledge and I've also added some information about the columns at the end of these sample ques
tions and answers. 

I'm currently passing this as a prefix parameter but I'm not sure if that's the best way to do this
. 

The idea is to have this pandas agent as something that can answer questions that don't require pandas as well. What
's the best way to build this? Thanks in advance!
```
---

     
 
all -  [ How ollama create embedding using llama2 ](https://www.reddit.com/r/ollama/comments/1dufiah/how_ollama_create_embedding_using_llama2/) , 2024-07-05-0911
```
ollama can use the llama2 model to generate embeddings (see: https://api.python.langchain.com/en/latest/embeddings/langc
hain\_community.embeddings.ollama.OllamaEmbeddings.html).

However, llama2 is not designed as an embedding model. Does a
nyone know how Ollama handled generating embeddings with Llama2?
```
---

     
 
all -  [ Fine-tune LLMs for classification task
 ](https://www.reddit.com/r/LangChain/comments/1duedyv/finetune_llms_for_classification_task/) , 2024-07-05-0911
```
I would like to use an LLM (Llama3 or Mistral for example) for a multilabel-classification task. I have a few 1000 examp
les to train the model on, but not sure what's the best way and library to do that. Is there any best practice how to fi
ne-tune LLMs for classification tasks?
```
---

     
 
all -  [ Is there a better approach for generating proper SQL queries for large databases to retrieve relevan ](https://www.reddit.com/r/LangChain/comments/1dudch2/is_there_a_better_approach_for_generating_proper/) , 2024-07-05-0911
```
Hello everyone,

I've been working on developing an enterprise platform for the past 2 months. This platform allows our 
clients to connect multiple data sources and create customized features powered by LLM for their products. The main issu
e I'm encountering is creating an appropriate query to retrieve relevant information from a large database with over 200
 tables, each containing approximately 30-50 columns. I've experimented with various approaches such as Langgraph, custo
m LCEL, retriever with Langgraph, and different LLMs, but I'm still not getting the desired response. If anyone has expe
rience with this type of problem, I would greatly appreciate it if you could share your knowledge. Thank you in advance.

```
---

     
 
all -  [ Can't get LlaMa 2 to generate responses ](https://www.reddit.com/r/LocalLLaMA/comments/1ducthu/cant_get_llama_2_to_generate_responses/) , 2024-07-05-0911
```
I've been trying to get the llama-2-7b-hf model to generate text for a simple use case but it doesn't seem to be doing a
nything when I run it. I'm using this code:

\`

    from langchain_huggingface import HuggingFacePipeline
    import to
rch
    from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline
    
    model_dir = 'C
://Users//shrim//Llama-2-7b-hf' #change the directory to the model later
    tokenizer = AutoTokenizer.from_pretrained(m
odel_dir)
    model = AutoModelForCausalLM.from_pretrained(
        model_dir, torch_dtype=torch.float16, trust_remote_c
ode=True, device_map='auto'
    )
    
    generation_config = GenerationConfig.from_pretrained(model_dir)
    generatio
n_config.max_new_tokens = 1024
    generation_config.temperature = 0.0001
    generation_config.top_p = 0.95
    generat
ion_config.do_sample = True
    generation_config.repetition_penalty = 1.15
    
    text_pipeline = pipeline(
        '
text-generation',
        model=model,
        tokenizer=tokenizer,
        generation_config=generation_config,
    )
 
   
    llm=HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={'temperature':0})
    
    result = llm.invoke(
  
      'Explain the difference between ChatGPT and open source LLMs in a couple of lines.'
    )
    print(result)
    fr
om langchain_huggingface import HuggingFacePipeline
    import torch
    from transformers import AutoTokenizer, AutoMod
elForCausalLM, GenerationConfig, pipeline
    
    
    model_dir = 'C://Users//shrim//Llama-2-7b-hf' #change the direct
ory to the model later
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForCausalLM.from_pr
etrained(
        model_dir, torch_dtype=torch.float16, trust_remote_code=True, device_map='auto'
    )
    
    
    ge
neration_config = GenerationConfig.from_pretrained(model_dir)
    generation_config.max_new_tokens = 1024
    generation
_config.temperature = 0.0001
    generation_config.top_p = 0.95
    generation_config.do_sample = True
    generation_co
nfig.repetition_penalty = 1.15
    
    
    text_pipeline = pipeline(
        'text-generation',
        model=model,
 
       tokenizer=tokenizer,
        generation_config=generation_config,
    )
    
    
    llm=HuggingFacePipeline(pip
eline=text_pipeline, model_kwargs={'temperature':0})
    
    
    result = llm.invoke(
        'Explain the difference 
between ChatGPT and open source LLMs in a couple of lines.'
    )
    print(result)

\`

When i run it, I get the follow
ing in the terminal, the response is not generated and the terminal continues to run:

`Loading checkpoint shards: 100%|
████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.87it/s]`

`WARNING:root:Some parameters are on th
e meta device device because they were offloaded to the cpu and disk.`

  
I have these dependencies installed in a virt
ual environment:  
`torch`  
`transformers`  
`langchain`  
`xformers`  
`chromadb`  
`sentence_transformers`  
`tokeniz
ers`  
`optimum`  
`unstructured`  
`langchain_huggingface`  
`accelerate`

My computer specifications:

OS: Windows 11 
Home  
CPU: 11th Gen Intel(R) Core(TM) i5-11300H @ 3.10GHz  
RAM: 16GB DDR4 3200MHz  
GPU: Intel(R) Iris(R) Xe Graphics 
 
Python version: 3.12.4

Any help would be appreciated, thanks.
```
---

     
 
all -  [ Routing in LangChain JS – Use LLM Classifiers to Call Different Prompts ](https://www.reddit.com/r/LangChain/comments/1duc2qb/routing_in_langchain_js_use_llm_classifiers_to/) , 2024-07-05-0911
```
Hey Everyone, I've made a short example of Routing in LangChain JS by using LLM Classifiers to call different prompts ba
sed on the content of the query:  
[https://www.js-craft.io/blog/routing-langchain-js-different-prompts-based-on-query-t
ype/](https://www.js-craft.io/blog/routing-langchain-js-different-prompts-based-on-query-type/)

In case someone may fin
d it useful :) 
```
---

     
 
all -  [ RAG pipline test ](https://www.reddit.com/r/LangChain/comments/1dubm7s/rag_pipline_test/) , 2024-07-05-0911
```
Hey everyone,

I'm currently testing the RAG (Retrieval-Augmented Generation) pipeline with RAGAS, but I'm facing some c
hallenges. The results aren't as good as expected. Could someone please help me with evaluating the RAG pipeline effecti
vely? Any tips or advice would be greatly appreciated  
#langchain 
```
---

     
 
all -  [ LLM Agent around a BigQuery table ](https://www.reddit.com/r/LangChain/comments/1dubb96/llm_agent_around_a_bigquery_table/) , 2024-07-05-0911
```
HI !

I'm trying to POC a little project.

I have a BigQuery table that looks like this:

|conversation\_id|message\_id|
message\_content|created\_at|
|:-|:-|:-|:-|
|1|1|Hello !|2014-11-19 06:14:03 UTC|
|1|2|Hey, nice to meet you !|2014-11-1
9 06:14:05 UTC|
|2|1|Hello, I have a problem|2015-04-10 11:25:50 UTC|
|2|2|What is your problem ?|2015-04-10 11:25:55 UT
C|

  
This table has millions of rows of conversations / messages.

I would like to configure an LLM around that table,
 so I could ask data analysis questions like:

**What were the main problems encountered by the users during the month o
f November 2014 ?**

  
What would you recommend ? Having such a large amount of data makes this a little tricky for me.

```
---

     
 
all -  [ I built *useful* AI agents with perplexity search and knowledge access. (Here's how) ](https://www.reddit.com/r/LangChain/comments/1duaiwv/i_built_useful_ai_agents_with_perplexity_search/) , 2024-07-05-0911
```
Hey everyone,

AI agents are all around but they lack real-world exposure. Asking them can be kinda hit-or-miss.

I foun
d a way to make them way more useful. I built an AI agent with Perplexity search capabilities, and now it can look stuff
 up on the internet and has access to memory and knowledge. It can be used for research, financial analysis, recipe find
ing, emailing and so on.

The agent is built using Phidata (a framework to build agents with access to knowledge, memory
 and tools).  Here’s a quick guide on how to do it using Portkey's AI Gateway. Link- [https://git.new/Portkey-Phidata](h
ttps://git.new/Portkey-Phidata)

Let me know your thoughts on this

Cheers!
```
---

     
 
all -  [ Image generation models with crewai ? ](https://www.reddit.com/r/crewai/comments/1du981q/image_generation_models_with_crewai/) , 2024-07-05-0911
```
I am looking for some guidance to use image generation models from stability diffusion or titan model. I am getting the 
following error because the model doesn't seem to be called properly and I didn't find any example on the internet on a 
crewai agent using an image generation model. Anyone can help me make this work please

    File /opt/conda/lib/python3.
10/site-packages/langchain_community/llms/bedrock.py:655, in BedrockBase._prepare_input_and_invoke_stream(self, prompt, 
system, messages, stop, run_manager, **kwargs)
        652     response = self.client.invoke_model_with_response_stream(
**request_options)
        654 except Exception as e:
    --> 655     raise ValueError(f'Error raised by bedrock service
: {e}')
        657 for chunk in LLMInputOutputAdapter.prepare_output_stream(
        658     provider, response, stop, 
True if messages else False
        659 ):
        660     yield chunk
    
    ValueError: Error raised by bedrock serv
ice: An error occurred (ValidationException) when calling the InvokeModelWithResponseStream operation: Malformed input r
equest, please reformat your input and try again.

...   

```
---

     
 
all -  [ RAG system with excel sheets  ](https://www.reddit.com/r/LangChain/comments/1du8uem/rag_system_with_excel_sheets/) , 2024-07-05-0911
```
For about a month or more, I've been trying to build an RAG system and understand the tools used for a project, which in
volves dealing with Excel data. I've encountered issues with the standard method like split and embedding the data ,espe
cially when dealing with excel file containing multiple sheets or pages. After researching, I found a few solutions incl
uding Langchain Agent, but encountered errors when trying to use it. Any solutions, suggestions, or resources that could
 help me? Thank you.
```
---

     
 
all -  [ Cost monitoring for Langchain apps using AWS Bedrock with Langfuse ](https://www.reddit.com/r/LangChain/comments/1du8aue/cost_monitoring_for_langchain_apps_using_aws/) , 2024-07-05-0911
```
Hello everyone,

Here's a new post on how to add cost monitoring to all your Langchain apps that use AWS Bedrock using L
angfuse. This is actually what I'm using in production and I recommand it!

Here's the link: [link](https://www.metadocs
.co/2024/07/03/monitor-your-langchain-app-cost-using-bedrock-with-langfuse/).

Have a nice read!
```
---

     
 
MachineLearning -  [ [P] Seeking Feedback on My GenAI Job Fit Project - New to LangChain/LangGraph ](https://www.reddit.com/r/MachineLearning/comments/1dgns9p/p_seeking_feedback_on_my_genai_job_fit_project/) , 2024-07-05-0911
```
Hi all,

Soo, i have been working on a a projectcalled [GenAI Job Fit](https://github.com/DAVEinside/GenAI_Job_Fit). It'
s an AI-driven system designed to enhance job applications by providing tailored recommendations based on individual pro
files.

I'm relatively new to LangChain and LangGraph, and I've incorporated them into this project. I would greatly app
reciate it if you could check out the repository and provide any feedback or suggestions for improvement.

Your insights
 on how I can better implement LangChain/LangGraph, or any other aspect of the project, would be incredibly valuable. I'
m eager to learn and make this project as robust as possible.

Thank you in advance for your time and feedback!

Repo Li
nk : [https://github.com/DAVEinside/GenAI\_Job\_Fit](https://github.com/DAVEinside/GenAI_Job_Fit)
```
---

     
 
MachineLearning -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-07-05-0911
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
deeplearning -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-07-05-0911
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
 
deeplearning -  [ What is ReAct Prompting? the most important piece in agentic frameworks ](https://www.reddit.com/gallery/1djk4nk) , 2024-07-05-0911
```
“What is ReAct Prompting? the most important piece in agentic frameworks” - A quick read from Mastering LLM (Large Langu
age Model) 'Coffee Break Concepts' Vol.6

This document deeps dive into the ReAct Prompting method and why it's importan
t:
1. Limitations of LLM
2. Why ReAct prompting matters?
3. How ReAct Works?
4. LangChain Implementation
5. Why Prompt w
ithin agentic frameworks Matters?

Comment below on which topic you want to understand next in this 'Coffee Break Concep
ts' series and we will include those topics in upcoming weeks.
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-07-05-0911
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
