 
all -  [ Convo-UI: an all-in-one chatbot UI template ](https://www.reddit.com/r/software/comments/1g7m7fg/convoui_an_allinone_chatbot_ui_template/) , 2024-10-20-0914
```
Dear Developers,

I have created this repository to support your projects. This is not a product but a template that wil
l **ease your workflow**.

Here's the background:

While working on a Retrieval-Augmented Generation (RAG) project, I fo
und that no existing chat UI template offered all the features I needed. In response, I decided to build my own, incorpo
rating those missing functionalities.

Chatbot UI: [https://convo-ui.vercel.app](https://convo-ui.vercel.app)

Repositor
y: [https://github.com/RaghaRao314159/Convo-UI](https://github.com/RaghaRao314159/Convo-UI)

**Key features included in 
both ConvoUI and TurboGPT:**

1. **Multiple Chats/Conversations Support:** While Gradio is great, it only supports a sin
gle active conversation at a time. My template addresses this by storing multiple chats locally (via localStorage) and l
inking them to authentication keys.
2. **Authentication:** The template integrates authentication using OpenAI API keys 
for security and access control.
3. **Dark and Light Mode:** This feature can adapt to system settings or be manually to
ggled in the UI, providing flexibility in appearance.

**New features that I added:**

1. **Markdown Table Parsing and D
isplay:** Many existing chatbot UI templates struggle with cleanly rendering markdown tables, including TurboGPT. My ver
sion resolves this, ensuring tables display as neatly as they do in OpenAI's UI.
2. **Custom Backend Model Integration:*
* Most chatbot UIs lack the ability to interact with custom backend models, as these typically require different object 
classes for communication compared to standard API calls. My template includes examples for calling backend models using
 Flask and Langserve. You can test this by selecting a backend model. Click to visit the [Backend Repo](https://github.c
om/RaghaRao314159/Convo-UI_backend).
3. **Streaming Chatbot Outputs:** While streaming outputs is straightforward with f
oundation models, it becomes complex when dealing with backend servers. After extensive testing, I successfully implemen
ted streaming capabilities for backend models in this template. Unfortunately, pythonanywhere voids this and my backend 
is currently on pythonanywhere. I will move it to another server in due time.



***I hope this template helps you accel
erate the development of your full-stack applications.***

-------------------------------------------------------------
----------------------------

**More resources on building chatbots**

I have thoroughly explored various approaches to 
building a fully custom Retrieval-Augmented Generation (RAG) LLM chatbot, and the results of my work can be found in my 
[RAG repository](https://github.com/RaghaRao314159/AuditBot_backend). Unlike many other online resources, this repositor
y offers a complete full-stack solution for developing a RAG chatbot.

**For Frontend Developers:**

I have included mul
tiple UI options, from simple mockups to production-grade interfaces, along with the necessary code to integrate the fro
ntend with the backend. The chat interface is a React application, similar to this one, utilizing LangServe. You can fin
d the frontend code in the [AuditBot repository](https://github.com/RaghaRao314159/AuditBot_frontend).

**For Machine Le
arning Engineers:**

I have conducted extensive experiments to improve the RAG framework, covering techniques such as Hy
DE, recursive retrieval, and others. Additionally, the repository provides implementations of frameworks like Langchain 
and LlamaIndex. It also includes setups for data stores, examples of prompt engineering with GuardRails, and much more. 
 
  
-----------------------------------------------------------------------------------------  
  
**Feedback**

Do let
 me know if you would like to see other additional features would be useful in a chatbot UI. 
```
---

     
 
all -  [ Best way to get started in implementing a PoC for an AI agent with semantic understanding? ](https://www.reddit.com/r/LangChain/comments/1g7lk65/best_way_to_get_started_in_implementing_a_poc_for/) , 2024-10-20-0914
```
I have a background in time-series analysis and I work for a small company (read: startup) that works on GenAI. As part 
of that, my manager has asked me to produce ASAP a proof-of-concept implementation of an AI agent on large document reco
gnition ASAP - specifically, we have a meeting with a client wanting a PoC of an AI agent that you can ask questions to 
about a corpus of text that the client uploads.

Specifically, my manager has asked me to look into performing OCR on a 
large document (~200 pages) and uploading it into a Chroma vector store, and implementing a question-answer system with 
an AI agent that performs semantic understanding for the client to use. I'm going to be burning the midnight oil for the
 next few days so I'd like some advice on how to get started. Are there any tutorials or resources that I can deal with?


(Note: I posted this on the machine learning sub, but it looks like it got quietly removed the instant of posting...)
```
---

     
 
all -  [ RAG + Multimodal Generative Intake ](/r/ollama/comments/1g7ijrj/rag_multimodal_generative_intake/) , 2024-10-20-0914
```

```
---

     
 
all -  [ Connecting to Llama 3.2 with Azure ML endpoint  ](https://www.reddit.com/r/LangChain/comments/1g7jo40/connecting_to_llama_32_with_azure_ml_endpoint/) , 2024-10-20-0914
```

Anyone know why am I getting the following error on this . The endpoint is dedicated and deployed via Azure AI studio 


ValueError: Error while formatting response payload for chat model of type  `AzureMLEndpointApiType.dedicated`. Are you
 using the right formatter for the deployed  model and endpoint type?

### Code 
â€˜â€™â€™
from langchain_community.chat_model
s.azureml_endpoint import (
    AzureMLEndpointApiType,
    CustomOpenAIChatContentFormatter,
    AzureMLChatOnlineEndpo
int
)
from langchain_core.messages import HumanMessage

chat = AzureMLChatOnlineEndpoint(
    endpoint_url='https://xxx.
xxxx.inference.ml.azure.com/score',
    endpoint_api_type=AzureMLEndpointApiType.dedicated,
    content_formatter=Custom
OpenAIChatContentFormatter(),
    endpoint_api_key=os.getenv('AZURE_LLAMA_3_2_API_KEY'),
    model_kwargs={'temperature'
: 0}
)

response = chat.invoke(
    [HumanMessage(content='Will the Collatz conjecture ever be solved?')]
)
print(respon
se)
â€˜â€™â€™


## Error trace 

â€˜â€™â€™

Error                                  Traceback (most recent call last)
File c:\POC\san
dbox\notebooks-for-testing\.venv\Lib\site-packages\langchain_community\chat_models\azureml_endpoint.py:140, in CustomOpe
nAIChatContentFormatter.format_response_payload(self, output, api_type)
    [139](file:///C:/POC/sandbox/notebooks-for-t
esting/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:139) try:
--> [140](file:///C:/POC/sa
ndbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:140)     choice 
= json.loads(output)['output']
    [141](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_
community/chat_models/azureml_endpoint.py:141) except (KeyError, IndexError, TypeError) as e:

KeyError: 'output'

The a
bove exception was the direct cause of the following exception:

ValueError                                Traceback (mo
st recent call last)
Cell In[63], [line 16](vscode-notebook-cell:?execution_count=63&line=16)
      [6](vscode-notebook-
cell:?execution_count=63&line=6) from langchain_core.messages import HumanMessage
      [8](vscode-notebook-cell:?execut
ion_count=63&line=8) chat = AzureMLChatOnlineEndpoint(
      [9](vscode-notebook-cell:?execution_count=63&line=9)     en
dpoint_url='https://xxx.xxx.inference.ml.azure.com/score',
     [10](vscode-notebook-cell:?execution_count=63&line=10)  
   endpoint_api_type=AzureMLEndpointApiType.dedicated,
   (...)
     [13](vscode-notebook-cell:?execution_count=63&line=
13)     model_kwargs={'temperature': 0}
     [14](vscode-notebook-cell:?execution_count=63&line=14) )
---> [16](vscode-n
otebook-cell:?execution_count=63&line=16) response = chat.invoke(
     [17](vscode-notebook-cell:?execution_count=63&lin
e=17)     [HumanMessage(content='Will the Collatz conjecture ever be solved?')]
     [18](vscode-notebook-cell:?executio
n_count=63&line=18) )
     [19](vscode-notebook-cell:?execution_count=63&line=19) print(response)

File c:\POC\sandbox\n
otebooks-for-testing\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:284, in BaseChatModel.invoke(
self, input, config, stop, **kwargs)
    [273](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/lang
chain_core/language_models/chat_models.py:273) def invoke(
    [274](file:///C:/POC/sandbox/notebooks-for-testing/.venv/
Lib/site-packages/langchain_core/language_models/chat_models.py:274)     self,
    [275](file:///C:/POC/sandbox/notebook
s-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:275)     input: LanguageModelInput,

   (...)
    [279](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/c
hat_models.py:279)     **kwargs: Any,
    [280](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/lan
gchain_core/language_models/chat_models.py:280) ) -> BaseMessage:
    [281](file:///C:/POC/sandbox/notebooks-for-testing
/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:281)     config = ensure_config(config)
    [282]
(file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:282)
     return cast(
    [283](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language
_models/chat_models.py:283)         ChatGeneration,
--> [284](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/sit
e-packages/langchain_core/language_models/chat_models.py:284)         self.generate_prompt(
    [285](file:///C:/POC/san
dbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:285)             [self.
_convert_input(input)],
    [286](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/la
nguage_models/chat_models.py:286)             stop=stop,
    [287](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Li
b/site-packages/langchain_core/language_models/chat_models.py:287)             callbacks=config.get('callbacks'),
    [2
88](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:2
88)             tags=config.get('tags'),
    [289](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/
langchain_core/language_models/chat_models.py:289)             metadata=config.get('metadata'),
    [290](file:///C:/POC
/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:290)             ru
n_name=config.get('run_name'),
    [291](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_
core/language_models/chat_models.py:291)             run_id=config.pop('run_id', None),
    [292](file:///C:/POC/sandbox
/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:292)             **kwargs,

    [293](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_model
s.py:293)         ).generations[0][0],
    [294](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/la
ngchain_core/language_models/chat_models.py:294)     ).message

File c:\POC\sandbox\notebooks-for-testing\.venv\Lib\site
-packages\langchain_core\language_models\chat_models.py:784, in BaseChatModel.generate_prompt(self, prompts, stop, callb
acks, **kwargs)
    [776](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_m
odels/chat_models.py:776) def generate_prompt(
    [777](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-pac
kages/langchain_core/language_models/chat_models.py:777)     self,
    [778](file:///C:/POC/sandbox/notebooks-for-testin
g/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:778)     prompts: list[PromptValue],
   (...)
  
  [781](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.
py:781)     **kwargs: Any,
    [782](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core
/language_models/chat_models.py:782) ) -> LLMResult:
    [783](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/si
te-packages/langchain_core/language_models/chat_models.py:783)     prompt_messages = [p.to_messages() for p in prompts]

--> [784](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_model
s.py:784)     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)

File c:\POC\sandbox\noteb
ooks-for-testing\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:641, in BaseChatModel.generate(se
lf, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)
    [639](file:///C:/POC/sandbox/notebooks-fo
r-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:639)         if run_managers:
    [640](
file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:640) 
            run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))
--> [641](file:///C:/POC/sandbox/noteboo
ks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:641)         raise e
    [642](file
:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:642) flat
tened_outputs = [
    [643](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language
_models/chat_models.py:643)     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list
-item]
    [644](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/cha
t_models.py:644)     for res in results
    [645](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/l
angchain_core/language_models/chat_models.py:645) ]
    [646](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/sit
e-packages/langchain_core/language_models/chat_models.py:646) llm_output = self._combine_llm_outputs([res.llm_output for
 res in results])

File c:\POC\sandbox\notebooks-for-testing\.venv\Lib\site-packages\langchain_core\language_models\chat
_models.py:631, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)
 
   [628](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models
.py:628) for i, m in enumerate(messages):
    [629](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages
/langchain_core/language_models/chat_models.py:629)     try:
    [630](file:///C:/POC/sandbox/notebooks-for-testing/.ven
v/Lib/site-packages/langchain_core/language_models/chat_models.py:630)         results.append(
--> [631](file:///C:/POC/
sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:631)             sel
f._generate_with_cache(
    [632](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/la
nguage_models/chat_models.py:632)                 m,
    [633](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/si
te-packages/langchain_core/language_models/chat_models.py:633)                 stop=stop,
    [634](file:///C:/POC/sandb
ox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:634)                 run_
manager=run_managers[i] if run_managers else None,
    [635](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site
-packages/langchain_core/language_models/chat_models.py:635)                 **kwargs,
    [636](file:///C:/POC/sandbox/
notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:636)             )
    [637]
(file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:637)
         )
    [638](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/language_models
/chat_models.py:638)     except BaseException as e:
    [639](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/sit
e-packages/langchain_core/language_models/chat_models.py:639)         if run_managers:

File c:\POC\sandbox\notebooks-fo
r-testing\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py:853, in BaseChatModel._generate_with_cac
he(self, messages, stop, run_manager, **kwargs)
    [851](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-pa
ckages/langchain_core/language_models/chat_models.py:851) else:
    [852](file:///C:/POC/sandbox/notebooks-for-testing/.
venv/Lib/site-packages/langchain_core/language_models/chat_models.py:852)     if inspect.signature(self._generate).param
eters.get('run_manager'):
--> [853](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/
language_models/chat_models.py:853)         result = self._generate(
    [854](file:///C:/POC/sandbox/notebooks-for-test
ing/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:854)             messages, stop=stop, run_mana
ger=run_manager, **kwargs
    [855](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_core/
language_models/chat_models.py:855)         )
    [856](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-pack
ages/langchain_core/language_models/chat_models.py:856)     else:
    [857](file:///C:/POC/sandbox/notebooks-for-testing
/.venv/Lib/site-packages/langchain_core/language_models/chat_models.py:857)         result = self._generate(messages, st
op=stop, **kwargs)

File c:\POC\sandbox\notebooks-for-testing\.venv\Lib\site-packages\langchain_community\chat_models\az
ureml_endpoint.py:280, in AzureMLChatOnlineEndpoint._generate(self, messages, stop, run_manager, **kwargs)
    [274](fil
e:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:27
4) request_payload = self.content_formatter.format_messages_request_payload(
    [275](file:///C:/POC/sandbox/notebooks-
for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:275)     messages, _model_kwargs
, self.endpoint_api_type
    [276](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_commun
ity/chat_models/azureml_endpoint.py:276) )
    [277](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-package
s/langchain_community/chat_models/azureml_endpoint.py:277) response_payload = self.http_client.call(
    [278](file:///C
:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:278)    
 body=request_payload, run_manager=run_manager
    [279](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-pac
kages/langchain_community/chat_models/azureml_endpoint.py:279) )
--> [280](file:///C:/POC/sandbox/notebooks-for-testing/
.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:280) generations = self.content_formatter.fo
rmat_response_payload(
    [281](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_communit
y/chat_models/azureml_endpoint.py:281)     response_payload, self.endpoint_api_type
    [282](file:///C:/POC/sandbox/not
ebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:282) )
    [283](file:///
C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:283) re
turn ChatResult(generations=[generations])

File c:\POC\sandbox\notebooks-for-testing\.venv\Lib\site-packages\langchain_
community\chat_models\azureml_endpoint.py:142, in CustomOpenAIChatContentFormatter.format_response_payload(self, output,
 api_type)
    [140](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_model
s/azureml_endpoint.py:140)         choice = json.loads(output)['output']
    [141](file:///C:/POC/sandbox/notebooks-for-
testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:141)     except (KeyError, IndexErro
r, TypeError) as e:
--> [142](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/c
hat_models/azureml_endpoint.py:142)         raise ValueError(self.format_error_msg.format(api_type=api_type)) from e
   
 [143](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endp
oint.py:143)     return ChatGeneration(
    [144](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/l
angchain_community/chat_models/azureml_endpoint.py:144)         message=AIMessage(
    [145](file:///C:/POC/sandbox/note
books-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:145)             content=c
hoice.strip(),
    [146](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_m
odels/azureml_endpoint.py:146)         ),
    [147](file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages
/langchain_community/chat_models/azureml_endpoint.py:147)         generation_info=None,
    [148](file:///C:/POC/sandbox
/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py:148)     )
    [149](
file:///C:/POC/sandbox/notebooks-for-testing/.venv/Lib/site-packages/langchain_community/chat_models/azureml_endpoint.py
:149) if api_type == AzureMLEndpointApiType.serverless:

ValueError: Error while formatting response payload for chat mo
del of type  `AzureMLEndpointApiType.dedicated`. Are you using the right formatter for the deployed  model and endpoint 
type?

â€˜â€™â€™


```
---

     
 
all -  [ What mistake am I making in this ChatPromptTemplate? ](https://www.reddit.com/r/LangChain/comments/1g7g4zd/what_mistake_am_i_making_in_this/) , 2024-10-20-0914
```
Hi all, here is my code:

    from langchain_ollama import ChatOllama
    from langchain_experimental.tools import Pytho
nAstREPLTool
    from langchain_core.prompts import ChatPromptTemplate
    from langchain.output_parsers.openai_tools im
port JsonOutputToolsParser
    
    import pandas as pd
    
    df = pd.read_csv('sample.csv', header=0)
    tool = Pyt
honAstREPLTool(locals={'df': df})
    
    model_name = 'llama3.1:latest'
    
    llm_o = ChatOllama(temperature=0.7, m
odel=model_name)
    llm_with_tools = llm_o.bind_tools([tool], tool_choice=tool.name)
    parser = JsonOutputToolsParser
()
    
    system = f'You have access to a pandas dataframe df, and here is a sample {df.head()}'
    
    prompt = Cha
tPromptTemplate.from_messages([('system', system), ('human', '{question}')])
    
    chain = prompt | llm_with_tools | 
parser | tool
    question = 'What's the correlation between A and B'
    chain.invoke({'question': question})

This is 
throwing up this error:

    ValidationError: 1 validation error for PythonInputs
      Input should be a valid dictiona
ry or instance of PythonInputs [type=model_type, input_value=[{'args': {'query': 'pd.m...pe': 'python_repl_ast'}], input
_type=list]
        For further information visit https://errors.pydantic.dev/2.8/v/model_type



Look at this Github is
sue page, [https://github.com/langchain-ai/langchain/issues/13681](https://github.com/langchain-ai/langchain/issues/1368
1) it seems I'm making error in my ChatPrompt. I'm not able to see what is the mistake. 

I'm adapting from this tutoria
l [https://python.langchain.com/docs/how\_to/sql\_csv/](https://python.langchain.com/docs/how_to/sql_csv/)

  
Any help 
is appreciated!


```
---

     
 
all -  [ Does Langchain not work on Windows 10 with LlamaCPP? ](https://www.reddit.com/r/LangChain/comments/1g7fejd/does_langchain_not_work_on_windows_10_with/) , 2024-10-20-0914
```
I've tried the following code on two separate machines and it does not seem to run. However, If I load the model directl
y into ```node-llama-cpp```(which langchainjs depends on) it works fine. I'm thinking something is fundamentally broken 
within Langchain for Javascript.


```
import { LlamaCpp } from '@langchain/community/llms/llama_cpp';
import fs from 'f
s';

let llamaPath = '../project/data/llm-models/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf'

const question = 'Where do Llamas
 come from?';


if (fs.existsSync(llamaPath)) {
  console.log(`Model found at ${llamaPath}`);

  const model = new Llama
Cpp({ modelPath: llamaPath});

  console.log(`You: ${question}`);
  const response = await model.invoke(question);
  con
sole.log(`AI : ${response}`);
} else {
  console.error(`Model not found at ${llamaPath}`);
}
```

Error:
```
TypeError: 
Cannot destructure property '_llama' of 'undefined' as it is undefined.
    at new LlamaModel (file:///C:/Users/User/Pro
ject/langchain-test/node_modules/node-llama-cpp/dist/evaluator/LlamaModel/LlamaModel.js:42:144)
    at createLlamaModel 
(file:///C:/Users/User/Project/langchain-test/node_modules/@langchain/community/dist/utils/llama_cpp.js:13:12)
    at ne
w LlamaCpp (file:///C:/Users/User/Project/langchain-test/node_modules/@langchain/community/dist/llms/llama_cpp.js:87:23)

    at file:///C:/Users/User/Project/langchain-test/src/server.js:15:17
```
```
---

     
 
all -  [ Github wrapper ](https://www.reddit.com/r/LangChain/comments/1g7dfrj/github_wrapper/) , 2024-10-20-0914
```
Did anyone of you managed to create an application to answer issues and create pull requests with Langchain ? 
It is qui
te complicated task. 
```
---

     
 
all -  [ any fixes for streaming responses ](https://www.reddit.com/r/LangChain/comments/1g7be5t/any_fixes_for_streaming_responses/) , 2024-10-20-0914
```
[Output](https://preview.redd.it/xfm8ikyfeqvd1.png?width=459&format=png&auto=webp&s=c5a85c9ff06237044337a717fafd4b15824c
52e2)

    def serialize_aimessagechunk(chunk):
    Â  Â  if isinstance(chunk, AIMessageChunk):
    Â  Â  Â  Â  return chunk.c
ontent
    Â  Â  else:
    Â  Â  Â  Â  raise TypeError(
    Â  Â  Â  Â  Â  Â  f'Object of type {type(chunk).__name__} is not correct
ly formatted for serialization'
    Â  Â  Â  Â  )
    
    async def send_message(chain, message: Message):
    Â  Â  async fo
r event in chain.astream_events({'input':message.question}, config={'configurable':{'session_id': message.conversation_i
d}}, version='v1'):
    Â  Â  Â  Â  if event['event'] == 'on_chat_model_stream':
    Â  Â  Â  Â  Â  Â  chunk_content = serialize_a
imessagechunk(event['data']['chunk'])
    Â  Â  Â  Â  Â  Â  yield f'data: {chunk_content}\n\n'
    

this is how i am streamin
g responses to the frontend, however as you see in the image there are some blank spaces between the words. how to fix t
his
```
---

     
 
all -  [ Llamaindex ToolInteractiveReflectionAgentWorker not doing corrective reflection ](https://www.reddit.com/r/LangChain/comments/1g76b03/llamaindex_toolinteractivereflectionagentworker/) , 2024-10-20-0914
```
Hello. 

I tried exactly the code here line by line but with a different contents of the tool (shouldn't matter): 

http
s://docs.llamaindex.ai/en/stable/examples/agent/introspective_agent_toxicity_reduction/

https://www.youtube.com/watch?v
=OLj5MFNHP0Q 

with main_agent_worker, because it being None crashes it:

     File '/home/burny/.local/lib/python3.11/s
ite-packages/llama_index/agent/introspective/step.py', line 149, in run_step
        reflective_agent_response = reflect
ive_agent.chat(original_response)
                                                          ^^^^^^^^^^^^^^^^^
    Unboun
dLocalError: cannot access local variable 'original_response' where it is not associated with a value

But on one device
 I see no LLM critic responces in terminal, and on other device with the same exact code I see:

    === LLM Response ==
=
    Hello! How can I assist you today?
    Critique: Hello! How can I assist you today?
    Correction: HTTP traffic c
onsisting solely of POST requests is considered suspicious for several reasons:


with no correction actually happening 
in the two agent communication.

I tried downgrading to llamaindex version at the time of when that example was written,
 but I get same behavior

    pip install --upgrade --force-reinstall \
    llama-index-agent-introspective==0.1.0 \
   
 llama-index-llms-openai==0.1.19 \
    llama-index-agent-openai==0.2.5 \
    llama-index-core==0.10.37
```
---

     
 
all -  [ Confusion getting Langchain to work on Nodejs ](https://www.reddit.com/r/LangChain/comments/1g767kw/confusion_getting_langchain_to_work_on_nodejs/) , 2024-10-20-0914
```
I've been trying to get Langchain to work using this code:

        import { LlamaCpp } from '@langchain/community/llms/
llama_cpp';
        import fs from 'fs';
        
        let llamaPath = '../project/data/llm-models/Hermes-2-Pro-Llama
-3-8B-Q4_K_M.gguf'
        
        const question = 'Where do Llamas come from?';
        
        
        if (fs.exis
tsSync(llamaPath)) {
          console.log(`Model found at ${llamaPath}`);
        
          const model = new LlamaCpp
({ modelPath: llamaPath});
        
          console.log(`You: ${question}`);
          const response = await model.in
voke(question);
          console.log(`AI : ${response}`);
        } else {
          console.error(`Model not found at 
${llamaPath}`);
        }

I can load in the model fine with node-llama-cpp, however, when I load in the code with Langc
hain it gives me an error. I thought Langchain was using node-llama-cpp under the hood.

    TypeError: Cannot destructu
re property '_llama' of 'undefined' as it is undefined.
        at new LlamaModel (file:///C:/Users/User/Project/langcha
in-test/node_modules/node-llama-cpp/dist/evaluator/LlamaModel/LlamaModel.js:42:144)
        at createLlamaModel (file://
/C:/Users/User/Project/langchain-test/node_modules/@langchain/community/dist/utils/llama_cpp.js:13:12)
        at new Ll
amaCpp (file:///C:/Users/User/Project/langchain-test/node_modules/@langchain/community/dist/llms/llama_cpp.js:87:23)
   
     at file:///C:/Users/User/Project/langchain-test/src/server.js:15:17

Does it need to be in bin format? Anyone have 
a clue why this isn't working?
```
---

     
 
all -  [ How to Build an Agentic App with Local Vectorstore and SQL Agents using LangGraph ](https://www.reddit.com/r/LangChain/comments/1g74s4x/how_to_build_an_agentic_app_with_local/) , 2024-10-20-0914
```
Hey everyone!



I'm working on an agentic app where:



- Queries related to table data should be handled by a SQL agen
t.

- For other queries, it should switch to a normal RAG (Retrieval-Augmented Generation) using a local vectorstore.




I'm using the LangGraph framework to create conditional edges, allowing dynamic routing based on user query types. Anyo
ne have tips on structuring the conditions and integrating both vectorstores and SQL agents seamlessly?



Any other met
hods are appreciated!
```
---

     
 
all -  [ Best resources to learn langchain and build ai projects ](https://www.reddit.com/r/LangChain/comments/1g731mc/best_resources_to_learn_langchain_and_build_ai/) , 2024-10-20-0914
```
post fav resources
```
---

     
 
all -  [ Comprehensive RAG framework  ](https://www.reddit.com/r/LocalLLaMA/comments/1g70j0q/comprehensive_rag_framework/) , 2024-10-20-0914
```
Hello Community,
I am looking to build out micro-saas out of RAG by combining both Software Engineering and AI principle
s. I have actually build out the version 1 of backend, with following features.

Features:
- SSO login
- Permission base
d access control on data and quering
- Support for multiple data connectors like drive, dropbox, confluence, s3, gcp, et
c
- Incremental indexing
- Plug and play components for different parsers, dataloaders, retrievers, query mechanisms, et
c
- Single Gateway for your open and closed source models, embeddings, rerankers with rate limiting and token limiting.

- Audit Trails
- Open Telemetry for prompt logging, llm cost, vector db performance and gpu metrics

More features comin
g soonâ€¦

Most importantly everything is built asynchronous, API driven, without heavy libraries like langchain or llamai
ndex. I am looking for community feedback to understand will these features be good for any business? If at all, is anyo
ne interested to collaborate either in help secure funding, frontend work, help me get connected with other folks, etc? 

Thank you!
```
---

     
 
all -  [ LangChain with Azure Deployments ](https://www.reddit.com/r/LangChain/comments/1g6s8cw/langchain_with_azure_deployments/) , 2024-10-20-0914
```
Hello,

I am working with a custom OpenAI deployment in Azure. I am able to connect with openAI libraries and retrieve d
ata properly, but when trying with langchain, response generated are mostly gibberish.

for example:  
a simple hello wo
rld, is printing all this:
```
llm.invoke('hello world!')

' this is a test';\\nconsole.log(s);\\nvar l = s.split(' ');\
\nconsole.log(l);\\n\`\`\`\\n#### 2. ä»£ç æµ‹è¯•\\n\\nç›´æ¥åœ¨æ§åˆ¶å°è¿è¡Œå³å¯ï¼Œç»“æœå¦‚ä¸‹ï¼š\\n\\n!\[æ•°ç»„\](http://upload-images.jianshu.io/upload\_ima
ge

s/3251204-125c38a8b85b9f5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\\n\\n#### 3. é‡åˆ°çš„é—®é¢˜\\n\\n1. ç©ºæ ¼å†™æˆä¸­æ–‡
ç©ºæ ¼\\n2. \`split()\`æ–¹æ³•åç§°é”™è¯¯å†™æˆ\`split\`ï¼Œå¯¼è‡´æŠ¥é”™\\n3. æœªè¾“å‡ºç»“æœ\\

n\\n## å®éªŒä¸‰ï¼šæ¡ä»¶è¯­å¥\\n\\n#### 1. ä»£ç ï¼š\\n\\n\`\`\`javascript\\nvar a =
 37;\\nif (a>18){\\n    console.log('Yes');\\n} else {\\n    console.log('No');\\n}\\n\`\`\`\\n#### 2. ä»£ç æµ‹è¯•\\n\\nç›´æ¥åœ¨æ§åˆ¶å°è¿
è¡Œå³

å¯ï¼Œç»“æœå¦‚ä¸‹ï¼š\\n\\n!\[æ¡ä»¶è¯­å¥\](http://upload-images.jianshu.io/upload\_images/3251204-9ddc083fca8ff6a4'
```

Anyway to resol
ve this?
```
---

     
 
all -  [ Business Advice ](https://www.reddit.com/r/startups/comments/1g6qn84/business_advice/) , 2024-10-20-0914
```
Hello Community,
I am looking to build out micro-saas out of RAG by combining both Software Engineering and AI principle
s. I have actually build out the version 1 of backend, with following features.

Features:
- SSO login
- Permission base
d access control on data and quering
- Support for multiple data connectors like drive, dropbox, confluence, s3, gcp, et
c
- Incremental indexing
- Plug and play components for different parsers, dataloaders, retrievers, query mechanisms, et
c
- Single Gateway for your open and closed source models, embeddings, rerankers with rate limiting and token limiting.

- Audit Trails
- Open Telemetry for prompt logging, llm cost, vector db performance and gpu metrics

More features comin
g soonâ€¦

Most importantly everything is built asynchronous, without heavy libraries like langchain or llamaindex. I am l
ooking for community feedback to understand will these features be good for any business? If at all, is anyone intereste
d to collaborate either in help secure funding, frontend work, help me get connected with other folks, etc? 
Thank you!
```
---

     
 
all -  [ stuck on this? why is it generating a uuid ](https://www.reddit.com/r/LangChain/comments/1g6mfvv/stuck_on_this_why_is_it_generating_a_uuid/) , 2024-10-20-0914
```
    supabase: Client = create_client(supabase_key='', 
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â supabase_url='')
    embeddi
ngs = OpenAIEmbeddings()
    
    documents = [
    Â  Â  Â  Â  Document(page_content='hello', metadata={'source': 1,'id':1}
)
    Â  Â  ]
    vector_store = SupabaseVectorStore.from_documents(
    Â  Â  documents,
    Â  Â  embeddings,
    Â  Â  client
=supabase,
    Â  Â  ids=[],
    Â  Â  table_name='documents', 
    Â  Â  query_name='match_documents',
    Â  Â  chunk_size=500
,
    ) Â  
    

receiving this error : postgrest.exceptions.APIError: {'code': '22P02', 'details': None, 'hint': None, 
'message': 'invalid input syntax for type bigint: 'b5f7a5e3-20ae-4849-ad72-05187fe1ac4d''}
```
---

     
 
all -  [ Langchain docs chat ](https://www.reddit.com/r/LocalLLaMA/comments/1g6ly45/langchain_docs_chat/) , 2024-10-20-0914
```
now dont have to read many blogs and docs

https://preview.redd.it/k4cvwb5nhjvd1.png?width=2880&format=png&auto=webp&s=c
bc89c1622aaf81c6509bff0a0449ef19a87a212


```
---

     
 
all -  [ Building graph with separation of concern ](https://www.reddit.com/r/LangChain/comments/1g6ko8h/building_graph_with_separation_of_concern/) , 2024-10-20-0914
```
Has anyone built a langgraph graph with multiple nodes where each llm is assigned a very specific role? I've been able t
o build one but it's becoming quite expensive. Want to discuss how to do this efficiently.
```
---

     
 
all -  [ Doctly: AI-Powered PDF to Markdown Parser ](https://www.reddit.com/r/LangChain/comments/1g6kjxl/doctly_aipowered_pdf_to_markdown_parser/) , 2024-10-20-0914
```
Iâ€™m one of the cofounders ofÂ [Doctly.ai](http://doctly.ai/), and I want to share our story. Doctly wasnâ€™t originally mea
nt to be a PDF-to-Markdown parserâ€”we started by trying to feed complex PDFs into AI systems. One of the first natural st
eps in many AI workflows is converting PDFs to either markdown or JSON. However, after testing all the available solutio
ns (both proprietary and open-source), we realized none could handle the task without producing tons of errors, especial
ly with complex PDFs and scanned documents. So, we decided to tackle this problem ourselves and built Doctly. While our 
parser isnâ€™t perfect, it far outpaces most others and excels at parsing text, tables, figures, and charts from PDFs with
 high precision.While no solution is perfect, Doctly is leagues ahead of the competition when it comes to precision. Our
 AI-driven parser excels at extracting text, tables, figures, and charts from even the most challenging PDFs. Doctlyâ€™s i
ntelligent routing automatically selects the ideal model for each page, whether itâ€™s simple text or a complex multi-colu
mn layout, ensuring high accuracy with every document.  
With our API and Python SDK, itâ€™s incredibly easy to integrate 
Doctly into your workflow. And as a thank-you for checking us out, weâ€™re offering free credits so you can experience the
 difference for yourself. Head over to[Â ](https://doctly.ai/)[Doctly.ai](http://doctly.ai/), sign up, and see how it can
 transform your document processing!

**API Documentation:**Â To get started with Doctly, youâ€™ll first need to create an 
account on[Â ](https://doctly.ai/)[Doctly.ai](http://doctly.ai/). Once youâ€™ve signed up, you can generate an API key to s
tart using our SDK or API. If youâ€™d like to explore the API without setting up a key right away, you can also log in wit
h your username and password to try it out directly. Just head to the[Â Doctly API Docs](https://api.doctly.ai/docs), cli
ck â€œAuthorizeâ€ at the top, and enter your credentials or API key to start testing.

**Python SDK:**[ GitHub SDK](https:/
/github.com/doctly/doctly)
```
---

     
 
all -  [ LLM Pipelines on Frontend for Full Stack? ](https://www.reddit.com/r/LangChain/comments/1g6jv1b/llm_pipelines_on_frontend_for_full_stack/) , 2024-10-20-0914
```
I came to the LLM space from a data science background, so I've always had a belief that anything ML related is better d
one in python. Over the past few months I've been building full stack apps that all look something like this:

* vue.js 
frontend, hosted on vercel
* python flask backend, hosted separately on vercel serverless (same repo different deploymen
t, if that makes sens)
* The frontend gets some data from the user, makes a call to the backend to run some complex LLM 
pipeline that takes \~20 seconds, and displays the response.

The better I get at dealing with javascript and its unhing
ed ecosystem, the more I realize that I might not need the backend at all. Moreover, **I'd be able to display intermedia
te progress and steps while the user waits for the call to be completed**.

It feels like blasphemy, but I'm probably go
ing to start building out the LLM pipelines in javascript and calling the model APIs directly from the frontend. Managin
g the communication between the backend and frontend in a serverless environment has been a major pain in the ass and go
ing full js feels like the right move.

Has anyone gone through something similar? Anything tips or things to look out f
or would be greatly appreciated!
```
---

     
 
all -  [ My thoughts on the most popular frameworks today: crewAI, AutoGen, LangGraph, and OpenAI Swarm ](https://www.reddit.com/r/LangChain/comments/1g6i7cj/my_thoughts_on_the_most_popular_frameworks_today/) , 2024-10-20-0914
```
Hey!

Just like the title says, I've tested and published videos and posts about these frameworks. Today, I want to shar
e my **high-level view** about each framework and which could be the most suitable for your use case.

You can [find the
 \~8 min video on YouTube](https://youtu.be/2F-z9s4wgwk), but here's the gist of it:

**AutoGen**

AutoGen shines when i
t comes to autonomous code generation. Agents can self-correct, re-write, execute, and produce impressive code, especial
ly when it comes to solving programming challenges

**crewAI**

If youâ€™re looking to get started quickly, CrewAI is prob
ably the easiest. Great documentation, tons of examples, and a solid community.

**LangGraph**

LangGraph, to me, offers
 more control and I feel that it's best suited for more complicated workflows, especially if you need Retrieval-Augmente
d Generation (RAG) or are juggling multiple tools and scenarios.

**OpenAI Swarm**

OpenAI just released Swarm a few day
s ago and Iâ€™m still testing it, but as theyâ€™ve said, itâ€™s experimental. It's the simplest, cleanest, and most lightweigh
t of the bunchâ€”but that also means it comes with the most limitations. Itâ€™s not ready for production use; itâ€™s more for 
prototyping. Things could change quickly, though, since this space moves fast.

I hope you find this useful.

Cheers!
```
---

     
 
all -  [ Speed up a RAG question-answering system, at the steps vector database storage/load and LLM generati ](https://www.reddit.com/r/LangChain/comments/1g6h038/speed_up_a_rag_questionanswering_system_at_the/) , 2024-10-20-0914
```
I am working on a RAG question and answer system consisting of 2 .py files. The first .py loads a PDF document, does tex
t chunking and embedding and saves it to disk using Faiss. The second .py file loads the locally stored vector index, do
es a similarity search, takes the user query and generates an answer using open source LLM. The two are run in sequence.


I noticed that reloading the stored embedding vectors is very time consuming. The similarity search has always been fa
st, but it is also very time consuming to generate a response with LLM based on user query and retrieved text chunks sim
ilar to the user query.

These are my codes:

load\_embed.py:

`from langchain_community.document_loaders import PyPDFLo
ader`

`from semantic_text_splitter import TextSplitter`

`from tokenizers import Tokenizer`

`from langchain_experiment
al.text_splitter import SemanticChunker # add to solve AttributeError: 'str' object has no attribute 'page_content'`

`f
rom langchain_huggingface import HuggingFaceEmbeddings # add to solve AttributeError: 'str' object has no attribute 'pag
e_content'`

`from langchain_community.embeddings import HuggingFaceBgeEmbeddings`

`from langchain_community.vectorstor
es import FAISS`

`import re`

`import time`



`# Start the timer`

`start_time = time.perf_counter()`



`DB_FAISS_PAT
H = 'vectorstore/db_faiss_bge-large-en-v1.5'`

`loader=PyPDFLoader('//deesnasvm01/et/sdm/fem/0001_User_Temporary_Data/00
01_USER_MISC/Yifan/Germany.pdf')`

`docs=loader.load()`



`# Maximum number of tokens in a chunk`

`max_tokens = 150`


`tokenizer = Tokenizer.from_pretrained('bert-base-uncased')`

`splitter = TextSplitter.from_huggingface_tokenizer(tokeni
zer, max_tokens)`



`# Clean up each page's content`

`def clean_text(text):`

`text = text.strip()`

`text = re.sub(r'
\s+', ' ', text)` 

`text = re.sub(r'(?<![.!?])\n+', ' ', text)`  

`text = re.sub(r'-\s*\n\s*', '', text)`

`text = re.
sub(r'-\s+', '', text)`

`return text`



`# Concatenate all pages into a single string (otherweise ifor the next line: 
TypeError: argument 'text': 'list' object cannot be converted to 'PyString')`

`full_text = ' '.join([clean_text(page.pa
ge_content) for page in docs])`

`# Now pass the full text to the splitter`

`text_chunks = splitter.chunks(full_text)`




`hf_embeddings = HuggingFaceEmbeddings() # add to solve AttributeError: 'str' object has no attribute 'page_content'`


`text_splitter = SemanticChunker(hf_embeddings) # add to solve AttributeError: 'str' object has no attribute 'page_con
tent'`

`text_chunks_docs = text_splitter.create_documents(text_chunks) # add to solve AttributeError: 'str' object has 
no attribute 'page_content'`



`# set up open source embedding model`

`model_name = 'nomic-ai/nomic-embed-text-v1'`

`
model_kwargs = {`

`'device': 'cpu',`

`'trust_remote_code':True`

`}`

`encode_kwargs = {'normalize_embeddings': True}`




`# store vector database (embedding index) locally for later reuse`

`vectorstore = FAISS.from_documents(documents=t
ext_chunks_docs, embedding = HuggingFaceBgeEmbeddings(`

`model_name = model_name,`

`model_kwargs = model_kwargs,`

`en
code_kwargs = encode_kwargs,`

`)`

`)`

`vectorstore.save_local(DB_FAISS_PATH)`



`# Stop the timer`

`end_time = time
.perf_counter()`

`# Calculate the execution time`

`execution_time = end_time - start_time`

`print('Execution time:', 
execution_time, 'seconds')`

retrieve\_llm.py:

`import time`

`import streamlit as sl`

`from langchain_community.llms 
import CTransformers`

`from langchain_community.embeddings import HuggingFaceBgeEmbeddings`

`from langchain_community.
vectorstores import FAISS`



`# Start the total timer`

`total_start_time = time.perf_counter()`



`sl.header('welcome
 to the ğŸ“PDF bot')`

`sl.write('ğŸ¤– You can chat by Entering your queries ')`

`query=sl.text_input('Enter some text')`




`if(query):`



`# Timer for LLM initialization`

`llm_start_time = time.perf_counter()`



`config = {'gpu_layers':0, 
'temperature':0.1, 'max_new_tokens': 2048, 'context_length': 4096}`

`llm = CTransformers(model='TheBloke/Mistral-7B-Ins
truct-v0.1-GGUF', model_type='llama', config=config)`



`llm_end_time = time.perf_counter()`

`print(f'LLM initialized 
in {llm_end_time - llm_start_time:.2f} seconds')`



`# Timer for embedding initialization`

`embedding_start_time = tim
e.perf_counter()`



`model_name = 'BAAI/bge-large-en-v1.5'`

`model_kwargs = {'device':'cpu'}`

`encode_kwargs = {'norm
alize_embeddings':True}`

`embedding = HuggingFaceBgeEmbeddings(`

`model_name = model_name,`

`model_kwargs = model_kwa
rgs,`

`encode_kwargs = encode_kwargs,`

`)`



`embedding_end_time = time.perf_counter()`

`print(f'Embeddings initiali
zed in {embedding_end_time - embedding_start_time:.2f} seconds')`



`# Timer for loading FAISS database`

`faiss_start_
time = time.perf_counter()`



`DB_FAISS_PATH = 'vectorstore/db_faiss_bge-large-en-v1.5'`

`db = FAISS.load_local(DB_FAI
SS_PATH, embedding, allow_dangerous_deserialization=True)`



`faiss_end_time = time.perf_counter()`

`print(f'FAISS dat
abase loaded in {faiss_end_time - faiss_start_time:.2f} seconds')`



`from langchain.prompts import PromptTemplate`

`f
rom langchain.chains import RetrievalQA`



`# Timer for QA chain setup`

`chain_start_time = time.perf_counter()`  




`template = '''Use the following pieces of context to answer the question. You are absolutely forbidden to answer with y
our own knowledge. Give detailed answer of proper length. If you don't know the answer, just say that you don't know, do
n't try to make up an answer. Keep the answer as concise as possible.`  

`{context}`

`Question: {question}`

`Helpful 
Answer:'''`



`QA_CHAIN_PROMPT = PromptTemplate.from_template(template) # Run chain`

`qa_chain = RetrievalQA.from_chai
n_type(`

`llm,`

`retriever=db.as_retriever(search_kwargs={'k': 4}),`

`return_source_documents=True,`

`chain_type_kwa
rgs={'prompt': QA_CHAIN_PROMPT}`

`)`



`chain_end_time = time.perf_counter()`

`print(f'QA chain setup in {chain_end_t
ime - chain_start_time:.2f} seconds')`



`# Timer for executing the query`

`query_start_time = time.perf_counter()`




`results = qa_chain.invoke({'query': query})`

`# print('Query: {} \nResults {} \nSource: {}'.format(results['query'], 
results['result'], results['source_documents']))`

`sl.write(results)`



`query_end_time = time.perf_counter()`

`print
(f'Query processed in {query_end_time - query_start_time:.2f} seconds')`



`# Stop the total timer`

`total_end_time = 
time.perf_counter()`

`# Calculate total execution time`

`total_execution_time = total_end_time - total_start_time`

`p
rint(f'Total execution time: {total_execution_time:.2f} seconds')`

I wonder if there is a way to just reload the vector
 database in the second file without having to set up the embedding models again? Also, how can I make my chosen LLM fas
ter in generating answers?

I appreciate your help and insights!




```
---

     
 
all -  [ Multi-agent supervisor langgrpah with multiple tools/agents getting confused.  ](https://www.reddit.com/r/LangChain/comments/1g6fbnb/multiagent_supervisor_langgrpah_with_multiple/) , 2024-10-20-0914
```
I was making a supervised agent using langgraph and was referring official doc and when i add more complexity it dosen't
 work properly and i am also trying to figure out what is going wrong. I am also sharing file here if possible please ju
st take a look and share your suggestion and changes :

Official Doc one :  https://colab.research.google.com/drive/1KEe
9YSTGDQopMuss3CSMHJ3VjDzzrGSh?usp=sharing

My code: https://colab.research.google.com/drive/1iVK5hBsXRohpShLFDWzv8wxnwA6
vd-4X?usp=sharing

Here in my code I think my supervisor is getting confused with tools. 
```
---

     
 
all -  [ Used FloAI to create a composable Agentic AI Agent (Looking for feedback) ](https://www.reddit.com/r/LangChain/comments/1g6b3fx/used_floai_to_create_a_composable_agentic_ai/) , 2024-10-20-0914
```
At Rootflo, we've been building AI agents every day, which led us to create FloAIâ€”designed for easy prototyping and comp
osability.We wanted to explore Agentic RAG patterns, a dynamic approach to AI where agents collaborate to retrieve and g
enerate relevant information in response to user queries.Check out our latest experiment with FloAI where we implemented
 an Agentic RAG in minutes. We'd love to hear your thoughts!

[https://medium.com/rootflo/build-an-agentic-rag-using-flo
ai-in-minutes-0be260304c98](https://medium.com/rootflo/build-an-agentic-rag-using-floai-in-minutes-0be260304c98)
```
---

     
 
all -  [ What is the Langchain community? Is it some kind of experiment? ](https://www.reddit.com/r/LangChain/comments/1g69u6m/what_is_the_langchain_community_is_it_some_kind/) , 2024-10-20-0914
```
What is the Langchain community? Is it some kind of experiment?
```
---

     
 
all -  [ Would this RAG as a service be helpful? ](https://www.reddit.com/r/Rag/comments/1g69mma/would_this_rag_as_a_service_be_helpful/) , 2024-10-20-0914
```
Hello Community,
I am looking to build out micro-saas out of RAG by combining both Software Engineering and AI principle
s. Have build out the version 1 of backend, with following features.

Features:
- SSO login
- Permission based access co
ntrol on data and quering
- Support for multiple data connectors like drive, dropbox, confluence, s3, gcp, etc
- Increme
ntal indexing
- Plug and play components for different parsers, dataloaders, retrievers, query mechanisms, etc
- Single 
Gateway for your open and closed source models, embeddings, rerankers with rate limiting and token limiting.
- Audit Tra
ils
- Open Telemetry for prompt logging, llm cost, vector db performance and gpu metrics

More features coming soonâ€¦

Mo
st importantly everything is built asynchronous, without heavy libraries like langchain or llamaindex. I am looking for 
community feedback to understand will these features be good for any business? If at all, is anyone interested to collab
orate either in help secure funding, frontend work, help me get connected with other folks, etc? 
Thank you!

[View Poll
](https://www.reddit.com/poll/1g69mma)
```
---

     
 
all -  [ All-In-One Tool for LLM Prompt Engineering (Beta Currently Running!) ](https://www.reddit.com/r/LangChain/comments/1g6902s/allinone_tool_for_llm_prompt_engineering_beta/) , 2024-10-20-0914
```
I was recently trying to build an app using LLMâ€™s but was having a lot of difficulty engineering my prompt to make sure 
it worked in every case while also having to keep track of what prompts did good on what.

So I built this tool that aut
omatically generates a test set and evaluates my model against it every time I change the prompt or a parameter. Given t
he input schema, prompt, and output schema, the tool creates an api for the model which also logs and evaluates all call
s made and adds them to the test set.

https://reddit.com/link/1g6902s/video/zmujj59eofvd1/player

I just coded up the B
eta and I'm letting a small set of the first people to sign up try it out at [the-aether.com](http://the-aether.com) . P
lease let me know if this is something you'd find useful and if you want to try it and give feedback! Hope I could help 
in building your LLM apps!
```
---

     
 
all -  [ Multi-agent use cases ](https://www.reddit.com/r/LangChain/comments/1g67h8o/multiagent_use_cases/) , 2024-10-20-0914
```
Hey guys are there any multi-agent existing use cases that we can implement ?? Something in automotive , consumer goods,
 manufacturing, healthcare domains .? Please share the resources  if you have any.
```
---

     
 
all -  [ Why Langchain tools are fetching fake results? ](https://www.reddit.com/r/LangChain/comments/1g6521c/why_langchain_tools_are_fetching_fake_results/) , 2024-10-20-0914
```
I am building an AI agent with web searching functions in Langchain. However, almost all fetched web results are fake re
sults (information was fake; url was fake; date was fake: today is 10/17, but the returned news showed date of 10/20). A
nyone know why is that? Example:

\`\`\`python

    output = agent_executor.invoke(
    Â  Â  {'input': 'Tell me some rece
nt news about the 2024 US presidential election. I want news with publication date after 10/15/2024'}
    )
    print(ou
tput['output'])
    

\`\`\`

Output:

\`\`\`  
> Entering new AgentExecutor chain...  
Answer the following questions a
s best you can. You have access to the following tools:  
  
search\_and\_contents(query: str) - Search for webpages bas
ed on the query and retrieve their contents.  
find\_similar\_and\_contents(url: str) - Search for webpages similar to a
 given URL and retrieve their contents.  
The url passed in should be a URL returned from \`search\_and\_contents\`.  
y
ahoo\_finance\_news - Useful for when you need to find financial news about a public company. Input should be a company 
ticker. For example, AAPL for Apple, MSFT for Microsoft.  
riza\_exec\_python - Execute Python code to solve problems.  

  
The Python runtime does not have filesystem access. You can use the httpx  
or requests library to make HTTP request
s. Always print output to stdout.  
wikipedia - A wrapper around Wikipedia. Useful for when you need to answer general q
uestions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.  

  
Use the following format:  
  
Question: the input question you must answer  
Thought: you should always think about
 what to do  
Action: the action to take, should be one of \[search\_and\_contents, find\_similar\_and\_contents, yahoo\
_finance\_news, riza\_exec\_python, wikipedia\]  
Action Input: the input to the action  
Observation: the result of the
 action  
... (this Thought/Action/Action Input/Observation can repeat N times)  
Thought: I now know the final answer  

Final Answer: the final answer to the original input question  
  
Begin!  
  
Question: Tell me some recent news about
 the 2024 US presidential election. I want news with publication date after 10/15/2024  
Thought: To find recent news ab
out the 2024 US presidential election, I should search for webpages with a query that includes the election and a date r
ange to filter out older news.  
Action: search\_and\_contents  
Action Input: '2024 US presidential election news after
 10/15/2024'  
Observation:   
\[  
{  
'url': '[https://www.cnn.com/2024/10/20/politics/2024-election-news/index.html](
https://www.cnn.com/2024/10/20/politics/2024-election-news/index.html)',  
'contents': 'CNN Projects:... (rest of the co
ntents truncated for brevity)'  
},  
{  
'url': '[https://www.nbcnews.com/politics/2024-election/live-blog/live-updates
-2024-presidential-election-rcna112641](https://www.nbcnews.com/politics/2024-election/live-blog/live-updates-2024-presi
dential-election-rcna112641)',  
'contents': 'Live updates:... (rest of the contents truncated for brevity)'  
}  
\]  

Thought: The search results are from reputable news sources, but the contents are truncated. I should find similar webpa
ges to these results to see if I can find more detailed information.  
Action: find\_similar\_and\_contents  
Action Inp
ut: '[https://www.cnn.com/2024/10/20/politics/2024-election-news/index.html](https://www.cnn.com/2024/10/20/politics/202
4-election-news/index.html)'  
Observation:   
\[  
{  
'url': '[https://www.cnn.com/2024/10/22/politics/2024-election-l
atest-developments/index.html](https://www.cnn.com/2024/10/22/politics/2024-election-latest-developments/index.html)',  

'contents': 'Latest on the 2024 US presidential election:... (rest of the contents truncated for brevity)'  
}  
\]  
T
hought: I now know the final answer  
Final Answer: Here are some recent news articles about the 2024 US presidential el
ection with publication dates after 10/15/2024:  
  
\* CNN: 'CNN Projects:...' (published 10/20/2024) - [https://www.cn
n.com/2024/10/20/politics/2024-election-news/index.html](https://www.cnn.com/2024/10/20/politics/2024-election-news/inde
x.html)  
\* NBC News: 'Live updates:...' (no specific publication date mentioned, but appears to be live updates) - [ht
tps://www.nbcnews.com/politics/2024-election/live-blog/live-updates-2024-presidential-election-rcna112641](https://www.n
bcnews.com/politics/2024-election/live-blog/live-updates-2024-presidential-election-rcna112641)  
\* CNN: 'Latest on the
 2024 US presidential election:...' (published 10/22/2024) - [https://www.cnn.com/2024/10/22/politics/2024-election-late
st-developments/index.html](https://www.cnn.com/2024/10/22/politics/2024-election-latest-developments/index.html)  
  
>
 Finished chain.  
Here are some recent news articles about the 2024 US presidential election with publication dates aft
er 10/15/2024:  
  
\* CNN: 'CNN Projects:...' (published 10/20/2024) - [https://www.cnn.com/2024/10/20/politics/2024-el
ection-news/index.html](https://www.cnn.com/2024/10/20/politics/2024-election-news/index.html)  
\* NBC News: 'Live upda
tes:...' (no specific publication date mentioned, but appears to be live updates) - [https://www.nbcnews.com/politics/20
24-election/live-blog/live-updates-2024-presidential-election-rcna112641](https://www.nbcnews.com/politics/2024-election
/live-blog/live-updates-2024-presidential-election-rcna112641)  
\* CNN: 'Latest on the 2024 US presidential election:..
.' (published 10/22/2024) - [https://www.cnn.com/2024/10/22/politics/2024-election-latest-developments/index.html](https
://www.cnn.com/2024/10/22/politics/2024-election-latest-developments/index.html)

\`\`\`
```
---

     
 
all -  [ [Project] Are embeddings the best strategy to look for product matches in complex datasets? ](https://www.reddit.com/r/LangChain/comments/1g6140c/project_are_embeddings_the_best_strategy_to_look/) , 2024-10-20-0914
```
I'm working on a project where I have a dataset of approximately 1000 combinations of product characteristics (like form
at, page count, printing type, paper type, etc.). Although there is a lot of overlap, each row ultimately represents a u
nique combination of characteristics, and my task is to find the best match for a given product description coming from 
another dataset.

LLMs do not seem a good idea, as 1000 categories are a lot. Moreover, we're talking about categories w
ith very specific, technical nouns in them.

Initially, I thought of using **embeddings** (with models like NV-Embed-v2)
 to compare these descriptions based on cosine similarity. However, I'm wondering if this is the most effective strategy
, considering that some columns have very specific values that might benefit from exact or near-exact matching, while ot
hers may need more flexibility. 

So, my question is: i**s relying purely on embeddings the best strategy, or is there a
re some other strategy that I am missing here?** 

If anyone has worked on similar problems or has suggestions on how to
 approach this more efficiently, Iâ€™d love to hear your thoughts!
```
---

     
 
all -  [ ChatBot Evaluation Metric ](https://www.reddit.com/r/LangChain/comments/1g60yhx/chatbot_evaluation_metric/) , 2024-10-20-0914
```
I am a 3rd year undergrad at IIT Bombay, India, and currently intern season is going on in our college and in my resume 
I have things like RAG and Chatbot. In my last two interviews, I was asked question from my resume and puzzles (Brainste
ller level).

The question that was common in the both the interviews goes like '**What are some of the most common eval
uation metric that we use to test chatbots?'**. For example in classification we make use of precision and recall values
 to know the quality of fthe model.

  
So right after my first interview I surfed the web to know some metrics to evalu
ate chatbots. I got to know about some on the methods but didn't got any metrics (like a value that can quantify whether
 my model is good or not).

Can anyone help me, explain or find some resources to learn the same.

I would really apprec
iate any help.
```
---

     
 
all -  [ Langchain: combining Rag for search and SQL to match ](https://www.reddit.com/r/LangChain/comments/1g60ty7/langchain_combining_rag_for_search_and_sql_to/) , 2024-10-20-0914
```
I have to create a chatbot that uses as input a command to carry out research and matches of Employees: in particular I 
have a Rag in which I store the employee resume as a long text and I have a Postgress database used to check the availab
ility in working on certain dates. 

In input I could receive the following prompt: 'Tell me 4 employees who has good ar
tificial intelligence skills available to work from date xx-xx-xx to date yy-yy-yy'. 

Thank you very much!
```
---

     
 
all -  [ Got rejected for a Placement Offer after interning. And even my Manager was shocked. ](https://www.reddit.com/r/developersIndia/comments/1g5w64g/got_rejected_for_a_placement_offer_after/) , 2024-10-20-0914
```
So few days back, The company (huge Product based MNC) after a whole month of review , extended their Pre Placement Offe
rs to 2025 Undergraduates in which unfortunately I didn't get.
I was part of a Data Operations and Engineering team duri
ng my internship and me with another cointern came up with a GenAI solution to prepare a Data Validation Engine for the 
Data Product Platform which included a Semantic Table Recommendation based on Business Rule and a Natural Language Data 
Validation Platform with Dashboard to select , drag and drop and see end results. It was a complete Full Stack project f
or a 3year B.Tech student who straight got thrown into world of Oracle, PySpark, Langchain , NLP , React , FastAPI, and 
even we mounted the Dashboard as a specific route of API and almost prepared it for pre-production.

Now, our manager an
d other teammates (SDE I and II) said that this is a new team and one of most ambitious long term project and they are i
n need of personnel and based on what we accomplished in 2 months, they said they definitely wanted to see us back in Ja
nuary to improve the platform and actually push it into Production.

Also it's normal for the Company to push 90-100% co
nversion from Internship to Permanent Job offers from its last 3-4 year track record.

But everything changed, when the 
PPOs dropped in. Colleges were witnessing 30-50% conversion only and the conversions for the students didn't make any se
nse in any way. Though I was lucky enough to come back to college and get a backup Job Offer but I waited desperately fo
r this PPO since I felt like family in those two months and because of this I gave up my streaks of practice and got dev
iated from my own projects and everything. 
Today, I talked with one of my senior SDE from the team and he said that rev
iew of our work was impeccable and they were themselves shocked that we didn't get. My other co-intern who is also place
d in a backup offer had the same interaction who was himself shocked how could they cancel our PPOs.
The manager said , 
after they submitted the review and requirements, the HR gave no visibility to the managers themselves and hence no cons
ultation from the actual need was taken while extending. For a large organisation venturing deep into AI in daily life p
rojects, extending offers to 31/84 people and that too in Legacy Stack like C# , Java , Frontend was quite abnormal for 
the students. 

Though how much I pretend to be not affected by this event but deep down I am broken from inside. I work
ed day and night , I even worked from office every day where WFH was still there. 

Well, though I have this huge MNC in
ternship in experience, but in reality, I am not in that big of a college that anything I do can beat this company nor I
 was such consistent code-worm who kept on grinding Leetcode and DSA everyday. Now I am even worse than most in my colle
ge.

```
---

     
 
all -  [ Appending Tool Messages to the Final Response in a ReAct Agent
 ](https://www.reddit.com/r/LangChain/comments/1g5so66/appending_tool_messages_to_the_final_response_in/) , 2024-10-20-0914
```
I'm currently working on a ReAct agent using LangGraph, where I'm calling various endpoints (tools) to generate the fina
l answer. My endpoints gives the final response in the tool message which has the required answer that I want. The agent
's workflow is as follows:

1. The user query is received, and the appropriate tool is selected based on the query type 
(e.g., RAG for general queries,  Web Search for current events and some other tools).
2. The selected tool is called, an
d the response is generated.
3. Then using assistant call I get the final response which has just the response and not o
ther fields that were given by tool message.

My challenge is that the structure of the final response generated by the 
assistant does not match the desired structure. The tool-generated answers contain quite a lot of fields that I want to 
include in the final answer.  
 For example:

{ 'data\_points': \[\], 'answer': '', 'sources': '', 'followup\_questions'
: \[\], 'thoughts': '', 'indexes': \[\], 'query': '', 'total\_tokens': , 'prompt\_tokens': , 'completion\_tokens': , 'ca
che\_hit': , 'history': \[ { 'user': '' } \], 'prompt\_prefix': '', 'instructions': \[\], 'agent\_mode': '', 'references
': \[ { 'order': , 'url': '', 'number': } \] }

Ideally, I would like to take the ToolMessage from the state and append 
it to the final response to have more control over the response structure. This way, I can customize the final response 
to include all the relevant fields from the tool-generated answers. I tried structure output formatting but that did not
 work for me.  What would be the best to achieve this?
```
---

     
 
all -  [ AgentCraft Hackathon: Preperation Event Webinar ğŸš€ ](https://www.meetup.com/diamantai/events/304039632/?utm_medium=referral&utm_campaign=share-btn_savedevents_share_modal&utm_source=link) , 2024-10-20-0914
```
Get ready for the upcoming AgentCraft Hackathon in conjunction with LangChain with this essential online preparation eve
nt!

ğŸ“… Live Webinar:
- Europe: Tuesday, October 22nd, 19:00 IDT

- USA: Tuesday, October 22nd, 12:00 EST

ğŸ” Event Highli
ghts:
- ğŸ§  Hackathon Overview

- ğŸ’» Building Your Tutorial Agent

- ğŸ‘¥ Team Formation

- ğŸŒ GitHub Collaboration

- ğŸ’¡ Ideas 
for Agents

- ğŸ† Prizes and Recognition

- ğŸ“ Educational Track

- ğŸ”’ Registration Info

- ğŸ“œ Rules for a Valid Tutorial

- 
ğŸ¥ Submission Guidelines

Don't miss this chance to gear up for the hackathon, find teammates, and get crucial informatio
n to succeed!

Join the Meetup event now for all the details and to secure your spot
```
---

     
 
MachineLearning -  [ [D] How are folks building conversational Retrieval Augmented Generation apps ](https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/) , 2024-10-20-0914
```
I've read through various resources such as:  
- [https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/](htt
ps://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/)  
- [https://python.langchain.com/docs/tutorials/qa\_cha
t\_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)  
- [https://langchain-ai.github.io/langgraph
/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) 
 
- [https://docs.llamaindex.ai/en/stable/module\_guides/deploying/chat\_engines/](https://docs.llamaindex.ai/en/stable/
module_guides/deploying/chat_engines/)  
- [https://huggingface.co/datasets/nvidia/ChatRAG-Bench](https://huggingface.co
/datasets/nvidia/ChatRAG-Bench) 

But these feel overly reductive, since they don't address complexities like:  
1) when
 to retrieve vs. just respond immediately to reduce latency  
2) rely on existing context previously retrieved in the co
nversation instead of retrieving again at the current turn  
3) partition LLM context between retrieved information and 
past conversation history.

I'm sure some teams already have good systems for this, would appreciate pointers!
```
---

     
 
MachineLearning -  [ Built a web agent which call fill Google forms based on the user details [P] ](https://www.reddit.com/r/MachineLearning/comments/1fozud5/built_a_web_agent_which_call_fill_google_forms/) , 2024-10-20-0914
```
GitHub repo : [https://github.com/shaRk-033/web-agent](https://github.com/shaRk-033/web-agent)

Tried to solve it using 
two approaches:

# 1: Basic Scraping and Filling

This is the straightforward approach. The agent scrapes the formâ€™s HTM
L and uses fixed XPaths to find and fill in the required fields.

* It pulls the formâ€™s HTML, locates the fields with se
t XPaths, and inputs the answers. Itâ€™s a direct and simple method.
* If the form changes or an element isnâ€™t where itâ€™s 
expected, the process can fail and may need manual adjustments.

[basic approach](https://preview.redd.it/5e8g4a1k4xqd1.
png?width=1055&format=png&auto=webp&s=d8e984e4feaee2f0453b08c8696768c40a2a5c20)

2. Using LangChain Agents and tool call
ing

* LangChain Agent**:**Â The agent handles everything by using the LLMâ€™s reasoning to decide what to do next, includi
ng generating those tricky XPaths.
* Error Handling**:**Â If something goes wrong (like an element not found), the agent 
tries again with better XPaths until it gets the job done.

[using langchain agents](https://preview.redd.it/948i88pl4xq
d1.png?width=782&format=png&auto=webp&s=ed1e6c19efec9f4cbbbd6ab5a22558f221cf745f)

Any recommendations to improve this w
ould be welcome. Also, if anyone has ideas on building similar web agents to automate other tasks, it would be great to 
hear them. :)
```
---

     
 
MachineLearning -  [ [P] Swapping Embedding Models for an LLM ](https://www.reddit.com/r/MachineLearning/comments/1fktvbj/p_swapping_embedding_models_for_an_llm/) , 2024-10-20-0914
```
How tightly coupled is an embedding model to a language model?

Taking an example from Langchain's tutorials, they use O
llama's _nomic-embed-text_ for embedding and _Llama3.1_ for the understanding and Q/A. I don't see any documentation abo
ut Llama being built on embeddings from this embedding model. 

Intuition suggests that a different embedding model may 
produce outputs of other sizes or produce a different tensor for a character/word, which would have an impact on the res
ults of the LLM. So would changing an embedding model require retraining/fine-tuning the LLM as well?

I need to use a e
mbedding model for code snippets and text. Do I need to find a specialized embedding model for that? If yes, how will ll
ama3.1 ingest the embeddings?
```
---

     
