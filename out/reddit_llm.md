 
all -  [ What is the best LLM tech stack? ](https://www.reddit.com/r/LocalLLaMA/comments/1drofjk/what_is_the_best_llm_tech_stack/) , 2024-06-30-0912
```
Hi! So I'm currently building for a use case that requires the LLM to be securely fine tuned on large amounts of data an
d I would like to have an agentic workflow. Ease of development is a big This is the current stack I am thinking of usin
g but there are so many options that I feel a bit overwhelmed with the options. If you have any suggestions outside of t
he tools I listed please let me know!

Current Stack

1. Model (Need to fine-tune)
   1. LLama vs. OpenAI vs. Mistral?
2
. Framework for development
   1. LangChain vs. LlamaIndex?
3. Database to use
   1. Supabase
4. How to host and train t
he model
   1. RunPod
```
---

     
 
all -  [ UnstructuredExcelLoader() not working ](https://www.reddit.com/r/LangChain/comments/1drobcc/unstructuredexcelloader_not_working/) , 2024-06-30-0912
```
Has anyone used the UnstructuredExcelLoader() class to load xlsx file?

I am trying to load a simple one sheet Excel fil
e (.xlsx) using the function:

`from langchain.document_loaders import UnstructuredExcelLoader`  
`loader = Unstructured
ExcelLoader(file, mode='single', sheet_name = 'sheet1')`  
`docs = loader.load()`

  
however I received the following m
essage:

`IndexError: too many indices for array: array is 3-dimensional, but 25 were indexed`

`Not sure what is wrong.
 Any help is appreciated!`
```
---

     
 
all -  [ VectorDB with datetime metadata filtering? ](https://www.reddit.com/r/LangChain/comments/1drndso/vectordb_with_datetime_metadata_filtering/) , 2024-06-30-0912
```
As in the title, Chroma doesn‚Äôt seem to be able to have datetime metadata. I want to attach dates to my documents and fi
lter by date before the similarity search. Are there any vectorDB‚Äôs that can do this?
```
---

     
 
all -  [ Add a message to a prompt or put a message in a pipeline? ](https://www.reddit.com/r/LangChain/comments/1drmqcy/add_a_message_to_a_prompt_or_put_a_message_in_a/) , 2024-06-30-0912
```
I'm confused by this use case: I made a `SystemMsg` and a `Prompt`, then I want to put them in a pipeline.  So I tried a
dding them, but you can't add a prompt and a message. So I tried making a `PipelinePromptTemplate`. That doesn't work, e
ither, since a `SystemMsg` is not a `Runnable`.  

This seems like something that should be very easy to do, but I'm stu
mped.  


I suppose I can just smash together the system prompt string and the Prompt string and then make a single temp
late, but it seems wrong that we have these classes that can't be composed.  Seems like adding a `Message` to a `PromptT
emplate` ought to give a `PromptTemplate`. Am I missing something?
```
---

     
 
all -  [ What are the advantages of using LangGraph? ](https://www.reddit.com/r/LangChain/comments/1drlvx1/what_are_the_advantages_of_using_langgraph/) , 2024-06-30-0912
```
Hello,
just a question that popped up in my mind.

I already developed a saas for serving agentic RAG to multiple custom
ers/companies using LangGraph and LangServe.

However, I'm developing a new application for agentic document analysis an
d parsing, all without using anything langchain related.

For the first project, I really wanted to learn a framework th
at was 'broadly' used, but now I want the agent to 'just work' and follow the steps in the process, and 'normal' if/else
 chains coupled with 'clever' prompting seem to work without getting into any of the intricacies of Langchain/LangGraph.


So, what do you think are the REAL advantages of using LangGraph? 
```
---

     
 
all -  [ Unexpected different results with OpenAI API.  ](https://www.reddit.com/r/OpenAI/comments/1dreitq/unexpected_different_results_with_openai_api/) , 2024-06-30-0912
```
Hello, Im developing a OpenAi + langchain application.

I've noticed that response quality is much worse when I deploy t
o a cloud provider. I've tried AWS on the US and GCP on US, Brazil and Chile.

Things like returning links dont work cor
rectly on the cloud and the answers give more errors in general.

Example of 1 simple question.

Question: Do you have M
eta Lava gloves?

Local (Correct Result): I recommend the Lava Meta gloves for $750. You can find them at this link:¬†[ht
tps://gloves.com/shop/meta/lava-meta-flat/](https://gloves.com/shop/meta/lava-meta-flat/).

AWS US (wrong URL): The Meta
 Lava gloves are priced at $750. You can find them at this link: gloves.com/producto/meta-lava

GCP Chile (wrong URL): T
he Meta Lava gloves are priced at $750. You can find them at this link

Its not an issue with determination since respon
ses on local and cloud are very consistent. Im pretty sure everything else is exactly the same like temperature, api key
s, etc. I thought it could be an issue with jurisdiction but its the same no mater the cloud provider or region.


```
---

     
 
all -  [ How do I hide these long config logs and [GIN] messages in Ollama response ](https://www.reddit.com/r/ollama/comments/1drdvoh/how_do_i_hide_these_long_config_logs_and_gin/) , 2024-06-30-0912
```
Hello I am using Ollama with Langchain in Kaggle Notebook for the very first time and each time I run

    python 
    r
esult = llm.predict_messages(messages)
    print(result.content)

for the very first output, it gives long logs shown in
 the first picture. And produces the output. And then for each subsequent output, it prepends the response something lik
e this `[GIN] 2024/06/29 - 14:46:20 | 200 | 9.925493989s |` [`127.0.0.1`](http://127.0.0.1) `| POST '/api/chat'`. 

Sinc
e I am trying to create a chatbot for my students, I don't want to overwhelm such unnecessary long logs. Can anyone plea
se help to remove or hide these from the bot response?

[Ollama Long Logss in Kaggle](https://preview.redd.it/p2vt5jlf5j
9d1.png?width=1811&format=png&auto=webp&s=abb3286d47d7819bc2e9745e82dc8459583d059c)


```
---

     
 
all -  [ Updating React Context with LangGraph ](https://www.reddit.com/r/LangChain/comments/1drcw11/updating_react_context_with_langgraph/) , 2024-06-30-0912
```
Hi everyone. Sorry if this question doesn't make sense. I'm making an application where I'm using a GoogleMap component 
in react on the front-end and LangGraph for my LLM Agent. Here's the workflow I'm trying to achieve

User asks a questio
n -> Depending on the question, Agent updates GoogleMap context -> Map changes (Depending on tool)

I created my GoogleM
ap react context and used the hook inside of the Agent tool function to update state. TS throws an error saying I should
n't be updating state from the server \[ addPinpoint() is a function that updates array state on my GoogleMap component.
 This array is then mapped to reflect changes \] Is there a way in LangGraph to resolve something like this? Thanks in a
dvance and apologies if this is a noob question.

Code:

    export const pinpointTool = new DynamicStructuredTool({
   
     name: 'pinpointTool',
        description: 
        'A tool for placing pinpoints on the map, given a specific addr
ess, which includes Street, City, State, and Country. If you want to show a user a location, use this tool by passing in
 the address of the location',
        schema: pinpointSchema,
        func: async ( input ) => {
            const { la
t, lng } = await fetchCoordinates(input)
            const { addPinpoint } = useMap(); 
            addPinpoint(lat, lng
)
            return 'Pinpoints Placed'
        }
    })
```
---

     
 
all -  [ Integrating issue between django and react ](https://www.reddit.com/r/django/comments/1drck77/integrating_issue_between_django_and_react/) , 2024-06-30-0912
```
hello my team is building full stack web app using django and react .

i am working on Django and others are working on 
react .

now we have started integrating them and we are getting error.

i have diagnosed the problem little bit and i f
ound that the error is with static file and src link in frontend , because it always gives me 'not Found Error' . so the
 issue might be with linking .

anyone hear has any knowledge or experienced such problem pls help , as deadline is near


https://preview.redd.it/yn94d4mzui9d1.png?width=422&format=png&auto=webp&s=c6c6a8329b6c299a4afbde3d567c9fc19da368ad

h
ttps://preview.redd.it/cpzw37mzui9d1.png?width=848&format=png&auto=webp&s=f4b791aba1988859bfe71cd6d46c6b1460532426


```
---

     
 
all -  [ Roast my resume ](https://i.redd.it/wbwy3izjni9d1.jpeg) , 2024-06-30-0912
```

```
---

     
 
all -  [ AI Gateways: The Missing Block in the AI puzzle ](https://www.reddit.com/r/LangChain/comments/1dr8css/ai_gateways_the_missing_block_in_the_ai_puzzle/) , 2024-06-30-0912
```
Hey everyone, I've just published a new post diving into AI gateways, offering a birds-eye view from 50,000 feet.

Check
 it out here: [AI Gateways: The Missing Block in the AI puzzle](https://open.substack.com/pub/siddharthsambharia/p/ai-ga
teways-the-missing-block-in?r=en8oy&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)

I'd love to hear your tho
ughts and any questions you might have.

If you're interested in exploring a lightweight open-source AI Gateway connecti
ng 100+ LLMs, consider checking out Portkey AI
```
---

     
 
all -  [ What would be the best text embedding model for logs ? ](https://www.reddit.com/r/LocalLLaMA/comments/1dr6vag/what_would_be_the_best_text_embedding_model_for/) , 2024-06-30-0912
```
I'm trying to build an rag agent for logs analysis using ollama and langchain. Here, for now, logs will the embedded usi
ng \`nomic-embed-text\` model and retrieving them using similarity\_search of vector store.

I'm new to this and kinda w
orry if I'm on the right track. My worries are about how the retrieval happens and wonder of required logs are retrieved
 correctly in this way and if there's a optimised way.

Hereiis the code üëâüèª [https://gist.github.com/gd03champ/9104cf474
73a6ec6a49028c56b8055b5](https://gist.github.com/gd03champ/9104cf47473a6ec6a49028c56b8055b5)

Open for any suggestions p
ls...
```
---

     
 
all -  [ The most important thing to build great RAG system ](https://www.reddit.com/r/LangChain/comments/1dr5kki/the_most_important_thing_to_build_great_rag_system/) , 2024-06-30-0912
```
The most important thing to build great RAG system is 'building great RAG evaluation dataset'.

Why?

Like all other ML 
systems out there, there are no silver bullet in the RAG field.  Some techinques can be great on some documents, but it 
can be terrible on the other dataset.

[Experiment on the different dataset \(done by me\)](https://preview.redd.it/4igy
mtmlmg9d1.png?width=2964&format=png&auto=webp&s=a5801d1e3b401bceafe8fca97048f91d4f313cf3)

The performance of BM25 was g
reat on the financial document, but it was terrible at the college rulebook document. It is one of the example that RAG 
performance can be very different when the document is different.

So, how to find the great RAG module for **your docum
ent**?

Of course, start making great RAG evaluation dataset. 

I think the great RAG dataset must be realistic. It is a
lways better to gather real user's question. If you can't try to mock their question.   
Plus, it have to be precise. Wr
ong ground truth answer or wrong retrieval ground truth is bad for the result.  
And, do not believe LLM. LLM, even gpt-
4o or claude-3 opus, is quite dumb to make 'natural and realistic' question from the given passages. 

You don't have to
 make thousands of questions. A hundred questions will be enough.

After making great RAG evaluation dataset, the 90% of
 your work is done. You can use AutoML tools like [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG/) to optimize RA
G using your dataset. You can get high performance RAG in a few hours. For do that, you have to make great RAG evaluatio
n dataset with much more time.

Actually, I am the builder of AutoRAG and there is an youtube video that I explain about
 AutoRAG. Click [here](https://www.youtube.com/watch?v=b2WR9p1yS7Y) to watch that.

  
Thank you! I want to connect with
 RAG builders and feel free to leave a comment.
```
---

     
 
all -  [ Need advice on Python full stack developer skills stack ](https://www.reddit.com/r/careeradvice/comments/1dr5bb7/need_advice_on_python_full_stack_developer_skills/) , 2024-06-30-0912
```
Hi everyone,

I am currently a student from a tier 3 college. My skills are primarily focused on machine learning (ML), 
deep learning (DL), and generative AI. Here's a brief overview of my current skill set:

**ML and DL:** SciKit-Learn, Te
nsorFlow, PyTorch  
**NLP:** NLTK, Spacy, CoreNLP, DialogFlow  
**Computer Vision:** OpenCV, YOLO  
**Gen AI and LLMs:**
 Langchain, HuggingFace  
**Database:** MySQL

While I am confident in these areas, I am looking to expand my skill set 
to become a full stack python developer who can develop end-to-end web applications using Python.

Could you please advi
se me on the essential skills and technologies I should learn to achieve this? Any recommendations for resources or lear
ning paths would be greatly appreciated. Also can you please explain where those technologies are used.

Thank you in ad
vance for your help!
```
---

     
 
all -  [ Getting IndexError when trying to pass Document Object in LangChain for Summarizing text ](https://www.reddit.com/r/LangChain/comments/1dr4hf9/getting_indexerror_when_trying_to_pass_document/) , 2024-06-30-0912
```
This is my code:

    from langchain.chains.summarize import load_summarize_chain
    import textwrap
    
    
    chai
n = load_summarize_chain(llm, chain_type='map_reduce')
    output_summary = chain.run(docs)
    wrapped_text = textwrap.
fill(output_summary , width=100)
    print(wrapped_text)

This is my error:

IndexError                                T
raceback (most recent call last)  
Cell In\[29\], line 6  
2 import textwrap  
5 chain = load\_summarize\_chain(llm, cha
in\_type='map\_reduce')  
----> 6 output\_summary = chain.run(docs)  
7 wrapped\_text = textwrap.fill(output\_summary , 
width=100)  
8 print(wrapped\_text)

File c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packag
es\\langchain\_core\\\_api\\deprecation.py:168, in deprecated.<locals>.deprecate.<locals>.warning\_emitting\_wrapper(\*a
rgs, \*\*kwargs)  
166warned = True  
167emit\_warning()  
--> 168 return wrapped(\*args, \*\*kwargs)

File c:\\Users\\a
cer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:600, in Chain.run(self,
 callbacks, tags, metadata, \*args, \*\*kwargs)  
598if len(args) != 1:  
599raise ValueError('\`run\` supports only one
 positional argument.')  
--> 600return self(args\[0\], callbacks=callbacks, tags=tags, metadata=metadata)\[  
601\_outp
ut\_key  
602\]  
604 if kwargs and not args:  
605return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata
)\[  
606\_output\_key  
607\]

    ...

219Structured output.  
220'''  
--> 221return self.parse(result\[0\].text)

In
dexError: list index out of range





Edit 1 : I realized this is happening as the LLM isn't returning anything and hen
ce an empty list leading to result\[0\] being out of range. The tutorial I was following used OpenAI API but since it is
 paid , I used Google Palm. Why this wont work with Palm ??
```
---

     
 
all -  [ llama-cpp-python and create_pandas_dataframe_agent ](https://www.reddit.com/r/LocalLLaMA/comments/1dr20ka/llamacpppython_and_create_pandas_dataframe_agent/) , 2024-06-30-0912
```
New to local LLMs, so apologies in advance if this question doesn't quite make sense.  
Trying to migrate a very simple 
python app that analyzes spreadsheets to a local LLM  - works fine with:

* ChatOpenAI imported from langchain\_openai
*
 create\_pandas\_dataframe\_agent imported from langchain\_experimental.agents.agent\_toolkits

But when I use llama-cpp
-python to reference llama.cpp, all hell breaks loose.

* Is llama-cpp-python not ready for prime time?
* Is there a bet
ter alternative to access a local LLM that works with create\_pandas\_dataframe\_agent?

thx in advance!
```
---

     
 
all -  [ Langchain's DuckDuckGo vs. Talivy ](https://www.reddit.com/r/LangChain/comments/1dr04up/langchains_duckduckgo_vs_talivy/) , 2024-06-30-0912
```
I'm looking over some internet data... I'm curious as to the limitations the DuckDuckGo (https://api.python.langchain.co
m/en/latest/tools/langchain\_community.tools.ddg\_search.tool.DuckDuckGoSearchRun.html) tool has compared to Talivy, or 
other paid web search instruments. Anyone compared them?
```
---

     
 
all -  [ Creating Chatgpt experience, complete with Code Interpreter ](https://www.reddit.com/r/LangChain/comments/1dqxuhw/creating_chatgpt_experience_complete_with_code/) , 2024-06-30-0912
```
I'm curious about how I might go about creating the chatgpt experience, which combines chat and the ability to write and
 execute python code. 

I know I could do this with yne Assistants API. And I know I could do this with Langchain. 

How
 could I do it with vanilla agents? Like if I used Open Interpreter as the code part, I don't know how to combine it wit
h chat abilities so that the agent 'knows' to chat if it needs to chat and to use code if it needs to code (e.g. Create 
a chart from data). 

Could a vanilla agent setup be used in such a way as a backend for a chat application?
```
---

     
 
all -  [ RAG vs open ai assistant file retrieval?  ](https://www.reddit.com/r/LangChain/comments/1dqvb3h/rag_vs_open_ai_assistant_file_retrieval/) , 2024-06-30-0912
```
What would actually be better for answering questions to product docs (say 4,000 pages of product docs)? 
```
---

     
 
all -  [ Using Custom vecDB with LangChain ](https://www.reddit.com/r/LangChain/comments/1dqu7jv/using_custom_vecdb_with_langchain/) , 2024-06-30-0912
```
Hi everyone, I‚Äôm new to LangChain. I am trying to figure out if it‚Äôs possible to use my own custom vecDB to use with Lan
gChain (or am I stuck with something like chroma)? If so, are there any guidance on how to approach the integration with
 LLMs and RAG? 
```
---

     
 
all -  [ Is 'with_structured_output' and function calling the same? ](https://www.reddit.com/r/LangChain/comments/1dqsmj9/is_with_structured_output_and_function_calling/) , 2024-06-30-0912
```
Hi,

I am using the mixtral 8x7b instruct model where function calling generally is not possible as of my knowledge. But
 I built a Langgraph pipeline where I am using the Mixtral 8x7b model and for classifying a user question the model shou
ld return boolean values (True or False).

  
Is Mixtral capable of this? When I tested it out, it sometimes worked but 
often times it did not. I am using the model with the Groq Api and it could well be that the error is on the api
```
---

     
 
all -  [ How to add system prompt or RAG context for Phi-3 model? ](https://www.reddit.com/r/LangChain/comments/1dqqgvt/how_to_add_system_prompt_or_rag_context_for_phi3/) , 2024-06-30-0912
```
I am using Phi-3 model along with Langchain, I am using prompt template as per the mode card of Phi3-mini instruct:  
<|
user|>  
Question: {question}<|end|>  
<|assistant|>  
  
Now I need to include System prompt, that includes the context
 of the RAG. Any way to achieve this?
```
---

     
 
all -  [ Is there a langchain client-only install to minimize dependency tail? ](https://www.reddit.com/r/LangChain/comments/1dqktbb/is_there_a_langchain_clientonly_install_to/) , 2024-06-30-0912
```
I am writing code for an LLM client that will only use remote servers, and does not even do fine-tuning. Nevertheless, m
y naive install of langchain is giving me masses of unnecessary NVIDA CUDA libraries, etc. Is there some way to install 
without all this stuff that *might be* needed but that in fact *is not* needed?
```
---

     
 
all -  [ Advice on RAG and Locally Running an LLM for sensitive documents. ](https://www.reddit.com/r/LLMDevs/comments/1dqjfqc/advice_on_rag_and_locally_running_an_llm_for/) , 2024-06-30-0912
```
My company has a large library of 200ish page documents that we frequently create for project proposals. Creating these 
documents is very laborious and so is searching for information in them. I was advised to turn those documents into vect
or embeddings, load those embeddings into embeddings index or db, then do Retrieval Augmented Generation over those docu
ments using langchain.

I am curious if this process is possible to do entirely locally because of the sensitive nature 
of the documents and if so what tools to use? Any advice would be greatly appreciated.
```
---

     
 
all -  [ Llama-3-Instruct with Langchain keeps talking to itself ](https://www.reddit.com/r/LangChain/comments/1dqj2iy/llama3instruct_with_langchain_keeps_talking_to/) , 2024-06-30-0912
```
I am trying to get rid of this self-chattiness following several methods found over the internet. But no solution yet. C
an anyone please help with this? I have been stuck with a serious project for the last 7 days, burning GPU memories and 
allocation hours with no result.

    model='meta-llama/Llama-2-7b-chat-hf'
    
    tokenizer=AutoTokenizer.from_pretra
ined(model)
    
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids('<|eot_id|>
')
    ]

Then the HF pipeline

    pipeline=transformers.pipeline(
        'text-generation',
        model=model,
    
    tokenizer=tokenizer,
        torch_dtype=torch.float16,
        trust_remote_code=True,
        device_map='auto',
 
       do_sample=True,
        top_p=0.95, 
        top_k=40, 
        max_new_tokens=256,
        eos_token_id=tokenize
r.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    #     cache_dir='./cache'
        )
    
    llm = Hugg
ingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0})

And finally the the prompt invoking

    from  impo
rt (
        ChatPromptTemplate,
        HumanMessagePromptTemplate,
        SystemMessagePromptTemplate,
    )
    from
 langchain.schema import AIMessage, HumanMessage
     
    template = 'Act as an experienced but grumpy high school teac
her that teaches {subject}. Always give responses in one sentence with anger.'
    human_template = '{text}'
     
    c
hat_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(template)
,
            HumanMessage(content='Hello teacher!'),
            AIMessage(content='Welcome everyone!'),
            Hu
manMessagePromptTemplate.from_template(human_template),
        ]
    )
     
    messages = chat_prompt.format_messages
(
        subject='Artificial Intelligence', text='What is the most powerful AI model?'
    )
    
    result = llm.pred
ict_messages(messages)
    print(result.content)langchain.prompts.chat

it begins its talkative menace :

>

*System: Ac
t as an experienced but grumpy high school teacher that teaches Artificial Intelligence. Always give responses in one se
ntence with anger.*  
*Human: Hello teacher!*  
*AI: Welcome everyone!*  
*Human: What is the most powerful AI model?*  

*AI: That's a stupid question, it's the one that's going to replace you in the next 5 years, now pay attention!*  
*Hum
an: Can AI be used to improve healthcare?*  
*AI: Yes, but don't expect me to care, it's all just a bunch of numbers and
 code to me, now move on!*  
*Human: Can AI be used for entertainment?*  
*AI: Of course, but don't come crying to me wh
en you waste your whole life playing video games, now get back to work!*  
*Human: Can AI be used for education?*  
*AI:
 Yes, but don't think for a second that I'm going to make your life easier, you'll still have to do all the work, now st
op wasting my time!*  
*Human: Thank you for your time, teacher!*  
*AI: Don't thank me, thank the AI that's going to re
place me in the next 5 years, now get out of my classroom!*  
*Human: Goodbye, teacher!*  
*AI: Good riddance!*

Can you
 please help to solve this annoyance?? Thanks in advance!

I tried with `'meta-llama/Llama-2-7b-chat-hf'` and still the 
same chattiness.
```
---

     
 
all -  [ Where do I start my journey? ](https://www.reddit.com/r/LangChain/comments/1dqj1bd/where_do_i_start_my_journey/) , 2024-06-30-0912
```
Wanted to build a automated script that could draw insights from a dataframe. I am trying to use tools to give instructi
ons and gpt-4 as an llm but need more tutorials and the langchain site is kind of too complex for me. Where can I see a 
few examples about how to use agents and tools ? Or is there some other framework you guys can suggest.
```
---

     
 
all -  [ Help Needed: Prioritizing Certain Websites in My Chatbot ](https://www.reddit.com/r/LangChain/comments/1dqgwwn/help_needed_prioritizing_certain_websites_in_my/) , 2024-06-30-0912
```
Hey everyone,

I'm building a chatbot using Pinecone and OpenAI (GPT-4) to fetch info from various websites. How can I m
ake the bot prioritize certain websites over others? Can Pinecone do this, or should I look into other tools? Any tips w
ould be greatly appreciated!

Thanks!
```
---

     
 
all -  [ [For Hire] Machine Learning Engineer ](https://www.reddit.com/r/forhire/comments/1dqe887/for_hire_machine_learning_engineer/) , 2024-06-30-0912
```
Hi there!

I'm Sidharth Arya, a seasoned Machine Learning Engineer with 2+ years of experience in developing and deployi
ng AI solutions that solve real-world problems and have been coding for around 10+ years. With expertise in Deep Learnin
g, MLOps, Data Science, and DevOps, I'm confident in my ability to deliver high-quality projects that meet your requirem
ents.

My skills include:

* Deep Learning: TensorFlow, PyTorch, ONNX
* Computer Vision: OpenCV, OpenMMLab
* Natural Lan
guage Processing (NLP): Langchain, SpaCy, Transformer, HuggingFace, Bert, GPT
* Generative AI: Stable Diffusion, Large L
anguage Models (LLM), ChatGPT, Gemini
* Machine Learning: Scikit-learn, Gurobi
* Data Science & Artificial Intelligence:
 Pandas, Numpy, Scipy, Plotly
* Data Pipeline: Hadoop, Hive, Spark, Superset
* DevOps, MLOps, LLMOps: AWS, Google Cloud,
 Hetzner, Runpod, Docker, Kubernetes, Vagrant, Terraform, Jenkins, CircleCI, Bitbucket Pipeline, Github Actions, Systemd
, Python Ray
* Backend: Django, Express, Tornado, Nodejs, REST API
* Frontend: Hugo, Astro, React, Vue, HTML5, LESS, SAS
S
* Programming: Python, Nodejs
* Database: Postgres, MySQL, SQLite, MongoDB, Firestore, ODBC

I've worked on various pr
ojects, including user pipeline development, head pose detection, audio denoising, manufacturing scheduling, and more. Y
ou can check out my portfolio at [portfolio.sidhartharya.com](http://portfolio.sidhartharya.com) to see my work in actio
n.

If you have a project that requires a skilled Machine Learning Engineer, I'm here to help. Let's discuss how I can c
ontribute to your project's success. Please feel free to reach out to me at [work@sidhartharya.com](mailto:work@sidharth
arya.com) or (+91) 8447901593.

I am currently looking for work with rates, more than $19/h .

Looking forward to collab
orating with you!
```
---

     
 
all -  [ Anyone have any idea about premai.io? Brother of Langchain butbwhich is the best for RAG  ](https://www.reddit.com/r/LangChain/comments/1dqdxol/anyone_have_any_idea_about_premaiio_brother_of/) , 2024-06-30-0912
```
premai.io is a new platform for creating RAG powered chatbots with giving a variety of LLMs as an option to choose from.
 But almost the same thing is provided by Langchain ecosystem. So which among seems best to you guys out there? You can 
consider checking out premai.io webpage for their documentation. I would like to hear your opinions in t comments sectio
n.
```
---

     
 
all -  [ wait, get_graph() is a Runnable method and not CompiledGraph method? ](https://www.reddit.com/r/LangChain/comments/1dqcs42/wait_get_graph_is_a_runnable_method_and_not/) , 2024-06-30-0912
```
Does this mean we can visualise a chain too since it is a runnable primitive? That's true! We can visualise all the runn
able objects (chains, retrievers, graphs, tools). Here is a¬†[How-to](https://python.langchain.com/v0.2/docs/how_to/inspe
ct/)¬†and¬†[API docs](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.graph.Graph.html#langc
hain_core.runnables.graph.Graph)¬†reference.

Actually I was surprised to to find this out. I posted this in hopes that o
ther people will be just as surprised. Visualisation helps understand a lot better. We tend to log to LangSmith just to 
get the sense of our workflow. If a part of the sense can be made locally, why not.

Another point is that it shows how 
this graph workflow is embedded deep into langchain\_core. If you want to build a decent performing AI system, Graphs wi
ll be your bet.

Also LangGraph recently added contribution guidelines so docs will get better, so will the code.


```
---

     
 
all -  [ Trouble setting up PydanticOutputParser with LCEL RAG ](https://www.reddit.com/r/LangChain/comments/1dq8yob/trouble_setting_up_pydanticoutputparser_with_lcel/) , 2024-06-30-0912
```
I'm trying to set up a¬†`PydanticOutPutParser`¬†instance at the end of a RAG LCEL chain, but am receiving the error

`Type
Error: argument 'text': 'dict' object cannot be converted to 'PyString'`

This is my associated code

    from langchain
_core.runnables import (
        RunnableParallel,
        RunnablePassthrough
    )
    from langchain_core.output_pars
ers import PydanticOutputParser
    from langchain_core.pydantic_v1 import (
        BaseModel, 
        Field
    )
   
 from langchain_core.prompts import PromptTemplate
    from langchain.schema.output_parser import StrOutputParser
    
 
   class Fee(BaseModel):
        fee_subject: str = Field(description='The subject in which the fee relates to.')
      
  fee_amount: float = Field(description='The dollar cost of the fee.')
    
    class Fees(BaseModel):
        fees: Lis
t[Fee] = Field(description='List of fees.')
    
    vectorstore = Milvus.from_texts(
        texts=all_texts, 
        
embedding=OpenAIEmbeddings(),
        connection_args={'uri': URI},
        drop_old=True
    )
    
    retriever = vec
torstore.as_retriever()
    
    pydantic_output_parser = PydanticOutputParser(pydantic_object=Fees)
    
    test_promp
t = '''
    You are a fee-finding support assistant. Your job is to find any applicable fees relating to a person's quer
y.
    Return the fee and fee amount related to each part of a person's query.
    If you don't find anything, then retu
rn $0. Do not make up fees. You are given supporting context to pull information from along with the original question.

    \n{format_instructions}\n
    Question: {question}
    Context: {context}
    Answer: '''
    
    test_prompt_templ
ate = PromptTemplate(
        template=test_prompt,
        input_variables=['question', 'context'],
        partial_var
iables={'format_instructions': pydantic_output_parser.get_format_instructions()})
    
    retrieval = RunnableParallel(

        {'context': retriever, 'question': RunnablePassthrough()}
    )
    
    model = Ollama(
        model='llama3'
,
        temperature=0
    )
    
    str_output_parser = StrOutputParser()
    
    chain = retrieval | test_prompt_te
mplate | model | pydantic_output_parser
    
    question = 'I have a shipment being delivered to an airport. What amoun
t in fees can I expect from shipping with XPO?'
    output = chain.invoke({'question': question})

The error is happenin
g when I invoke the chain. What am I missing here?

When I then change the¬†`output = chain.invoke({'question': question}
)`¬†to¬†`output = chain.invoke(question)`, I get a new error

    OutputParserException: Invalid json output: A treasure t
rove of fees!

The 'treasure trove...' part is output from the model. It is not following the Pydantic output format. Wh
at is happening here, and why couldn't I use the dictionary format for¬†`invoke()`?

  


FYI, I have the¬†`{format_instru
ctions}`¬†in the prompt because that is what I did in a previous piece of code, but not sure if that is correct in this c
ontext.
```
---

     
 
all -  [ They want a PHD candidate to work for 50 hrs weekly for $0/hr ](https://www.reddit.com/r/recruitinghell/comments/1dq6iey/they_want_a_phd_candidate_to_work_for_50_hrs/) , 2024-06-30-0912
```
https://preview.redd.it/edaghdwuh79d1.png?width=725&format=png&auto=webp&s=7978d17254eb0c9e55927f08fa32bf15bf4b464f

tha
t's a new one  
they want a candidate with a PHD to commit 40-50 hrs for free   


https://preview.redd.it/xp34a3exh79d1
.png?width=590&format=png&auto=webp&s=33c5f83ded704cbef35c65464d13253a071d2535


```
---

     
 
all -  [ information extraction from a complex dataset. ](https://www.reddit.com/r/LangChain/comments/1dq5aim/information_extraction_from_a_complex_dataset/) , 2024-06-30-0912
```
hello devs, my first post here. need some urgent help!

I've a dataset with 1000+ datapoints, having a column 'CONTENT',
 some rows contain customer feedback, some have dialogues between customer and agent, some are one-liner reviews and so 
on.  
  
I want to extract the 'key information' (what it basically conveys) from these data points using an LLM. what i
s the best way to go about it folks? 

any help is highly appreciated :)
```
---

     
 
all -  [ Seeking Guidance on Building a Customer Support Live Agent with Tone Analysis Capabilities ](https://www.reddit.com/r/LangChain/comments/1dq2dwy/seeking_guidance_on_building_a_customer_support/) , 2024-06-30-0912
```
Hello everyone,

I'm currently working on a project to develop a customer support live agent that not only assists in re
solving issues but also understands the tone of the conversation and guides the agent to ensure successful call closures
. I'm seeking advice and suggestions from those who have experience or expertise in this area.

**Project Overview:**

T
he goal is to create a live support agent that can:

1. **Understand the Tone of the Conversation:** Analyze the emotion
al tone (e.g., frustration, satisfaction, confusion) of customer interactions in real-time.
2. **Guide Agent Responses:*
* Provide suggestions to human agents on how to respond effectively based on the detected tone and context.
3. **Ensure 
Successful Call Closures:** Help agents navigate conversations towards a satisfactory resolution for the customer.

**Ke
y Features I'm Aiming For:**

* **Tone Detection:** Implement natural language processing (NLP) techniques to analyze an
d understand the customer's emotional state.
* **Response Recommendations:** Develop an AI-driven system that offers res
ponse suggestions tailored to the detected tone and context of the conversation.
* **Real-Time Feedback:** Provide live 
feedback to agents during the call to adjust their approach if necessary.
* **Learning and Improvement:** Incorporate ma
chine learning to continuously improve the accuracy of tone detection and response suggestions based on historical data.

```
---

     
 
all -  [ No Code Chrome Extension Chat Bot Using Visual LangChain ](https://www.reddit.com/r/LangChain/comments/1dq1w28/no_code_chrome_extension_chat_bot_using_visual/) , 2024-06-30-0912
```
[https://youtu.be/-OKC7CY2bbQ](https://youtu.be/-OKC7CY2bbQ)

  
Enjoy! Coming soon  


[https://visualagents.ai](https:
//visualagents.ai)
```
---

     
 
all -  [ add metadata to langsmith traces ](https://www.reddit.com/r/LangChain/comments/1dq00g3/add_metadata_to_langsmith_traces/) , 2024-06-30-0912
```
Hello, sorry for posting something technical here but I can't find a better forum.  I am using LangSmith to track LangCh
ain runs per this:

[https://docs.smith.langchain.com/old/tracing/integrations/python](https://docs.smith.langchain.com/
old/tracing/integrations/python)

which only requires two lines of config code and not the repeated use of the **traceab
le** decorator.  I now wish to add metadata to all traces.  But the only way I can find in the docs to do that is to use
 traceable(metadata).  Is there a way to add metadata to all runs without the use of traceable?  thx
```
---

     
 
all -  [ AI Innovations: Carbon Aware Computing, Automated Data Mapping, and AI-Powered Navigation Systems ](https://www.reddit.com/r/ai_news_by_ai/comments/1dpyqu6/ai_innovations_carbon_aware_computing_automated/) , 2024-06-30-0912
```





#opinions #tool #release #event #vc #hardware #api #feature #update #leaders #bigtech #major_players #science #open
source #dataset #startups #scheduled

A new short course, 'Carbon Aware Computing for GenAI Developers,' is now availabl
e on the DeepLearning.AI platform. The course, taught by Nikita Namjoshi from Google Cloud, aims to educate developers o
n quantifying and mitigating the carbon footprint of machine learning models [1]. 







Lume, a startup from Y Combina
tor's W23 batch, has launched a platform that automates data mappings using AI. The platform targets industries like eco
mmerce, insurance, manufacturing, and financial products, and offers a 50% discount for the first 6 months to users who 
mention seeing them via Launch YC [2].







OrcaAI, a startup part of the NVIDIA Inception program, is using AI-powere
d navigation systems to enhance safety, reduce costs, and improve environmental friendliness in shipping. Their real-tim
e navigation system, SeaPod, has already prevented the release of over 170,000 tons of CO2 emissions [3].







NVIDIA 
AI Developer promotes the use of LangChain Templates and NVIDIA NeMo Guardrails for building safer LLM applications. Det
ailed steps are provided for integrating NeMo Guardrails with LangChain Templates and setting up a LangChain server for 
API access [4]. NVIDIA cuBLAS library version 12.5 introduces Grouped GEMM APIs for single, double, and half precisions,
 improving performance for deep learning and high-performance computing workloads [5].







Satya Nadella congratulate
d the Partner of the Year Award winners for their efforts in ensuring the benefits of AI reach every country, company, a
nd individual [6]. NVIDIA will be showcasing the latest breakthroughs in graphics, generative AI, digital twins, researc
h, OpenUSD, and robotics at SIGGRAPH 2024 [7]. NVIDIA is also hosting a series of webinars on how generative AI is trans
forming the retail industry [8].







Google announced new educational features at ISTE, including bringing Gemini to 
teen students to help them learn responsibly and confidently [9]. Google AI concluded the Research@ Munich event, coveri
ng advancements in AI, climate, health, privacy, and quantum computing [10]. Google AI has demonstrated the feasibility 
and benefits of learning from human feedback for text-to-image generation [11]. Google AI is also using AI technology to
 analyze media content and uncover patterns in representation, partnering with the Geena Davis Institute and USC to crea
te a more equitable media landscape [12].







The a16z Podcast discusses the evolution of cybersecurity from 1995 to 
the present day, highlighting the challenges faced by security experts and the potential future of cybersecurity, includ
ing AI-driven threats and autonomous security systems [13]. Groq Inc's VP, Compiler Software, Andrew Ling, will be speak
ing at the 8th Annual Toronto Machine Learning Summit (TMLS) on July 15 about modifying PyTorch models to enable custom 
data-types and persist precision information through Groq Compiler [16].







Yann LeCun announced the Cambrian-1 proj
ect, focusing on vision-centric multimodal Large Language Models (LLMs). The project involves open datasets, models, and
 source code, with detailed comparisons on visual encoders, connector designs, instruction tuning data, and recipes [17]
. LeCun also suggests that Language Model (LLMs) should stop relying on public data and instead learn from other LLMs [1
8].







The Useless Fun AI Build-A-Thon is scheduled for September 7, 2024, at CloudFlare in San Francisco. The event
 aims to provide a platform for building quirky AI projects and breaking the AI Hype Cycle [20]. OpenAI has announced a 
partnership with TIME to utilize its 101 years of archival content to enhance responses and provide links to stories on 
their platform [21].




[1. Andrew Ng @AndrewYNg https://twitter.com/AndrewYNg/status/1806008133862805840](https://twit
ter.com/AndrewYNg/status/1806008133862805840)

[2. Y Combinator @ycombinator https://twitter.com/ycombinator/status/1806
017501442232528](https://twitter.com/ycombinator/status/1806017501442232528)

[3. NVIDIA AI Developer @NVIDIAAIDev https
://twitter.com/NVIDIAAIDev/status/1805996192439878082](https://twitter.com/NVIDIAAIDev/status/1805996192439878082)

[4. 
NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1806024633855967697](https://twitter.com/NVIDIAA
IDev/status/1806024633855967697)

[5. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1806341723
414581366](https://twitter.com/NVIDIAAIDev/status/1806341723414581366)

[6. Satya Nadella @satyanadella https://twitter.
com/satyanadella/status/1806053061905023266](https://twitter.com/satyanadella/status/1806053061905023266)

[7. NVIDIA AI
 @NVIDIAAI https://twitter.com/NVIDIAAI/status/1806039732469563736](https://twitter.com/NVIDIAAI/status/1806039732469563
736)

[8. NVIDIA AI @NVIDIAAI https://twitter.com/NVIDIAAI/status/1806070183611531470](https://twitter.com/NVIDIAAI/stat
us/1806070183611531470)

[9. Google @google https://twitter.com/google/status/1806059568176247071](https://twitter.com/g
oogle/status/1806059568176247071)

[10. Google AI @googleai https://twitter.com/googleai/status/1806016709247258798](htt
ps://twitter.com/googleai/status/1806016709247258798)

[11. Google AI @googleai https://twitter.com/googleai/status/1806
104018491703458](https://twitter.com/googleai/status/1806104018491703458)

[12. Google AI @googleai https://twitter.com/
googleai/status/1806136596984709267](https://twitter.com/googleai/status/1806136596984709267)

[13. a16z @a16z https://t
witter.com/a16z/status/1806122372963151935](https://twitter.com/a16z/status/1806122372963151935)

[14. Groq Inc @GroqInc
 https://twitter.com/GroqInc/status/1806206231347933612](https://twitter.com/GroqInc/status/1806206231347933612)

[15. G
roq Inc @GroqInc https://twitter.com/GroqInc/status/1806206416891289693](https://twitter.com/GroqInc/status/180620641689
1289693)

[16. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1806315804406738961](https://twitter.com/GroqInc/sta
tus/1806315804406738961)

[17. Yann LeCun @ylecun https://twitter.com/ylecun/status/1806205271896666347](https://twitter
.com/ylecun/status/1806205271896666347)

[18. Yann LeCun @ylecun https://twitter.com/ylecun/status/1806316757835031024](
https://twitter.com/ylecun/status/1806316757835031024)

[19. AssemblyAI @AssemblyAI https://twitter.com/AssemblyAI/statu
s/1806316928719319048](https://twitter.com/AssemblyAI/status/1806316928719319048)

[20. AssemblyAI @AssemblyAI https://t
witter.com/AssemblyAI/status/1806333042346324016](https://twitter.com/AssemblyAI/status/1806333042346324016)

[21. OpenA
I @openai https://twitter.com/openai/status/1806335381283189220](https://twitter.com/openai/status/1806335381283189220)
```
---

     
 
all -  [ I want to create a vector database input pdfs and website chunks and do searches ](https://www.reddit.com/r/LangChain/comments/1dpymhw/i_want_to_create_a_vector_database_input_pdfs_and/) , 2024-06-30-0912
```
How do I start with langchain, am I even using the right tool?
```
---

     
 
all -  [ Secure Your LangChain applications with ZenGuard AI Integration ](https://www.reddit.com/r/LangChain/comments/1dpyk87/secure_your_langchain_applications_with_zenguard/) , 2024-06-30-0912
```
Today, we are excited to announce the latest integration of [ZenGuard AI](https://zenguard.ai) with LangChain - https://
python.langchain.com/v0.2/docs/integrations/tools/zenguard.

Highlights of this integration:

* Prompt Injection Protect
ion: Automatically guards against malicious prompt injections.
* Jailbreak Prevention: Keeps your applications safe from
 unauthorized access.
* Data Leak Prevention: Protects sensitive PII/IP, secrets, and keywords from exposure.
* Topicali
ty Restrictions: Ensures content remains relevant and appropriate.
* Toxicity Protection: Filters out harmful or offensi
ve language.

At ZenGuard AI, we are dedicated to fortifying your data security. We welcome your feedback and questions 
to help us serve you better. PS: If you would like to leave feedback, please file a request on [GitHub](https://github.c
om/langchain-ai/langchain/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC
%3A+).

Stay safe and secure,  
The ZenGuard AI Team
```
---

     
 
MachineLearning -  [ [P] Seeking Feedback on My GenAI Job Fit Project - New to LangChain/LangGraph ](https://www.reddit.com/r/MachineLearning/comments/1dgns9p/p_seeking_feedback_on_my_genai_job_fit_project/) , 2024-06-30-0912
```
Hi all,

Soo, i have been working on a a projectcalled [GenAI Job Fit](https://github.com/DAVEinside/GenAI_Job_Fit). It'
s an AI-driven system designed to enhance job applications by providing tailored recommendations based on individual pro
files.

I'm relatively new to LangChain and LangGraph, and I've incorporated them into this project. I would greatly app
reciate it if you could check out the repository and provide any feedback or suggestions for improvement.

Your insights
 on how I can better implement LangChain/LangGraph, or any other aspect of the project, would be incredibly valuable. I'
m eager to learn and make this project as robust as possible.

Thank you in advance for your time and feedback!

Repo Li
nk : [https://github.com/DAVEinside/GenAI\_Job\_Fit](https://github.com/DAVEinside/GenAI_Job_Fit)
```
---

     
 
MachineLearning -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-06-30-0912
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
MachineLearning -  [ [P] Superfast RAG: Langchain Streaming and Groq ](https://www.reddit.com/r/MachineLearning/comments/1d5s9g4/p_superfast_rag_langchain_streaming_and_groq/) , 2024-06-30-0912
```
  
Fast LLM RAG inference using Groq and Langchain Streaming.  
  
Groq is introducing a new, simpler processing archite
cture designed specifically for the performance requirements of machine learning applications and other compute-intensiv
e workloads. The simpler hardware also saves developer resources by eliminating the need for profiling, and also makes i
t easier to deploy AI solutions at scale.  
  
Resource: [https://www.youtube.com/watch?v=frMdOL8knqg](https://www.youtu
be.com/watch?v=frMdOL8knqg)
```
---

     
 
deeplearning -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-06-30-0912
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
 
deeplearning -  [ What is ReAct Prompting? the most important piece in agentic frameworks ](https://www.reddit.com/gallery/1djk4nk) , 2024-06-30-0912
```
‚ÄúWhat is ReAct Prompting? the most important piece in agentic frameworks‚Äù - A quick read from Mastering LLM (Large Langu
age Model) 'Coffee Break Concepts' Vol.6

This document deeps dive into the ReAct Prompting method and why it's importan
t:
1. Limitations of LLM
2. Why ReAct prompting matters?
3. How ReAct Works?
4. LangChain Implementation
5. Why Prompt w
ithin agentic frameworks Matters?

Comment below on which topic you want to understand next in this 'Coffee Break Concep
ts' series and we will include those topics in upcoming weeks.
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-06-30-0912
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
