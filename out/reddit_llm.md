 
all -  [ This thing annoys me a lot. PlanAndExecute mistyping 'title' instead of 'query' when trying to use t ](https://www.reddit.com/gallery/1b7gjco) , 2024-03-06-0910
```

```
---

     
 
all -  [ Autonomous agent framework comparison ](https://www.reddit.com/r/singularity/comments/1b7czo0/autonomous_agent_framework_comparison/) , 2024-03-06-0910
```
Does anyone have something that compares/contrasts the different autonomous agent frameworks out there these days (ie. l
angchain, autogen, etc.). There seems to be so many floating around but hard to discern the pros and cons of each.
```
---

     
 
all -  [ Confluence cleanup ](https://www.reddit.com/r/LangChain/comments/1b7byjo/confluence_cleanup/) , 2024-03-06-0910
```
 Hello,

I'm fairly new to LangChain, and I'm wondering if it's a use case that can be well executed with LangChain.   

I would like to create a helper/copilot (chatbot) in the first place to identify irrelevant, duplicate, or contradictory
 content and assist me in restructuring the navigation tree. 
```
---

     
 
all -  [ Seeking Advice for RAG App ](https://www.reddit.com/r/LangChain/comments/1b7b57n/seeking_advice_for_rag_app/) , 2024-03-06-0910
```
Hello,

I'm an intern with an ambitious project that could earn me a full-time position. Our company often engages in ex
tensive referencing of files, documents, and web research, which is time-consuming. My solution is to automate these tas
ks through a bot, leveraging LangChain and GPT-4 for intelligent query processing.

**Project Overview:**

The bot will 
integrate with Microsoft Teams, enabling users to submit queries and receive document references or research results. It
 will serve different departments, each possibly requiring access to a vast array of data (Word, Excel, PDFs, emails, et
c.). The data volumes we're looking at span gigabytes to terabytes. Given this, I'm leaning towards using a vertical dat
abase to manage this diverse and voluminous data efficiently.

**Seeking Advice on:**

1. **Vertical Database Selection:
** Given my inclination towards a vertical database for this project, which specific vertical database systems would you
 recommend for handling both structured and unstructured data across such a broad spectrum of file types and sizes?

2. 
**Database Structure:** Is it more advantageous to maintain a single comprehensive database or to deploy multiple databa
ses, one for each department, to streamline data management and querying processes?

3. **Project Architecture:** How sh
ould I structure the interaction between the vertical databases, the LangChain bot, and the GPT-4 API to ensure seamless
 operation and scalability? What considerations should I keep in mind to support the large data volume and diverse file 
types?

4. **Managing Large Datasets:** Any insights on efficiently processing and retrieving information from large Exc
el files filled with extensive numerical data would be especially helpful. Are there particular strategies or technologi
es that would facilitate handling such datasets?

I am very much open to learning from your experiences and suggestions,
 including any alternative approaches or tools that could enhance the project. My ultimate aim is to develop a convincin
g proof of concept that clearly demonstrates the potential benefits and feasibility of the initiative.

Thank you immens
ely for your guidance and support!

```
---

     
 
all -  [ Hirering full stack developer with Langchain knowledge ](https://www.reddit.com/r/LangChain/comments/1b7915m/hirering_full_stack_developer_with_langchain/) , 2024-03-06-0910
```
We are looking to hire a full stack developer to help us with ai projects. You must be excellent at these skills:
Englis
h
Langchain with Python
Setting up a frontend that works coherently with the backend and can be used by multiple users.

If you are interested, please message us you resume at Team@dialogintelligens.dk
```
---

     
 
all -  [ Llama plus text similarity ](https://www.reddit.com/r/LangChain/comments/1b7470g/llama_plus_text_similarity/) , 2024-03-06-0910
```
Hello guys, i am extracting information about someone using llama and then using this information to compare it to a tex
t of requirements. For example:

Req: person needs to have skills A or similar.

What can be the best technologies to do
 this comparison?  
I was trying embeddings models and then calculating the l2 distance between the requirements and the
 skills but i seem to be getting bad scores for people that should be 'good'.

Anyone ever dived into this realm? Can¬¥t 
seem to find good info online.
```
---

     
 
all -  [ Update: Langtrace Preview: An opensource LLM monitoring tool - achieving better cardinality compared ](https://www.reddit.com/r/LangChain/comments/1b6phov/update_langtrace_preview_an_opensource_llm/) , 2024-03-06-0910
```
This is with regards to: [https://www.reddit.com/r/LangChain/comments/1b4s7cw/building\_a\_open\_source\_llm\_monitoring
\_software/](https://www.reddit.com/r/LangChain/comments/1b4s7cw/building_a_open_source_llm_monitoring_software/)  


Ju
st wanted to share an update on my open source LLM monitoring tool. I do not have a UI yet, so asked chatGPT to plot the
 spans of a trace I generated for a langchain example code that uses agents. Below is the screenshot of my tool's trace 
plotted:

https://preview.redd.it/xvqgcukrgemc1.png?width=2980&format=png&auto=webp&s=0eaa0d298e047457520359017123054f65
570621

  
Same output from Langsmith:

https://preview.redd.it/lulyrgh6gemc1.png?width=778&format=png&auto=webp&s=db44f
54bf8d561ed379a2ea3e1dfe2319ee9ab84

&#x200B;

Feedback/comments/thoughts welcome
```
---

     
 
all -  [ Local Models w/ M1 Pro? ](https://www.reddit.com/r/LangChain/comments/1b6n1k9/local_models_w_m1_pro/) , 2024-03-06-0910
```
Hello. 

How capable is an M1 Pro 3.2 GHz and 32 GB of ram at running local models? I‚Äôm deciding whether to buy a used m
achine for $1K to power a Langchain based project I‚Äôm working on. 

I‚Äôm also considering using cloud services and wait u
ntil the new Mac hardware launches which will likely be optimized for LLMs. 

Thanks!üôè 
```
---

     
 
all -  [ Scale PDF Q&A App to 10K Users with GPUs ‚Äì <$250/Mo ](https://www.reddit.com/r/machinelearningnews/comments/1b6k0s7/scale_pdf_qa_app_to_10k_users_with_gpus_250mo/) , 2024-03-06-0910
```
Hello everyone,

Check out this step-by-step detailed tutorial on building and scaling a PDF Q&A Application using Pinec
one, Langchain and Inferless

&#x200B;

[Architecture](https://preview.redd.it/tnwmdqdfedmc1.png?width=1301&format=png&a
uto=webp&s=819867865e8a7361f306b5aa111919aa9a4d5bb9)

Alongside, the detailed quick deploy guide, it also includes cost 
analysis on how you can save upto 84% cost with an example of processing 3000 documents and nearly 10,000 queries every 
month, all while dramatically cutting your costs from $1800 ( AWS) to just $250 a month on Inferless.

Here is the tutor
ial - [https://cookbook.inferless.com/](https://cookbook.inferless.com/)

If you resonate, join the discussion on Hacker
news here - [https://news.ycombinator.com/item?id=39594588](https://news.ycombinator.com/item?id=39594588)
```
---

     
 
all -  [ Scale PDF Q&A App to 10K Users with GPUs ‚Äì <$250/Mo ](https://www.reddit.com/r/MLQuestions/comments/1b6jzz7/scale_pdf_qa_app_to_10k_users_with_gpus_250mo/) , 2024-03-06-0910
```
Hello everyone,

Check out this step-by-step detailed tutorial on building and scaling a PDF Q&A Application using Pinec
one, Langchain and Inferless

&#x200B;

[Architecture](https://preview.redd.it/ib26vnfaedmc1.png?width=1301&format=png&a
uto=webp&s=166e69be2a820476cd836f86f0eeece532807a3a)

Alongside, the detailed quick deploy guide, it also includes cost 
analysis on how you can save upto 84% cost with an example of processing 3000 documents and nearly 10,000 queries every 
month, all while dramatically cutting your costs from $1800 ( AWS) to just $250 a month on Inferless.

Here is the tutor
ial - [https://cookbook.inferless.com/](https://cookbook.inferless.com/)

If you resonate, join the discussion on Hacker
news here - [https://news.ycombinator.com/item?id=39594588](https://news.ycombinator.com/item?id=39594588)
```
---

     
 
all -  [ Scale PDF Q&A App to 10K Users with GPUs ‚Äì <$250/Mo ](https://www.reddit.com/r/mlops/comments/1b6jyl8/scale_pdf_qa_app_to_10k_users_with_gpus_250mo/) , 2024-03-06-0910
```
Hello everyone,

Check out this step-by-step detailed tutorial on building and scaling a PDF Q&A Application using Pinec
one, Langchain and Inferless

&#x200B;

[Architecture](https://preview.redd.it/v5rc3w9wddmc1.png?width=1301&format=png&a
uto=webp&s=d5736aeac19741ce95112dfebe14cd58cd7f11e1)

Alongside, the detailed quick deploy guide, it also includes cost 
analysis on how you can save upto 84% cost with an example of processing 3000 documents and nearly 10,000 queries every 
month, all while dramatically cutting your costs from $1800 ( AWS) to just $250 (Inferless) a month.

Here is the tutori
al - [https://cookbook.inferless.com/](https://cookbook.inferless.com/)

If you resonate, join the discussion on Hackern
ews here - [https://news.ycombinator.com/item?id=39594588](https://news.ycombinator.com/item?id=39594588)
```
---

     
 
all -  [ [D] : Scale PDF Q&A App to 10K Users with GPUs ‚Äì <$250/Mo ](https://www.reddit.com/r/MachineLearning/comments/1b6jv56/d_scale_pdf_qa_app_to_10k_users_with_gpus_250mo/) , 2024-03-06-0910
```
Hello everyone,

Check out this step-by-step detailed tutorial on building and scaling a PDF Q&A Application using Pinec
one, Langchain and Inferless

&#x200B;

[Architecture](https://preview.redd.it/zfta52cbddmc1.png?width=1301&format=png&a
uto=webp&s=440399212d3feb03e861759a31602e2cde0dc7fb)

Alongside, the detailed quick deploy guide, it also includes cost 
analysis on how you can save upto 84% cost with an example of processing 3000 documents and nearly 10,000 queries every 
month, all while dramatically cutting your costs from $1800 ( AWS) to just $250 a month on Inferless.

Here is the tutor
ial - [https://cookbook.inferless.com/](https://cookbook.inferless.com/)

If you resonate, join the discussion on Hacker
news here - [https://news.ycombinator.com/item?id=39594588](https://news.ycombinator.com/item?id=39594588)
```
---

     
 
all -  [ Effective way to summarize 10k page documents ](https://www.reddit.com/r/LangChain/comments/1b6fiyt/effective_way_to_summarize_10k_page_documents/) , 2024-03-06-0910
```
Hello,

Is there an effective way to summarize 10k page documents. 

So I have chunks of 300 words and their embeddings 
already stored in the database.

I can apply clustering algorithm like k means. But for such large documents, I think ef
fective cluster size would be 100-150, with each cluster of being approximately 25-30k tokens. 

That would mean 100 api
 calls to get chunk summary , and final api call yo get final summary. 

1. How can we optimize this ? Not necessarily u
sing clustering, any other way to  make it kore cost efficient. As passing 32k prompt to model is very expensive itself,
  and to add 100 api calls. 

2. For such large documents what metrics can I use to ensure summary is good and model isn
't hallucinating. 

Thanks! 
```
---

     
 
all -  [ Is using microsoft stack the only way to successfully build chatbots? ](https://www.reddit.com/r/generativeAI/comments/1b6fhb6/is_using_microsoft_stack_the_only_way_to/) , 2024-03-06-0910
```
In my team, we have been trying to build chatbots using Langchain and Azure OpenAI models ( with the plan to move to ope
n source models later hopefully), but for many different reasons it's not going well and not very successful. From my po
int of view, main reasons are no having dedicated developers to work on building chatbots, bad data quality (html files 
with lots of html tags and noise), not having a clear picture of end product. Today, I got an email from managers that w
e should move to Microsoft Azure AI Document Intelligence and they said: 'Chatbot will newer work without a AI knowledge
 search, and the AI knowledge search will never work without proper document/data structure.' Is using microsoft stack t
he only way to success? 
```
---

     
 
all -  [ Best Academic Papers on Using LLM's to Automate Development Tasks? ](https://www.reddit.com/r/LocalLLaMA/comments/1b6eww2/best_academic_papers_on_using_llms_to_automate/) , 2024-03-06-0910
```
Hi Everyone,

I am currently working on automating some R&D-type tasks/workflows with LLM's - specifically open source o
nes. I have been reading about Guidance, custom grammar files, and LMQL as methods to standardize the output of an LLM t
o make it more suitable for chaining prompts.

That being said, I think **conceptual** approaches to solving this issue 
are equally - if not more - important. 

So far, I have found this paper linked from the LangChain docs: [https://arxiv.
org/abs/2210.03629](https://arxiv.org/abs/2210.03629), which describes the ReAct method of prompting.

**What other nota
ble advancements have been made in this arena?** 

Also, are there any stand-out papers on utilizing fine-tuning in a sy
stematic way to nudge models into more effectively working as agents? Specifically, improving the accuracy of returning 
structured JSON responses? This seems like the golden ticket.

Looking to start a good discussion on this. Thanks!
```
---

     
 
all -  [ How can I store the QA object? ](https://www.reddit.com/r/LangChain/comments/1b69l6p/how_can_i_store_the_qa_object/) , 2024-03-06-0910
```
How can I store the QA object to preserve its state while implementing memory? For every question answered, we create a 
new RetrievalQA object, which flushes the chat\_history. I'm creating a chatbot where multiple projects can be made, and
 I don't want to lose the QA object's state. How can I save the QA in Django?
```
---

     
 
all -  [ Roast my resume. Or any suggestions for improvement. ](https://i.redd.it/2z238yga3bmc1.jpeg) , 2024-03-06-0910
```
Hi everyone. My first post here. Please suggest improvements I can do in my resume.
```
---

     
 
all -  [ TypeError: Object of type RetrievalQA is not JSON serializable ](https://www.reddit.com/r/LangChain/comments/1b67qnb/typeerror_object_of_type_retrievalqa_is_not_json/) , 2024-03-06-0910
```
How to serialize QA object?
```
---

     
 
all -  [ Best framework for LLM based applications in production ](https://www.reddit.com/r/LangChain/comments/1b67jkl/best_framework_for_llm_based_applications_in/) , 2024-03-06-0910
```
We've been building LLM based tools for months, but I think that there should be efficient frameworks by now that actual
ly add value. I tried langchain a while back but I felt like it was just an over complicated overhead where it was alway
s simpler to make everything from scratch each time. Guidance has been the only real improvement for me as it does way m
ore than basic prompt templating, but it is in no way a full framework.

Now there are LlamaIndex, TigerLab, Langchain..
. but I simply don't have the time to test them all.

We need to run the models by ourselves, so no Open AI api, ideally
 run something compatible with TGI / VLLM. We need to connect to proper databases and vectorDB (currently using Milvus).
 And I'm looking for something that is actually useful and I don't have to struggle and hack the library everytime I wan
t to do something slightly different.

Does any of you have a good recommendation? Everything changes so quickly I feel 
like I can't trust articles that are older than two months. So what are you currently using and what has been an overhyp
ed crap?
```
---

     
 
all -  [ How to use Rerankers in Langchain (both in Python and JS, but particularly in LangchainJS)? ](https://www.reddit.com/r/LangChain/comments/1b670b4/how_to_use_rerankers_in_langchain_both_in_python/) , 2024-03-06-0910
```
Hi üëã

I would like to know what can be done to perform ReRanking in a RAG pipeline beyond using the Cohere endpoint or t
hose made available by Langchain.

A couple of days ago Mixedbread released some very interesting ReRanking models that 
I would like to try, but perhaps due to a lack of detailed knowledge of the Langchain library, I don't know if it is pos
sible to include any ReRanking model I want within my pipeline.

How are you all doing it? üí¨
```
---

     
 
all -  [ Getting Error while storing QA object locally ](https://www.reddit.com/r/LangChain/comments/1b65y4h/getting_error_while_storing_qa_object_locally/) , 2024-03-06-0910
```
**I am storing QA object locally like this in PGVector:**

`class ProjectQA(models.Model):`

`id = models.AutoField(prim
ary_key=True) # Add primary key field`

`project = models.ForeignKey(ProjectName, on_delete=models.CASCADE)`

`qa_data =
 models.JSONField() # Store serialized qa data in JSONField`

`class Meta:`

`db_table = 'project_qa'`

`def save_qa(sel
f, qa_object):`

`self.qa_data = jsonpickle.encode(qa_object)`

`self.save()`

`def get_qa(self):`

`return jsonpickle.d
ecode(self.qa_data)`

`def __str__(self):`

`return f'QA for Project: {self.project.project_id}'`

I am able to store th
e QA object, but getting below error while getting QA :

object return {self.\_restore(v) for v in obj\[tags.SET\]} 

Ty
peError: unhashable type: 'dict
```
---

     
 
all -  [ Frustrating problems with langchain/LLMs? ](https://www.reddit.com/r/LangChain/comments/1b64qb5/frustrating_problems_with_langchainllms/) , 2024-03-06-0910
```
Starting a thread on the most frustrating problems you‚Äôre facing with the use of Langchain or LLMs in your projects
```
---

     
 
all -  [ Fancy Resume? ü§î ](https://i.redd.it/hw0i989te9mc1.jpeg) , 2024-03-06-0910
```
I custom made this in Figma around a year back, now I want to update it. I was wondering if a resume like this would hav
e any advantage over a traditional resume? I understand this would help in design related positions, but I will apply mo
stly to coding based positions. Also please provide any other feedback related to it too, thankyou.
```
---

     
 
all -  [ Local RAG Chat with ollama, gradio and langchain - POC ](https://www.reddit.com/r/LocalLLaMA/comments/1b5uibf/local_rag_chat_with_ollama_gradio_and_langchain/) , 2024-03-06-0910
```
I built a proof of concept notebook to enable a locally hosted RAG chat with LLama. 

The idea was to use langchain to e
.g. cut markdown files into chunks, embed them with a LLM hosted in ollama, in this case LLama, and then build the chat 
frontend with Gradio. 

If anyone has any ideas on how to improve the similarity search, please let me know :)

[https:/
/github.com/Tr33Bug/Open-Ollama-RAG-ChatApp](https://github.com/Tr33Bug/Open-Ollama-RAG-ChatApp)
```
---

     
 
all -  [ Embedding model error while Loading to Chroma db ](https://www.reddit.com/r/LangChain/comments/1b5pb5u/embedding_model_error_while_loading_to_chroma_db/) , 2024-03-06-0910
```
I am using openai embedding to convert the documents to embedding and post it to chroma db in external server.

collecti
ons = client.get_or_create_collection(name='pdf_of_ai',embedding_function=new OpenAIEmbeddings ())

I have already insta
lled the latest version of langxhain and chroma db but I am still facing this error .

raise ValueError( ValueError: Exp
ected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['args',
 'kwargs'])

Can anyone help me resolve this issue?

I even tried custom embedding class but still facing the same error
.
```
---

     
 
all -  [ Review my resume ](https://i.redd.it/k8gkr9km16mc1.jpeg) , 2024-03-06-0910
```
I have custom made it in Figma, I have to update it now with iOS experience wondering if this format reduces my hiring c
hances or is a good refresh over traditional resumes?
```
---

     
 
all -  [ Hi pretty new with txtai framework. I do have some questions ](https://www.reddit.com/r/txtai/comments/1b5evic/hi_pretty_new_with_txtai_framework_i_do_have_some/) , 2024-03-06-0910
```
Currently finding a candidate again for my tinyllama project (trying to stay away from langchain), I already made my ini
tial test with txtai but I do have questions

* Does this framework supports DPO Training? 

Also after training the mod
el with QLora

 

    train = HFTrainer()     
    train('TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T',         

    pd.read_csv(data),         
    task='language-generation',         
    columns=('source', 'target'),         
   
 prefix='some prefix i put here: ',         
    maxlength=512,         
    per_device_train_batch_size=4,         
   
 num_train_epochs=500,         
    output_dir=output,         
    overwrite_output_dir=True,         
    quantize=qua
ntize, 
    lora=lora     
    )  

I got an error when I called it with the LLM class

    from txtai.pipeline import L
LM 
    llm = LLM(output) 

error: {model} does not appear to have a file named config.json. Checkout 

so yeap any help
 would be nice thank you!
```
---

     
 
all -  [ How to reduce costs when chatting with database? ](https://www.reddit.com/r/LangChain/comments/1b5ecou/how_to_reduce_costs_when_chatting_with_database/) , 2024-03-06-0910
```
I want to reduce costs when chatting with my database. Below is how it works as of now;

    const datasource = new Data
Source({
        type: 'postgres',
        url: LOCAL_DATABASE_URL,
      });
    
      const db = await SqlDatabase.fr
omDataSourceParams({
        appDataSource: datasource,
      });
    
      const llm = new ChatOpenAI({
        openAI
ApiKey: 'sk-',
        modelName: 'gpt-3.5-turbo',
      });
    
      const prompt =
        PromptTemplate.fromTempla
te(`Based on the provided SQL table schema below, write a SQL query that would answer the user's question.
    ---------
---
    SCHEMA: {schema}
    ------------
    QUESTION: {question}
    ------------
    SQL QUERY:`);
    
      const s
qlQueryChain = RunnableSequence.from([
        {
          schema: async () => db.getTableInfo(),
          question: (i
nput: { question: string }) => input.question,
        },
        prompt,
        llm.bind({ stop: ['\nSQLResult:'] }),

        new StringOutputParser(),
      ]);
    
      await sqlQueryChain.invoke({
        question: message,
      });

    
      const finalResponsePrompt =
        PromptTemplate.fromTemplate(`Based on the table schema below, question, 
SQL query, and SQL response, write a natural language response:
      ------------
      SCHEMA: {schema}
      --------
----
      QUESTION: {question}
      ------------
      SQL QUERY: {query}
      ------------
      SQL RESPONSE: {resp
onse}
      ------------
      NATURAL LANGUAGE RESPONSE:`);
    
      const finalChain = RunnableSequence.from([
     
   {
          question: (input) => input.question,
          query: sqlQueryChain,
        },
        {
          schem
a: async () => db.getTableInfo(),
          question: (input) => input.question,
          query: (input) => input.query
,
          response: (input) => db.run(input.query),
        },
        finalResponsePrompt,
        llm,
        new S
tringOutputParser(),
      ]);
    
      const finalResponse = await finalChain.invoke({
        question: message,
   
   });
    
      console.log({ message, finalResponse });

The first obvious optimisation I found is to fine-tune the m
odel with the the SQL schema before asking the question to prevent sending it over and over again.

But out of curiosity
, how does one do this at scale and for the cheapest costs possible? I'm new to LLMs and looking for a simple way for my
 SaaS users to interact with their data. Thanks!
```
---

     
 
all -  [ Is there a software to monitor performance of open-source LLMs? ](https://www.reddit.com/r/LangChain/comments/1b5dscn/is_there_a_software_to_monitor_performance_of/) , 2024-03-06-0910
```
Hello. I have been playing around with a bunch open open-source LLMs, but they all vary in terms of quality of output an
d response times. I'm wondering whether there is an open-source software to monitor the performance of such LLMs that I 
can integrate into my system.
```
---

     
 
all -  [ Suggestion for robust RAG which can handel 5000 pages of pdf ](https://www.reddit.com/r/LangChain/comments/1b5d1m7/suggestion_for_robust_rag_which_can_handel_5000/) , 2024-03-06-0910
```
I'm working on a basic RAG which is really good with a snaller pdf like 15-20 pdf but as soon as i go about 50 or 100 th
e reterival doesn't seem to be working good enough. Could you please suggest me some techniques which i can use to impro
ve the RAG with large data.

What i have done till now : 
1)Data extraction using pdf miner.
2) Chunking with 1500 size 
and 200 overlap 
3) hybrid search (bm25+vector search(Chroma db)) 
4) Generation with llama7b 

What I'm thinking of doi
ng fir further improving RAG

1) Storing and using metadata to improve vector search, but i dont know how should i extra
ct meta data out if chunk or document. 

2) Using 4 Similar user queries to retrieve more chunks then using Reranker ove
r the reterived chunks.

Please Suggest me what else can i do or correct me if im doing anything wrong :)
```
---

     
 
all -  [ Is there a good tutorial on how to setup a RAG stack application? ](https://www.reddit.com/r/LangChain/comments/1b54w8h/is_there_a_good_tutorial_on_how_to_setup_a_rag/) , 2024-03-06-0910
```
I'm an experienced software engineer new to anything involving LLM development. I've consistently believed in upskilling
 throughout my career, so I'm begining to explore how to integrate LLMs into an application. The goal is to do a learnin
g project to understand from hands on experience the pros, cons, and pitfalls of doing so.

My research so far has point
ed me to taking a RAG stack approach, since my goal is to leverage my own data with the LLM to achieve the outcome I wan
t. I have a basic understanding of what is required, however I'm hoping to find a straightforward code-a-long tutorial t
o start dipping my feet in. After that I have a whole project planned out, but I digress.

Ideally this tutorial will no
t require me to run LLama2 or something similar on my machine as I don't have the GPU power for it on my home server. I'
m willing to pay for ChatGPT and use their cheaper options for this.

Thanks in advance for any guidance you can offer.
```
---

     
 
all -  [ Implementing RAG with Langchain on AWS Bedrock ](https://www.reddit.com/r/LangChain/comments/1b51f6n/implementing_rag_with_langchain_on_aws_bedrock/) , 2024-03-06-0910
```
Hello beautiful people,

I am developing a financial chatbot using langchain and chromadb, I want to use llama2 on aws.


Right now everything is on a local project on my machine, as I am going to move from gpt to llama (cost reduction), I w
ant to use llama2 through Bedrock, is it doable to implement a RAG architecture with Langchain on that?


```
---

     
 
all -  [ How to cache LLM responses in Langchain recent versions ](https://www.reddit.com/r/LangChain/comments/1b4y6fb/how_to_cache_llm_responses_in_langchain_recent/) , 2024-03-06-0910
```
I making an FAQ bot using latest langchain version, and pgvector as my vector datastore.  
I've looked for caching metho
ds and most of them very old posts, and the example in the official documentation doesn't work. I've looked into GPTCach
e and the project hasn't been active for a while.  
I want to save some API calls and also improve response time for the
 repeated and similar questions.

Can anyone point me to any projects, resources on how to cache LLM responses.   


Tha
nk you in advance
```
---

     
 
all -  [ Langchain for production  ](https://www.reddit.com/r/LangChain/comments/1b4y0pr/langchain_for_production/) , 2024-03-06-0910
```
Converting a streamlit based Langchain app with ChromaDB into a production ready app with Next js for a better UI. Apart
 from using FastAPI for asynchronous requests, how to achieve streaming within this framework? Also what are some low co
st options for the entire dev stack, what other things should I take into considering while building this ?
```
---

     
 
all -  [ Finetuning LLM with own PDFs ](https://www.reddit.com/r/LangChain/comments/1b4wa9v/finetuning_llm_with_own_pdfs/) , 2024-03-06-0910
```
Hi,

my company wants me to finetune an LLM with their own documents. I am aware of RAG an it is already running but i w
ant to see if it is possible to finetune a LLM with raw pdfs? 

So is it possible without a labeled dataset, just feedin
g the texts from the pdfs? 
```
---

     
 
all -  [ [D] What Is Your LLM Tech Stack in Production? ](https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/) , 2024-03-06-0910
```
Curious what everybody is using to implement LLM powered apps for production usage and your experience with these toolin
gs and advice. 

This is what I am using for some RAG prototypes I have been building for users in finance and capital m
arkets.

**Pre-processing\ETL:**
Unstructured.io + Spark, Airflow

**Embedding model:**
Cohere Embed v3
Previously using
 OpenAI Ada but Cohere has significantly better retrieval recall and precision for my use case. Also exploring other ope
n weights embedding models

**Vector Database:**
Elasticsearch previously but now using Pinecone

**LLM:**
Gone through 
quite a few including hosted and self-hosted options. Went with gpt4 early during prototyping then switched to gpt3.5-tu
rbo for more manageable costs and eventually open weights models. 

Now using a fine-tuned Llama2 70B model self hosted 
with vLLM 

**LLM Framework:**
Started with Langchain initially but found it cumbersome to extend as the app became more
 complex. Tried implementing it in LlamaIndex at some point just to learn and found it just as bad. Went back to Langcha
in and now I am in the midst of replacing it with my own logic

What is everyone else using?

Edit: correct model Llama2
 70B
```
---

     
 
all -  [ Building a open source LLM monitoring software ](https://www.reddit.com/r/LangChain/comments/1b4s7cw/building_a_open_source_llm_monitoring_software/) , 2024-03-06-0910
```
I am building an open source LLM monitoring software. I know there are bunch of other tools out there. But, what are som
e features you would like to see? I would like to solve for the ones that are not already solved by the other tools that
 exist today. 
```
---

     
 
all -  [ Maintain the chat history ](https://www.reddit.com/r/LangChain/comments/1b4rs0h/maintain_the_chat_history/) , 2024-03-06-0910
```
Hey guys, how to maintain the chat history?

I am creating a chat bot with OpenAI API and LangChain in Django. I've chec
ked LangChain, there are several Conversational Retrieval agents, but seems they're not what i need, because they requir
es to save docs in vector db. I don‚Äôt need docs at all.

I just want something like this on my chatbot UI:

    To GPT: 
hey, how are you? 
    GPT:    I am doing good. 
    To GPT: what did i ask you? 
    GPT:    You asked me how i am doin
g.

I couldn't figure out how to achieve above from OpenAI and LangChain.

Any similar project written in Django are muc
h appreciated!
```
---

     
 
all -  [ Streaming with Langchain python and Next JS ](https://www.reddit.com/r/LangChain/comments/1b4r4nj/streaming_with_langchain_python_and_next_js/) , 2024-03-06-0910
```
Building an app with Langchain python  + ChromaDB + Next js. How can I perform streaming for the LLM responses? Also wha
t should the rest of my architecture look like for a production ready system? Currently built a basic PoC with streamlit
 but I am trying to move it to a more production-like Proof of concept. 
```
---

     
 
all -  [ Reflection Agents ](https://www.reddit.com/r/LangChain/comments/1b4oydd/reflection_agents/) , 2024-03-06-0910
```
Hello everyone, is there any example using reflection agents to decide if an answer is adequate or not for the user but 
using more than one tools afterwards? cause i cant find any such example
```
---

     
 
MachineLearning -  [ [D] Graphs + vectordbs? Need your input: Cognee.ai . AI Data Pipelines for Real-World Production (Pa ](https://www.reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/) , 2024-03-06-0910
```
Hey there, Redditors!

I'm back with the latest installment on creating dependable AI data pipelines for real-world prod
uction.

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://top
oteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba40a
ab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines.

After a few months of work
, we integrated cognitive architecture with [keepi.ai](https://www.keepi.ai) 

We aim to explore with our demo:

**1. Co
ntext sanitization**  
The world of AI is fast-moving, and we've realized that the context is becoming a building block 
we refer to as a crucial part of future cognitive architecture.  
**2. Best Practices for AI Memory**  
In this rapidly 
evolving landscape, there are no established best practices. You'll need to make educated bets on tools and processes, k
nowing that things will change. We assume that having traditional data engineering practices + frameworks + classifiers 
and other AI solutions can solve a lot of standard hurdles  
**3. AI Frameworks**  
They are trying to do too much, too 
fast, too broad. We want to find a pattern and a correct layer of abstraction for the AI memory to fit new industry.  



&#x200B;

How does it work? 

The Github repo is l:

  


[How cognee works](https://preview.redd.it/yuiabmyihyjc1.png?
width=1633&format=png&auto=webp&s=4384c4441b615f72caf1e0591c5ab23aee735fab)

Github repo is [here](https://github.com/to
poteretes/cognee)

Next steps:  
I have questions for you:

1. Is context sanitization relevant for you?
2. How do you m
anage metadata? 
3. How do you prepare data for LLMs?
4. Are there any data enrichment steps you perform?

Check out the
 blog post:

[Link to part 4](https://topoteretes.notion.site/Going-beyond-Langchain-Weaviate-Level-4-towards-production
-fe90ff40e56e44c4a49f1492d360173c?pvs=4)

*Remember to give this post an upvote if you found it insightful!*  
*And also
 star our* [Github repo](https://github.com/topoteretes/cognee)
```
---

     
 
MachineLearning -  [ [D] AI projects Suggestions ](https://www.reddit.com/r/MachineLearning/comments/1aunkmw/d_ai_projects_suggestions/) , 2024-03-06-0910
```
Hi Everyone, I need a suggestion to create AI courses for students ( Hands-on AI projects). I am thinking about the late
st AI trends such as Langchain, RAG, and vector databases. In each project, there can be multiple tasks, and the main th
ing is each task should have an automated system in which we can verify whether students have done it correctly or not.


For example: Project with visualization cannot be automatically tested. 

For example: A project with visualization can
not be automatically tested. . em can verify if the length of the text is smaller we can verify that it is correct.
```
---

     
 
MachineLearning -  [ Whats in your RAG setup? [D] ](https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/) , 2024-03-06-0910
```
What frameworks and libraries are you using in your RAG? 

I'm most curious if  LangChain is as popular as it was?

Here
's mine at a high-level: 

*  langchain to use OpenAI for creating embeddings
* Pinecone for storing embedding
* langcha
in to load document splitters and characters splitters for chunking
* Mongo for conversations memory

&#x200B;
```
---

     
 
deeplearning -  [ [D] WebVoyager: Navigating Digital Cosmos with LangGraph & Multimodal Models ](https://www.reddit.com/r/deeplearning/comments/1altlca/d_webvoyager_navigating_digital_cosmos_with/) , 2024-03-06-0910
```
Embark on a journey through the digital cosmos with WebVoyager, a groundbreaking Large Multimodal Model (LMM) web agent 
designed to navigate the vastness of the online universe. In collaboration with Langchain, WebVoyager represents a parad
igm shift in autonomous web agents, seamlessly integrating visual and textual information to complete user instructions 
end-to-end by interacting with real-world websites.

Link: [https://medium.com/@andysingal/webvoyager-navigating-digital
-cosmos-with-langgraph-multimodal-models-dace64196c2f](https://medium.com/@andysingal/webvoyager-navigating-digital-cosm
os-with-langgraph-multimodal-models-dace64196c2f)
```
---

     
