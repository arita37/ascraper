 
all -  [ Online prompt management best practice ](https://www.reddit.com/r/LangChain/comments/1fopx64/online_prompt_management_best_practice/) , 2024-09-25-0912
```
Hey,

I'd like some advice on how safe it is to retrieve prompts from an online platform (like LangSmith or LangFuse) fo
r an LLM app in prod. 

What is usually done to add safeguards in case of accidental prompt deletion, platform failure, 
or whatever? If you have any insights, that would be great. Thank you very much!
```
---

     
 
all -  [ Discussion: Best Way to Plot Charts Using LLM? ](https://www.reddit.com/r/LangChain/comments/1fopcus/discussion_best_way_to_plot_charts_using_llm/) , 2024-09-25-0912
```
Hi guys, how are you plotting charts or graphs? Currently, I am using structured output and sending it to the frontend t
o plot with Plotly React.

I've seen [chat2plot](https://github.com/nyanp/chat2plot) use a dataframe with the data, quer
ying it from the LLM and then structuring the output, but it only uses the dataframe to plot the chart. The LLM never di
rectly accesses that data.

In my current approach, I get the data from an API and then pass it to the LLM, which format
s the data and the structure for the chart. However, this is currently slow.

What approach do you recommend for handlin
g chart generation in this kind of setup?

Regards!
```
---

     
 
all -  [ Why is this Python wrapper class code so hard to understand? ](https://i.redd.it/icgrxyoh3uqd1.png) , 2024-09-25-0912
```
I'm working with a Python class that wraps around Google Generative Al, but the code is tough to follow. It imports adva
nced libraries. I'm struggling to understand why it's so complicated.

I asked AI to explain but it's still very technic
al not able to understand.

```
---

     
 
all -  [ [3 YoE, Unemployed, DevOps Engineer, Canada] ](https://www.reddit.com/r/resumes/comments/1fomi9t/3_yoe_unemployed_devops_engineer_canada/) , 2024-09-25-0912
```
[Resume](https://preview.redd.it/sn3ip6l6htqd1.png?width=1700&format=png&auto=webp&s=6f49b7977bb936e473ae57ca15471a02e10
8dbde)

I moved to Canada over a month ago. I have been applying to jobs for sometime. Could you please look and give me
 some suggestions on my resume. It would be very appreciated! **I seek a cloud engineering, DevOps engineering, or Site 
Reliability Engineering jobs.**

Notes:

* I graduated May of 2024, but I was working full time at my last company as a 
Cloud and DevOps Engineer.
* I also have 2-3 years of Miscellaneous experience, including Sales Jobs and Event Managemen
t, should I include that as points in my resume or LinkedIn.
```
---

     
 
all -  [ OllaLab-Lean to rapidly set up and begin working on LLM-based projects ](https://www.reddit.com/r/ollama/comments/1follho/ollalablean_to_rapidly_set_up_and_begin_working/) , 2024-09-25-0912
```
Hey there. I've taken the offer to work for the GSA - Technology Transformation Service (TTS) which is really big on ope
n source and dev community development. [OllaLab-Lean](https://github.com/GSA/FedRAMP-OllaLab-Lean) is the first project
 I released under GSA TTS and I believe [OllaLab-Lean](https://github.com/GSA/FedRAMP-OllaLab-Lean) can help both novice
 and experienced developers rapidly set up and begin working on LLM-based projects. Installation is simple and supports 
both Docker and Podman.

The project includes several key components.
- Pre-made prompt templates and applications to ac
celerate research and developments.
- Ollama for managing local openweight Large Language Models (LLMs).
- LangChain for
 orchestrating LLM pipelines, allowing users to seamlessly connect, manage, and optimize their workflows.
- Streamlit se
rver to locally host dynamic LLM-based web applications.
- Jupyter Lab server as the integrated development environment 
(IDE), providing users with an interactive space to write, test, and iterate code efficiently.
- Neo4J vector database s
upporting retrieval-augmented generation (RAG) tasks.
- Data analysis, AI, ML tools such as: DuckDB, AutoGluon, AutoViz,
 GenSim, etc.
- Monitoring and Logging tools such as: Elastic Search, Kibana, Grafana, Prometheus.

A few sample use cas
es are:
- Use pre-made prompt templates and the provided Simple Chat Streamlit application to generate initial codes for
 R&D projects in any language.
- Use the provided 'Chat with Local Folder' to interact with multiple documents stored in
 a local folder for research and learning purposes.
- Use Jupyter Lab and the provided Jupyter Notebooks to learn and ex
periment with cutting edge topics such as Graph-based Retreival Augmented Generation (RAG), other advanced RAG technique
s, knowledge graph algorithms, and so on.
- Use Jupyter Lab and the installed AutoML, AutoViz packages to efficiently ex
ecute Data Science/AI/ML tasks.

Sorry for the long text because I am so passionate about this project. More bells and w
hitles will come in later version of [OllaLab-Lean](https://github.com/GSA/FedRAMP-OllaLab-Lean). For now, I hope you wi
ll give it a spin !! 
```
---

     
 
all -  [ LLMs & RAGs: showcase project ideas ](https://www.reddit.com/r/learnmachinelearning/comments/1fojy0y/llms_rags_showcase_project_ideas/) , 2024-09-25-0912
```
Hello everyone, 

I have a good experience with Computer vision models, especially with object detection model such as Y
OLOv7, Grounding DINO or OWL-ViT. 

I am currently looking for a showcase project to do in order to improve my skills in
 LLMs and especially with RAGs, since almost every ML job opportunities are asking for those skills. For now I found thi
s notebook tutorial: [https://github.com/GURPREETKAURJETHRA/RAG-using-Llama3-Langchain-and-ChromaDB](https://github.com/
GURPREETKAURJETHRA/RAG-using-Llama3-Langchain-and-ChromaDB)

Thank you in advance for your help!
```
---

     
 
all -  [ Interactive AI Agents Market Landscape Map (Sept 2024) ](https://www.reddit.com/r/PostAI/comments/1fohvju/interactive_ai_agents_market_landscape_map_sept/) , 2024-09-25-0912
```
here is Ai Agents Market Landscape Map (Sep 2024) grouped by functionality. you can play with it here and double click o
n each agent to review demo and details.

[https://aiagentsdirectory.com/landscape](https://aiagentsdirectory.com/landsc
ape)

https://preview.redd.it/6zsmhga2jsqd1.png?width=1319&format=png&auto=webp&s=d4026f49a69b52f50a012acb2e07cc0ca0826e
fd


```
---

     
 
all -  [ [Article] The Essential Guide to Large Language Model’s Structured Output, and Function Calling ](https://www.reddit.com/r/LangChain/comments/1fogun6/article_the_essential_guide_to_large_language/) , 2024-09-25-0912
```
For the past year, I’ve been building production systems using LLMs. When I started back in August 2023, materials were 
so scarce that many wheels had to be reinvented first. As of today, things have changed, yet the community is still in d
ire need of educational materials, especially from a production perspective.



Lots of people talk about LLMs, but very
 few actually apply them to their users/business.



Here is my new contribution to the community, “[The Essential Guide
 to Large Language Model’s Structured Output, and Function Calling](https://pavelbazin.com/post/the-essential-guide-to-l
arge-language-models-structured-output-and-function-calling?utm_source=reddit&utm_medium=social&utm_campaign=structured_
output&utm_content=sub_langchain)” article.



It is a hands-on guide (long one) on structured output and function calli
ng, and how to apply them from 0 to 1. Not much of requirements, just some basic Python, the rest is explained.



I had
 quite a bit of success applying it at the company to the initiative “Let's solve all customer support issues via LLMs f
or 200K+ users.” We haven’t hit 100% of the goal yet, but we are getting there fast, and structured output in particular
 is what made it possible for us.



Spread the word, and let’s share more on our experience of applied LLMs beyond demo
s.

  
*Note: article does not use LangChain or any other frameworks, it rather encourages you to understand how LangCha
in does things under the hood, and why you might need it, or not.*
```
---

     
 
all -  [ All-in-one AI assistant shared by LangChain ](https://i.redd.it/v7fygp2j1sqd1.jpeg) , 2024-09-25-0912
```

```
---

     
 
all -  [ StopIteration Error on tool use while Streaming ](https://www.reddit.com/r/LangChain/comments/1foffkc/stopiteration_error_on_tool_use_while_streaming/) , 2024-09-25-0912
```
Using astream\_events I **inconsitently** get the attached error message. Sometimes it just works fine, sometimes the ex
ception is thrown directly when the LLM want to use a tool.

Error message:

    coroutine raised StopIteration
    Trac
eback (most recent call last):
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-cha
tbot/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py', line 804, in arun
        raise error_to_raise
  
    File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/lib/python3.12/site-pa
ckages/langchain_core/tools/base.py', line 757, in arun
        tool_args, tool_kwargs = self._to_args_and_kwargs(tool_i
nput)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File '/Users/kesslf/Documents/dev/inte
rgrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py', line
 574, in _to_args_and_kwargs
        tool_input = self._parse_input(tool_input)
                     ^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/lib/pytho
n3.12/site-packages/langchain_core/tools/base.py', line 507, in _parse_input
        key_ = next(iter(get_fields(input_a
rgs).keys()))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    StopIteration
    
    The above exception wa
s the direct cause of the following exception:
    
    Traceback (most recent call last):
      File '/Users/kesslf/Doc
uments/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/lib/python3.12/site-packages/nicegui/events.py', 
line 417, in wait_for_result
        await result
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegu
i-integrated-ui-chatbot/src/ui/layout.py', line 37, in send
        await handle_user_query(question, orchestrator_agent
, history)
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/src/ui/handlers
.py', line 23, in handle_user_query
        await _process_agent_response(executable_agent, history)
      File '/Users/
kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/src/ui/handlers.py', line 61, in _process_age
nt_response
        await executable_agent.run(history, message)
      File '/Users/kesslf/Documents/dev/intergrated-ui-
chatbots/nicegui-integrated-ui-chatbot/src/agents/executable_agent.py', line 53, in run
        async for event in agent
_executor.astream_events(
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/
.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py', line 1384, in astream_events
        async for eve
nt in event_stream:
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/
lib/python3.12/site-packages/langchain_core/tracers/event_stream.py', line 1006, in _astream_events_implementation_v2
  
      await task
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/lib
/python3.12/site-packages/langchain_core/tracers/event_stream.py', line 962, in consume_astream
        async for _ in e
vent_streamer.tap_output_aiter(run_id, stream):
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-
integrated-ui-chatbot/.venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py', line 201, in tap_outpu
t_aiter
        async for chunk in output:
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integ
rated-ui-chatbot/.venv/lib/python3.12/site-packages/langchain/agents/agent.py', line 1810, in astream
        async for 
step in iterator:
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-chatbot/.venv/li
b/python3.12/site-packages/langchain/agents/agent_iterator.py', line 266, in __aiter__
        async for chunk in self.a
gent_executor._aiter_next_step(
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbots/nicegui-integrated-ui-ch
atbot/.venv/lib/python3.12/site-packages/langchain/agents/agent.py', line 1556, in _aiter_next_step
        result = awa
it asyncio.gather(
                 ^^^^^^^^^^^^^^^^^^^^^
      File '/Users/kesslf/Documents/dev/intergrated-ui-chatbot
s/nicegui-integrated-ui-chatbot/.venv/lib/python3.12/site-packages/langchain/agents/agent.py', line 1589, in _aperform_a
gent_action
        observation = await tool.arun(
                      ^^^^^^^^^^^^^^^^
    RuntimeError: coroutine ra
ised StopIteration

Any ideas?
```
---

     
 
all -  [ Consulta cobro proyecto ](https://www.reddit.com/r/chileIT/comments/1fobv3z/consulta_cobro_proyecto/) , 2024-09-25-0912
```
Hola cabros, ayúdenme con esta consulta porfa ¿cuánto cobrarían por el desarrollo de una REST API que utilice OpenAI y l
angchain para uso de herramientas? Es una empresa grande, el desarrollo ya lo tengo listo pero tengo que adaptarlo a las
 necesidades de la empresa y estimo que me demoraría unos dos meses en implementarlo con horario flexible de manera remo
ta (2 horitas diarias).
```
---

     
 
all -  [ What is the best Code Embedding model you have seen so far?? ](https://www.reddit.com/r/LangChain/comments/1foblka/what_is_the_best_code_embedding_model_you_have/) , 2024-09-25-0912
```
I am searching for an excellent code-embedding model for retrieval tasks. What models do you guys think are the best?
```
---

     
 
all -  [ I made an landscape map for Autonomous Web Agents tools ](https://www.reddit.com/r/rpa/comments/1fob4gy/i_made_an_landscape_map_for_autonomous_web_agents/) , 2024-09-25-0912
```
I've been exploring tools for connecting LangChain with web applications. Made a list of the best tools I came across, f
or all to enjoy — [Awesome Autonomous Web](https://github.com/Agent-Tools/awesome-autonomous-web)
```
---

     
 
all -  [ Get n outputs for n inputs in one single LLM call ](https://www.reddit.com/r/LLMDevs/comments/1fo9r6x/get_n_outputs_for_n_inputs_in_one_single_llm_call/) , 2024-09-25-0912
```
So, let's say I have a large number of sentences (30,000).

I want to perform a task so that I get an output for each of
 them, but I can only make 250 requests every minute (Sonnet 3.5 in Amazon Bedrock). This means 2 hours and it's too lon
g.

My idea is a batch approach, with e.g. 100 samples each batch.

With the batch I input the sentences with <sentence>
\[sentence\]</sentence> and tell the LLM to do the same, with an appropriate example.

Now I make 40 requests at the sam
e time, which take 1 minute to complete. This brings down the time to 30,000 / (40 \* 100) \* 1 = 7.5 minutes.

Sadly, t
he output for some of the batches doesn't have 100 <sentence></sentence>, but 99, 98, or whatever. When this happens, I 
have to discard the whole batch.

I've tried LangChain and Pydantic classes but it only worked with 10 examples/batch or
 so, so I need another approach.

**TLDR:** Single LLM call, get n inputs, n outputs, all ordered, no skipping
```
---

     
 
all -  [ LLMChain dropping performance ](https://www.reddit.com/r/Rag/comments/1fo91ge/llmchain_dropping_performance/) , 2024-09-25-0912
```
I have been trying to prompt Llama3.1 8B to create an Agent. Using my VLLM client the LLM was doing its task succesfully
.

As part of our solution and to maintain consistency, I used a Langchain LLMChain and generated its prompt template.


It has been performing really bad with the same prompt. I don't understand why. Did anyone face this drop in performance
?
```
---

     
 
all -  [ What Companies want off a Fresh Grad ](https://www.reddit.com/r/csMajors/comments/1fo8pp9/what_companies_want_off_a_fresh_grad/) , 2024-09-25-0912
```
Literally the bare minimum to be able to apply to companies as a fresh grad, their expectations had gone up so much.

If
 anyone wants to be not rejected at resume screening stage, one would need:

1. ICPC REGIONALS BRONZE or above
2. 1 fron
tend stack such as React / angular
3. 1 backend stack such as Nest.js , spring boot, PHP, ASP.NET 
4. Associated securit
y measures such as Spring Security, etc, with web security framework such as jwt, oauth2, HTTPS configuration
5. Reverse
 Proxy (nginx)
6. High throughput connection such as WebRTC, gRPC
7. Message queue, data streams such as RABBITMQ, KAFKA

8. CICD with jenkins, github actions
9. Containerization: docker
10. Node management (Kubernetes)
11. Deployment : AWS/
 GCLOUD/ AZURE
12. AI: Langchain/ pytorch/ Tensorflow/ Keras (LLM welcome)
13. REST + GRAPHQL, (API) with documentation 
such as swagger
14. Advanced Testing such as Load testing with Apache Jmeter, end to end testing
15. Observability such 
as opentelemetry, Elastic
16. Asynchronous I/O and reactive programming frameworks such as coroutine/ webflux
17. profic
iency in at least 2RDBMS dialect and Nosql such as Cassandra(time based), Mongo(general purpose)
18. Managed a service w
ith high throughput levels
19. Try out auto scaling, proof of contingency operations, linking telemetry data to AWS aler
ts
20. Additional specializations
```
---

     
 
all -  [ Creating a Test Automation App with OpenAI, Streamlit, and LangChain ](https://www.reddit.com/r/softwaretesting/comments/1fo85ue/creating_a_test_automation_app_with_openai/) , 2024-09-25-0912
```
Creating a Test Automation App using OpenAI, Streamlit, and LangChain offers a groundbreaking way to improve software te
sting processes. This approach integrates the power of AI with simple-to-use tools, making it accessible to all team mem
bers while enhancing efficiency.

The development process begins by obtaining an OpenAI API key to access the language m
odels necessary for generating intelligent responses. Next, you set up a coding environment, adding essential libraries 
like Streamlit, OpenAI, and LangChain. With these components in place, the actual coding process starts. You create a Py
thon file (e.g., streamlit\_app.py), import the necessary libraries, and set up the user interface where users can input
 prompts and receive AI-generated test cases.

Deploying the app is simple with GitHub and Streamlit Community Cloud, ma
king it easy for others to use the app once it's live. The combination of low-code, AI-powered solutions enables efficie
nt test automation, cutting down maintenance time, and increasing test reliability.

By leveraging this technology, team
s can break down traditional silos, allowing developers, testers, and product managers to collaborate in creating robust
, efficient, and intelligent test cases. This tutorial provides a comprehensive guide to building an automated test app 
that will drive innovation and enhance collaboration across development teams.

By: [Dr. Ernesto Lee](https://drlee.io/?
source=post_page-----90ccacd4e159--------------------------------)

Read: [https://drlee.io/creating-a-test-automation-a
pp-with-openai-streamlit-and-langchain-90ccacd4e159](https://drlee.io/creating-a-test-automation-app-with-openai-streaml
it-and-langchain-90ccacd4e159)
```
---

     
 
all -  [ Creating a Test Automation App with OpenAI, Streamlit, and LangChain ](https://www.reddit.com/r/CodingJag/comments/1fo85ji/creating_a_test_automation_app_with_openai/) , 2024-09-25-0912
```
Creating a Test Automation App using OpenAI, Streamlit, and LangChain offers a groundbreaking way to improve software te
sting processes. This approach integrates the power of AI with simple-to-use tools, making it accessible to all team mem
bers while enhancing efficiency.

The development process begins by obtaining an OpenAI API key to access the language m
odels necessary for generating intelligent responses. Next, you set up a coding environment, adding essential libraries 
like Streamlit, OpenAI, and LangChain. With these components in place, the actual coding process starts. You create a Py
thon file (e.g., streamlit\_app.py), import the necessary libraries, and set up the user interface where users can input
 prompts and receive AI-generated test cases.

Deploying the app is simple with GitHub and Streamlit Community Cloud, ma
king it easy for others to use the app once it's live. The combination of low-code, AI-powered solutions enables efficie
nt test automation, cutting down maintenance time, and increasing test reliability.

By leveraging this technology, team
s can break down traditional silos, allowing developers, testers, and product managers to collaborate in creating robust
, efficient, and intelligent test cases. This tutorial provides a comprehensive guide to building an automated test app 
that will drive innovation and enhance collaboration across development teams.

  
By: [Dr. Ernesto Lee](https://drlee.i
o/?source=post_page-----90ccacd4e159--------------------------------)

  
Read: [https://drlee.io/creating-a-test-automa
tion-app-with-openai-streamlit-and-langchain-90ccacd4e159](https://drlee.io/creating-a-test-automation-app-with-openai-s
treamlit-and-langchain-90ccacd4e159)
```
---

     
 
all -  [ How To Build a RAG Agent With Nvidia NIM and LangChain ](https://thenewstack.io/how-to-build-a-rag-agent-with-nvidia-nim-and-langchain/) , 2024-09-25-0912
```
... maybe someone finds this helpful 🤔 
```
---

     
 
all -  [ BindTools vs. Router LLM Node - Which one is better? ](https://www.reddit.com/r/LangChain/comments/1fo7vft/bindtools_vs_router_llm_node_which_one_is_better/) , 2024-09-25-0912
```
Hi everyone,

I'm working on creating an agent that assists with market research on companies. The goal is to allow user
s to ask questions like:

* 'What is the revenue of Company X?'
* 'What is the pricing of Service Y from Company Z?'

To
 accomplish this, I've been using the `bindtools` function to connect different tools to the agent. Each tool is special
ized—for example:

* **Pricing Tool**: Scrapes company websites where pricing information is likely found.
* **Revenue T
ool**: Searches specific websites that typically display company revenue figures.

However, as I add more tools, I've no
ticed that sometimes the agent doesn't call the correct tool. I suspect this might be due to overlapping tool descriptio
ns or limitations in how `bindtools` handles tool selection.

I'm considering an alternative approach:

1. **Option 1**:
 **Continue using** `bindtools`, possibly refining tool descriptions to improve accuracy.
2. **Option 2**: **Implement a
 Router LLM Node using LangGraph**:
   * Use an LLM node to read the user's query and determine the category (e.g., reve
nue, pricing, team).
   * Output the category and use `add_conditional_edge` to direct the query to the appropriate node
.
   * In this setup, tools wouldn't use decorators but would be functions represented as nodes.

My questions to the co
mmunity are:

* **Which of these two options is better for ensuring accurate tool selection as I scale up?**
* **Are the
re other strategies or best practices that might suit my use case even better?**

Any insights or experiences you can sh
are would be greatly appreciated!

Thanks in advance for your help!
```
---

     
 
all -  [ LangGraph for Complex Dialogue Flows – Advice and Tips? ](https://www.reddit.com/r/LangChain/comments/1fo6ej9/langgraph_for_complex_dialogue_flows_advice_and/) , 2024-09-25-0912
```
Hey everyone!

Has anyone here experimented with building complex dialogue or conversational flows using LangGraph? I’ve
 been working with the traditional method of intent classification and entity recognition, manually branching through di
alogue flows. While it works, it’s getting a bit difficult with all the moving parts. I’m considering making the switch 
to LangGraph to streamline the process.

For those who have experience with it, is LangGraph suited for this kind of use
 case? If so, what should I be mindful of when making the transition? Any tips or insights would be greatly appreciated!
 Thanks in advance!
```
---

     
 
all -  [ Can't get AWS bedrock to respond at all ](https://www.reddit.com/r/Rag/comments/1fo69fz/cant_get_aws_bedrock_to_respond_at_all/) , 2024-09-25-0912
```
Hi at my company I am trying to use the AWS bedrock FMs , I have been given an endpoint url and the region as well and c
an list the foundational models using boto3 and client.list_foundation_models()

But when trying to access the bedrock L
LMs through both invoke_model of client object and through BedrockLLM class of Langchain I can't get the output 
Example
 1: 
Trying to access the invoke_model 
brt = boto3.client(service_name='bedrock-runtime',region_name='us-east-1', endpo
int_url='https://someprovidedurl')
body = json.dumps({
    'prompt': '\n\nHuman: Explain about French revolution in shor
t\n\nAssistant:',
    'max_tokens_to_sample': 300,
    'temperature': 0.1,
    'top_p': 0.9,
})

modelId = 'arn:aws:....
'
# (arn resource found from list of foundation models)
accept = 'application/json'
contentType = 'application/json'

re
sponse = brt.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
print(response)
response_b
ody = json.loads(response.get('body').read())
print(response_body)
# text
print(response_body.get('completion'))
 The re
sponse Mera data in this case is with status code 200 but output in response_body is 
{'Output': {'__type': 'com.amazon.
coral.service#UnknownOperationException'}, 'Version': '1.0'}

I tried to find this issue on Google/stackoverflow as well
 but the coral issue is for other AWS services and solutions not suitable for me

Example 2: 
I tried with BedrockLLM 
l
lm = BedrockLLM(
     
     client = brt,
     #model_id='anthropic.claude-instant-v1:2:100k',
     region_name='us-east
-1',
    
     model_id='arn:aws:....',
     model_kwargs={'temperature': 0},
     provider='Anthropic'
)
response = llm
.invoke('What is the largest city in Vermont?')
print(response)

It is not working as well 😞 
With error TypeError: 'Non
eType' object is not subscriptable 

Can someone help please 
```
---

     
 
all -  [ Code Executor Agent using LLM and LangChain  ](https://www.reddit.com/r/ArtificialInteligence/comments/1fo50zo/code_executor_agent_using_llm_and_langchain/) , 2024-09-25-0912
```
I tried experimenting to create a code execution agent i.e. the LLM generates and executed the code using LangChain agen
t and tool concept. Check out the code demo here : https://youtu.be/tg8mIsJ2KSY
```
---

     
 
all -  [ How to learn Python for AI, Langchain and RAG ? ](https://www.reddit.com/r/Rag/comments/1fo4x7m/how_to_learn_python_for_ai_langchain_and_rag/) , 2024-09-25-0912
```
Hi guys ,   
I'm a web dev ( JS background ) . I've been learning about AI, Langchain , RAG since past 3 months now beca
use the company that I work for shifted towards the AI narrative.

I along with 6 other devs are getting trained in AI (
 self learning ) , I was the 4th one in the queue . Devs ahead of me started with Langchain using Python , so automatica
lly I was made to learn in Python so that we can collaborate on projects .

I started with a Langchain playlist , then t
utorials , tutorials , tutorials , and from the past 1.5 months I've started with building projects like a simple PDF RA
G app , currently working on multi agent system using Langgraph .  
  
I don't know Python till now ( just beginner leve
l  stuff ) and I feel that's why I'm still not able to write efficient and modular code.

Can you guys suggest me a road
map like how much Python to learn to become proficient in building AI apps using Langchain , Langgraph ?

Any tutorials 
that you followed when you were at level 0 ?
```
---

     
 
all -  [ LangGraph Studio release timeline ](https://www.reddit.com/r/LangChain/comments/1fo4mp4/langgraph_studio_release_timeline/) , 2024-09-25-0912
```
LangGraph Studio is available in beta for only Apple Silicon chip, so when it will be released for other operating syste
m platform like windows and linux? 
```
---

     
 
all -  [ RAG Chatbot for a Real Estate Platform ](https://www.reddit.com/r/LangChain/comments/1fo498m/rag_chatbot_for_a_real_estate_platform/) , 2024-09-25-0912
```
I am planning to build a RAG Chatbot for a real estate platform for a pet project. What sort of data would be appropriat
e to build the vector database? Where can I get the data?
```
---

     
 
all -  [ True AI Agents ](https://www.reddit.com/r/LangChain/comments/1fo40j1/true_ai_agents/) , 2024-09-25-0912
```
I'm seeing a trend in AI Agents and AI in general where it is becoming yet another thing to make people better employees
. This just seems fundamentally wrong to me. Who cares about Salesforce agents? And for the people afraid of there jobs 
being take, I get it but there's another way of looking at it. 

The true juice in AI Agents is going to be the freedom 
it gives everyone to build whatever they want. You'll agents building you a business overnight, helping you build an AAA
 game in a day, creating generative art in your style. It's gonna give every person enterprise level resources at the fi
ngertips. They call social media the 'creator economy' but agents will enable a true creator economy.

What do you guys 
think? And assuming agents become this powerful in the next 5 years, how should people build and prepare?
```
---

     
 
all -  [ Gemini(or Any other AI) God - A Minecraft Mod that Brings AI-Powered Gameplay to Life! ](https://www.reddit.com/r/MinecraftMod/comments/1fo0c94/geminior_any_other_ai_god_a_minecraft_mod_that/) , 2024-09-25-0912
```
This has been inspired by the [**minecraft-gpt-god-plugin**](https://github.com/YOUSY0US3F/minecraft-gpt-god-plugin) I s
aw it, and wanted to create something like that but better, so I thought why not with Gemini? Or Maybe even Langchain? B
ut I just thought I'll go forward with it and see where it goes. Even though it's massively WIP but here are a couple of
 things.

  
**What is Gemini God?**

Gemini God is a Minecraft mod built using the Fabric modding framework. The mod ai
ms to create a dynamic and immersive gameplay experience by integrating AI-powered features that allow players to intera
ct with the game world in new and exciting ways. With Gemini God, you'll be able to communicate with an AI-powered 'God'
 that can understand and respond to your commands, creating a unique and personalized gameplay experience.

  
**What's 
planned for the future?**

While we're still in the early stages of development, I have big plans for Gemini God. Some o
f the features I'm considering include:

Advanced command parsing and execution

Dynamic world context embedding 

AI-po
wered NPCs and entities 

Customizable AI personalities and behaviors



**Why gather a community now?**

You might be w
ondering why I'm posting about this project before it's even finished. The reason is that I want to gather a community o
f interested players and developers who can help shape the direction of the project. By getting feedback and input from 
the community, I can ensure that Gemini God meets the needs and expectations of its users.

  
**How can you get involve
d**?

If you're interested in learning more about Gemini God or want to contribute to the project, I'd love to hear from
 you! You can comment below with your thoughts, suggestions, or questions. I'll also be setting up a Discord server for 
the project, where we can discuss development, share updates, and collaborate on new features.
```
---

     
 
all -  [ Is there a way to put REPL code into a model response? ](https://www.reddit.com/r/LangChain/comments/1fo04vy/is_there_a_way_to_put_repl_code_into_a_model/) , 2024-09-25-0912
```
I want to put the repl code in the response, create a plotly chart with it, convert it to pltoly json, and send the resu
lt to the frontend.  


but, currently gpt does not include the repl code in the model's final response.  


Is there a 
way to solve this?

https://preview.redd.it/nlenqdd3mnqd1.png?width=944&format=png&auto=webp&s=bd35b84a490e77be7b70891a5
22423a353897986


```
---

     
 
all -  [ Vision QA ](https://www.reddit.com/r/LangChain/comments/1fnxzxo/vision_qa/) , 2024-09-25-0912
```
Which is better: google/deplot or qwen2-VL?
```
---

     
 
MachineLearning -  [ [P] Swapping Embedding Models for an LLM ](https://www.reddit.com/r/MachineLearning/comments/1fktvbj/p_swapping_embedding_models_for_an_llm/) , 2024-09-25-0912
```
How tightly coupled is an embedding model to a language model?

Taking an example from Langchain's tutorials, they use O
llama's _nomic-embed-text_ for embedding and _Llama3.1_ for the understanding and Q/A. I don't see any documentation abo
ut Llama being built on embeddings from this embedding model. 

Intuition suggests that a different embedding model may 
produce outputs of other sizes or produce a different tensor for a character/word, which would have an impact on the res
ults of the LLM. So would changing an embedding model require retraining/fine-tuning the LLM as well?

I need to use a e
mbedding model for code snippets and text. Do I need to find a specialized embedding model for that? If yes, how will ll
ama3.1 ingest the embeddings?
```
---

     
 
MachineLearning -  [ [P] Review and suggest ideas for my chatbot ](https://www.reddit.com/r/MachineLearning/comments/1fb2mwl/p_review_and_suggest_ideas_for_my_chatbot/) , 2024-09-25-0912
```
Ok, so I am currently trying to build support chatbot with following technicalities 
1. FastAPI for web server(Need to m
ake it faster)
2. Qdrant as Vector Data Base(Found it to be the fastest amongst Chromadb, Elastic Search and Milvus)
3. 
MongoDB for storing all the data and feedback.
4. Semantic chunking with max token limit of 512.
5. granite-13b-chat-v2 
as the LLM(I know it's not good but I have limited options available)
6. The data is structured as well as unstructured.
 Thinking of having involving GraphRAG with current architecture.
7. Multiple data sources stored in multiple collection
s of vector database because I have implemented an access control.
8. Using mongoengine currently as a ORM. If you know 
something better please suggest.
9. Using all-miniLM-l6-v2 as vector embedding currently but planning to use stella_en_4
00M_v5.
10. Using cosine similarity to retrieve the documents.
11. Using BLEU, F1 and BERT score for automated evaluatio
n based on golden answer.
12. Using top_k as 3.
13. Currently using basic question answering prompt but want to improve 
it. Any tips? Also heard about Automatic Prompt Evaluation.
14. Currently using custom code for everything. Looking to u
se Llamaindex or Langchain for this. 
15. Right now I am not using any AI Agent, but I want to know your opinions. 
16. 
It's a simple RAG framework and I am working on improving it.
17. I haven't included reranker but I am planning to do so
 too.

I think I mentioned pretty much everything I am using for my project. So please share your suggestions, comments 
and reviews for the same. Thank you!!
```
---

     
 
MachineLearning -  [ [P] Lessons from Retrieval Augmented Generation ](https://www.reddit.com/r/MachineLearning/comments/1f9tvg7/p_lessons_from_retrieval_augmented_generation/) , 2024-09-25-0912
```
I implemented Rag in my organization and just wrote a blog about what we learned here:   
[https://www.b-yond.com/post/t
ransforming-telco-troubleshooting-our-journey-building-telcogpt-with-rag](https://www.b-yond.com/post/transforming-telco
-troubleshooting-our-journey-building-telcogpt-with-rag)

Hoping it would be helpful for those in this area. Covers rag 
evaluation (ragas), sql db, langchain agents vs chains, weaviate vector db, hybrid search, reranking, and more.

Some ad
ditional insights on ranking and hybrid search here:

[https://www.linkedin.com/posts/drzohaib\_transforming-telco-troub
leshooting-our-journey-activity-7232072089837486081--Le1?utm\_source=share&utm\_medium=member\_android](https://www.link
edin.com/posts/drzohaib_transforming-telco-troubleshooting-our-journey-activity-7232072089837486081--Le1?utm_source=shar
e&utm_medium=member_android)
```
---

     
 
deeplearning -  [ What is the best approach for Parsing and Retrieving Code Context Across Multiple Files in a Hierarc ](https://www.reddit.com/r/deeplearning/comments/1fh58oz/what_is_the_best_approach_for_parsing_and/) , 2024-09-25-0912
```
I want to implement a Code-RAG system on a code directory where I need to:

* Parse and load all the files from folders 
and subfolders while excluding specific file extensions.
* Embed and store the parsed content into a vector store.
* Ret
rieve relevant information based on user queries.

However, I’m facing two major challenges:

**File Parsing and Loading
:** What’s the most efficient method to parse and load files in a hierarchical manner (reflecting their folder structure
)? Should I use Langchain’s directory loader, or is there a better way? I came across the Tree-sitter tool in Claude-dev
’s repo, which is used to build syntax trees for source files—would this be useful for hierarchical parsing?

**Cross-Fi
le Context Retrieval:** If the relevant context for a user’s query is spread across multiple files located in different 
subfolders, how can I fine-tune my retrieval system to identify the correct context across these files? Would reranking 
resolve this, or is there a better approach?

**Query Translation:** Do I need to use Something like Multi-Query or RAG-
Fusion to achieve better retrieval for hierarchical data?

\[I want to understand how tools like [continue.dev](http://c
ontinue.dev/) and [claude-dev](https://github.com/saoudrizwan/claude-dev) work\]
```
---

     
 
deeplearning -  [ Month of August in AI ](https://www.reddit.com/r/deeplearning/comments/1f6zfz0/month_of_august_in_ai/) , 2024-09-25-0912
```
🔍 I**nside this Issue:**

* 🤖 La*test Breakthroughs: *This month it’s all about A*gents, LangChain RAG, and LLMs evaluat
ion challenges.*
* 🌐 AI Monthly News: Discover how these stories are revolutionizing industries and impacting everyday l
ife: E*U AI Act, California’s Controversial SB1047 AI regulation act, Drama at OpenAI, and possible funding at OpenAI by
 Nvidia and Apple.*
* 📚 Editor’s Special: This covers the interesting talks, lectures, and articles we came across recen
tly.

Follow me on Twitter and LinkedIn at [**RealAIGuys**](https://twitter.com/RealAIGuys) and [**AIGuysEditor**](https
://www.linkedin.com/in/vishal-rajput-999164122/) to get insight on new AI developments.

>**Please don't forget to subsc
ribe to our Newsletter:** [**https://medium.com/aiguys/newsletter**](https://medium.com/aiguys/newsletter)

# Latest Bre
akthroughs

Are Agents just simple rules? Are Agents just enhanced reasoning? The answer is yes and no. Yes, in the sens
e that agents have simple rules and can sometimes enhance reasoning capabilities compared to a single prompt. But No in 
the sense that agents can have a much more diverse functionality like using specific tools, summarizing, or even followi
ng a particular style. In this blog, we look into how to set up these agents in a hierarchal manner just like running a 
small team of Authors, researchers, and supervisors.

[**How To Build Hierarchical Multi-Agent Systems?**](https://mediu
m.com/aiguys/how-to-build-hierarchical-multi-agent-systems-dc26b19201d2?sk=90958e39e1a28f5030872a90f8e3f3da)

**TextGrad
**. It is a powerful framework performing automatic “differentiation” via text. **It backpropagates textual feedback pro
vided by LLMs to improve individual components of a compound AI system.** In this framework, LLMs provide rich, general,
 natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular struc
tures. TextGrad showed effectiveness and generality across various applications, from question-answering and molecule op
timization to radiotherapy treatment planning.

[**TextGrad: Improving Prompting Using AutoGrad**](https://medium.com/ai
guys/textgrad-controlling-llm-behavior-via-text-2a82e2073d10?sk=3633a9aa63b884c97469bce659265921)

The addition of RAG t
o LLMs was an excellent idea. It helped the LLMs to become more specific and individualized. Adding new components to an
y system leads to more interactions and its own sets of problems. Adding RAG to LLMs leads to several problems such as h
ow to retrieve the best content, what type of prompt to write, and many more.

In this blog, we are going to combine the
 **LangChain RAG with DSPy**. We deep dive into how to evaluate the RAG pipeline quantitatively using **RAGAs** and how 
to create a system where instead of manually tweaking prompts, we let the system figure out the best prompt.

[**How To 
Build LangChain RAG With DSPy?**](https://medium.com/aiguys/how-to-build-langchain-rag-with-dspy-ce9154fbafaa?sk=b41d104
05f84c767cf9cd6a58d1ebac0)

As the field of natural language processing (NLP) advances, the evaluation of large language
 models (LLMs) like GPT-4 becomes increasingly important and complex. Traditional metrics such as accuracy are often ina
dequate for assessing these models’ performance because they fail to capture the nuances of human language. In this arti
cle, we will explore why evaluating LLMs is challenging and discuss effective methods like BLEU and ROUGE for a more com
prehensive evaluation.

[**The Challenges of Evaluating Large Language Models**](https://medium.com/aiguys/the-challenge
s-of-evaluating-large-language-models-ec2eb834a349)

# AI Monthly News

# AI Act enters into force

On 1 August 2024, th
e European Artificial Intelligence Act (AI Act) enters into force. The Act aims to foster responsible artificial intelli
gence development and deployment in the EU. The AI Act introduces a uniform framework across all EU countries, based on 
a forward-looking definition of AI and a risk-based approach:

* **Minimal risk:** most AI systems such as spam filters 
and AI-enabled video games face no obligation under the AI Act, but companies can voluntarily adopt additional codes of 
conduct.
* **Specific transparency risk:** systems like chatbots must clearly inform users that they are interacting wit
h a machine, while certain AI-generated content must be labelled as such.
* **High risk:** high-risk AI systems such as 
AI-based medical software or AI systems used for recruitment must comply with strict requirements, including risk-mitiga
tion systems, high-quality of data sets, clear user information, human oversight, etc.
* **Unacceptable risk:** for exam
ple, AI systems that allow “social scoring” by governments or companies are considered a clear threat to people’s fundam
ental rights and are therefore banned.

**EU announcement:** [**Click here**](https://commission.europa.eu/news/ai-act-e
nters-force-2024-08-01_en)

https://preview.redd.it/nwyzfzgm4cmd1.png?width=828&format=png&auto=webp&s=c873db37ca0dadd5b
510bea70ac9f633b96aaea4

# California AI bill SB-1047 sparks fierce debate, Senator likens it to ‘Jets vs. Sharks’ feud


**Key Aspects of SB-1047:**

* Regulation Scope: Targets “frontier” AI models, defined by their immense computational t
raining requirements (over 10²⁶ operations) or significant financial investment (>$100 million).
* Compliance Requiremen
ts: Developers must implement safety protocols, including the ability to immediately shut down, cybersecurity measures, 
and risk assessments, before model deployment.
* Whistleblower Protections: Encourages reporting of non-compliance or ri
sks by offering protection against retaliation.
* Safety Incident Reporting: Mandates reporting AI safety incidents with
in 72 hours to a newly established Frontier Model Division.
* Certification: Developers need to certify compliance, pote
ntially under penalty of perjury in earlier drafts, though amendments might have altered this.

**Pros:**

* Safety Firs
t: Prioritizes the prevention of catastrophic harms by enforcing rigorous safety standards, potentially safeguarding aga
inst AI misuse or malfunction.
* Incentivizes Responsible Development: By setting high standards for AI model training, 
the company encourages developers to think critically about the implications of their creations.
* Public Trust: Enhance
s public confidence in AI by ensuring transparency and accountability in the development process.

**Cons:**

* Innovati
on Stagnation: Critics argue it might stifle innovation, especially in open-source AI, due to the high costs and regulat
ory burdens of compliance.
* Ambiguity: Some definitions and requirements might be too specific or broad, leading to leg
al challenges or unintended consequences.
* Global Competitiveness: There’s concern that such regulations could push AI 
development outside California or the U.S., benefiting other nations without similar restrictions.
* Implementation Chal
lenges: The practicalities of enforcing such regulations, especially the “positive safety determination,” could be compl
ex and contentious.

**News Article:** [**Click here**](https://www.thenation.com/article/society/sb-1047-ai-big-tech-fi
ght/)

**Open Letter:** [**Click here**](https://safesecureai.org/open-letter)

https://preview.redd.it/ib96d7nk4cmd1.pn
g?width=828&format=png&auto=webp&s=0ed5913b5dae72e203c8592393e469d9130ed689

# MORE OpenAI drama

OpenAI co-founder John
 Schulman has left the company to join rival AI startup Anthropic, while OpenAI president and co-founder Greg Brockman i
s taking an extended leave until the end of the year. Schulman, who played a key role in creating the AI-powered chatbot
 platform ChatGPT and led OpenAI’s alignment science efforts, stated his move was driven by a desire to focus more on AI
 alignment and hands-on technical work. Peter Deng, a product manager who joined OpenAI last year, has also left the com
pany. With these departures, only three of OpenAI’s original 11 founders remain: CEO Sam Altman, Brockman, and Wojciech 
Zaremba, lead of language and code generation.

**News Article:** [**Click here**](https://techcrunch.com/2024/08/05/ope
nai-co-founder-leaves-for-anthropic/)

https://preview.redd.it/0vdjc18j4cmd1.png?width=828&format=png&auto=webp&s=e9de60
4c26aed3e47b50df3bdf114ef61f967080

# Apple and Nvidia may invest in OpenAI

Apple, which is planning to integrate ChatG
PT into iOS, is in talks to invest. Soon after, [*Bloomberg* also](https://www.bloomberg.com/news/articles/2024-08-29/nv
idia-has-held-discussions-about-joining-openai-s-funding-round?srnd=homepage-americas) reported that Apple is in talks b
ut added that Nvidia “has discussed” joining the funding round as well. The round is reportedly being led by Thrive Capi
tal and would value OpenAI at more than $100 billion.

**News Article:** [**Click here**](https://www.theverge.com/2024/
8/29/24231626/apple-nvidia-openai-invest-microsoft)

https://preview.redd.it/ude6jguh4cmd1.png?width=828&format=png&auto
=webp&s=3603cbca0dbb1be3e6d0efcf06c3a698428bbdd6

# Editor’s Special

* The AI Bubble: Will It Burst, and What Comes Aft
er?: [**Click here**](https://www.youtube.com/watch?v=91SK90SahHc&t=317s)
* Eric Schmidt Full Controversial Interview on
 AI Revolution (Former Google CEO): [**Click here**](https://www.youtube.com/watch?v=mKVFNg3DEng)
* AI isn’t gonna keep 
improving [**Click here**](https://www.youtube.com/watch?v=Y8Ym7hMR100)
* General Intelligence: Define it, measure it, b
uild it: [**Click here**](https://www.youtube.com/watch?v=nL9jEy99Nh0)
```
---

     
