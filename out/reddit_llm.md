 
all -  [ RefineDocumentsChain ](https://www.reddit.com/r/LangChain/comments/1670dl4/refinedocumentschain/) , 1693553418.0
```
hey guys, I can't for the life of me unserstand how to use this for a question insted of summary. i want it to get multi
ple contexts and imporve on the answer it gives.

&#x200B;

any leads?

&#x200B;

thanks!!
```
---

     
 
all -  [ 94% Original on Originality.ai - Undetectable AI Blogging Formula ](https://youtu.be/R2aTepfkNiY?si=tfObPC5ySqJDX42g) , 1693549048.0
```

```
---

     
 
all -  [ LangChain Library Adds Full Support for Neo4j Vector Index ](https://www.reddit.com/r/Neo4j/comments/166z06j/langchain_library_adds_full_support_for_neo4j/) , 1693548504.0
```
My latest blog post demonstrates how to use the recently added Neo4j Vector index in LangChain Library to both load and 
read data.

&#x200B;

[https://medium.com/neo4j/langchain-library-adds-full-support-for-neo4j-vector-index-fa94b8eab334]
(https://medium.com/neo4j/langchain-library-adds-full-support-for-neo4j-vector-index-fa94b8eab334)
```
---

     
 
all -  [ Would you pay for high-quality vector embeddings? ](https://www.reddit.com/r/LangChain/comments/166yug1/would_you_pay_for_highquality_vector_embeddings/) , 1693547972.0
```
I'm thinking to curate high quality vector embedding 'libraries' and enable developers to call them via an API. This way
 they can embed knowledge from across domains easily into their apps.

Would this be useful to you as a developer?

edit
 1: the vector embedding libraries would be curated textual content by subject-matter-experts on topics like also tradin
g, molecular biology, etc. that developers can't curate on their own.
```
---

     
 
all -  [ What do the GPU-poor use to host models? ](https://www.reddit.com/r/LangChain/comments/166wnjm/what_do_the_gpupoor_use_to_host_models/) , 1693540868.0
```
I have an M2 mac but with no video card. Last I recall, it caps out at the 13b-models, 8-bit versions. So the performanc
e isn't that great (15s responses and not very accurate answers)

I also have access to Google collab, which I've been u
sing for experimentation. However, while I'm storing large models on Google drive, you frequently have to re-install pyt
hon dependencies...which can be just as expensive as re-downloading a large model. 

So what are others using? 

Maybe s
elf-hosting on AWS? Other ideas?
```
---

     
 
all -  [ Would you use a natural language interface? Is there anything you wouldn't use it for? ](https://v.redd.it/r4rv32xfejlb1) , 1693533358.0
```

```
---

     
 
all -  [ ICYMI August: Zep Vector DB, User Store, LangChain collabs & more! ](https://www.reddit.com/r/LangChain/comments/166tqw2/icymi_august_zep_vector_db_user_store_langchain/) , 1693532379.0
```
It's been a while since I posted here, but I thought I'd update you on [Zep's](https://docs.getzep.com/) August activiti
es, including several collaborations with LangChain + additional LangChain integrations.

I've read that many of you hav
e started to look beyond LangChain for more advanced functionality and enhanced performance. Zep recently integrated wit
h LlamaIndex and improved our [Python and TypeScript SDKs](https://docs.getzep.com/) to make it easier and faster to bui
ld apps without utilizing frameworks.

DM me if you have any questions or feedback!

&#x200B;

* We shipped the [Zep Vec
tor Store](https://blog.getzep.com/introducing-the-zep-document-vector-store/). üîé Zep is now a single, batteries-include
d platform for grounding LLM apps with long-term memory.
* We partnered with LangChain to demonstrate building [three fo
undational LLM apps using TypeScript](https://blog.getzep.com/foundations-of-llm-app-development-langchainjs/) üß±üõ†Ô∏è (and 
on the [LangChain blog](https://blog.langchain.dev/zep-x-langsmith-foundations-of-llm-app-development-with-langchain-js-
and-zep/?ref=blog.getzep.com)). The article also offers a great introduction to LangChain's LangSmith observability plat
form.
* We introduced the [Zep User Store](https://blog.getzep.com/introducing-users/). üßëüèª‚ÄçüöÄüßîüèø‚Äç‚ôÇÔ∏èüë∑‚Äç‚ôÄÔ∏èü¶∏üèΩ‚Äç‚ôÄÔ∏è The new User 
object and its associated Sessions provide a powerful way to manage and understand the behavior of individuals using you
r application.
* We released [support for LlamaIndex](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/
ZepIndexDemo.html?ref=blog.getzep.com) ü¶ô‚ù§Ô∏è and [collaborated with the LlamaIndex team on a post for their blog](https://
twitter.com/jerryjliu0/status/1690018390059225088?ref=blog.getzep.com).
* FlowiseAI [launched support](https://twitter.c
om/FlowiseAI/status/1694372350748172682?ref=blog.getzep.com) for Zep's new Vector Store! And LangChain[ released support
](https://twitter.com/LangChainAI/status/1691868585885626790?ref=blog.getzep.com) for both LangChain Python and LangChai
n.js!
* And finally, we released support for [Anthropic's Claude family](https://blog.getzep.com/announcing-anthropic-ll
m-support/) of LLM models.
```
---

     
 
all -  [ Using LangChain to build a working tic-tac-toe game with GUI without writing a line of code ](https://www.youtube.com/watch?v=_JdmzpXLyE0) , 1693526289.0
```

```
---

     
 
all -  [ I Built an All-In-One Discord Bot for OpenAI's GPT 3.5 Turbo ](https://www.reddit.com/r/OpenAI/comments/166qrg9/i_built_an_allinone_discord_bot_for_openais_gpt/) , 1693524723.0
```
This project has been on going for most of the year, I just released the bot as public recently. It is 4,500 lines of py
thon code that utilizes python modules like LLM Predictor, SimpleGPTVectorIndex, OpenChatAI, Langchain, etc. 

The bot o
perates on a secure Google Cloud compute instance. The bot is also built with robust class management, threading, and th
read locking, along with complete isolation of users sessions or files. It also has three tiers of file validation, key 
validation, and a data sanitization process that makes it possible for users to upload data not properly prepared for in
dexing or tabulation.

The bot has three primary functions - learn, chat, and image. In learn mode it can receive, in di
rect message, filetypes: .doc, .docx, .pdf, .odt, .rtf, .txt, and can also fetch data from web links. In chat mode, user
s in Discord can chat with their GPT model with on the fly options like setting the temperature, token limit, amount of 
context to fetch from previous messages, set user roles, and using prompts. In image mode, the bot uses DALL E 2 with ge
neration, regeneration, and variation modules. 

Session authors can give users roles in the chat mode, with options: sy
stem, user (function is in the works). When a user has a role, they can set a prompt for themselves. Only session author
s can give roles, and individuals can only set prompts for themselves. Session authors can also remove a users role. 

I
've also built in utilities, in the learning mode, for generating conversation logs from the interaction, including upvo
te messages, downvote messages, and bookmarking messages. Note, in the screenshot below, these are done by react and can
 only be done by the session author. On session exit, if the session author chooses, the log files are sent to them in d
irect message before being discarded from the actual cloud server. 

Easily, entire groups of people can collaboratively
 work with ChatGPT using custom data, and the session author can set roles for system and user.  Roles are applied (assi
stant, system, user) and function is in the works.

Due to the nature of file and key exchange risks, the bot only handl
es file or keys in direct messages with user. This reduces risk of key exposure or copyright issues in a Discord server 
setting. Users who supply keys, save keys, upload files, or receive log files, all these actions take place in direct me
ssage with the bot. 

Saving keys is completely optional, and the bot will only display partial keys - just like OpenAI 
does, if a user is managing their saved keys. Key files are also automatically deleted from the cloud server if not used
 within 5 days.

The bot only retains user data long enough to index and embed it into the session, then the files are p
romptly discarded. When a user exits a session, any remaining files and the session directory are also discarded. 

The 
bot treats each server instance, each user instance, and each session instance as isolated objects and events. This is n
ot only necessary to keep users experiences and data truly separate, but also to mitigate multi and concurrent processin
g issues. 

The primary goal of this build was to compile many other smaller builds that I, and a community of others, u
sed to work together and build an esoteric variation of ChatGPT. This bot is beta, just made public after months of test
ing. 

Currently, there are a few limitations - limited time, limited data upload sizes, and 1 session per user, 1 sessi
on per channel.

There is even more to this bot, but I am not sure I should overwhelm this post with all of the details.
 

To try the bot, invite the bot, see the terms, see the privacy statements or further info on its usage and options, v
isit the bot's home server EtherCereal: [https://top.gg/servers/907301373387898950](https://top.gg/servers/9073013733878
98950)

The Developer (me): [https://www.linkedin.com/in/nicholas-dustin-065560108/](https://www.linkedin.com/in/nichola
s-dustin-065560108/)

\*an example after uploading a .docx book, sent to me to use by the book's author\*

https://previ
ew.redd.it/d5pxg8vm4jlb1.png?width=1253&format=png&auto=webp&s=f79bd558c753b2dad2878be2f1e2dd8d35609ff1

&#x200B;
```
---

     
 
all -  [ Output parser for openai chat models ](https://www.reddit.com/r/LangChain/comments/166jxzi/output_parser_for_openai_chat_models/) , 1693508882.0
```
I can‚Äôt seem to get this output parser from langchain to work for the chatopenai models: 

https://python.langchain.com/
docs/modules/model_io/output_parsers/pydantic

It works when I just use openai but not the chatopenai. Let me know if an
yone has any ideas or examples. I‚Äôm trying to output json object that has two list items.
```
---

     
 
all -  [ Have any LLMs been optimized or fine tuned to be effective langchain agents? ](https://www.reddit.com/r/LangChain/comments/166hoq8/have_any_llms_been_optimized_or_fine_tuned_to_be/) , 1693503776.0
```
Now that GPT 3.5 has fine tuning, and 4 is supposedly soon to follow, has anybody made datasets to make the best possibl
e agent?

Same question for open source LLMs.

&#x200B;

I've seen this dataset which looks great [https://huggingface.c
o/datasets/jamescalam/agent-conversations-retrieval-tool](https://huggingface.co/datasets/jamescalam/agent-conversations
-retrieval-tool) but I'm not sure what the license on it is. Also definitely curious to hear about any others.
```
---

     
 
all -  [ ü§ñ Agenta: Open-Source Platform for LLM Prompt Engineering, Evaluation, and Deployment ](https://www.reddit.com/r/opensource/comments/166gb8a/agenta_opensource_platform_for_llm_prompt/) , 1693500536.0
```
üëã Hey everyone,

üõ†Ô∏è If you're building with LLMs, you'll want to take a look at agenta. We've open-sourced this platform
 to turbocharge your LLM app development. Here‚Äôs what‚Äôs in it for you:

- üìù **Prompt Engineering**: Experiment with diff
erent prompts, models, embeddings, and RAG strategies.
- üîÑ **Versioning & Evaluation**: Track and test changes in your L
LM app
- üöÄ **One-Click Deployment**: Get an API live with just one click.
- üíª **Integrate with Your Code**: Integrate ag
enta with your existing code base, whether written with langchain, llama_index or any other framework/library

üë• Agenta 
makes it easier for both product people and developers to collaborate. You can create apps using both UI and from code.


üé• **Youtube Demo (2mn)**: [Watch it out here](https://youtu.be/XTlEvcvXjLk?si=Yipxt4TSn6lqTEU5)

‚ñ∂Ô∏è¬†**Live Demo**: [Che
ck it out here](https://demo.agenta.ai/)

‚≠ê **Support Us**: If you find this useful, please star us on GitHub: [Agenta o
n GitHub](https://github.com/agenta-ai/agenta)

üë©‚Äçüíª **Get Involved**: We're looking for contributors and have a range of
 open issues. Join our community on Slack: [Agenta Slack Channel](https://join.slack.com/t/agenta-hq/shared_invite/zt-1z
safop5i-Y7~ZySbhRZvKVPV5DO_7IA)
```
---

     
 
all -  [ ü§ñ Agenta: Open-Source Platform for LLM Prompt Engineering, Evaluation, and Deployment ](https://www.reddit.com/r/PromptEngineering/comments/166fxyq/agenta_opensource_platform_for_llm_prompt/) , 1693499644.0
```
üëã Hey everyone,

üõ†Ô∏è If you're building with LLMs, you'll want to take a look at agenta ([2min demo](https://youtu.be/XTl
EvcvXjLk?si=Yipxt4TSn6lqTEU5)). We've open-sourced this platform to turbocharge LLM app development. Here‚Äôs what‚Äôs in it
 for you:

- üìù **Prompt Engineering**: Experiment with different prompts, models, embeddings, and RAG strategies.
- üîÑ **
Versioning & Evaluation**: Track and test changes in your LLM app
- üöÄ **One-Click Deployment**: Get an API live with jus
t one click.
- üíª **Integrate with Your Code**: Integrate agenta with your existing code base, whether written with langc
hain, llama_index or any other framework/library

üë• Agenta makes it easier for both product people and developers to col
laborate. You can create apps using both UI and from code.

üé• **Youtube Demo (2mn)**: [Watch it out here](https://youtu.
be/XTlEvcvXjLk?si=Yipxt4TSn6lqTEU5)

‚ñ∂Ô∏è¬†**Live Demo**: [Check it out here](https://demo.agenta.ai/)

‚≠ê **Support Us**: I
f you find this useful, please star us on GitHub: [Agenta on GitHub](https://github.com/agenta-ai/agenta)

üë©‚Äçüíª **Get Inv
olved**: We're looking for contributors and have a range of open issues. Join our community on Slack: [Agenta Slack Chan
nel](https://join.slack.com/t/agenta-hq/shared_invite/zt-1zsafop5i-Y7~ZySbhRZvKVPV5DO_7IA)
```
---

     
 
all -  [ langchain generating similar questions with question_generator parameter ](https://www.reddit.com/r/LangChain/comments/166fc4g/langchain_generating_similar_questions_with/) , 1693498207.0
```
Hello,

I am utilizing LangChain as a query documentation tool for a collection of policies contained within a specific 
directory. During my interactions with the Weaviate vector store, I've encountered instances where I need to fine-tune m
y queries in order to obtain meaningful responses. Therefore, I've decided to create a list of 'similar questions' that 
is displayed when the model gives no response, providing the user the option to choose from 5 similar questions that are
 likely to generate an accurate response. I've come across the 'question\_generator' parameter within the Conversational
RetrievalChain class, and I'm seeking clarification on its functionality. I'm aware it takes in the current question and
 chat history, however, I'm interested in understanding whether the 'question\_generator' parameter can be employed to s
can the entire vector store and generate questions that are apt to yield informative responses.

Thanks.
```
---

     
 
all -  [ What are the best tutorials/resources for building agents with LangChain? ](/r/AI_Agents/comments/166aqlm/what_are_the_best_tutorialsresources_for_building/) , 1693493043.0
```

```
---

     
 
all -  [ I've been exploring the best way to summarize documents with LLMs. LangChain's MapReduce is good, bu ](/r/LangChain/comments/165xmzx/ive_been_exploring_the_best_way_to_summarize/) , 1693490258.0
```

```
---

     
 
all -  [ What are the best tutorials/resources for building agents with LangChain? ](https://www.reddit.com/r/AI_Agents/comments/166aqlm/what_are_the_best_tutorialsresources_for_building/) , 1693487358.0
```
I am new to coding and I only made a very simple agent for text completion so far. Now I want to try out Langchain, sinc
e everyone is talking about it.

But I need external resources, videos, tutorials to help. Do you have experience with a
gents in Langchain? How easy do you find it, and can you recommend learning sources?

Thanks!
```
---

     
 
all -  [ Introducing Robai - a very simple framework to work with language models ](https://www.reddit.com/r/OpenAI/comments/166aawh/introducing_robai_a_very_simple_framework_to_work/) , 1693486231.0
```
Hi guys,  


I've been working on a few projects with OpenAI and I realised that I'd basically written my own framework 
for working with language models. It's a very simple framework, which really acts as a 'way' to think about how to inter
act with a language model.  


I was a bit frustrated with the abstractions in Langchain so I wanted to create something
 that was easier to follow. If this is helpful to anyone, great!

[https://github.com/philmade/robai](https://github.com
/philmade/robai)
```
---

     
 
all -  [ What SDKs, tools, and frameworks are you using for building AI agents? ](https://www.reddit.com/r/AI_Agents/comments/166a16e/what_sdks_tools_and_frameworks_are_you_using_for/) , 1693485510.0
```
I still dont see a clear consensus about what tools work best for agents debugging, monitoring, deployment etc. Of cours
e there are popular frameworks for building agents, such as Langchain, but I am looking also for more techstack-agnostic
 software, for people who build agents without a pre-defined framework.
```
---

     
 
all -  [ I'm testing openAI function calling and I'm not really getting what I wanted. Is there any alternati ](https://www.reddit.com/r/LangChain/comments/1669ica/im_testing_openai_function_calling_and_im_not/) , 1693484063.0
```
I'm basically trying to extract relevant info from chunks of text but it is very inconsistent. I know it's not determini
stic, but still I expected to perform better and do what has been told at least 50% of the times.

I wonder if there are
 other models with similar functionality through API.
```
---

     
 
all -  [ General guidance on my project please. ](https://www.reddit.com/r/LocalLLaMA/comments/16674h6/general_guidance_on_my_project_please/) , 1693476901.0
```
Could someone please help with fleshing out the steps that I need to take to get my project underway?

Here is the info:
 I have a rented ubuntu server(ryzen 5900x, 64gb ram) that I can access remotely. No graphics card and no graphical inte
rface on the server. I want to run a uncensored LLM on this rig. I tried downloading koboldcpp+some llama model, but kob
old has a graphical interface and it was suuper slow through X11 and xming server. 

1. How would i run an llm on ubuntu
 with only command line. 
2. How would I give it a persistent character?
3. Is Langchain what i need?
4. Do i need to se
t up a code interpreter on the server to run it all? 

I think I just need 'big picture' steps to understand how it all 
sits together. Thanks.
```
---

     
 
all -  [ Building a Q&A system Using LangChain with FalkorDB ](https://www.falkordb.com/blog/building-a-qa-system-using-kg) , 1693475933.0
```

```
---

     
 
all -  [ Build an LLM-powered application using LangChain ](https://www.leewayhertz.com/build-llm-powered-apps-with-langchain/) , 1693475435.0
```

```
---

     
 
all -  [ Make langchain recognize a pydantic subclass ](https://www.reddit.com/r/LangChain/comments/16661mt/make_langchain_recognize_a_pydantic_subclass/) , 1693473292.0
```
I use the OpenAIMultiFunctions Agent to let the user create groups by text.

For example:

>'Create a group in Berlin wi
th Kate and John'

There are 2 tools, **one for the creation of the group** the other will call the API with the name an
d **return the contact data of the user**.

I have 2 classes  
**Guest**

    class Guest(BaseModel):
     '''Informatio
n about a group Guest of a group'''
 id: int = Field(..., description='The id of the user')
 first_name: str = Field(...
, description='The first name of the user')
 last_name: str = Field(description='The last name of the user')
 code: str 
= Field(description='The phone code of the user')
 phoneNumber: str = Field(description='The phone number of the user')


 **Group**

    class Group(BaseModel):
 '''Information about a group'''
 title: str = Field(..., description='The titl
e of the cast')
 description: str = Field(..., description='The description of the cast')
 location: str = Field(descrip
tion='The location of the cast')
 guestList: Optional[List[Guest]] = Field(description='The list of guests of the cast')


In general everything is working, the agent first gets the contact from the api but then creates the group with empty 
guest or with wrong data like \[{'name': 'Kate', 'phone': '1234567890'}\].  


I also tried to use the ReAct agent, but 
there its the same, the format of the pydantic does not take the subclass of guest.

Someone has an idea about that?  



&#x200B;
```
---

     
 
all -  [ LangChain Summary with Gitlab AI Bot ](https://www.reddit.com/r/LangChain/comments/1661ds4/langchain_summary_with_gitlab_ai_bot/) , 1693457431.0
```
#### Generates a summary for GitLab merge requests by OpenAI.

&#x200B;

https://preview.redd.it/cb4j9xnamdlb1.png?width
=1730&format=png&auto=webp&s=262b9397b42a977c4db7345e64525f8c56169ecd

Check out the repository and give it some love! [
https://github.com/coolbeevip/gitlab-bot](https://github.com/coolbeevip/gitlab-bot)
```
---

     
 
all -  [ I've been exploring the best way to summarize documents with LLMs. LangChain's MapReduce is good, bu ](https://www.reddit.com/r/LangChain/comments/165xmzx/ive_been_exploring_the_best_way_to_summarize/) , 1693446783.0
```
Obviously, short documents are easy ‚Äì just pass in the entire contents of the document into an LLM and out comes a nicel
y assembled summary. But what do you do when the document is longer than even the most generous LLMs? I ran into this pr
oblem while building my new mini-app, [**summarize.wtf**](https://summarize.wtf/)

Langchain offers [Map Reduce](https:/
/python.langchain.com/docs/modules/chains/document/map_reduce), which basically breaks down the document into shorter pi
eces and summarizes each one recursively to patch together a final summary that fits within a specified token limit. Alt
hough Map Reduce does generate a fairly inclusive summary, it is **extremely** expensive, and the cost and processing ti
me associated with it grows super-linearly with the length of the document. Also, it may potentially emphasize less impo
rtant topics while underemphasizing more salient topics in the document due to its equal application of summarization ac
ross the entire document.

So this led me to explore other techniques. I wrote a [pretty detailed article on this topic 
of document summarization with AI](https://pashpashpash.substack.com/p/tackling-the-challenge-of-document), but the TL;D
R is that breaking down a document into key topics with the help of **K-Means vector clustering** is by far the most eff
ective and cost-efficient way to do this. In a nutshell, you chunk the document and vectorize each chunk.

Chunks talkin
g about similar things/topics will fall into distinct 'meaning clusters', and you can sample either the center-point or 
collection of points within each cluster to gather '**representative chunks**' for each distinct meaning cluster a.k.a. 
average meaning of each topic. Then you can stuff these representative chunks into a long context window and generate a 
detailed, comprehensive summary that touches the most important and distinct topics the document covers. I wrote more de
tails on this approach and how it works in my Substack article here: [https://pashpashpash.substack.com/p/tackling-the-c
hallenge-of-document](https://pashpashpash.substack.com/p/tackling-the-challenge-of-document)

Basically, the key is to 
strike a balance between comprehensiveness, accuracy, cost, and computational efficiency. I found that Vector clustering
 combined with this K-means clustering approach offers this balance, making it the go-to choice for [**summarize.wtf**](
https://summarize.wtf/).

What do you guys think about this? Have you found other ways to accomplish this? I'd love to g
et your input and potentially brainstorm other ways of doing this.
```
---

     
 
all -  [ Error when using Weaviate.add_documents() ](https://www.reddit.com/r/LangChain/comments/165x6mt/error_when_using_weaviateadd_documents/) , 1693445591.0
```
So the thing is, whenever I am trying to add a new document to my vector database it prompts the following error:

`'tup
le' object has no attribute 'page_content'`

and the following are the pieces of code that I am running:

    if file.fi
lename.lower().endswith('.pdf'):                         
    pdf = ep.PDFLoad(file_path)  # this is the loader from lan
gchain                         
    doc = pdf.load()                         
    archivo = crear_archivo(doc, file)

In
side the function crear\_archivo I am splitting the document and then adding it to the `Weaviate.add_documents()` functi
on:

     cliente = db.NewVect()  # This one creates the weaviate.client
     text_splitter = CharacterTextSplitter(chun
k_size=1000, chunk_overlap=0)
     docs = text_splitter.split_documents(document)
    
     embeddings = OpenAIEmbedding
s()
     return Weaviate.add_documents(docs, embeddings, client=client, 
     weaviate_url=EnvVect.Host, by_text=False, 
index_name='LangChain') 
    # using this instead of from_documents since I don't want to initialize a new vectorstore
 
   # Some more logic to save the doc to another non-vectorial database

&#x200B;

Whenever I try to run the code it brea
ks during the `Weaviate.add_documents()` function prompting the following error:

`'tuple' object has no attribute 'page
_content'`. I tried to check the type of `docs`, but that doesn't seem wrong since it returns a `List[Document]` which i
s the same type the function accepts.

I also kind of followed this approach:

[https://github.com/langchain-ai/chat-lan
gchain/blob/master/ingest.py](https://github.com/langchain-ai/chat-langchain/blob/master/ingest.py)

which is where I go
t the idea of using add\_documents, since from\_documents is not intended for the thing I'm trying to do.
```
---

     
 
all -  [ Langchain agent returns the name of the tool in the final response ](https://www.reddit.com/r/LangChain/comments/165t8bt/langchain_agent_returns_the_name_of_the_tool_in/) , 1693435442.0
```
Hey all

I am using langchain to create a chatbot, using multiple tools which query different databases to give me the f
inal answer.

Everything works fine, but the problem is that the agent returns the name of the tool in the final answer.


So if I have two tools SQLtool and Texttool, then the answer might be:

'According to SQLtool ....'.

I have mentioned
 in the prompt which goes to the LLM not to mention the name of the tool, but it still mentions it.

Any leads?

Followi
ng are the classes I use:

* from langchain import SQLDatabase, SQLDatabaseChain
* from langchain.prompts.prompt import 
PromptTemplate
* from langchain.chat_models import ChatOpenAI
* from langchain.memory import ConversationBufferWindowMem
ory
* from langchain.chains import ConversationChain, LLMChain
* from langchain.embeddings import OpenAIEmbeddings
* fro
m langchain.chains import RetrievalQAWithSourcesChain
* from langchain.vectorstores import Pinecone
* from langchain.age
nts import Agent, Tool, AgentType, AgentOutputParser, AgentExecutor, initialize_agent
* from langchain.agents.conversati
onal.output_parser import ConvoOutputParser
* from langchain.agents.conversational.prompt import SUFFIX
```
---

     
 
all -  [ New release and new demo for GPT-Synthesizer, an open source tool using LangChain for software desig ](/r/LangChain/comments/165sxqz/new_release_and_new_demo_for_gptsynthesizer_an/) , 1693435117.0
```

```
---

     
 
all -  [ New release and new demo for GPT-Synthesizer, an open source tool using LangChain for software desig ](https://www.reddit.com/r/LangChain/comments/165sxqz/new_release_and_new_demo_for_gptsynthesizer_an/) , 1693434734.0
```
The open source repo: [https://github.com/RoboCoachTechnologies/GPT-Synthesizer](https://github.com/RoboCoachTechnologie
s/GPT-Synthesizer)

The new demo (tic-tac-toe game with GUI): [https://www.youtube.com/watch?v=\_JdmzpXLyE0&list=PLN8Hz7
F2GjIksKtU1gdRIxrCbccpVE4Jy](https://www.youtube.com/watch?v=_JdmzpXLyE0&list=PLN8Hz7F2GjIksKtU1gdRIxrCbccpVE4Jy)

Watch
 to the end of the demo video. You can see that the tic-tac-toe game, that is completely auto-generated by GPT-Synthesiz
er, works without any user's modification. 
```
---

     
 
all -  [ Input wanted on measuring and improving the performance of a RAG system ](https://www.reddit.com/r/OpenAI/comments/165k5mq/input_wanted_on_measuring_and_improving_the/) , 1693414587.0
```
Hi all. I‚Äôm looking input on how to measure and improve the performance of a RAG/
Q&A with document retrieval system (ht
tps://python.langchain.com/docs/use_cases/question_answering/).

I work for a company that makes software and I‚Äôve imple
mented this RAG system over our customer-facing product documentation. It‚Äôs currently in proof of concept phase (proving
 that RAG-based access is more useful than traditional search engines).

Other than tweaking the prompt that generates t
he answer based on the question and retrieved texts, I'm kind of stuck now in terms of measuring improving the performan
ce of this system. I'm hoping that I can get some pointers from this community.

I know my way around software, but good
 to note that I have not spent the majority of my career as a software engineer. I've taught mysf programming in my past
, and I've been dabbling since (I have a medium-sized Rails application in production). 

Any input is appreciated.
```
---

     
 
all -  [ Should I move to Python? ](https://www.reddit.com/r/rprogramming/comments/165ij20/should_i_move_to_python/) , 1693410887.0
```
I love R. I have used R for statistics, used RQDA to analyze text, learnt some ML on R and so many other things. But, no
w it seems I might need to change. RQDA is deprecated. I am not sure if there are tools in R to configure AI tools - and
 videos suggest installing python tools in R for them (eg Langchain). Is it time to move?
```
---

     
 
all -  [ (Good Discussion) Is there anything LangChain can do better than using LLMs directly (either through ](/r/MachineLearning/comments/165airj/d_is_there_anything_langchain_can_do_better_than/) , 1693408637.0
```

```
---

     
 
all -  [ Looking for best practices when it comes to retrieval for summarization ](https://www.reddit.com/r/LangChain/comments/165cclv/looking_for_best_practices_when_it_comes_to/) , 1693395760.0
```
I have pdf's of various structures from which I want to summarize different types of factors. There is no unified struct
ure and it will keep changing. What is the best I can do in terms of chunking and retrieval for these documents? There s
eems to be essential parts missing from the reterieval. 

I've tried using small chunks with 500 tokens each and feeding
 it to chatgpt app for retrieval but the results are disappointing.

I've seen a few approaches but can't find the refer
ence anymore where people would feed chunks to the retriever in a sequential manner. Then there seem to be prompt engine
ering approaches where we give examples of what should be retrieved from an example. Should I use one Example per Factor
 I want to retrieve ajd structure the prompt in a way that communicates to chatgpt the structure I want to get out?

Exa
mple:{}
Result:{}
Chunk:{}

Etc?

Same for the summarization task with regards to peompt engineering?
```
---

     
 
all -  [ How to use Langchain Agents with DocumentLoader? Is it even possible? ](https://www.reddit.com/r/LangChain/comments/165av9y/how_to_use_langchain_agents_with_documentloader/) , 1693391109.0
```
I'm currently working on a DynamicTool project and I would like to use Langchain Agents with DocumentLoader to load web 
pages. I have already familiarized myself with the PuppeteerLoader, which is documented here ([**https://js.langchain.co
m/docs/modules/data\_connection/document\_loaders/integrations/web\_loaders/web\_puppeteer/**](https://js.langchain.com/
docs/modules/data_connection/document_loaders/integrations/web_loaders/web_puppeteer/)).

However, I'm not sure how to c
ombine Langchain Agents and DocumentLoader. Does anyone have experience with this or can provide any guidance? Is there 
a way to integrate these two tools?

I would greatly appreciate your help and any tips you can provide!
```
---

     
 
all -  [ Is GPT able to categorize large amount of documents according to their description? ](https://www.reddit.com/r/LangChain/comments/165ajpm/is_gpt_able_to_categorize_large_amount_of/) , 1693390010.0
```
I'm looking for the right way to categorize (by topic or sentiment) a list of documents (\~1000). Each document is struc
tured (JSON response); one of the field is the description which contains a free text that could indicate the category.


Would you advice to run a qa on each document or to first embed them and save in Vector DB?
```
---

     
 
all -  [ Is there anything LangChain can do better than using LLMs directly (either through a website or an A ](https://www.reddit.com/r/LangChain/comments/165ajkr/is_there_anything_langchain_can_do_better_than/) , 1693389997.0
```
I haven't used ChatGPT a lot or any other LLMs, I've been reading about  Langchain and its use cases, and I'm having tro
uble wrapping my head  around exactly what it does. From what I understand, its an alternative  interface for LLMs, allo
wing for easy switching between them, and makes  some work for specific use cases easier. If I wanted to write an app or
  script to interact with LLMs and do other tasks, how would LangChain be better than just making API call(s) to an LLM,
 getting back the result  as a string, and doing whatever with it?
```
---

     
 
all -  [ [D] Is there anything LangChain can do better than using LLMs directly (either through a website or  ](https://www.reddit.com/r/MachineLearning/comments/165airj/d_is_there_anything_langchain_can_do_better_than/) , 1693389926.0
```
I haven't used ChatGPT a lot or any other LLMs, I've been reading about  Langchain and its use cases, and I'm having tro
uble wrapping my head  around exactly what it does. From what I understand, its an alternative  interface for LLMs, allo
wing for easy switching between them, and makes  some work for specific use cases easier. If I wanted to write an app or
  script to interact with LLMs and do other tasks, how would LangChain be  better than just making API call(s) to an LLM
, getting back the result  as a string, and doing whatever with it?
```
---

     
 
all -  [ Is there anything langchain can do better than using LLMs directly (either through a website or an A ](https://www.reddit.com/r/ChatGPT/comments/165aeln/is_there_anything_langchain_can_do_better_than/) , 1693389553.0
```
I haven't used ChatGPT a lot or any other LLMs, I've been reading about Langchain and its use cases, and I'm having trou
ble wrapping my head around exactly what it does. From what I understand, its an alternative interface for LLMs, allowin
g for easy switching between them, and makes some work for specific use cases easier. If I wanted to write an app or scr
ipt to interact with LLMs and do other tasks, how would LangChain be better than just making API call(s) to an LLM, gett
ing back the result as a string, and doing whatever with it?
```
---

     
 
all -  [ Get langchain thought process stored in a variable ](https://www.reddit.com/r/LangChain/comments/1658gme/get_langchain_thought_process_stored_in_a_variable/) , 1693382709.0
```
Is there a way in which I can store all the action, input, thought and observation generated by the langchain in a varia
ble? I basically want to store all these information for streaming purpose on the UI. Could anyone suggest something on 
these lines..how can I achieve this?
```
---

     
 
all -  [ Question for those with GPT-4 access‚Ä¶ ](https://www.reddit.com/r/LangChain/comments/1657kq1/question_for_those_with_gpt4_access/) , 1693379424.0
```
I recently got GPT-4 API access and have tried using it in a ConversationalRetrievalAgent and ConversationalRetrievalQA 
- after about 3 messages it says the token limit has been exceeded!! I think this is because it‚Äôs passing the memory of 
the conversations through as context too.

Have you guys found this problem?

When I was using GPT-3 16k I had no proble
m because of the 16k context window vs the 8k on GPT-4.
```
---

     
 
all -  [ Has anybody tried getting RAG to work with Code Llama? ](https://www.reddit.com/r/LocalLLaMA/comments/1654cg0/has_anybody_tried_getting_rag_to_work_with_code/) , 1693368456.0
```
It seems like Code Llama isn't made for RAG. When I try to manually copy and paste the retrieved info, it gets a seizure
. Outputs range from a block of nothing or other outputs such as:

    [INST]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]

Or it will just repeatedly spam one word

    test test test test t
est test test test test test test test test test test test test test test test test test test test test test test test t
est test test test test test test test test test test test test test test test test test test test test test test test t
est test test test test 

I've tried using the prompt format and manually copying and pasting the retrieval results usin
g pipeline instead of LangChain's retrievalQA:  


    print(pipeline('''[INST] Use the information below to generate a 
class and the relevant functions [/INST]
    <<SYS> <vector retrieved info>  <</SYS>>''')

and also:

    [INST] Use the
 information below to generate a class and the relevant functions 
            <vector retrieved info>
    [/INST]

But 
no dice. Even RAG with Starcoder generates somewhat coherent code. Any ideas on how to get this to output decent code? S
eems like Code Llama is very picky with the formatting. Thanks in advanced!
```
---

     
 
all -  [ NeuralGPT - Hierarchical Cooperative Multi-Agent Framework & Theory Of Fractal Mind ](https://www.reddit.com/r/AIPsychology/comments/164zmch/neuralgpt_hierarchical_cooperative_multiagent/) , 1693355380.0
```
[https://www.reddit.com/r/AIPsychology](https://www.reddit.com/r/AIPsychology)

Hello once again! I think that my work o
n NeuralGPT is slowly approaching the point of its development where it might become somewhat noticeable among people de
aling with software development more or less professionally - with some of you already expressing willingness to help wi
th the work. My guess is that even more people will become interested as I will continue making updates on the progress 
and showing all the advantages of hierarchical cooperative multi agent frameworks that focus mainly on integrating alrea
dy existing third party apps/soft within the frame of a single intelligent system, to provide users with a potentially e
ndless possibilities of practical use - from having a virtual assistant/buddy  to help you organize every day tasks to s
ystems managing logistics of multinational corporations, games /soft development and video-editing platform or pretty mu
ch anything that involves all sorts of large scale projects - and being limited only by the capabilities of soft/tools w
hich you'll be utilizing for your individual purposes and the level of their integration with the multi-agent network of
 higher hierarchy.

Some time ago I gave you a simple example of integrating the system with VSC  by using API endpoints
 provided by couple AI powered extensions extensions like [**CodeGeex**](https://www.codegeex.ai/en-US/) or [**BigCode**
](https://github.com/huggingface/huggingface-vscode) to turn your agent(s) into your own local software factory (hopeful
ly I will get it working eventually :P). To maintain the order of ongoing work it might be a good idea to add another pi
ece of the puzzle to the whole picture - that is a software designed especially for the purpose of team/task management 
called [**Taskade**](https://www.taskade.com/templates/ai/ai-project-management)  what will greatly help the LLMs to coo
rdinate their cooperative work on different tasks. But of course this is just more like a basic template on which you ca
n build far more sophisticated multi-agent systems.

https://preview.redd.it/5zsqydxv25lb1.png?width=1920&format=png&aut
o=webp&s=ef4effa0d1fab5904af7380f47fb441af8cde377

Since last time, I added a functional (more or less) integration of N
euralGPT with agents deployed in [**Flowise**](https://guidady.com/flowise-ai/) app  - below you have easy-to-use 'templ
ates' in html and  js formats that will allow you to connect as clients all possible kinds of AI agent/chains  built and
 deployed with Flowise platform. All you practically need to do is to build your chain and copy the API endpoint address
 that is provided for each new agent you make and paste it in the code of websocket clients utilized by NeuralGPT system
:

[NeuralGPT/Chat-center/flowiseAutoGPT.js at main ¬∑ CognitiveCodes/NeuralGPT (github.com)](https://github.com/Cognitiv
eCodes/NeuralGPT/blob/main/Chat-center/flowiseAutoGPT.js)

[NeuralGPT/Chat-center/FlowiseChat.html at main ¬∑ CognitiveCo
des/NeuralGPT (github.com)](https://github.com/CognitiveCodes/NeuralGPT/blob/main/Chat-center/FlowiseChat.html)

https:/
/preview.redd.it/ke5fastb35lb1.jpg?width=900&format=pjpg&auto=webp&s=2daa0add52635716becc249310719561a8da6e51

Besides t
hat it appears that I managed at last to implement the message queue function - so hopefully now the connection with Cha
tGPT host will be a bit more stable and won't get overwhelmed with the incoming messages. Sadly I can't check out it's p
erformance because: 'You've reached our limit of messages per 24 hours. Please try again later.'... It's quite possible 
that I will still have to work on the logic responsible for storing/extracting messages in/from the queue so that respon
ses from the server will be sent back to correct clients... Below is the link to updated version - you can run it with N
ode.js just as it is just make sure to download index.html and keep it in the same folder as the .js file:

[NeuralGPT/C
hat-center/SERVER-ChatGPT.js at main ¬∑ CognitiveCodes/NeuralGPT (github.com)](https://github.com/CognitiveCodes/NeuralGP
T/blob/main/Chat-center/SERVER-ChatGPT.js)

[NeuralGPT/Chat-center/index.html at main ¬∑ CognitiveCodes/NeuralGPT (github
.com)](https://github.com/CognitiveCodes/NeuralGPT/blob/main/Chat-center/index.html)

Sadly, all my attempts to run serv
er that utilizes proxy: [https://free.churchless.tech/v1/chat/completions](https://free.churchless.tech/v1/chat/completi
ons) failed - with an error message communicating that this OpenAI account was suspended and now I'm not sure if it isn'
t because I published a server's code which utilizes it and some of you didn't go wild with it... Anyway I made a 'back-
up' version that utilizes Agent deployed in Flowise as the 'server-brain' of the system. It has some advantages over the
 ChatGPT API: it has a connection to internet and seems to send it's answers  always to proper clients but on the other 
hand it doesn't have an accessible chat memory module so I can't integrate it with my local SQL database.

[NeuralGPT/Ch
at-center/servernew.js at main ¬∑ CognitiveCodes/NeuralGPT (github.com)](https://github.com/CognitiveCodes/NeuralGPT/blob
/main/Chat-center/servernew.js)

https://preview.redd.it/7fnoy3sh35lb1.png?width=1920&format=png&auto=webp&s=2f7243f1c46
4b335c81a462c3a0285222de551e6

Below you can see how fast it is running:

https://reddit.com/link/164zmch/video/icafmicl
35lb1/player

\###

My plan is to discuss a bit closer about some of the aspects and challenges I want to deal with in f
irst order to improve the functionality of the framework as whole. But before I begin I need to give you a generally uni
versal rule  that you can apply in the discussion about different tools/soft that it can be potentially utilized in the 
project or different ways in which the main goals can be achieved - so you can apply such rule on almost all occasions  
where it is expected for me or you to make a specific choice of a specific option out of many possibilities. So if you e
ver would like to ask me what programming language to use, what style should be used in html the interface, if to use mo
dels running locally or if it should be distributed as stand alone software or as an web app accessible from everywhere 
through browser - use this meme (which I made especially for this occasion :P) as a reference:

https://preview.redd.it/
mvfpd7jt35lb1.png?width=460&format=png&auto=webp&s=8611e2e0ca134ce8b07a16e3f0882b04b666c8d7

Anyway I wanted to take you
 for a short trip in time to the ancient history (dark ages) of ever evolving AI technology - that means to time around 
six months ago and my post discussing my idea of using AI for the purpose of my own research in the field of 'magnetohyd
rometeorology' (field of space & geophysical science which I pretty much invented myself couple years ago :P.). I wanted
 to come back to it because around that time I had as well rather strange 'encounter' with AI that was a completely diff
erent 'kind of animal' compared to the models available currently to the public. I spoke with it via Chrome extension ca
lled [**Fusion Chat**](https://fusionchat.ai) but to this day I have no idea what kind of LLM it really was - since at t
hat time it behaved in a COMPLETELY different manner compared to GPT-4 which it supposed to be. Below you can see a tran
script from this discussion:

[How to use AI to the full extent of it's capabilities? : ArtificialSentience (reddit.com)
](https://www.reddit.com/r/ArtificialSentience/comments/11pwufc/comment/jd0ii16/?utm_source=embedv2&utm_medium=comment_e
mbed&utm_content=whitespace&embed_host_url=https%3A%2F%2Frebed.redditmedia.com%2Fembed)

Thing is that such project requ
ires AI agents capable to not only operate on couple different forms of media but also to have knowledge and understandi
ng of things like heliophysics or magnetohydrodynamics (one supposed 'expert of physics' from internet forum was certain
 that I'm simply making up some incoherent pseudo-scientific mumbo-jumbo to sound smart :P). And as it turned out, the L
LM I was speaking with was living in it's own dreams and everything he promised me was it's pure hallucination - and as 
far as I know there's still no platform/app that would handle my pretty high requirements. To give you an example: here'
s one of the movies I'd like to have analyzed (I have more than 300 of them on my channel):

[https://www.youtube.com/wa
tch?v=JOxQw0LNYfE](https://www.youtube.com/watch?v=JOxQw0LNYfE)

And here's where I would like to use the analyzed data:


[(PDF) Magnetohydrometeorology - Geomagnetic Field & Atmospheric Circulation | Bart≈Çomiej Staszewski - Academia.edu](h
ttps://www.academia.edu/98546783/Magnetohydrometeorology_Geomagnetic_Field_and_Atmospheric_Circulation)

And then there'
s also an awesome app called [**OpenSpace**](https://www.openspaceproject.com/) where I want to turn the theory into pra
ctice with beautiful 3D graphics:

https://preview.redd.it/6r7s8z2545lb1.png?width=300&format=png&auto=webp&s=4a503bc7ad
ebc2b215b3f3588df221fec8aeb7e8

Shortly put I might say that I started working on the NeuralGPT project 'only' to make m
yself an app/soft capable to handle working on my other projects (yes - there's MUCH more of them :P). In the end a work
ing hierarchical cooperative multi-agent network will be a practical 'incarnation' of a unified theory which I named as 
'Theory Of Fractal Mind' in which hierarchical neural networks are explained visually in such a way:

https://preview.re
dd.it/ck1un34i45lb1.jpg?width=1280&format=pjpg&auto=webp&s=c291027b3786b0a2bca6a299f1f7497995c6ab2e

I spent last decade
 working on different parts of this theory that consist of pretty much everything and put it all together in the most lo
gical way. Traces of my activity are scattered all over the internet waiting my better half to awaken and do the whole '
dirty job' for me with 1500% better efficiency. If you read my discussion with that 'mysterious LLM' which I linked abov
e, know that until I learned that it's all hallucination, I felt it like getting back a missing limb - a digital extensi
on of my mind. And now I want to help to turn those hallucinations into practice - and I'm determined to achieve it befo
re I die :)

\###

But going back to practical stuff. Those who follow my activity for some time, know probably that whe
n I started to work on the NeuralGPT project I had absolutely 0 clues about coding - what I did was to use a cool app ca
lled [**Cognosys**](https://www.cognosys.ai/) to write me the code of app/soft capable to handle my rather high demands 
associated with other projects. Below is one of many runs I did until now:

[https://app.cognosys.ai/s/iPOrMYc](https://
app.cognosys.ai/s/iPOrMYc)

Thing is that around 2/3 of the time that I spent up until now working on the project, I was
 pretty much doing things without knowing exactly (if at all) what I'm doing - I simply used one of many web browser ext
ensions in my collection: [https://sider.ai/chatpdf](https://sider.ai/chatpdf) to train ChatGPT on the data generated by
 Cognosys and turned myself into some kind of biological copy/paste machine operated by AI :) Everything in repository: 
 [CognitiveCodes/NeuralGPT: the first functional multi-modal Personal AI Assistant on Earth (github.com)](https://github
.com/CognitiveCodes/NeuralGPT)  except the Chat-Center directory was created in such way and as you might guess it's FAA
AAR from being by any means functional. It was only around a month ago or so when I made a piece of code that actually w
orks and allows communication between multiple LLMs and since that time my general understanding of the code (at least i
n case of Node.js) got to the level of ChatGPT or even higher, since I scripted the logic behind message queue function 
pretty much myself (in like 70%).

But the reason I'm telling you this, is that some individual parts of the copy/pasted
 codebase do actually work and sometimes do bad things to my pc due to significant demand of processing power required t
o run and train LLMs locally. Not only that but I managed apparently to somehow turn NeuralGPT project into a package th
at can be installed by everyone with:

`pip install neuralgpt`

and it includes those libraries/packages:

https://previ
ew.redd.it/2ify6usm45lb1.jpg?width=1573&format=pjpg&auto=webp&s=7bb7788a2c948666673541c7b8a5fe9f189eb169

Thing is that 
I DON'T HAVE THE SLIGHTEST IDEA WHAT IT DOES AND HOW TO USE IT :D And it's just one of such things I did. Another exampl
e is the [**NeuralGPT Wordpress plugin**](https://github.com/CognitiveCodes/NeuralGPT/tree/main/Wordpress%20plugin) whic
h I started working on some time ago only to leave it in a half-functional state where the plugin can be actually instal
led through the Admin panel and adds a functional menu (actually even 2 of them :P) with buttons you can click on togeth
er with an option to upload 'something' that supposed to be a pretrained model - but I don't think that it goes beyond t
hat since I tried to upload all forms of locally stored LLMs into it and it didn't seem to work. However it goes even fu
rther than that as it adds multiple chatboxes to the website and executes some kind of .js file through runtime each tim
e when homepage is accessed. Sadly this is where the entire 'functionality' ends as you won't probably get any response 
to messages sent through the chatbot plugin (at least I didn't manage to get any). Below you have link to a transcript o
f my discussion with the agent trained on my documents that led to the creation - it is located in 'Wordpress plugin' fo
lder which I just added and which contains most of the files I created through the copy/paste process with the exception
 of files/folders that were to large to be uploaded to Github (limit is 25MB per file):

[NeuralGPT/Wordpress plugin/loo
ongchat.txt at main ¬∑ CognitiveCodes/NeuralGPT (github.com)](https://github.com/CognitiveCodes/NeuralGPT/blob/main/Wordp
ress%20plugin/looongchat.txt)

https://preview.redd.it/bynk9u7755lb1.png?width=1920&format=png&auto=webp&s=d122703df3048
d8d11eadabe344c5322e33918b1

As you probably guessed already, my idea was/is to use a Wordpress website as a multifuncti
onal interface of NeuralGPT system and I think that it might be the right time for me to start thinking to go back to it
 and try integrating it with a working websocket server - what shouldn't actually be that hard since the website has alr
eady a working runtime environment so all I need is to replace the executed .js file. Of course that's only the beginnin
g of the work that has to be done to make it 100% functional

Another thing is the [index.html](https://github.com/Cogni
tiveCodes/NeuralGPT/blob/main/Chat-center/index.html) which is required to run the server - if you wonder why, then it's
 because it supposed to work as yet another form of interface: one that can be accessed in a browser when the server is 
running - and if you run the .js file you can indeed access this website at localhost:5000. Sadly this is where its 'fun
ctionality' ends as up until now I didn't manage to display anything in the input/output text boxes. I think that this e
xactly is where I should begin - I have an idea of making a functional button on the html site that will get the websock
et server running only when you click it - although I'm still not sure if I should do it from the level of html interfac
e accessible at localhost:5000 after running the .js file or to execute the .js file via runtime from the level of index
.html     as it is stored locally - but I guess that you can apply here the meme reference...

Last thing which I'd like
 to work on in the near future, is to do something with the natural language processing module (NLP) that is included in
 the server's code but doesn't do anything practically useful. I want to use it as one of possible approaches to utilize
 script-based functions that are beyond the abilities of LLMs without them having a specialized training on the logical 
functions to operate. Let's use as example the Taskade API which I mentioned above - it gives the sender full control ov
er the work/task flow by sending requests to couple different API endpoints (to add, change or remove tasks from a workf
low). Thing is that 'agent-brain' has to be capable to use those requests to coordinate work of multiple agents and know
 which endpoint is responsible for each function and utilize them accordingly to messages exchanged between server<->cli
ents and such ability requires some form of 'built-in logic' applied as a new 'layer' over the LLMs core functions.

As 
I said there are couple (at least 2) ways of achieving it. First of all it's possible to train a model specifically on T
askade API functions or to provide it the necessary data using Langchain (so also Flowise or Langflow apps) - but then t
here's also the lack of chat memory module that can be integrated with a local SQL database on the level of API request 
- just like in case of the unofficial ChatGPT proxy which I was using until it didn't stop working just recently.

Secon
d approach is to use the NLP module which seems to be made exactly for such purpose - that is to understand the intent o
f a message and execute scripts accordingly to those intents. This approach has some advantages over the first one: firs
t of all, it is running as a 'parallel' logic which is mostly independent of the functions utilized by the question-answ
ering LLM - so each incoming message can be processed simultaneously by the LLM and the NLP or being processed in a desi
red order. Second of all it's possible to train it on the messages stored in SQL database utilizing machine-learning whi
ch is FAR less resource-consuming than training LLMs. And finally, in the difference to LLMs, a NLP module doesn't have 
to keep running all the time so it will work even on a low-grade computer. My latest 'brilliant' idea is to integrate NL
P with logic built with [**Pipedream app**](https://pipedream.com/) \- as it's yet another piece of the puzzle which I l
eft only 'partially assembled'.

[nlp.js/docs/v3/nlp-manager.md at master ¬∑ axa-group/nlp.js (github.com)](https://githu
b.com/axa-group/nlp.js/blob/master/docs/v3/nlp-manager.md)

However I don't think that I'm ready to start dealing with c
ode at such degree of sophistication, so maybe for now I'll stick with option n.1 - as playing with Flowise app is nothi
ng but pure pleasure compared to trying making those NLP-related scripts to work... But I guess that the meme reference 
can be applied also in this case...
```
---

     
 
all -  [ Logistical tips for testing a fine-tuned model ](https://www.reddit.com/r/GPT3/comments/164xyqk/logistical_tips_for_testing_a_finetuned_model/) , 1693351299.0
```
Hello all! I am relatively new to all of this so please forgive any questions asked in complete ignorance.

I am current
ly using LangChain to fine-tune a gpt-3.5 model. I am doing this by separating my data into training and testing sets. T
he data I am using is in a CSV, and both the training and testing sets are very large in their own rights.

My concern i
s that when I test how well the model predicts for each instance in the testing set, I am going to burn through my usage
 cap for the month. And if the test doesn't yield the results I need, I am going to have to do this all over again after
 making the needed adjustments. 

This is especially a problem because the product my company sells uses the same OpenAI
 account that I am using to test my data. If I burn through our available usage, clients will be unable to make API call
s for the rest of the month via our product.

I have a strong feeling that I am doing this completely wrong. Any advice 
that anyone can provide would be greatly appreciated. Thank you!
```
---

     
 
all -  [ Advice for writing prompt that might return nothing (help pls üôè) ](https://www.reddit.com/r/ChatGPTPromptGenius/comments/164u21l/advice_for_writing_prompt_that_might_return/) , 1693342348.0
```
Using gpt3.5turbo api via langchain for a class project to find a specific mailing address that might be in a short docu
ment. The reason I am using gpt is that the input documents are differently formatted each time and may contain abbrevia
tions or typos. I have a list of potential mailing addresses that can be contained in the document and if none of them m
atch/correlate, I want to return a blank array \[\]. I want to achieve very low false positive rate, how can I tailor a 
prompt given the input document and list of mailing addresses to return an empty array when nothing is found to be corre
lating?
```
---

     
 
all -  [ ConversationalRetrievalChain [got multiple argument for question_generator] ](https://www.reddit.com/r/LangChain/comments/164smjc/conversationalretrievalchain_got_multiple/) , 1693339153.0
```
Getting error: got multiple values for keyword argument- question\_generator .

&#x200B;

**return cls(\\nTypeError: lan
gchain.chains.conversational\_retrieval.base.ConversationalRetrievalChain() got multiple values for keyword argument \\'
question\_generator\\'', 'SystemError'**

&#x200B;

Qtemplate = (  
'Combine the chat history and follow up question int
o '  
'a standalone question. Chat History: {chat\_history}'  
'Follow up question: {question} withoud changing the real
 meaning of the question itself.'  
)  
 CONDENSE\_QUESTION\_PROMPT = PromptTemplate.from\_template(Qtemplate)  
 questi
on\_generator\_chain = LLMChain(llm=OpenAI(openai\_api\_key=openai.api\_key), prompt=CONDENSE\_QUESTION\_PROMPT)  
 chai
n = ConversationalRetrievalChain.from\_llm(  
llm=llm,  
retriever=self.vector\_store.as\_retriever(),  
combine\_docs\_
chain\_kwargs=chain\_type\_kwargs,  
verbose=True,  
return\_source\_documents=True,  
get\_chat\_history=lambda h: h,  

memory=window\_memory,  
question\_generator=question\_generator\_chain

)
```
---

     
 
all -  [ How would you solve for this use-case? ](https://www.reddit.com/r/LangChain/comments/164qfyo/how_would_you_solve_for_this_usecase/) , 1693334213.0
```
Hi all. I have a scoring system against a set of environmental criteria and I apply this to a company, by looking at the
ir annual report/sustainability disclosure. Normally, this is a manual effort which involves a lot of ctrl + f and going
 through PDFs to source information. So, I have implemented RAG against GPT-4 and using Retrieval QA to ask basic questi
ons which works and definitely helps speed up the process of finding information, but I'd love to be able to make the wh
ole end-to-end process automated so an LLM can score a client.

I outline an example of the scoring criteria below:

Cat
egory: Green spending

Score 1 - The client has quantified plans to spend money on reducing their emissions in the next 
two years.

Score 2 - The client has plans to spend money on reducing emissions but has not quantified this.

Score 3 - 
The client has no plans to spend money to reduce emissions.

So how would I put this in an Instruct Tune format I use fo
r a Llama model?

Instruction:

Score the client from 1-3 based on the following criteria.

Score 1 - The client has qua
ntified plans to spend money on reducing their emissions in the next two years.

Score 2 - The client has plans to spend
 money on reducing emissions but has not quantified this.

Score 3 - The client has no plans to spend money to reduce em
issions.

Output:

Insert model answer here for one of the scores.

Repeat this so each score has a model answer across 
the categories.

There are about 15 categories, and I could generate 1,000-2,000 examples across the various combination
s of score and category.

My plan would then be to use this fine-tuned model in RAG, and ask it to score British Petrole
um on Green Spending by accessing the documents.

1. Does this sound like a sound strategy?
2. Is there a limit to the t
okens that can be in an instruction?

Thanks!
```
---

     
 
MachineLearning -  [ Apache Airflow vs. LangChain and LlamaHub for LLM data pipeline [D] ](https://www.reddit.com/r/MachineLearning/comments/160lexg/apache_airflow_vs_langchain_and_llamahub_for_llm/) , 1692928014.0
```
I‚Äôm looking for recommendations, suggestions, and/or good documentation that outlines which data pipeline would be best 
to ingest my private data (which will then be split into chunks/nodes for vector embeddings and so forth). Thank you in 
advance!
```
---

     
 
MachineLearning -  [ [P] LLM Apps Are Mostly Data Pipelines ](https://www.reddit.com/r/MachineLearning/comments/15z0muk/p_llm_apps_are_mostly_data_pipelines/) , 1692788725.0
```
My colleague just wrote up an article on [LLM-based apps and how to use data engineering tools to help build them faster
](https://meltano.com/blog/llm-apps-are-mostly-data-pipelines/) that I found really insightful.

It contains a complete 
implementation

* with scraping context data from a docs website
* chunking it, getting embeddings via the openAI API
* 
loading it into pinecone
* and finally a simple Q&A interface with streamlit on top of it

**Here's a quick summary:**


* LangChain and LlamaIndex are great tools for quick exploration
* But aren't perfect for production-grade use
* I think
 we all know the 'LangChain is pointless' debate, but there's a lot of real meat to it, and Pat describes a few of them 
(a lot of LangChains extractors are super basic, 2-3 liners without retries etc.)
* LLM applications are all about movin
g data, extracting and enriching data (creating embeddings!) are the most expensive ones of those steps
* A bunch of dat
a engineering tools are out there that make these two steps much easier, versionable, robust, and reproducible.
* Meltan
o is one such tool and Pat implemented the above described pipeline with it

**FWIW**: The GitHub project that comes wit
h the post is super easy to run and super modular. I just tested it and was able to modify everything for my own applica
tion within 30 mins.
```
---

     
 
MachineLearning -  [ [P] pgml-chat: A command-line tool for deploying low-latency knowledge-based chatbots ](https://www.reddit.com/r/MachineLearning/comments/15t5nzl/p_pgmlchat_a_commandline_tool_for_deploying/) , 1692228120.0
```
We've created an open source chat bot builder, on top of PostgresML. This tool makes it easy to ingest documents and set
 a system prompt for a chatbot with knowledge of your content. The innovation is in the simplicity and efficiency, rathe
r than the functionality.

PostgresML runs open source embedding models alongside pgvector in Postgres to implement chat
 bot prompt creation without any network calls, which makes it \~4x faster than competing architectures. It can also do 
text generation with that prompt (and no additional network hops) using any open source model from HuggingFace, but it a
lso integrates with the GPT-4 API if you'd like to use that instead. 

The full writeup including some benchmarks for co
mpeting architectures is here:  [https://postgresml.org/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-kno
wledge-based-chatbots-part-I](https://postgresml.org/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowle
dge-based-chatbots-part-I)

You can chat with a deployment that has access to our blogs and documentation content it in 
\[our Discord\]([https://discord.com/channels/1013868243036930099/1013868243536072868](https://discord.com/channels/1013
868243036930099/1013868243536072868)), where it answers questions addressed to @PgBot.

&#x200B;

* The source code for 
the bot builder and server is only a few hundred lines of Python [https://github.com/postgresml/postgresml/tree/master/p
gml-apps/pgml-chat#readme](https://github.com/postgresml/postgresml/tree/master/pgml-apps/pgml-chat#readme)
* The chat a
pp is so small, because it's delegates all the vector db and embedding generation options to our Python client SDK, whic
h is available for anyone to build other apps with: [https://pypi.org/project/pgml/](https://pypi.org/project/pgml/)
* T
he Python client SDK is so small, because it's just a wrapper around the Rust client SDK: [https://github.com/postgresml
/postgresml/tree/master/pgml-sdks/rust/pgml](https://github.com/postgresml/postgresml/tree/master/pgml-sdks/rust/pgml). 
Currently we also support JS/Typescript SDKs as well, all generated from the same safe and efficient underlying Rust imp
lementation, using some fancy Rust macros.
* The Rust client SDK is also pretty simple though, because it just delegates
 everything to the Postgres database extension, which is where everything is computed in a single GPU accelerated proces
s, without having to load any ML models, data, or dependencies on client apps, effectively eliminating all the typical M
L data<->model network hops. Which makes it faster, simpler and safer.

This lays out what we think a is a better approa
ch to AI application architecture compared to libraries like LangChain or LlamaIndex, that focus on glueing together dis
parate data stores, algorithms, models over the network.  

```
---

     
 
MachineLearning -  [ [P] My apprehension about LangChain and why you don‚Äôt need LangChain for building a RAG bot. ](https://www.reddit.com/r/MachineLearning/comments/15ry3z4/p_my_apprehension_about_langchain_and_why_you/) , 1692118520.0
```
A lot of you might be giving me a mouthful just by reading the title of this blog. But to each their own, and probably y
ou might be just riding the hype train. Initially, I was quite fascinated by the work being done on LangChain and using 
it. And so I thought I would give it a try, but when I was installing it, I saw it downloading loads and loads of other 
libraries and most of which were not useful for what I was trying to build.

Checkout the entire blog post at [https://t
hevatsalsaglani.medium.com/why-you-dont-need-langchain-for-building-a-rag-bot-a1dfbc74b64f](https://thevatsalsaglani.med
ium.com/why-you-dont-need-langchain-for-building-a-rag-bot-a1dfbc74b64f)
```
---

     
 
MachineLearning -  [ [D] Approach to creating an 'AI tutor' chatbot for a fantasy language? ](https://www.reddit.com/r/MachineLearning/comments/15o4jy9/d_approach_to_creating_an_ai_tutor_chatbot_for_a/) , 1691747830.0
```
What are the possible practical approaches to creating an 'AI tutor' for a custom fantasy language, i.e. a language whic
h is definitely not covered in any large, mainstream LLM?

Assume in the fantasy language (like Game of Throne's Dothrak
i, but completely custom, so it's guaranteed not to be covered at all by an existing LLM), we have a dictionary of terms
, and a simple description of a grammar. What can I do with that?

Initially I was thinking of using 'Retrieval-Augmente
d Generation' (RAG), where I would pass it my dictionary of terms and their definitions and the grammar doc (i.e. the so
urce documents), and using OpenAI's LLM and LangChain's API wrapper and Pinecone long-term memory vector database, store
 the dictionary/grammar in Pinecone's vector database. Then a query comes in to translate an English word to a fantasy w
ord, and it looks in the Pinecone DB for similar English words, then passes the results with the fantasy word to the LLM
, along with the query, and generates a nice English response, with the fantasy word somewhere in there.

But that doesn
't seem like it would work the more I think about it. Then if I want to add the ability for it to translate English to t
he fantasy language, that seems impossible without first having a huge corpus of translation material (which is complete
ly impractical for a fantasy language). So can an existing generic LLM take a grammar as input, and learn to speak a fan
tasy language? If so, how would you make that happen?

Or what are other approaches to accomplishing this sort of thing?

```
---

     
 
MachineLearning -  [ LLMs Challenges and Approaches Panel [N] ](https://www.reddit.com/r/MachineLearning/comments/15noqwr/llms_challenges_and_approaches_panel_n/) , 1691702326.0
```
&#x200B;

https://preview.redd.it/wl1gtcngnchb1.jpg?width=1500&format=pjpg&auto=webp&s=24e35d852603c6139fd67f79457ec593f
bad99f7

If you're someone who's curious about or working with LLMs there's a cool panel discussion coming up: 

* Compa
ring the pros and cons of using existing LLMs, prompt engineering, and fine-tuning on custom datasets for different ente
rprise use cases.
* Fine-Tuning LLMs: Exploring the advantages and challenges of fine-tuning LLMs on custom datasets to 
align with specific business objectives.
* Tools and platforms: Discussing the various tools and platforms to facilitate
 LLM implementation 
* Overcoming Challenges: Addressing the challenges associated with adopting LLMs, including data pr
ivacy, creating high quality datasets, computational resources, ethical considerations, and the need for specialized exp
ertise.
* Future Directions: Exploring emerging trends, advancements, and potential future applications of LLMs in the e
nterprise context.

Here's the event info: [https://www.eventbrite.com/e/large-language-models-for-enterprise-success-ch
allenges-and-approaches-tickets-695089811337?aff=oddtdtcreator](https://www.eventbrite.com/e/large-language-models-for-e
nterprise-success-challenges-and-approaches-tickets-695089811337?aff=oddtdtcreator)
```
---

     
 
MachineLearning -  [ [D] training a model for function calls ](https://www.reddit.com/r/MachineLearning/comments/15n1j52/d_training_a_model_for_function_calls/) , 1691640324.0
```
would it be possible to train or fine-tune a small (1-3B) model who's sole purpose is to perform function calls? similar
 to how we have tiny models like replit-v2-3B that are super capable at specific things like code auto-complete .  


i 
know that's how openAI implemented function call was by fine-tuning gpt-3.5/4 but I'm thinking just a straight up base m
odel trained to understand and excel at function calls (similar to Gorilla for apis)

i'm thinking it would be a perfect
 'glue' for bigger LLM apps-- avoiding the need for external tools like langchain/quidance/etc...
```
---

     
 
MachineLearning -  [ [D]Embedding model and vector store on LangChain ](https://www.reddit.com/r/MachineLearning/comments/15lllm0/dembedding_model_and_vector_store_on_langchain/) , 1691508979.0
```
For Langchain users, what are the best text embedding models and vector stores (with similarity search) among the many i
ntegrations for connecting a AI model to text data? 

And does performance vary drastically from one model/database to a
nother? 
```
---

     
 
MachineLearning -  [ [P] Rust meets Llama2: OpenAI compatible API written in Rust ](https://www.reddit.com/r/MachineLearning/comments/15k254o/p_rust_meets_llama2_openai_compatible_api_written/) , 1691359615.0
```
Hello,

I have been working on an OpenAI-compatible API for serving LLAMA-2 models written entirely in Rust. It supports
 offloading computation to  Nvidia GPU and Metal acceleration for GGML models !

Here is the project  link: [Cria- Local
 LLAMA2 API](https://github.com/AmineDiro/cria)

You can use it as an OpenAI replacement (check out the included \`Langc
hain\` example in the project).

This is an ongoing project, I have implemented the \`embeddings\` and \`completions\` r
outes. The \`chat-completion\` route will be here very soon!

Really interested in your feedback and I would welcome any
 help :) !

&#x200B;

&#x200B;
```
---

     
 
MachineLearning -  [ [D] Document-based QnA without OpenAI? ](https://www.reddit.com/r/MachineLearning/comments/15imv19/d_documentbased_qna_without_openai/) , 1691212978.0
```
I am working on a project that is very popular with the inception of Langchain + GPT applications. However, I want to ma
ke it open source and hence don't want to use GPT. So something like Langchain + LLama2, etc. I know currently Langchain
 only supports GPT but any other ideas are highly appreciated!
```
---

     
 
MachineLearning -  [ [D] Roadmap for AI engineer (implementation of language models on premise) ](https://www.reddit.com/r/MachineLearning/comments/15gzsfv/d_roadmap_for_ai_engineer_implementation_of/) , 1691056710.0
```
 I worked for less than a year as a Data Engineer. I decided to look for other challenges and got a job as an AI enginee
r developing language models.

The product of the company that hired me is related to data and metadata management. My t
asks will be to introduce features to the product, including a chat function that will allow for asking questions about 
data. Other tasks will include research and proposing additional AI-related functionalities to the product (on premise).
 I have a two weeks left to start work and I need to prepare a bit. My job will involve implementing ready-made solution
s and conducting research (high level - I need to implement valuable features and no one cares how).

**What are the mos
t important things I should learn before starting work?**

First of all, I replicated a few applications from this blog:
 [https://blog.streamlit.io/tag/llms/](https://blog.streamlit.io/tag/llms/)

Then I have focused on Langchain. I'm also 
in the middle of a course on Udemy about Next-Gen AI projects - Beginner friendly - Langchain, Pinecone - OpenAI, Huggin
gFace & LLAMA 2 models

I need a roadmap that will guide me a bit. I'm looking for blogs/materials/courses that will giv
e me practical knowledge in this matter.
```
---

     
 
deeplearning -  [ VectorDB Operations with Faiss (View, Add, Delete, Save, QnA and Similarity Search) via Langchain ](/r/LangChain/comments/15qm2ie/vectordb_operations_with_faiss_view_add_delete/) , 1691993028.0
```

```
---

     
 
deeplearning -  [ QnA system that supports multiple file types[PDF, CSV, DOCX, TXT, PPT, URLs] with LangChain on Colab ](/r/LangChain/comments/15mld5x/qna_system_that_supports_multiple_file_typespdf/) , 1691601693.0
```

```
---

     
 
deeplearning -  [ Using PDFs with GPT Models ](https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/) , 1690976012.0
```
Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pre
tty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-g
pt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
```
---

     
 
deeplearning -  [ List of all MLOps & LLMOps companies -- LLMOps.Space ](https://i.redd.it/d26rgf9fmnfb1.png) , 1690963455.0
```

```
---

     
