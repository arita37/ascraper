 
all -  [ RetrievalQA chain returning generated answer embedded in prompt even when return_only_outputs=True ](https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/) , 2024-04-10-0910
```
Hi all,

I am using RetrievalQA chain with custom prompt. When invoked with a question it is returning prompt with answe
r embedded in it even when the return_only_outputs is set to True.

I was wondering how can I get only the generated ans
wer without the prompt (System message + Context + Question)?
```
---

     
 
all -  [ How to deploy RAG Chatbot ](https://www.reddit.com/r/Chatbots/comments/1c06b8h/how_to_deploy_rag_chatbot/) , 2024-04-10-0910
```
Hi,  
I can create a chatbot using LangChain or other tools in a Jupyter notebook, and thus can create a \*.py file to d
efine and create the RAG Chatbot. I am having difficulty in figuring out how to deploy the Chatbot, which would have an 
API interface, and then making it available on my WIX website. Any ideas on how to deploy? For some reason I keep thinki
ng about Docker and Kubernetes, or NVidia, or AWS... What are your thoughts? Thanks.
```
---

     
 
all -  [ Need teammates for a RAG hackathon ](https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/) , 2024-04-10-0910
```
So guys there‚Äôs vectara‚Äôs upcoming hackathon,anybody interested to participate and needs a team.DM me‚Ä¶
```
---

     
 
all -  [ The Ultimate Guide To Vector Database Success In AI ](https://vectorize.io/what-is-a-vector-database/) , 2024-04-10-0910
```

```
---

     
 
all -  [ Filling in Null values in a dataset using LLM's ](https://www.reddit.com/r/LocalLLaMA/comments/1bzxw9e/filling_in_null_values_in_a_dataset_using_llms/) , 2024-04-10-0910
```
Hey guys , currently have a small text based dataset consisting of few columns all containing text and has  lot of null 
values .

I would love to fill in null values present in the dataset by synthetically generating the responses and then 
filling it . Tried using Langchain but it isn't able to generate consistently and half the time fails when i would want 
to generate more than 30 odd responses.

Been trying for the past few days. would love to receive help .

do let me know
 asap if there are any resources or links for doing this .feel free to suggest and dm.
```
---

     
 
all -  [ Need help with my chatbot application ](https://www.reddit.com/r/StreamlitOfficial/comments/1bztb1x/need_help_with_my_chatbot_application/) , 2024-04-10-0910
```
I believe it's really simple how to solve it but I don't know how. I'm not really proficient with programming in general
 and I'm stuck with this problem : I'm doing a chatbot on streamlit that use a vector store for its knowledge. But I tri
ed a couple of things, i can't understand how to make it works on streamlit. My problem is that whenever i write my seco
nd message into the chat, my first question (I use the streamlitchathistory from langchain) is being replaced by the thi
ng on the second screenshot.   


Can someone help me and provide me a simple solution ? I'm stuck with this for a few d
ays now.   


&#x200B;

[This is a snippet of my app](https://preview.redd.it/s91txa0gpgtc1.png?width=1031&format=png&au
to=webp&s=45fc9e1aba38cc6358928870f1b788e125c81467)

[Whenever i write my second question, the first one is transformed]
(https://preview.redd.it/nhksxhljpgtc1.png?width=713&format=png&auto=webp&s=570c3cb58a891e87698b47ff8fba3d011be2d055)
```
---

     
 
all -  [ Interacting with LLMs in steps  ](https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/) , 2024-04-10-0910
```
I would like interact with LLMs in sequential way like:

Step 1 : load documents 
Step 2 : ask about the products descri
bed in the documents
Step 3:  based on the response lookup where can I buy these products
Step 4: check if the store has
 other options
‚Ä¶

I am currently doing it in a very basic way. Loading the documents, retrieving using a question, using
 the output as input for another retriever , ‚Ä¶
Is there a more sophisticated way of doing it ?
```
---

     
 
all -  [ Comparing Agent Cloud and CrewAI ](https://www.reddit.com/r/AutoGenAI/comments/1bzrxbv/comparing_agent_cloud_and_crewai/) , 2024-04-10-0910
```
A good comparison blog between AI agents.

Agent Cloud is like having your own GPT builder with a bunch extra goodies.


# The Top GUI features Are:

* RAG pipeline which can natively embed 260+ datasources
* Create Conversational apps (like
 GPTs)
* Create Multi Agent process automation apps (crewai)
* Tools
* Teams+user permissions. Get started fast with Doc
ker and our [install.sh](https://install.sh)

Under the hood, Agent Cloud uses the following open-source stack:

* Airtb
yte for its ELT pipeline
* RabbitMQ for message bus.
* Qdrant for vector database.

They're OSS and you can check their 
repo [GitHub](https://github.com/rnadigital/agentcloud)

# CrewAI

CrewAI is an open-source framework for multi-agent co
llaboration built on Langchain. As a multi-agent runtime, Its entire architecture relies heavily on Langchain.

# Key Fe
atures of CrewAI:

The following are the key features of CrewAI:

* **Multi-Agent Collaboration**: Multi-agent collabora
tion is the core of CrewAI‚Äôs strength. It allows you to define agents, assign distinct roles, and define tasks. Agents c
an communicate and collaborate to achieve their shared objective.
* **Role-Based Design:** Assign distinct roles to agen
ts to promote efficiency and avoid redundant efforts. For example, you could have an ‚Äúanalyst‚Äù agent analyzing data and 
a ‚Äúsummary‚Äù agent summarizing the data.
* **Shared Goals**: Agents in CrewAI can work together to complete an assigned t
ask. They exchange information and share resources to achieve their objective.
* **Process Execution**: CrewAI allows th
e execution of agents in both a sequential and a hierarchical process. You can seamlessly delegate tasks and validate re
sults.
* **Privacy and Security**: CrewAI runs each crew in standalone virtual private servers (VPSs) making it private 
and secure.

What are your thoughts, looks like If anyone is looking for a good solution for your RAG then agentcloud pe
ople are doing good job. 

[Blog link](https://dev.to/agentcloud/agent-cloud-vs-crewai-986)
```
---

     
 
all -  [ Data poision in llms  ](https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/) , 2024-04-10-0910
```
How can we prevent data poision in llms , for example if our database it self is corrupt and we need llm not to send tha
t data , how can we achieve that 
```
---

     
 
all -  [ OpenAI API key integration mismatch ](https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/) , 2024-04-10-0910
```
I've been using Langchain and Langsmith to create a benchmarking workflow for my LLM prompts. Everything's been working 
well, until I had to delete and re-create my OpenAI API key.

I've updated the API key in the env.py as the main environ
mental variable, and this works- I've tested this with the OpenAI Chat Completion.

I've also updated the API key in Lan
gSmith by going into the requested prompt's playground > Secrets & API keys > updated it manually. I even checked under 
my organization's Settings > Secrets, and see the API key is updated.

However, when I try to use the 'arun\_on\_dataset
' function in the aforementioned Python environment, it gives me an error-

    'AuthenticationError('Error code: 401 - 
{'error': {'message': 'Incorrect API key provided: sk-O6ZSB***************************************TNxS. You can find you
r API key at https://platform.openai.com/account/api-keys.' 

The prefix for the API key the error throws is indeed the 
previous, non-functional API key, but I don't know where else to adjust it so it'll read the current API key.

Nothing w
as changed in the code, which ran well previously, and the only external adjustment was the API key.

Should I change th
e API key elsewhere?

Any thoughts and ideas are welcome!
```
---

     
 
all -  [ Redis as vector database  ](https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/) , 2024-04-10-0910
```
I am using redis as vector database . I am getting no permission error when adding text to redis using langchain , manua
lly I am able to add it , but getting error when using langchain , can someone suggest an alternative or how can we achi
eve it with langchain 
```
---

     
 
all -  [ How do I send an audio from INMP441 mic connected to ESP32 DevKit V1 to my laptop and also receive a ](https://www.reddit.com/r/esp32/comments/1bznxsc/how_do_i_send_an_audio_from_inmp441_mic_connected/) , 2024-04-10-0910
```
I made a chatbot code in my Mac which answers users queries by listening to the user through my Mac's microphone and giv
es out the answer using text to speech in real time. I'm trying to create a code that uses the INMP441 mic connected to 
my ESP32 DevKit V1 and receive the user's query through wifi, send it to my chatbot code, processs the query and send it
 back to the ESP (through wifi), which will be played out through a speaker. I have a MAX98357A I2S connector connected 
to a speaker.  Basically, my laptop acts like a server which gets the query from the user and answers it back. I'll prov
ide my chatbot code below. I couldn't figure out the right code for the ESP. I tried a lot of codes, yet I end up in one
 of the two problems: 1)The mic doesn't listen 2)The audio will not be sent

Btw I have connected INMP441 to esp32 devki
t v1 in the following manner :

GND TO GND

VDD TO 3V3

SD TO D33

L/R TO GND

WS TO D25

SCK TO D32

I have connected t
hem in following order with my esp32 DevKit V1

VIN - 3V3

GND - GND

GAIN - GND

DIN - GP22

BCLK - GP26

LRC - GP25

C
HATBOT CODE:  
`import os`  
`import textwrap`  
`import speech_recognition as sr`  
`from langchain.document_loaders im
port TextLoader`  
`from langchain.text_splitter import CharacterTextSplitter`  
`from langchain.embeddings import Huggi
ngFaceEmbeddings`  
`from langchain.vectorstores import FAISS`  
`import pyttsx4`  
`os.environ['HUGGINGFACEHUB_API_TOKE
N'] = '**********************************'`  
`os.environ['TOKENIZERS_PARALLELISM'] = 'false'`  
`loader = TextLoader('/
Users/sanjaykrishna/Documents/CODE_CAP/custom_dataset.txt')`  
`loader.load()`  
`document = loader.load()`  
`def wrap_
text_preserve_newlines(text, width=100):`  
`lines = text.split('\n')`  
`wrapped_lines = [textwrap.fill(line, width=wid
th) for line in lines]`  
`wrapped_text = '\n'.join(wrapped_lines)`  
`return wrapped_text`  
`text_splitter = Character
TextSplitter(chunk_size=1000, chunk_overlap=0)`  
`docs = text_splitter.split_documents(document)`  
`embeddings = Huggi
ngFaceEmbeddings()`  
`db = FAISS.from_documents(docs, embeddings)`  
`engine = pyttsx4.init()`  
`rate = engine.getProp
erty('rate')`  
`new_rate = rate - 10`  
`engine.setProperty('rate', new_rate)`  
`def listen_for_wake_word():`  
`recog
nizer = sr.Recognizer()`  
`with sr.Microphone() as source:`  
`print('Listening for wake word 'hey buddy'...')`  
`reco
gnizer.adjust_for_ambient_noise(source, duration=1)`  
`audio = recognizer.listen(source)`  
`try:`  
`wake_word = recog
nizer.recognize_google(audio)`  
`if 'hey buddy' in wake_word.lower():`  
`print('Wake word 'hey buddy' detected.')`  
`
engine.say('Yes? How may I help you?')`  
`engine.runAndWait()`  
`return True`  
`except sr.UnknownValueError:`  
`pass
`  
`except sr.RequestError as e:`  
`print('Error fetching results: {0}'.format(e))`  
`return False`  
`def handle_que
ry():`  
`print('Speak your query.')`  
`while True:`  
`query = get_query_from_speech()`  
`if query:`  
`while True:` 
 
`if 'goodbye' in query.lower():`  
`print('User said goodbye. Exiting...')`  
`engine.say('Bye! Take care.')`  
`engin
e.runAndWait()`  
`return`  
`doc = db.similarity_search(query)`  
`answer = wrap_text_preserve_newlines(str(doc[0].page
_content))`  
`print('Answer:', answer)`  
`engine.say(answer)`  
`engine.runAndWait()`  
`engine.say('anything else?')`
  
`engine.runAndWait()`  
`break`  
`def get_query_from_speech():`  
`recognizer = sr.Recognizer()`  
`with sr.Micropho
ne() as source:`  
`recognizer.adjust_for_ambient_noise(source, duration=1)`  
`audio = recognizer.listen(source)`  
`tr
y:`  
`query = recognizer.recognize_google(audio)`  
`print('User Query:', query)`  
`return query`  
`except sr.Unknown
ValueError:`  
`print('Could not understand audio')`  
`except sr.RequestError as e:`  
`print('Error fetching results: 
{0}'.format(e))`  
`return ''`  
`def main():`  
`print('Initializing chatbot silently...')`

`# Start interaction loop`
  
`while True:`  
`# Listen for wake word`  
`if listen_for_wake_word():`  
`handle_query()`  
`if __name__ == '__main_
_':`  
`main()`  
It is for a college project and I have been trying this for several weeks now. Still I couldn't comple
te it.  Also this is my first reddit post, I just created an account just to ask this question. Can anyone help me out w
ith this?üòì

Thank you.
```
---

     
 
all -  [ Using user feedback to optimize RAG ](https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/) , 2024-04-10-0910
```
I'm building several chat based apps with LangChain for clients. I'm asking for feedback with each answer, users can lea
ve a üëç or üëé.

Often I get the question: 'does this 'self-improve'?'

This got me thinking, why not use the positive feed
back to improve future answers? Has anyone tried something like this:

1. Store (positive) user feedback in a VectorDB w
ith questions-answer pairs.
2. When a new question is asked, run the usual pipeline (RAG for example).
3. Then also quer
y the feedback VectorDB and add the top-k feedback question-answer pairs with high relevance to the question and add it 
as extra context.
4. Let the LLM answer the question using the context and top-k feedback items.

Looking forward to you
r experience, otherwise I might build this, it doesn't seem to hard to make.
```
---

     
 
all -  [ SQL Agent in production? ](https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/) , 2024-04-10-0910
```
I‚Äôd be interested in whether anyone here is using¬†¬†LangChain‚Äôs SQL Agent (or similar self-built agents with LangChain or
 autogen). I‚Äôd love to conenct to learn from your experiences as I have not seen it be used in productive systems yet!
```
---

     
 
all -  [ Seeking Help with Whether to Use ChatGPT's API or LangChain in My Chatbot Web App ](https://www.reddit.com/r/MLQuestions/comments/1bzmv6n/seeking_help_with_whether_to_use_chatgpts_api_or/) , 2024-04-10-0910
```
Hey guys!

I need a little bit of assistance and guidance as to what should I use in my project. I'm trying to create

a
 web app (React - frontend | Flask - backend), designed to include a chatbot that offers personalized video game  recomm
endations. The goal is for the chatbot to suggest games based on user inputs, like preferences for similar games, demogr
aphic details, or specific game features they enjoy.

Given my very limited experience in this sphere of AI and ML,  I'm
 at a crossroads on how best to integrate this chatbot functionality. Should I integrate ChatGPT's API into my code (and
 perhaps further fine-tuning it to better map the project's requirements and idea) or go with LangChain (a framework tha
t I've stumbled upon when searching for ways to implement my ideas into code and have very basic knowledge about it as o
f this moment)?

Do you have any other advices for me to take into consideration and maybe follow in the process of crea
ting this project? Perhaps other APIs or tools that may ease the process of developing the app and save me some time on 
some aspects.

Thanks a ton to everyone taking the time to read and respond! Your help is really appreciated!
```
---

     
 
all -  [ How to create a chatbot to chat with csv files and internet (Bing API)? ](https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/) , 2024-04-10-0910
```
So I have a requirement of being able to chat with csv files and when the chatbot can't find any relevant information fr
om the csv files it should use the Bing API to search on the web and gather information and answer. I tried to make a cu
stom langchain agent with Bing API as a tool but it's not able to perform the observation, action loop, the model I'm us
ing is Mistral-7B-Instruct-v0.1 which I can't change I think model is not powerful enough for this task. But still does 
anybody have idea how can I make this possible? 
```
---

     
 
all -  [ Multi-Agent Interview using LangGraph  ](https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/) , 2024-04-10-0910
```
Checkout how you can leverage Multi-Agent Orchestration for developing an auto Interview system where the Interviewer as
ks questions to interviewee, evaluates it and eventually shares whether the candidate should be selected or not. Right n
ow, both interviewer and interviewee are played by AI agents.
https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP
```
---

     
 
all -  [ LangChain Embeddings ](https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/) , 2024-04-10-0910
```
Hello,

I‚Äôm very new to LangChain and LLM altogether. Very excited!

I started following a LangChain example:
https://py
thon.langchain.com/docs/use_cases/question_answering/chat_history/

I modified it to use a DirectoryLoader. Next, I invo
ked a prompt. The response is narrowed down to a particular chunk when the custom data is split and misses information f
rom other document chunks. For example, the custom data contains information about best practices spread across several 
pages. My prompt is ‚Äúlist 5 of the best practices‚Äù. The response would only show one best practice, while ignoring the o
ther document chunks that contain other practices.

Other example of a prompt is ‚ÄúSummary the most important best practi
ce‚Äù. The response seems to randomly pick a document chunk and consider that the most important.

How should I go about e
nsure that all chunks of every document is used as part of the response?

Thanks!
```
---

     
 
all -  [ Im worried about privacy and was wondering if there is an LLM I can run locally on my i7 Mac that ha ](https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/) , 2024-04-10-0910
```
Does an LLM like this exist? Also, will this kill my machine? Im not needing a large model trained on a lot of tokens be
cause I just want it to work with the data I provide it in the context during inference but I the context I have is abou
t 21k. 

Can I use Amazon to host a private LLM instead of running locally? Is that what Bedrock offers? 

Any insights 
are appreciated. Thanks.
```
---

     
 
all -  [ Seeking Advice: Simple and Fast Solution for Classifying Chat Messages into Tasks ](https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/) , 2024-04-10-0910
```
I'm building a knowledge management system. I have implemented each individual functionality separately. Since accessing
 everything separately is cumbersome, I have decided to use a chat interface with function calling. I'm currently using 
the Mistral 8x7b 4-bit quantized version as my LLM. However, due to hardware limitations, directly performing function c
alling is slow and does not give the expected results.

Then, I decided to use a semantic-router with OpenAI's Ada model
 to classify each input to determine which task it belongs to and then send it to the LLM to extract relevant informatio
n. This approach works well, but since I intend to use a local solution, OpenAI's model is not an option.

I tried using
 local models, but the faster ones produce suboptimal results, while the better ones are slower. Is there any simple and
 fast solution that can classify chat messages into the appropriate task? My current tasks include image search, Retriev
al Augmented Generation (RAG), document summarization, and document search.

I thought of building a simple classifier u
sing traditional machine learning methods like TF-IDF, Gradient Boosting, Logistic Regression, etc. Would that work? Has
 anyone done this kind of work before? Thanks in advance.
```
---

     
 
all -  [ Run this to optimize your numpy programs automatically ](https://www.reddit.com/r/Numpy/comments/1bzfdf2/run_this_to_optimize_your_numpy_programs/) , 2024-04-10-0910
```
Hi! I am Saurabh. I love writing fast programs and I've always hated how slow Python code can sometimes be. To solve thi
s problem, I have created Codeflash, which is the first automatic code performance optimizer.

`codeflash` is a Python p
ackage that uses state of the art AI to figure out the most performant way  to rewrite a Python code. It not only optimi
zes the performance but also verifies the correctness of the new code, i.e. makes sure that the new code follows exactly
 the same behavior as your original code. This automates the manual optimization process.

It can improve algorithms, da
ta structures, fix logic, use better optimized libraries etc to speed up your code. It particularly works really well fo
r numpy programs. For numpy, it finds the best algorithms, best numpy call for your use case and a lot more to really sp
eed up your code. [This PR](https://github.com/langchain-ai/langchain/pull/8151/files) on Langchain is a great example o
f numpy algorithmic speedups made through codeflash.

Website - [https://www.codeflash.ai/](https://www.codeflash.ai/) ,
 get started here.

PyPi - [https://pypi.org/project/codeflash/](https://pypi.org/project/codeflash/)

Really interested
 to see what optimizations you discover. Since we are early, it is free to use codeflash.

If you have a Python project,
 it should take you less than 5 minutes to setup codeflash - `pip install codeflash` and `codeflash init.`

After you ha
ve set it up, Codeflash can also optimize your entire Python project! Run `codeflash --all` and codeflash will optimize 
your project, function by function, and create PRs on GitHub when it finds an optimization. This is super powerful. We h
ave already optimized some popular open source projects with this.

You can also install codeflash as a GitHub actions c
heck that runs on every new PR you create, to  ensure that all new code is performant. This makes your code expert-level
. This ensures that your project stays at peak performance everytime. Its like magic ‚ú®

How it works

Codeflash works by
 optimizing the code path under a function. So if there is a function `foo(a, b):`, codeflash finds the fastest implemen
tation of the function foo and all the other functions it calls. The optimization procedure preserves  the signature of 
the function foo and then figures out a new optimized implementation that results in exactly the same return values as t
he  original foo. The behavior of the new function is verified to be correct  by running your unit tests and generating 
a bunch of new regression  tests. The runtime of the new code is measured and the fastest one is  recommended.

Let me k
now what optimizations it found, and any ideas you may have for us. Very interested to hear what you may want to speed u
p.

Cheers,

Saurabh
```
---

     
 
all -  [ Creating agents with 'Scripted conversations' ](https://www.reddit.com/r/LangChain/comments/1bz80zh/creating_agents_with_scripted_conversations/) , 2024-04-10-0910
```
Hello, after working on a classic RAG chatbot to answer questions about IT support, we want to improve it so that, inste
ad of showing a list of steps retrieved from the KB, it can interact with the user asking questions. For example:  
U: X
 does not work.  
A: please try A, did it work?  
U: No  
A: ok, if A did not work, please try B.  
U:  it worked.  
A: 
happy to be of help.  
etc. etc.  
Of course the conversation can become longer with multiple steps, etc.  
I can imagin
e mixing a classic chatbot with a scripted flow with an LLM to get better understanding of utterances, a bit like the ex
perimental mode of Rasa does, but I'm asking myself if there is a better and more modern way to handle such data collect
ion/scripted conversations. At the end it's a kind of data gathering mixed with a decision tree and wondering what's the
 more modern approach to such a problem.  
What do you guys think?
```
---

     
 
all -  [ Recommendations for vectorDBs (local & serializable) ](https://www.reddit.com/r/vectordatabase/comments/1bz7zu9/recommendations_for_vectordbs_local_serializable/) , 2024-04-10-0910
```
I am looking for recommendations for vectorDBs that can operate in a browser. I'd like to be able to pre-compute embeddi
ngs for a large number of documents, load to the vectorDB, save it, and subsequently reload the vectorDB without having 
to re-compute the embeddings.

I have tried voy-search already, but there's a bug with their serialize and deserialize f
unctionality. I am also trying MemoryVectorStore with Langchain but there's no easy way I can see to export the vectorDB
 for later use.

Any help is greatly appreciated!
```
---

     
 
all -  [ Wrote a semantic chunker for RAG pipelines in Typescript ](https://www.reddit.com/r/LLMDevs/comments/1bz3p2n/wrote_a_semantic_chunker_for_rag_pipelines_in/) , 2024-04-10-0910
```
Langchain python had this for some time now, but the typescript implementation lacked this chunking mechanism. So, follo
wed [this](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Leve
ls_Of_Text_Splitting.ipynb) notebook by Greg Kamadt and implemented it myself.  


Feel free to use the code :) Everythi
ng is documented in jsDoc  


[https://github.com/tsensei/Semantic-Chunking-Typescript](https://github.com/tsensei/Seman
tic-Chunking-Typescript)
```
---

     
 
all -  [ Wrote a semantic chunker for RAG pipelines in Typescript ](https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/) , 2024-04-10-0910
```
Langchain python had this for some time now, but the typescript implementation lacked this chunking mechanism. So, follo
wed [this](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Leve
ls_Of_Text_Splitting.ipynb) notebook by Greg Kamadt and implemented it myself.  


You're free to use the code :) Everyt
hing is documented in jsDoc  


[https://github.com/tsensei/Semantic-Chunking-Typescript](https://github.com/tsensei/Sem
antic-Chunking-Typescript)
```
---

     
 
all -  [ Need help building better RAG chatbot using azure  ](https://www.reddit.com/r/AZURE/comments/1bz35jp/need_help_building_better_rag_chatbot_using_azure/) , 2024-04-10-0910
```
I'm building RAG chat bot with azure open ai and azure ai search

using streamlit as frontend and maintain chat history 
using session state (not focusing on persistent state rgt now)

Data ingestion pipeline is big headache for me(working w
ith static data)
Working with formats PDF , PPT , docx , excel(mostly technical documentation and excel is logs of ticke
t)

Each pdf have different structure,Ppt is in  different structure ,Docx is in diff structure 

Right now , i converte
d ppt's and docx's to pdf and used pymupdf to extract text and chucked with recursive character split(size =1024, overla
p=120)
Used direct pymupdf lib instead of langchain abstract why you ask?
LET us consider PDF A 
PDF A consist most of i
mages with little text so chunking page wise  results in character length of 150

So I extracted whole pdf text applied 
chunking on that.
**Is this optimal way ? I lose meta data by this method.**

**Coming to logs of excel** .
TBH I don't 
know how work with this.
I combined important columns as one column.

For Eg:
(Title:
....
Desc:
....)
I combined Title 
and desc column as one with column heading in each row so context not missed out.
Here also Each row converted as docume
nt object with size  comes around 250 characters.

I feel wasting resource too much 

What best ways you would suggest?


Thanks in advance

**I know there is atleast 3 templates from azure. I find difficulty to implement those because of my
 access level. Im working with deployed resources, cant create new resource like storage container and document intellig
ence**

```
---

     
 
all -  [ Need help finding better methods implementing RAG ](https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/) , 2024-04-10-0910
```
I'm building RAG chat bot with azure open ai and azure ai search

Right now I'm only developing POC 

using streamlit as
 frontend and maintain chat history using session state (not focusing on persistent state rgt now)

Data ingestion pipel
ine is big headache for me even with static data(no updation once used )

Working with PDF , PPT , docx , excel(mostly t
echnical documentation and excel is logs of ticket)

Each pdf have different structure 
Ppt is in  different structure 

Docx is in diff structure 

Right now , i converted ppt and docx to pdf and used pymupdf to extract text and chucked wit
h recursive character split(size =1024, overlap=120)
Used direct pymupdf lib instead of langchain abstract why you ask?


LET us consider PDF A 

PDF A consist most of images with little text so chunking page wise  results in character lengt
h of 150

So I extracted whole pdf text applied chunking on that.
**Is this optimal way ? I lose meta data by this metho
d.**

Coming to logs of excel .
TBH I don't know how work with this.
I combined important columns as one column.

For Eg
:
(Title:
....
Desc:
....)
I combined Title and desc column as one with column heading in each row so context not missed
 out.
Here also Each row converted as document object with size  comes around 250 characters.

I feel wasting resource t
oo much 

What best ways you guys would suggest?

Thanks in advance

```
---

     
 
all -  [ Migrating my prompts to open source language models ](https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/) , 2024-04-10-0910
```
Open source language models are no serious competitors. I have been migrating a lot of my prompts to open source models,
 and I wrote up this tutorial about how I do it.

https://blog.promptlayer.com/migrating-prompts-to-open-source-models-c
21e1d482d6f
```
---

     
 
all -  [ Text-to-text generation: finetuning vs RAG vs few-shot? ](https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/) , 2024-04-10-0910
```
So I want to automate the conversion of a legal document (5-20 pages) into a different type of document with plain/lay E
nglish + adhere to a specific style and format guidelines (20-100 pages) that are in 3 separate reference pdf documents.


I tried the simplest approach I could think of at first, which was extracting and then providing the expected output f
ormat (headers/sub-headers) in my prompt using a 'custom GPT' on the openai front-end, as well as a one-shot example pai
r of legal doc/converted doc in the prompt window, plus I also uploaded the reference docs to the customgpt (which I thi
nk is used as RAG by the GPT?). 

The result is okay - it gets the format right for the most part, but it ignores many o
f the style guidelines, and summarizes a lot needlessly which leads to information loss. 

I want to now try either more
 advanced RAG (I am a python user and with the exception of recent LCEL releases, am familiar with Langchain as well as 
LlamaIndex), but was also considering finetuning llama-2 with 4-bit quantization. 

Is finetuning without a label even p
ossible in this case? What RAG retrievers or embeddings would you suggest? Any other suggestions? Thanks!
```
---

     
 
all -  [ Should I partner with a publication for my book on Generative AI? ](https://i.redd.it/beyqpwcpu9tc1.png) , 2024-04-10-0910
```
Recently I launched my debut book (self-published) on Generative AI i.e. 'LangChain in your Pocket: Beginners guide to b
uilding Generative AI applications using LLMs' which is going a bestseller since release. A big technical book publicati
on Packt has reached out to me for distribution of this book. This would definitely help me reach a wider audience but t
he price may go up exponentially high and hence unaffordable, specially in India where I've set a slightly lower price. 
What should I do? 
```
---

     
 
all -  [ Insights and Learnings from Building a Complex Multi-Agent System ](https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/) , 2024-04-10-0910
```
tldr: Some insights and learnings from a LLM enthusiast working on a complex Chatbot using multiple agents built with La
ngGraph, LCEL and Chainlit.


Hi everyone! I have seen a lot of interest in multi-agent systems recently, and, as I'm cu
rrently working on a complex one, I thought I might as well share some feedback on my project. Maybe some of you might f
ind it interesting, give some useful feedback, or make some suggestions.

## Introduction: Why am I doing this project?


I'm a business owner and a tech guy with a background in math, coding, and ML. Since early 2023, I've fallen in love wi
th the LLM world. So, I decided to start a new business with 2 friends: a consulting firm on generative AI. As expected,
 we don't have many references. Thus, we decided to create a tool to demonstrate our skillset to potential clients.

Aft
er a brainstorm, we quickly identified that a) RAG is the main selling point, so we need something that uses a RAG; b) W
e believe in agents to automate tasks; c) ChatGPT has shown that asking questions to a chatbot is a much more human-frie
ndly interface than a website; d) Our main weakness is that we are all tech guys, so we might as well compensate for tha
t by building a seller.

From here, the idea was clear: instead, or more exactly, alongside our website, build a chatbot
 that would answer questions about our company, 'sell' our offer, and potentially schedule meetings with our consultants
. Then make some posts on LinkedIn and pray...

Spoiler alert: This project isn't finished yet. The idea is to share som
e insights and learnings with the community and get some feedback.

## Functional specifications

The first step was to 
list some specifications:
* We want a RAG that can answer any question the user might have about our company. For that, 
we will use the content of the company website. Of course, we also need to prevent hallucination, especially on two topi
cs: the website has no information about pricing, and we don't offer SLAs.
* We want it to answer as quickly as possible
 and limit the budget. For that, we will use smaller models like GPT-3.5 and Claude Haiku as often as possible. But that
 limits the reasoning capabilities of our agents, so we need to find a sweet spot.
* We want consistency in the response
s, which is a big problem for RAGs. Questions with similar meanings should generate the same answers, for example, 'What
's your offer?', 'What services do you provide?', and 'What do you do?'.
* Obviously, we don't want visitors to be able 
to ask off-topic questions (e.g., 'How is the weather in North Carolina?'), so we need a way to filter out off-topic, pr
ompt injection, and toxic questions.
* We want to demonstrate that GenAI can be used to deliver more than just chatbots,
 so we want the agents to be able to schedule meetings, send emails to visitors, etc.
* Ideally, we also want the agents
 to be able to qualify the visitor: who they are, what their job is, what their organization is, whether they are a tech
 person or a manager, and if they are looking for something specific with a defined need or are just curious about us.
*
 Ideally, we also want the agents to 'sell' our company: if the visitor indicates their need, match it with our offer an
d 'push' that offer. If they show some interest, let's 'push' for a meeting with our consultants!

## Architecture

### 
Stack

We aren't a startup, we haven't raised funds, and we don't have months to do this. We can't afford to spend more 
than 20 days to get an MVP. Besides, our main selling point is that GenAI projects don't require as much time or budget 
as ML ones.

So, in order to move fast, we needed to use some open-source frameworks:
* For the chatbot, the data is pub
lic, so let's use GPT and Claude as they are the best right now and the API cost is low.
* For the chatbot, Chainlit pro
vides everything we need, except background processing. Let's use that.
* Langchain and LCEL are both flexible and unify
 the interfaces with the LLMs.
* We'll need a rather complicated agent workflow, in fact, multiple ones. LangGraph is mo
re flexible than crew.ai or autogen. Let's use that!

### Design and early versions

#### First version

From the start,
 we knew it was impossible to do it using a 'one prompt, one agent' solution. So we started with a 3-agent solution: one
 to 'find' the required elements on our website (a RAG), one to sell and set up meetings, and one to generate the final 
answer.

The meeting logic was very easy to implement. However, as expected, the chatbot was hallucinating a lot: 'Here 
is a full project for 1k‚Ç¨, with an SLA 7/7 2 hours 99.999%'. And it was a bad seller, with conversations such as 'Hi, wh
o are you?' 'I'm Sellbotix, how can I help you? Do you want a meeting with one of our consultants?'

At this stage, afte
r 10 hours of work, we knew that it was probably doable but would require much more than 3 agents.

#### Second version


The second version used a more complex architecture: a guard to filter the questions, a strategist to make a plan, a se
ller to find some selling points, a seeker and a documentalist for the RAG, a secretary for the schedule meeting functio
n, and a manager to coordinate everything.

It was slow, so we included logic to distribute the work between the agents 
in parallel. Sadly, this can't be implemented using LangGraph, as all agent calls are made using coroutines but are awai
ted, and you can't have parallel branches. So we implemented our own logic.

The result was much better, but far from pe
rfect. And it was a nightmare to improve because changing one agent's system prompt would generate side effects on most 
of the other agents. We also had a hard time defining what each agent would need to see and what to hide. Sending every 
piece of information to every agent is a waste of time and tokens.

And last but not least, the codebase was a mess as w
e did it in a rush. So we decided to restart from scratch.

## Third version, WIP

So currently, we are working on the t
hird version. This project is, by far, much more ambitious than what most of our clients ask us to do (another RAG?). An
d so far, we have learned a ton. I honestly don't know if we will finish it, or even if it's realistic, but it was worth
 it. 'It isn't the destination that matters, it's the journey' has rarely been so true.

Currently, we are working on th
e architecture, and we have nearly finished it. Here are a few insights that we are using, and I wanted to share with yo
u.

### Separation of concern

The two main difficulties when working with a network of agents are a) they don't know wh
en to stop, and b) any change to any agent's system prompt impacts the whole system. It's hard to fix. When building a c
omplex system, separation of concern is key: agents must be split into groups, each one with clear responsibilities and 
interfaces.

The cool thing is that a LangGraph graph is also a Runnable, so you can build graphs that use graphs. So we
 ended up with this: a main graph for the guard and final answer logic. It calls a 'think' graph that decides which subg
raphs should be called. Those are a 'sell' graph, a 'handle' graph, and a 'find' graph (so far).

### Async, parallelism
, and conditional calls

If you want a system to be fast, you need to NOT call all the agents every time. For that, you 
need two things: a planner that decides which subgraph should be called (in our think graph), and you need to use `async
io.gather` instead of letting LangGraph call every graph and await them one by one.

So in the think graph, we have plan
ner and manager agents. We use a standard doer/critic pattern here. When they agree on what needs to be done, they gener
ate a list of instructions and activation orders for each subgraph that are passed to a 'do' node. This node then create
s a list of coroutines and awaits an `asyncio.gather`.

### Limit what each graph must see

We want the system to be fas
t and cost-efficient. Every node of every subgraph doesn't need to be aware of what every other agent does. So we need t
o decide exactly what each agent gets as input. That's honestly quite hard, but doable. It means fewer tokens, so it red
uces the cost and speeds up the response.

## Conclusion

This post is already quite long, so I won't go into the detail
s of every subgraph here. However, if you're interested, feel free to let me know. I might decide to write some addition
al posts about those and the specific challenges we encountered and how we solved them (or not). In any case, if you've 
read this far, thank you!

If you have any feedback, don't hesitate to share. I'd be very happy to read your thoughts an
d suggestions!
```
---

     
 
all -  [ Agentic Workflow Distortion in the Absence of Systemic Self-Reflection ](https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/) , 2024-04-10-0910
```
I wrote this as the intro to a problem I am working in. anyone else thinking about this?

**Strategic Risk Reduction in 
AI Operations: Enhancing Systemic Controls in Agentic Workflows**

It is possible for a workflow comprised of a chain of
 agentic assistants to drift away from the desired operational baseline without hallucinating or scoring poorly on stand
ardized quality tests such as those for relevance, faithfulness, and alignment. Agentic assistants may be observed, meas
ured, and managed on an individualized basis, but nodal evaluations may accurately analyze a point in time step in a wor
kflow while failing to capture systemic distortion only observable at the workflow level while the agentic workflows are
 in motion.

**Agentic Workflow Distortion in the Absence of Systemic Self-Reflection**

The concepts of Agentic Workflo
w Distortion and Systemic Self-Reflection pertain to the dynamics and evaluation within automated systems, particularly 
those that are structured around the autonomous operation of individual agents, referred to as 'agentic workflows.' ....
.tbc

\*\* I have the rest of this writeup if is anyone is interested 
```
---

     
 
all -  [ Use Llamaindex Retriever in Langchain chain ](https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/) , 2024-04-10-0910
```
Hi,

I was wondering if anyone tried out to use a retriever from Llamaindex as retriever in a Langchain chain?

For me t
his is interesting because for now it is difficult to persistently save a ParentDocumentRetriever in Langchain but I thi
nk this is possible with Llamaindex. So I thought I am just using the Llamaindex retriever and pass the results to my ch
ain.

Is there anything I should consider or are there any expericences with this approach?
```
---

     
 
all -  [ GitHub - Upsonic/Tiger: Neuralink for your AI Agents ](https://www.reddit.com/r/aidevtools/comments/1bytoos/github_upsonictiger_neuralink_for_your_ai_agents/) , 2024-04-10-0910
```
Tiger: Neuralink for AI Agents (MIT) (Python)

Hello, we are developing a superstructure that provides an AI-Computer in
terface for AI agents created through the frameworks like LangChain and AutoGen, we have published it completely openly 
under the MIT license.

What it does: Just like human developers, it has some abilities such as running the codes it wri
tes, making mouse and keyboard movements, writing and running Python functions for functions it does not have. AI litera
lly thinks and the interface we provide transforms with real computer actions.

Those who want to contribute can provide
 support under the MIT license and code conduct. [https://github.com/Upsonic/Tiger](https://github.com/Upsonic/Tiger)
```
---

     
 
all -  [ Langtrace: Preview of the new Evaluation dashboard ](https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/) , 2024-04-10-0910
```
Hey,

  
I am building an open source project called Langtrace which lets you monitor, debug and evaluate the LLM reques
ts made by your application.

  
[https://github.com/Scale3-Labs/langtrace](https://github.com/Scale3-Labs/langtrace) . 
The integration is only 2 lines of code.

Currently building an Evaluations dashboard which is launching this week. It l
ets you do the following:

1. Create tests - like factual accuracy, bias detection etc. 

2. Automatically capture the L
LM calls to specific tests by passing a testId to the langtrace SDK installed in your code.

3. Evaluate and measure the
 overall success % and how success % trends over time.

The goal here is to get confidence with the model or RAG before 
deploying it to production.

  
Please check out the repository. Would love to hear your thoughts! Thanks!

https://prev
iew.redd.it/5bracw5ki7tc1.png?width=2932&format=png&auto=webp&s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f

 
```
---

     
 
all -  [ First NextJS project : Ralla.ai - Itinerary Planner w AI and Chatbot ](https://www.reddit.com/r/nextjs/comments/1byrfo5/first_nextjs_project_rallaai_itinerary_planner_w/) , 2024-04-10-0910
```
Hi guys,

So I started learning React about 8 months ago and quickly made the switch to NextJS, at that time they were s
witching from the pages to app router and introduction of server actions so it was quite a journey with a lot of confusi
on on how to approach certain things in my app but I think we're at a point where I know what I'm doing, sort of... some
times. Could you share your thoughts, recommendations regarding my application below?

[Ralla.ai](https://ralla.ai)

The
re is a registration built in, I'm not trying to spam you but it's just because I need to assign the itinerary to a spec
ific user.. Sorry!

Technology I used : NextJS, Supabase, OpenAI, Langchain

&#x200B;

Many thanks!
```
---

     
 
all -  [ HTML/MARKDOWN splitter vs Code splitter with html/markdown as language. ](https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/) , 2024-04-10-0910
```
While going through the Langchain documentation, I came across the fact that LC is providing separate html and markdown 
splitter and you also have an option for the same two in code splitter as well.   
What is the difference between the tw
o?
```
---

     
 
all -  [ Creation of website to visualize responses generated by LangChain pandas dataframe agent ](https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/) , 2024-04-10-0910
```
I want to make a website where I want to display the responses I am generating using langchain agent. It is actually ana
lysis of educational data, and generated responses are helpful to be used as teaching material. Hence, I want to documen
t it. What would be the easiest and best way to direclty document it onto the website? It would be better to document th
e intermediate steps generated by the agent as well. Thank you
```
---

     
 
all -  [ LangChain vs DSPy ](https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/) , 2024-04-10-0910
```
Do you guys really think that using DSPy is a good idea over Langchain? 
For me I think, DSPy is not mature enough and L
angChain provides so many things.
```
---

     
 
all -  [ Help for tutorial ](https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/) , 2024-04-10-0910
```
I just joined here not long ago.
I have learned knowledges from the official documents,but i think it's not suitable for
 me.
I have some programming basics in python,i hope to get some friendly and excellent tutorials that use ollama to cre
ate rather than openai based in langchain.In additional,i want to know how to get high-quality information sources.Engli
sh is not my native language,i'm sorry to any grammar errors.
```
---

     
 
all -  [ Recommendatios for an LLM-Based Conversational Product Search Interface ](https://www.reddit.com/r/LocalLLaMA/comments/1byl8cr/recommendatios_for_an_llmbased_conversational/) , 2024-04-10-0910
```
I'm diving into building a super cool feature for our marketplace ‚Äì a search utility that understands plain English and 
can run edge-or database functions to assist customers. 

Conceptually, the customer enters 'I'm looking for art from th
e Pacific Northwest' and receives product recommendations or contextual questions like ‚ÄúThere are a few styles; do any o
f these [items] fit what you're thinking?‚Äù, or ‚Äúwelcome back, your package is x days out‚Äù

The project‚Äôs toolkit is alre
ady scoped with Supabase, Next.js, TS, Postgres, and a mix of Node and Python on the backend, plus Stripe for the cash f
low. 

Basically, I'm looking for pointers or recommendations on LLMs or tools that play nice or fit with what I'm think
ing. I've had experience with a variety of tools (pytorch, Openai‚Äôs API, Hugging Face, langchain, LLlama, and a variety 
of projects), so nothing is really an overly heavy lift.
```
---

     
 
MachineLearning -  [ [D] How to get the source documents from the retrieved context for RAG?  ](https://www.reddit.com/r/MachineLearning/comments/1bvoc1t/d_how_to_get_the_source_documents_from_the/) , 2024-04-10-0910
```
I'm not using Lanchain but only making API calls to an LLM service provider. The retriever is connected to a vector DB, 
and I would like to know what the LLM refers to WITHIN that retrieved context whenever it provides an answer, similar to
 how return_source_documents works in Langchain.

I'm using AzureOpenAI. I couldn't find much in their docs that related
 to returning the source documents. Any help will be greatly appreciated!

```
---

     
 
MachineLearning -  [ [P] RAG pipeline to query the ML Engineering Open Book ](https://www.reddit.com/r/MachineLearning/comments/1bu4wyx/p_rag_pipeline_to_query_the_ml_engineering_open/) , 2024-04-10-0910
```
I built a quick RAG implementation using Langchain to make it easy to query the [ML Engineering Open Book](https://githu
b.com/stas00/ml-engineering) by [Stas Bekman](https://twitter.com/StasBekman). The Multi-Vector retriever gave pretty go
od results and was quite easy to set up too. 

Github link - [https://github.com/shreyansh26/RAG-ML-Engg-Open-Book](http
s://github.com/shreyansh26/RAG-ML-Engg-Open-Book)

Hope this is useful for folks!
```
---

     
 
MachineLearning -  [ [Project] FinancialAdvisorGPT : LLM+RAG Boilerplate Project ](https://www.reddit.com/r/MachineLearning/comments/1btlpgd/project_financialadvisorgpt_llmrag_boilerplate/) , 2024-04-10-0910
```
FinancialAdvisorGPT is a boilerplate project designed for RAG (Retriever-Augmented Generation) and LLM (Large Language M
odel) applications in financial analysis. Built on a technology stack including MongoDB, MongoDB VectorDB, Chroma, FastA
PI, Langchain, and React submodule for UI, it offers a framework for developers to implement and customize RAG+LLM proje
cts. Leveraging parallelized data pipelines, FinancialAdvisorGPT processes and integrates various data sources such as s
tock data, news, SEC filings, and local PDFs.

Github project includes long documentation on how the project is structur
ed, how LLM+RAG works for specific task :¬†[https://github.com/mburaksayici/FinancialAdvisorGPT](https://github.com/mbura
ksayici/FinancialAdvisorGPT)
```
---

     
 
MachineLearning -  [ [Project] Picachain, image search made simple. ](https://www.reddit.com/r/MachineLearning/comments/1bt7epv/project_picachain_image_search_made_simple/) , 2024-04-10-0910
```
I am working on creating something for image search, basically something like langchain for images. Probably add videos 
too.

Currently supports chromaDB. Working on pinecone and other vector dbs. [https://github.com/d1pankarmedhi/picachain
](https://github.com/d1pankarmedhi/picachain)

Will you use something like this? What else should I add? Feel free to ad
d your views.

Appreciate your feedback or any feature ideas :)

&#x200B;
```
---

     
 
deeplearning -  [ Tengyu Ma on Voyage AI - Weaviate Podcast #91! ](https://www.reddit.com/r/deeplearning/comments/1bjft8i/tengyu_ma_on_voyage_ai_weaviate_podcast_91/) , 2024-04-10-0910
```
**Voyage AI** is the newest giant in the embedding, reranking, and search model game!

I am SUPER excited to publish our
 latest Weaviate podcast with Tengyu Ma, Co-Founder of Voyage AI and Assistant Professor at Stanford University!

We beg
an the podcast with a deep dive into everything embedding model training and contrastive learning theory. Tengyu deliver
ed a **masterclass** in everything from scaling laws to multi-vector representations, neural architectures, representati
on collapse, data augmentation, semantic similarity, and more! I am beyond impressed with Tengyu's extensive knowledge a
nd explanations of all these topics.

The next chapter dives into a case study Voyage AI did **fine-tuning an embedding 
model for the LangChain documentation.** This is an absolutely fascinating example of the role of continual fine-tuning 
with very new concepts (for example, very few people were talking about chaining together LLM calls 2 years ago), as wel
l as the data efficiency advances in fine-tuning.

We concluded by discussing ML systems challenges in serving an embedd
ings API. Particularly the challenge of detecting if a request is for batch or query inference and the optimizations tha
t go into either say \~100ms latency for a query embedding or maximizing throughput for batch embeddings.

I hope you fi
nd the podcast interesting, more than happy to discuss any of these topics with you or answer any questions about the co
ntent in the podcast! Thank you so much!

YouTube: [https://www.youtube.com/watch?v=xPdyivfheqI](https://www.youtube.com
/watch?v=xPdyivfheqI)

Spotify: [https://spotifyanchor-web.app.link/e/u6XPLYfF7Hb](https://spotifyanchor-web.app.link/e/
u6XPLYfF7Hb)
```
---

     
