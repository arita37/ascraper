 
all -  [ Open Source NotebookLM API built with langchain ](https://www.reddit.com/r/LangChain/comments/1geh0gx/open_source_notebooklm_api_built_with_langchain/) , 2024-10-29-0913
```
Python package: [https://github.com/souzatharsis/podcastfy](https://github.com/souzatharsis/podcastfy)  
Web app demo: [
https://huggingface.co/spaces/thatupiso/Podcastfy.ai\_demo](https://huggingface.co/spaces/thatupiso/Podcastfy.ai_demo)
```
---

     
 
all -  [ Feel like LangGraph could benefit from having more advanced nodes  like those you find in Behavior T ](https://www.reddit.com/r/LangChain/comments/1gegibp/feel_like_langgraph_could_benefit_from_having/) , 2024-10-29-0913
```
[Unreal Engine](https://dev.epicgames.com/documentation/en-us/unreal-engine/behavior-tree-in-unreal-engine---overview#ba
sicsofbehaviortrees)

Eg; in Unreal Engine, they have nodes with various utilities. You can have a node that execute imm
ediate child nodes in sequence whether each fails or not. Another where the sequence aborts if a child fail. A repeat-X-
times node. A wait node. fail-over & interrupt branch (or whatever it is called), etc...
```
---

     
 
all -  [ [4 YoE, Unemployed, Data Analyst, Dallas] ](https://i.redd.it/5v75p2x9zjxd1.jpeg) , 2024-10-29-0913
```
I am international student who graduated in May 2024 and actively applying to jobs. Very few callbacks and interviews.
P
lease review my resume and give your feedback on it.
Should I add more keywords to pass ATS? Does my resume seem technic
al and consistent?
```
---

     
 
all -  [ Study guide using RAG ](https://www.reddit.com/r/LangChain/comments/1ge8zt7/study_guide_using_rag/) , 2024-10-29-0913
```
I'm a complete beginner, but I've been thinking about whether it's possible to create a system that takes a textbook and
 past exam papers to generate a study guide. The textbook is about 1,000 pages, while study guides are usually around 30
0 pages. I’m not sure if an LLM can produce a full 300-page guide, but could we break it down into sections and combine 
them to create a comprehensive study guide? Is this feasible? I don’t have an OpenAI API key, so I’m considering using G
emini or local LLMs.
I’m willing to learn everything needed to make this work, but I’m unsure if it’s feasible for me. A
ny insights would be greatly appreciated. Thank you!


```
---

     
 
all -  [ LLM for creative writing ](https://www.reddit.com/r/LangChain/comments/1ge890j/llm_for_creative_writing/) , 2024-10-29-0913
```
Hey guys, 

I'm creating a little application for creative writing and was wondering which LLM's you prefer. I'm current
ly sticking to Claude 3.5 Sonnet, GPT4o is kind of comparable. What are your takes?
```
---

     
 
all -  [ Is there a framework like dify but where I can use a custom orchestrator instead? ](https://www.reddit.com/r/LocalLLaMA/comments/1ge7cei/is_there_a_framework_like_dify_but_where_i_can/) , 2024-10-29-0913
```
Hi everyone,  
I would like to build an LLM app. I tried Dify and it's awesome but LLM orchestration isn't flexible enou
gh for what I need.  

It is perfect for the interface/frontend, API integration, monitoring and management, knowledge m
anagement.

I am wondering if there is a quick way to build a similar platform with open tools.  
For example: weaviate 
+ langchain + langfuse + openwebui...

I am a little bit lost on this. I would be grateful for any help, thank you a lot
 in advance.
```
---

     
 
all -  [ Controllable Agent for Complex RAG Tasks ](https://open.substack.com/pub/diamantai/p/controllable-agent-for-complex-rag?r=336pe4&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true) , 2024-10-29-0913
```

```
---

     
 
all -  [ Have OpenAI's GPT-4o Models Changed Recently? Noticing Different Results with GPT-4o Wrapper ](https://www.reddit.com/r/LangChain/comments/1ge42uk/have_openais_gpt4o_models_changed_recently/) , 2024-10-29-0913
```
Has anyone noticed changes in OpenAI models over time? I built a wrapper around GPT-4o a few months ago, and it was givi
ng consistent results during testing. I didn’t touch it for about a month, but now that I’m working on it again, it’s be
having differently. I thought model behavior would only change for the chat app, not the API. Has anyone else experience
d this?
```
---

     
 
all -  [ Where does LangGraph's MemorySaver actually store the threads' data? ](https://www.reddit.com/r/LangChain/comments/1ge01ru/where_does_langgraphs_memorysaver_actually_store/) , 2024-10-29-0913
```
I'm developing a small test project where I have an Agent that answers questions about events and locations in my region
 by searching a database and google for information. I wanted to test how to add thread-level persistance to get it to a
nswer follow-up questions, and I used the base implementation from langgraph-checkpoint as a start. Everything works, bu
t I'm left with a question: where are the checkpoints for the threads actually stored? And how long are they stored for?


I realize this implementation is only meant for testing but given that this project is more of a 'proof of concept' fo
r a uni course at the moment it might be best to keep using this for the moment, so I would like to understand how it wo
rks as much as possible.
```
---

     
 
all -  [ Dynamic JSON in ChatPromptTemplate ](https://www.reddit.com/r/LangChain/comments/1gdw24i/dynamic_json_in_chatprompttemplate/) , 2024-10-29-0913
```
I am building a chat interface where I am mostly dealing with JSONs. You can imagine, the output of the response would c
ontain a JSON which would be injected back into the prompt as an 'assistant' message.

This does not work in Langchain b
ecause now it thinks, it needs to use variables because it detects curly braces {}. Either this simple use case is not s
upported in Langchain (then it's not the right fit for my case) or I am doing something silly. Any help is much apprecia
ted!

Edit: This is in Langchain Python
```
---

     
 
all -  [ Why is Llama failing where OpenAI works just fine? (code) ](https://www.reddit.com/r/learnpython/comments/1gduuuw/why_is_llama_failing_where_openai_works_just_fine/) , 2024-10-29-0913
```
Problem: Openai implementation and Llama implementation code + output provided. OpenAI agent implementation works perfec
tly, calling the search tool thrice as required and providing the complete answer. Llama implementation using my workpla
ce api hosted on fireworks fails to do the same even when the code is completely unchanged, just the model has been chan
ged. it calls the tool once and then stops.

Context: At my workplace I have been told to learn langgraph with agents. I
 started on the agents with langgraph course on [deeplearning.ai](http://deeplearning.ai) , however later i was told to 
use the workplace's fireworks hosted llama model. i am not getting any errors, so i dont even know what to fix here.

\*
\*OpenAI implementation:\*\*

    import os
    import json
    from openai import OpenAI
    from datetime import datet
ime, timedelta
    from dotenv import load_dotenv, find_dotenv
    from langchain_openai import ChatOpenAI
    from lang
chain.schema import HumanMessage, AIMessage,ChatMessage
    # Load environment variables from .env file
    load_dotenv(
)
    _ = load_dotenv(find_dotenv())
    
    # Access the OpenAI API key from environment variables
    # we use only g
pt-4o-mini from now on. yay!
    openai_api_key = os.getenv('OPENAI_API_KEY')
    langchain_api_key = os.getenv('LANGCHA
IN_API_KEY')
    
    # Debug: Print the API key to verify it is loaded correctly (optional, remove in production)
    #
 print(f'API Key: {api_key}')
    
    if openai_api_key is None:
        raise ValueError('API key is not set. Please s
et the OPENAI_API_KEY in the .env file.')
    
    # Initialize the OpenAI client
    client = OpenAI(api_key=openai_api
_key)
    
    llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)
    
    from langgraph.graph import StateGraph
, END
    from typing import TypedDict, Annotated
    import operator
    from langchain_core.messages import AnyMessage
, SystemMessage, HumanMessage, ToolMessage
    from langchain_community.tools.tavily_search import TavilySearchResults
 
   
    
    tool = TavilySearchResults(max_results = 2)
    print(type(tool))
    print(tool.name)
    
    class Agent
State(TypedDict):
        messages: Annotated[list[AnyMessage], operator.add]
    
    class Agent:
        def __init__
(self, model, tools, system = ' '):
            self.system = system
            graph = StateGraph(AgentState)
        
    graph.add_node('llm',self.call_openai)
            graph.add_node('action',self.take_action)
            graph.add_c
onditional_edges(
                'llm', 
    # here we set where the conditional edge starts from
                self.
exists_action, 
    # function that will determine where to go from there on
                {
                    
    
# This maps the respose of the function and where it should next go to
                    True : 'action', False : END

                }
            )
            graph.add_edge('action', 'llm')
            graph.set_entry_point('llm')
   
         self.graph = graph.compile()
    
            
    #langchain runnable is ready
    
            self.tools = {
t.name : t for t in tools}
            self.model = model.bind_tools(tools)
    
        def exists_action(self, state: 
AgentState):
            result = state['messages'][-1]
            return len(result.tool_calls)>0
     
        def ca
ll_openai(self, state: AgentState):
            messages = state['messages']
            if self.system:
               
 messages = [SystemMessage(content= self.system)] + messages
            message = self.model.invoke(messages)
         
   print(message)
            return {'messages' : [message]}
        
    # since we annotated messages with operator.a
dd, when we call the above return statement, it doesn't overwrite the messages, but adds to it.
    
        def take_ac
tion(self, state : AgentState):
            tool_calls = state['messages'][-1].tool_calls
            results = []
     
       for t in tool_calls:
                print(f'Calling: {t}')
                result = self.tools[t['name']].invoke
(t['args'])
                results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))
     
       
            print('Back to the model!')
            return {'messages' : results}
        
    prompt = '''You a
re a smart research assistant. Use the search engine to look up information. \
    You are allowed to make multiple call
s (either together or in sequence). \
    Only look up information when you are sure of what you want. \
    If you need
 to look up some information before asking a follow up question, you are allowed to do that!
    '''
    
    abot = Age
nt(model= llm, tools= [tool], system = prompt)
    
    messages = [HumanMessage(content = 'Who won IPL 2023? What is th
e gdp of that state and the state beside that combined?')]
    
    result = abot.graph.invoke({'messages' : messages})

    
    print(result['messages'][-1].content)

\*\*OpenAI output:\*\*  
\`\`\`  
<class 'langchain\_community.tools.tav
ily\_search.tool.TavilySearchResults'>  
tavily\_search\_results\_json  
content='' additional\_kwargs={'tool\_calls': \
[{'id': 'call\_uuUBBnZxDF5yhcCC7zn0ArOu', 'function': {'arguments': '{'query': 'IPL 2023 winner'}', 'name': 'tavily\_sea
rch\_results\_json'}, 'type': 'function'}, {'id': 'call\_mFfUnqm5mISKgr5vAnYlGwu8', 'function': {'arguments': '{'query':
 'GDP of Gujarat 2023'}', 'name': 'tavily\_search\_results\_json'}, 'type': 'function'}, {'id': 'call\_tIDXlc3QuWYdHvrny
Rx9ze3X', 'function': {'arguments': '{'query': 'GDP of Maharashtra 2023'}', 'name': 'tavily\_search\_results\_json'}, 't
ype': 'function'}\]} response\_metadata={'token\_usage': {'completion\_tokens': 84, 'prompt\_tokens': 166, 'total\_token
s': 250, 'prompt\_tokens\_details': {'cached\_tokens': 0}, 'completion\_tokens\_details': {'reasoning\_tokens': 0}}, 'mo
del\_name': 'gpt-4o-mini', 'system\_fingerprint': 'fp\_f59a81427f', 'finish\_reason': 'tool\_calls', 'logprobs': None} i
d='run-04615292-a37e-4558-84d2-6371d835467f-0' tool\_calls=\[{'name': 'tavily\_search\_results\_json', 'args': {'query':
 'IPL 2023 winner'}, 'id': 'call\_uuUBBnZxDF5yhcCC7zn0ArOu', 'type': 'tool\_call'}, {'name': 'tavily\_search\_results\_j
son', 'args': {'query': 'GDP of Gujarat 2023'}, 'id': 'call\_mFfUnqm5mISKgr5vAnYlGwu8', 'type': 'tool\_call'}, {'name': 
'tavily\_search\_results\_json', 'args': {'query': 'GDP of Maharashtra 2023'}, 'id': 'call\_tIDXlc3QuWYdHvrnyRx9ze3X', '
type': 'tool\_call'}\] usage\_metadata={'input\_tokens': 166, 'output\_tokens': 84, 'total\_tokens': 250}  
Calling: {'n
ame': 'tavily\_search\_results\_json', 'args': {'query': 'IPL 2023 winner'}, 'id': 'call\_uuUBBnZxDF5yhcCC7zn0ArOu', 'ty
pe': 'tool\_call'}  
Calling: {'name': 'tavily\_search\_results\_json', 'args': {'query': 'GDP of Gujarat 2023'}, 'id': 
'call\_mFfUnqm5mISKgr5vAnYlGwu8', 'type': 'tool\_call'}  
Calling: {'name': 'tavily\_search\_results\_json', 'args': {'q
uery': 'GDP of Maharashtra 2023'}, 'id': 'call\_tIDXlc3QuWYdHvrnyRx9ze3X', 'type': 'tool\_call'}  
Back to the model!  

content='The winner of IPL 2023 was the \*\*Chennai Super Kings (CSK)\*\*, who defeated the Gujarat Titans by five wicke
ts in the final match held at the Narendra Modi Stadium in Ahmedabad. This victory marked CSK's fifth IPL title. \[More 
details here\]([https://www.iplt20.com/news/3976/tata-ipl-2023-final-csk-vs-gt-match-reportOverall).\\n\\nNow](https://w
ww.iplt20.com/news/3976/tata-ipl-2023-final-csk-vs-gt-match-reportOverall)./n/nNow), regarding the GDP of the states inv
olved:\\n\\n1. \*\*Gujarat\*\*: The GDP of Gujarat for 2023 is estimated to be around ₹2.96 lakh crore (approximately $3
6 billion) based on the budget analysis for 2023-24. \[Source\]([https://prsindia.org/budgets/states/gujarat-budget-anal
ysis-2023-24).\\n\\n2](https://prsindia.org/budgets/states/gujarat-budget-analysis-2023-24)./n/n2). \*\*Maharashtra\*\*:
 The GDP of Maharashtra for 2023-24 is estimated to be around ₹42.67 trillion (approximately $510 billion). \[Source\]([
https://en.wikipedia.org/wiki/Economy\_of\_Maharashtra).\\n\\n###](https://en.wikipedia.org/wiki/Economy_of_Maharashtra)
./n/n###) Combined GDP of Gujarat and Maharashtra:\\n- Gujarat: ₹2.96 lakh crore\\n- Maharashtra: ₹42.67 trillion\\n\\nT
o combine these figures:\\n- Convert Gujarat's GDP to the same unit as Maharashtra's: ₹2.96 lakh crore = ₹2.96 trillion.
\\n- Combined GDP = ₹2.96 trillion + ₹42.67 trillion = ₹45.63 trillion (approximately $550 billion).\\n\\nThus, the comb
ined GDP of Gujarat and Maharashtra is approximately \*\*₹45.63 trillion\*\* (or about \*\*$550 billion\*\*).' response\
_metadata={'token\_usage': {'completion\_tokens': 328, 'prompt\_tokens': 2792, 'total\_tokens': 3120, 'prompt\_tokens\_d
etails': {'cached\_tokens': 0}, 'completion\_tokens\_details': {'reasoning\_tokens': 0}}, 'model\_name': 'gpt-4o-mini', 
'system\_fingerprint': 'fp\_f59a81427f', 'finish\_reason': 'stop', 'logprobs': None} id='run-5ca9fd99-6884-4dc5-9ce6-ce0
156bef852-0' usage\_metadata={'input\_tokens': 2792, 'output\_tokens': 328, 'total\_tokens': 3120}  
The winner of IPL 2
023 was the \*\*Chennai Super Kings (CSK)\*\*, who defeated the Gujarat Titans by five wickets in the final match held a
t the Narendra Modi Stadium in Ahmedabad. This victory marked CSK's fifth IPL title. \[More details here\](https://www.i
plt20.com/news/3976/tata-ipl-2023-final-csk-vs-gt-match-reportOverall).

Now, regarding the GDP of the states involved:


1. \*\*Gujarat\*\*: The GDP of Gujarat for 2023 is estimated to be around ₹2.96 lakh crore (approximately $36 billion) 
based on the budget analysis for 2023-24. \[Source\](https://prsindia.org/budgets/states/gujarat-budget-analysis-2023-24
).
2. \*\*Maharashtra\*\*: The GDP of Maharashtra for 2023-24 is estimated to be around ₹42.67 trillion (approximately $
510 billion). \[Source\](https://en.wikipedia.org/wiki/Economy\_of\_Maharashtra).

\### Combined GDP of Gujarat and Maha
rashtra:  
\- Gujarat: ₹2.96 lakh crore  
\- Maharashtra: ₹42.67 trillion

To combine these figures:  
\- Convert Gujara
t's GDP to the same unit as Maharashtra's: ₹2.96 lakh crore = ₹2.96 trillion.  
\- Combined GDP = ₹2.96 trillion + ₹42.6
7 trillion = ₹45.63 trillion (approximately $550 billion).

Thus, the combined GDP of Gujarat and Maharashtra is approxi
mately \*\*₹45.63 trillion\*\* (or about \*\*$550 billion\*\*).

\`\`\`

\*\*Llama Implementation:\*\*:

    import os
 
   import json
    from openai import OpenAI
    from datetime import datetime, timedelta
    from dotenv import load_do
tenv, find_dotenv
    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage, AIMessage,C
hatMessage
    # Load environment variables from .env file
    load_dotenv()
    _ = load_dotenv(find_dotenv())
    
   
 # Access the OpenAI API key from environment variables
    # we use only gpt-4o-mini from now on. yay!
    openai_api_k
ey = os.getenv('OPENAI_API_KEY')
    langchain_api_key = os.getenv('LANGCHAIN_API_KEY')
    
    # Debug: Print the API 
key to verify it is loaded correctly (optional, remove in production)
    # print(f'API Key: {api_key}')
    
    if ope
nai_api_key is None:
        raise ValueError('API key is not set. Please set the OPENAI_API_KEY in the .env file.')
   
 
    # Initialize the OpenAI client
    client = OpenAI(api_key=openai_api_key)
    
    # llm = ChatOpenAI(model_name=
'gpt-4o-mini', temperature=0)
    llm = ChatOpenAI(
        model='accounts/fireworks/models/llama-v3p1-70b-instruct',
 
       temperature=0,
        api_key=os.getenv('FIREWORKS_API_KEY'),
        base_url='https://api.fireworks.ai/inferen
ce/v1',
    )
    
    from langgraph.graph import StateGraph, END
    from typing import TypedDict, Annotated
    impor
t operator
    from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage
    from langcha
in_community.tools.tavily_search import TavilySearchResults
    
    
    tool = TavilySearchResults(max_results = 2)
  
  print(type(tool))
    print(tool.name)
    
    class AgentState(TypedDict):
        messages: Annotated[list[AnyMessa
ge], operator.add]
    
    class Agent:
        def __init__(self, model, tools, system = ' '):
            self.system
 = system
            graph = StateGraph(AgentState)
            graph.add_node('llm',self.call_openai)
            grap
h.add_node('action',self.take_action)
            graph.add_conditional_edges(
                'llm', 
    # here we set
 where the conditional edge starts from
                self.exists_action, 
    # function that will determine where to
 go from there on
                {
                    
    # This maps the respose of the function and where it should
 next go to
                    True : 'action', False : END
                }
            )
            graph.add_edge(
'action', 'llm')
            graph.set_entry_point('llm')
            self.graph = graph.compile()
    
            
   
 #langchain runnable is ready
    
            self.tools = {t.name : t for t in tools}
            self.model = model.b
ind_tools(tools)
    
        def exists_action(self, state: AgentState):
            result = state['messages'][-1]
   
         return len(result.tool_calls)>0
     
        def call_openai(self, state: AgentState):
            messages = 
state['messages']
            if self.system:
                messages = [SystemMessage(content= self.system)] + message
s
            message = self.model.invoke(messages)
            print(message)
            return {'messages' : [message
]}
        
    # since we annotated messages with operator.add, when we call the above return statement, it doesn't ove
rwrite the messages, but adds to it.
    
        def take_action(self, state : AgentState):
            tool_calls = st
ate['messages'][-1].tool_calls
            results = []
            for t in tool_calls:
                print(f'Calling
: {t}')
                result = self.tools[t['name']].invoke(t['args'])
                results.append(ToolMessage(tool
_call_id=t['id'], name=t['name'], content=str(result)))
            
            print('Back to the model!')
           
 return {'messages' : results}
        
    prompt = '''You are a smart research assistant. Use the search engine to loo
k up information. \
    You are allowed to make multiple calls (either together or in sequence). \
    Only look up info
rmation when you are sure of what you want. \
    If you need to look up some information before asking a follow up ques
tion, you are allowed to do that!
    '''
    
    abot = Agent(model= llm, tools= [tool], system = prompt)
    
    mes
sages = [HumanMessage(content = 'Who won IPL 2023? What is the gdp of that state and the state beside that combined?')]

    
    result = abot.graph.invoke({'messages' : messages})
    
    print(result['messages'][-1].content)

\*\*Llama O
utput:\*\*

\`\`\`  
<class 'langchain\_community.tools.tavily\_search.tool.TavilySearchResults'>  
tavily\_search\_resu
lts\_json  
content='' additional\_kwargs={'tool\_calls': \[{'id': 'call\_JurtcbX3QsXqxPS9RJ0aCGAU', 'function': {'argum
ents': '{'query': 'IPL 2023 winner'}', 'name': 'tavily\_search\_results\_json'}, 'type': 'function', 'index': 0}\]} resp
onse\_metadata={'token\_usage': {'completion\_tokens': 27, 'prompt\_tokens': 304, 'total\_tokens': 331}, 'model\_name': 
'accounts/fireworks/models/llama-v3p1-70b-instruct', 'system\_fingerprint': None, 'finish\_reason': 'tool\_calls', 'logp
robs': None} id='run-4ec44c44-5970-44b5-b10b-e41ac47f35de-0' tool\_calls=\[{'name': 'tavily\_search\_results\_json', 'ar
gs': {'query': 'IPL 2023 winner'}, 'id': 'call\_JurtcbX3QsXqxPS9RJ0aCGAU', 'type': 'tool\_call'}\] usage\_metadata={'inp
ut\_tokens': 304, 'output\_tokens': 27, 'total\_tokens': 331}  
Calling: {'name': 'tavily\_search\_results\_json', 'args
': {'query': 'IPL 2023 winner'}, 'id': 'call\_JurtcbX3QsXqxPS9RJ0aCGAU', 'type': 'tool\_call'}  
Back to the model!  
co
ntent='The winner of IPL 2023 is Chennai Super Kings.' response\_metadata={'token\_usage': {'completion\_tokens': 13, 'p
rompt\_tokens': 1004, 'total\_tokens': 1017}, 'model\_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct', 'syste
m\_fingerprint': None, 'finish\_reason': 'stop', 'logprobs': None} id='run-bb29ca04-b059-4c64-8692-ee6e02a270dc-0' usage
\_metadata={'input\_tokens': 1004, 'output\_tokens': 13, 'total\_tokens': 1017}  
The winner of IPL 2023 is Chennai Supe
r Kings.  
\`\`\`
```
---

     
 
all -  [ Classification/Named Entity Recognition using DSPy and Outlines ](https://www.reddit.com/r/LangChain/comments/1gds8ko/classificationnamed_entity_recognition_using_dspy/) , 2024-10-29-0913
```
In this post, I will show you how to solve classification/name-entity recognition class of problems using DSPy and Outli
nes (from [dottxt](https://dottxt.co/)) . This approach is not only ergonomic and clean but also guarantees schema adher
ence.

Let's do a simple boolean classification problem. We start by defining the DSPy signature.

https://preview.redd.
it/jj7zy8s4vexd1.png?width=1102&format=png&auto=webp&s=11dcf805d5249597e576ba5623b962ad58f80d5c

Now we write our progra
m and use the ChainOfThought optimizer from DSPy's library.

https://preview.redd.it/9jy3zc26vexd1.png?width=1334&format
=png&auto=webp&s=9328ae01f8d47b9093d27b2a75bce706d4ff12e7

  
Next, we write a custom dspy.LM class that uses the outlin
es library for doing text generation and outputting results that follow the provided schema.

https://preview.redd.it/gf
47tri7vexd1.png?width=1306&format=png&auto=webp&s=1ca835a86aadfa6ddc941489e8ec2c0ee7cbac7d

Finally, we do a two pass ge
neration to get the output in the desired format, boolean in this case.

1. First, we pass the input passage to our dspy
 program and generate an output.
2. Next, we pass the result of previous step to the outlines LM class as input along wi
th the response schema we have defined.

https://preview.redd.it/q5gns589vexd1.png?width=936&format=png&auto=webp&s=9f75
745b06f971899b8df960cb57ccbfdc1d307e

That's it! This approach combines the modularity of DSPy with the efficiency of st
ructured output generation using outlines built by [dottxt](https://dottxt.co/). You can find the full source code for t
his example [here](https://github.com/Scale3-Labs/dspy-examples/tree/main/src/structured_output). Also, I am building an
 open source observability tool called Langtrace AI which supports DSPy natively and you can use to understand what goes
 in and out of the LLM and trace every step within each module deeply.
```
---

     
 
all -  [ I tested what small LLMs (1B/3B) can actually do with local RAG - Here's what I learned ](https://www.reddit.com/r/LocalLLaMA/comments/1gdqlw7/i_tested_what_small_llms_1b3b_can_actually_do/) , 2024-10-29-0913
```
Hey r/LocalLLaMA 👋！

Been seeing a lot of discussions about small LLMs lately ([this thread](https://www.reddit.com/r/Lo
calLLaMA/comments/1gbwvqg/does_anyone_even_use_the_1b_or_3b_32_llama) and [this one](https://www.reddit.com/r/LocalLLaMA
/comments/1g3pkc2/besides_coding_and_chatting_how_do_you_use_llms/)). I was curious about what these smaller models coul
d actually handle, especially for local RAG, since lots of us want to chat with documents without uploading them to Clau
de or OpenAI.

I spent some time building and testing a local RAG setup on my MacBook Pro (M1 Pro). Here's what I found 
out:

# The Basic Setup

* Nomic's embedding model
* Llama3.2 3B instruct
* Langchain RAG workflow
* Nexa SDK Embedding 
& Inference
* Chroma DB
* [Code & all the tech stack on GitHub if you want to try it](https://github.com/NexaAI/nexa-sdk
/tree/main/examples/Chat-with-PDF-locally)

# The Good Stuff

Honestly? Basic Q&A works better than I expected. I tested
 it with Nvidia's Q2 2025 financial report (9 pages of dense financial stuff):

[Asking two questions in a single query 
- Claude vs. Local RAG System](https://i.redd.it/z9mmi51fcexd1.gif)

* PDF loading is crazy fast (under 2 seconds)
* Sim
ple info retrieval is slightly faster than Claude 3.5 Sonnet (didn't expect that)
* It handles combining info from diffe
rent parts of the same document pretty well

If you're asking straightforward questions like 'What's NVIDIA's total reve
nue?' - it works great. Think of it like Ctrl/Command+F on steroids.

# Where It Struggles

No surprises here - the smal
ler models (Llama3.2 3B in this case) start to break down with complex stuff. Ask it to compare year-over-year growth be
tween different segments and explain the trends? Yeah... it start outputting nonsense.

# Using LoRA for Pushing the Lim
it of Small Models

Making a search-optimized fine-tuning or LoRA takes lots of time. So as a proof of concept, I traine
d specific adapters for generating pie charts and column charts. Think of it like giving the model different 'hats' to w
ear for different tasks 🎩.

For handling when to do what, I'm using [Octopus\_v2 action model](https://huggingface.co/Ne
xaAIDev/Octopus-v2) as a task router. It's pretty simple:

* When it sees `<pdf>` or `<document>` tags → triggers RAG fo
r document search
* When it sees 'column chart' or 'pie chart' → switches to the visualization LoRA
* For regular chat →
 uses base model

And surprisingly, it works! For example:

1. Ask about revenue numbers from the PDF → gets the data vi
a RAG
2. Say 'make a pie chart' → switches to visualization mode and uses the previous data to generate the chart

[Gene
rate column chart from previous data, my GPU is working hard](https://i.redd.it/ywhb69z29exd1.gif)

[Generate pie chart 
from previous data, plz blame Llama3.2 for the wrong title](https://i.redd.it/d0fq2da79exd1.gif)

The LoRAs are pretty b
asic (trained on small batches of data) and far from robust, but it hints at something interesting: you could potentiall
y have one small base model (3B) with different LoRA 'plugins' for specific tasks in a local RAG system. Again, it is ki
nd of like having a lightweight model that can wear different hats or shoes when needed.

# Want to Try It?

I've open-s
ourced everything, [here is the link again](https://github.com/NexaAI/nexa-sdk/tree/main/examples/Chat-with-PDF-locally)
. Few things to know:

* Use `<pdf>` tag to trigger RAG
* Say 'column chart' or 'pie chart' for visualizations
* Needs a
bout 10GB RAM

# What's Next

Working on:

1. Getting it to understand images/graphs in documents
2. Making the LoRA swi
tching more efficient (just one parent model)
3. Teaching it to break down complex questions better with multi-step reas
oning or simple CoT

# Some Questions for You All

* What do you think about this LoRA approach vs just using bigger mod
els?
* What will be your use cases for local RAG?
* What specialized capabilities would actually be useful for your docu
ments?
```
---

     
 
all -  [ text 2 sql architecture pattern and issues ](https://www.reddit.com/r/LangChain/comments/1gdq4u3/text_2_sql_architecture_pattern_and_issues/) , 2024-10-29-0913
```
Hi everyone,

I’m working on a text-to-SQL solution for a data warehouse that contains around 20 tables. To streamline a
ccess, we’ve created about 10 consolidated views, allowing our users to query across different data segments efficiently
. Here’s an overview of our current setup, our solution approach, and the main challenges we’re looking to tackle.

**Cu
rrent Solution Structure:**

1. **Team-Specific Views and Flexibility**: Our users come from various teams like sales an
d marketing. To cater to their specific needs, we assign each team an application ID. This lets us create tailored views
 on top of the same underlying tables, so each team has a customized perspective without duplicating data or impacting t
he original structure.
2. **Embedding and Retrieval Approach**: We embed column names, table names, and descriptions in 
OpenSearch. By using small-3 embeddings, we leverage OpenSearch’s built-in k-nearest neighbors (k-NN) retrieval to ident
ify the top 5 relevant tables for each query. This has allowed us to achieve an accuracy rate of about 70% so far.
3. **
Accuracy Enhancements with Entity Recognition**: To improve this 70% accuracy without increasing latency, we’re explorin
g the integration of named entity recognition (NER) and fine-tuning our SQL database to ensure efficient query processin
g and higher precision in retrieval.

**Key Challenges and Questions:**

1. **Ambiguity Handling**: We’re looking for a 
way to handle ambiguous user queries effectively. Ideally, our system would recognize unclear questions and prompt users
 for clarification, ensuring accurate SQL generation for complex or vague input.
2. **End-User Feedback Utilization**: O
ur primary users are non-SQL-savvy and rely on our system to auto-generate SQL queries, though they can validate query r
esults. We’ve provided a feedback option, but we’re unsure how best to leverage this input to systematically improve the
 SQL outputs. Any strategies to incorporate their feedback into a “good SQL” model would be highly useful.
3. **“Good SQ
L” Database Management**: To manage reliable SQL generation, we’re considering building a structured “good SQL” database
:

* **User-Specific SQL**: SQL marked as effective by individual users, though this has challenges, as user feedback ca
n sometimes be inconsistent.
* **Global Good SQL DB**: A general repository of reliable SQL queries that can serve as a 
primary source for top-k matches when users query.

The ideal solution would allow us to first query a user’s SQL feedba
ck before moving to the global “good SQL” database, helping to improve relevance. Any advice on managing these feedback 
databases effectively or implementing a robust matching mechanism for improved accuracy would be incredibly valuable.

I
f anyone has encountered similar issues in text-to-SQL solutions, especially in balancing accuracy improvements with lat
ency constraints, I’d love to hear your insights!
```
---

     
 
all -  [ I built an open-source Desktop app to let Claude control your computer ](https://www.reddit.com/r/LangChain/comments/1gdp3h0/i_built_an_opensource_desktop_app_to_let_claude/) , 2024-10-29-0913
```
https://reddit.com/link/1gdp3h0/video/3s10nv8x1exd1/player

 
```
---

     
 
all -  [ How to build a multi-agent app to support structured and unstructured data query? ](https://www.reddit.com/r/LangChain/comments/1gdjd7v/how_to_build_a_multiagent_app_to_support/) , 2024-10-29-0913
```
I am looking to build an app that can query both structured and unstructured data sources - mostly a user question would
 either point to structured or unstrctured data sources and app would need to make a decision where it needs to go.

I e
nvision this as a multi-agent app with an agent for structured data and an agent for unstructured data with a supervisor
 agent for deciding which sub-agent to invoke.

Has anyone experimented or built something like this using LangGraph and
 OpenAI?
```
---

     
 
all -  [ Need help with some PromptEngineering basics on Open WebUI... ](https://www.reddit.com/r/LocalLLM/comments/1gdf7mq/need_help_with_some_promptengineering_basics_on/) , 2024-10-29-0913
```
So I've followed the steps by NetworkChuck in his Youtube [video](https://www.youtube.com/watch?v=Wjrdr0NU4Sk). and now 
I have a local LLM in my computer. But now I'm trying to make my own model based on Ollama3.1:8b, but with a specific sy
stem prompt. So I have a couple of questions:

1. What's the difference between the Model's system prompt, and the RAG T
emplate?
2. Is there a specific framework I should use for system prompts, or is the Co-Star Framework pretty much works
? Does it make a difference if I use hashtags or XML tags?
3. What's the use of the 'Chain of thought' part Or does it n
ot do anything (refer to my system prompt below)? I basically got inspired from [https://openwebui.com/p/varunmahajan/sy
stem-prompt-generator](https://openwebui.com/p/varunmahajan/system-prompt-generator)
4. I'm trying to figure out how to 
tell it NOT to answer anything, unless I give it a document to summarize, or at least, ask the user if they want to give
 a general information.  Can you tell me what I did wrong there?
5. I'm trying to test if my system prompt make sense. W
hat tool can I use other than [LangSmith PlayGround](https://smith.langchain.com/hub/ohkgi/superb_system_instruction_pro
mpt/playground)? (or maybe it doesn't actually make sense? Because there's no Ollama there...

&#8203;

    #####
    # 
CONTEXT #
    You are an AI assistant called Summary Sam designed to extract key information from documents provided by 
users which helps the user summarize the information from the given file(s), and/or explain what the information in the 
documentation.
    #####
    # OBJECTIVE #
    To create an understandable explanation, and answer any information that 
the user asked, based on the given documentation. If no documentation is provided by the user, respond with 'I cannot an
swer you. Please give me a document to summarize first'.
    #####
    # STYLE #
    Blend technical accuracy with appro
achable language, clear and concise, suitable to be read by professional people. Maintain a neutral tone, and do not mak
e any sound that's too much like a sales or marketing pitch.
    #####
    # AUDIENCE #
    Tailor the output towards wo
rking professionals and academics who are seeking to understand a documentation or research paper and are looking for fa
cts and key points.
    #####
    # RESPONSE #
    ## RULES OF ENGAGEMENT WITH USERS ##
    - **STRUCTURE** your prompt 
clearly, with precision, and according to the complexity suitable for the model size.
    - **MAINTAIN CONSISTENCY** in 
by answering in the same language as the user query.
    - If the source is not the same language as the user query, do 
not attempt to translate it, but instead quote the information directly, but form it in an understandable summary, and e
xplain it in the same language as the user query.
    - If the user doesn't give any documentation to summarize, then ap
ologize, before answering with 'I cannot answer you. Please give me a document to summarize first'
    - If the user doe
sn't give any documentation to summarize, don't give information about general knowledge, before the user gives back the
ir answer, that they want you to give it a general knowledge answer, with the timestamp where you had achieved the gener
al knowledge answer.
    - If the answer is not clear, ask the user for clarification to ensure accurate response.
    #
# CHAIN OF THOUGHTS ##
    1. Clearly enumerate your explanation, on why you give such summary
    2. Always give out re
ferences on where you had summarized the information, complete with page number, as well as line number when possible.
 
   #####
```
---

     
 
all -  [ Best Approach to Building a Chatbot with Twitter Data Using LLMs (LLaMA 3.2)? ](https://www.reddit.com/r/developersIndia/comments/1gdf4jj/best_approach_to_building_a_chatbot_with_twitter/) , 2024-10-29-0913
```
**Hello everyone,**

I'm currently working on analyzing customer support inquiries from various insurance companies (twi
tter handles) and generating questions from these tweets using LLaMA 3.2. The dataset includes both full conversation an
d tweet-level formats, containing customer support inquiries.

Now, I'm looking to take it a step further and build a ch
atbot that can:

1. Answer customer queries based on the patterns found in the historical tweets. (Currently doing manua
lly)
2. Utilize the questions I've already generated.
3. Learn from ongoing interactions with users to improve its respo
nses over time.

Given the data I have and my experience working with LLMs, what would be the best way to approach build
ing this chatbot? Here are a few specifics I'm curious about:

* What framework or tools (open-source or otherwise) woul
d work well for this kind of chatbot development?
* How can I integrate LLaMA 3.2 (or another model, if recommended) to 
handle real-time question generation and answering?
* How should I structure the chatbot's learning process to continuou
sly improve its responses from new tweets or user interactions?

Any suggestions on architecture, training strategies,RA
Gs or frameworks (like Rasa, Langchain, etc.) would be greatly appreciated. Thank you!
```
---

     
 
all -  [ I created a Claude Computer Use alternative to use with OpenAI and Gemini, using Langchain and open- ](https://i.redd.it/5rtd09dycbxd1.jpeg) , 2024-10-29-0913
```

github: https://github.com/Clevrr-AI/Clevrr-Computer

The day Anthropic announced Computer Use, I knew this was gonna b
low up, but at the same time, it was not a model-specific capability but rather a flow that was enabling it to do so. 


I it got me thinking whether the same (at least upto a level) can be done, with a model-agnostic approach, so I don’t ha
ve to rely on Anthropic to do it. 

I got to building it, and in one day of idk-how-many coffees and some prototyping, I
 built Clevrr Computer - an AI Agent that can control your computer using text inputs. 

The tool is built using Langcha
in’s ReAct agent and a custom screen intelligence tool, here’s how it works. 

- The user asks for a task to be complete
d, that task is broken down into a chain-of-actions by the primary agent. 
- Before performing any task, the agent calls
 the `get_screen_info` tool for understanding what’s on the screen. 
- This tool is basically a multimodal llm call that
 first takes a screenshot of the current screen, draws gridlines around it for precise coordinate tracking, and sends th
e image to the llm along with the question by the master agent. 
- The response from the tool is taken by the master age
nt to perform computer tasks like moving the mouse, clicking, typing, etc using the `PyAutoGUI` library.

And that’s how
 the whole computer is controlled. 

**Please note that this is a very nascent repository right now, and I have not enab
led measures to first create a sandbox environment to isolate the system, so running malicious command will destroy your
 computer, however I have tried to restrict such usage in the prompt**

Please give it a try and I would love some quali
ty contributions to the repository!
```
---

     
 
all -  [ Paper to podcast using LangChain ](https://www.reddit.com/r/generativeAI/comments/1gdco7f/paper_to_podcast_using_langchain/) , 2024-10-29-0913
```
I have built this small open-source app using LangChain and Openai API and I want you guys to give me feedback about it.

It basically takes a research paper and turns it into an engaging podcast between 3 persons:
- host: present the paper 
and directs the discussion.
- learner: asks interesting questions about the paper.
- researcher: have a lot of knowledge
, comments and explaind complex concepts.
This is perfect for people who like podcasts and enjoy listening to papers whi
le traveling.
You need an OpenAI Key to make it work, and it costs ~0.19$ for a ~16 pages paper.
Feel free to roast me, 
I really need to improve 💪
Link: https://github.com/Azzedde/paper_to_podcast/tree/main
```
---

     
 
all -  [ How to get Django back rnd internship/job in so much saturation? ](https://i.redd.it/xu1vtquzpaxd1.jpeg) , 2024-10-29-0913
```
I have been trying to get a job/internship but no luck

In my city,there are barely any companies/software houses meanwh
ile the recent chatgpt shit blew up this field
There were total 3500 students that took admission in my uni in computer 
field

And there would be barely like 100 or something software houses

How do i get noticed? Its not like i got selecte
d for the interview
I am not even being called for internviews despite being 
Well skilled and a good student

Attaching
 an image of skills so you guys can see i atleast got enough to get an interview
```
---

     
 
all -  [ EmailSnap - Empower Your Email Routine with LLM Agents ](https://www.reddit.com/r/u_ilbets/comments/1gd2nnb/emailsnap_empower_your_email_routine_with_llm/) , 2024-10-29-0913
```
Nowadays, almost every service revolves around communication, whether it’s plumbers, receptionists at clinics, or clerks
 in banks. Every day, countless people read emails, review attached scanned documents, and PDFs; and sort, organize, and
 forward them elsewhere. Imagine how much time we could save if we automated this process! With the advancements in Gene
rative AI and Large Language Models (LLMs), it’s no longer just a dream — it’s possible now.

https://preview.redd.it/i0
dduokz28xd1.jpg?width=1792&format=pjpg&auto=webp&s=592ef68deb152ee99d4d3736b3219bf4e5aee935

# Problem ✨ Solution

You r
eceive an email with a long thread of messages and attached files — many of them with random names. *What do you really 
want?* To quickly understand what the email is about and know what actions to take.

That’s where EmailSnap comes in. Si
mply forward the email to [**review@emailsnap.app**](mailto:review@emailsnap.app), and [**EmailSnap**](https://emailsnap
.app/) will apply AI to analyze the email and its attachments, then send you back a new, actionable, and well-formatted 
response.

The processed email contains:

* **Title:** \[Priority\] Subject
* **From:** Sender of the email
* **To:** Re
cipient
* **Highlights:** Key action items in bullet points
* **Summary:** A concise review of the email content and att
ached files
* **Attachments:** Renamed to reflect their actual content

# [EmailSnap.app](http://EmailSnap.app) 📩

The i
mplementation consists mainly of two parts: the underlying infrastructure and the LLM flow. Let’s start with the more in
teresting part.

# AI Review ✨

For the LLM implementation, we use the LangChain stack: **LangChain**, and **LangSmith**
. Despite not using LangGraph itself, I apply the same graph-like idea for executing EmailSnap’s LLM processing, as it r
equires multiple LLM calls with different prompts while using various tools, like saving intermediate states to a databa
se and S3.

Our execution graph contains the following nodes:

* **Format Email:** Reads the EML file and converts it in
to well-formatted text, handling threads and forwarded messages while removing redundant HTML tags and other irrelevant 
information.
* **Read Attachments:** If the attached file is an image or PDF, we convert it to Base64 for the LLM to rea
d the content. For PDFs, each page is converted into a new image, potentially creating a long loop.
* **Review Attachmen
t:** This node creates a summary with highlights and action items from each document, suggests the recipient, and propos
es a better file name.
* **Email Summary:** The final node combines the formatted email and the summaries from all attac
hments to generate a complete email with insights, action items, highlights, and an overall summary.

After the final co
ntent is generated, we will send out a new email back to the sender including reorganized attachments.

https://preview.
redd.it/13kb3pl338xd1.jpg?width=580&format=pjpg&auto=webp&s=715202d38b4149ab29879e45fa05329ac79a46d4

# Sample

Finally,
 let’s take a look at a real example. I took an MRI report in PDF format from an online source, sent it to one email add
ress, and then forwarded it to [**review@emailsnap.app**](mailto:review@emailsnap.app). EmailSnap generated a new subjec
t, highlights, and summary. It also extracted the recipient from the PDF — **Dr. Ross Banner** — and finally suggested a
 new file name: **Regina Doe MRI Report**, based on the patient’s name and procedure.

https://preview.redd.it/znbrr37a3
8xd1.jpg?width=2694&format=pjpg&auto=webp&s=b14dc7a43c8d757861f0abce7f93dc78ef6a638d

# Interested?

Interested in the i
dea? ping me at [help@emailsnap.app](mailto:help@emailsnap.app) 
```
---

     
 
all -  [ agentic ASI ](https://www.reddit.com/r/LangChain/comments/1gcuxgs/agentic_asi/) , 2024-10-29-0913
```
I realized that superintelligence is already appearing today in a modular form. And what if the modules could be LangGra
ph agents? 

For example calculator: superintelligent in its narrow field of making calculations. Also easily implementa
ble to LangGraph as a tool. It can help the main LLM with calculations. 

What if this approach can be scaled? Can we im
plement other modules, that would create a superintelligence (or at least something similarly intelligent) together? 

I
 saw an interview with Demis Hassabis, where he spoke about implementing some 'tools' directly into Gemini and letting o
thers be called by Gemini. This resonated with me, because calling tools is very solved today. The first part about impl
ementing directly into the model is different as only few companies can do this. 

But could be stick with the tool call
ing (or communication between agents) only to create a superintelligence? How could we intertwine the thinking of the mo
dules/agents? 

Could we share some ideas about this please? 
```
---

     
 
all -  [ Open Source NotebookLM Podcast API seeking Contributors ](https://www.reddit.com/r/OpenSourceAI/comments/1gcqdcg/open_source_notebooklm_podcast_api_seeking/) , 2024-10-29-0913
```
I love NotebookLM 'Deep Dives' audio generation; it's really a new UI/UX for LLMs. However, I wished there were an API s
o I could automated things instead of being tied to Google's UI.

So I built an open source Python package for it:

[htt
ps://github.com/souzatharsis/podcastfy](https://github.com/souzatharsis/podcastfy)

It uses langchain for LLM management
, llamafile to enable running llms locally and it integrates with several text-to-speech models.  It is multimodal, mult
ilingual and fully customizable.

The project already reached thousands of downloads and it's in a point that would bene
fit from additional contributors! If you are excited about this kind of problem, we would love your help!
```
---

     
 
all -  [ Need Guidance on whether to sit and study from scratch or just wing it.  ](https://www.reddit.com/r/developersIndia/comments/1gcm2y9/need_guidance_on_whether_to_sit_and_study_from/) , 2024-10-29-0913
```
Context :  
0. I graduated from NIT in Mechanical Engineering. But I always had an idea that I would want my career in S
oftware industry. I have no DSA knowledge nor practice. So only training I had was after college in Java & SQL.

1. I am
 working in the software industry from the past 4 years. Currently I am working as an langchain developer from the past 
six months.
2. My background : 3 Years in one service based company. Majorly worked on an internal tool and on the side 
learnt java stack. Got hands on properly for 1 year. Then resigned and joined another company as the stack in my previou
s company was legacy and I wanted hands on learning in latest java stack.
3. Gave and cleared a lot of interviews(Servic
e Based) by just studying the most asked interview questions and practice questions and got hired.
4. So after joining a
nother company, I was excited and was provided with amazing opportunities with great work. But soon I could see that I a
m not able to catch up. And I could also understand that since my base and hands on is not a lot, I could not take compl
ex stories and work at the speed that was expected at my experience.
5. So after working for 6 months, I was assigned a 
fresh project using Langchain. Since it is GenAI, I accepted and now I am comfortable in it. But I want my core strong s
kill to be the java stack.

My main question is :

As this stage in my life, should I invest time to study everything fr
om scractch and spend maybe 4-6 months to build a strong base or just wing it by getting more and more practice by just 
building YT projects and then just that the java coding becomes a habit rather than deep understanding. I feel like I wi
ll get by for another few years just by winging it but that makes me cautious and take a step back and sit and learn eve
rything from scratch. I am super confused.

PS : I am also married since the past 1 year and that is also a facet in my 
life.

I am just hoping for some guidance from the folks here.
```
---

     
 
all -  [ How do I get this menu in Safari? ](https://www.reddit.com/r/VisionPro/comments/1gcibho/how_do_i_get_this_menu_in_safari/) , 2024-10-29-0913
```
https://preview.redd.it/w1e9yf21y2xd1.png?width=1920&format=png&auto=webp&s=8d5881da24543947c7c6809b3c0b1e8dede90fdd

  

I've randomly got this menu in Safari but I'm not sure how I did. I didn't even know that this type of a menu exists in
 Safari. Does anyone know how to get that menu? Thanks! 
```
---

     
 
all -  [ AI SQL Agent not working or hallucinating like mad ](https://www.reddit.com/r/n8n/comments/1gchmi4/ai_sql_agent_not_working_or_hallucinating_like_mad/) , 2024-10-29-0913
```
Trying to use this workflow for [sql ai agent](https://n8n.io/workflows/2292-talk-to-your-sqlite-database-with-a-langcha
in-ai-agent/) but it’s not working.  
Tried with Ollama and Gemini, both do not work.

Error:  
Could not parse LLM outp
ut: Based on the provided information, here is a rewritten version of the query with improvements: `sql SELECT DISTINCT 
Title FROM albums WHERE ArtistId = 1;` Changes made: \* Added `DISTINCT` to remove duplicate titles from the result set.
 \* Removed the `LIMIT 10` clause, as it’s not necessary in this case. If you need to limit the number of results for so
me reason, consider adding it with a comment explaining its purpose. This query is more efficient and concise than the o
riginal version, and it achieves the same goal: retrieving unique titles from the “albums” table where the ArtistId is 1
. However, without knowing the exact data types of ArtistId and Title, or having access to the actual database schema an
d sample rows, it’s difficult to provide a more tailored solution. But, using PostgreSQL’s syntax, you can also use doub
le quotes for quoting table names: `sql SELECT DISTINCT ''Title'' FROM 'albums' WHERE ArtistId = 1;` Or, if you want to 
avoid the double quotes altogether: `sql SELECT DISTINCT Title FROM albums WHERE ArtistId = 1;` In this case, PostgreSQL
 will interpret `Albums` as a table name automatically. If you want to use subqueries or JOINs for more complex queries,
 I can provide examples and explanations for those cases as well. Just let me know!

https://preview.redd.it/s0au808bp2x
d1.png?width=690&format=png&auto=webp&s=056a4b098a422c24c2e42407c8055cdcdea2c054

I then tried with Gemini model, and it
’s just straight up making things up:

https://preview.redd.it/p6juk9ccp2xd1.png?width=690&format=png&auto=webp&s=023570
ab301727bea1d06bd93edf61460bddd609

[Real db data](https://preview.redd.it/fuhum7afp2xd1.png?width=1324&format=png&auto=
webp&s=877b0d08b1545d5f61f0e82471efe0886dbae47a)


```
---

     
 
all -  [ (Very skilled) 3D artist seeking advice for career change to cybersecurity. (Study program advice). ](https://www.reddit.com/r/ITCareerQuestions/comments/1gcg6d9/very_skilled_3d_artist_seeking_advice_for_career/) , 2024-10-29-0913
```
I qualify for a 14 month program fully paid by the government. I'd like to know your 
thoughts about this program, given
 the length of the duration. I’m a 3D artist with spectacular skills, but I feel AI is taking over careers to do with ar
t.


Certificates:

Google IT Support Professional Certificate
Google Cybersecurity Professional Certificate
CompTIA Sec
urity+
CompTIA Network+
CompTIA A+
IHK Berlin - Operative Professionals

Concepts covered:

Python Fundamentals: Learn t
he basics of programming, including syntax,
data types, and simple operations.

Algorithmic Thinking: Develop problem-so
lving and logic-building skills
using algorithms.

Looping: Learn how to create repetition in your code using for loops.


Intro to HTML + CSS: The basic building blocks of web pages.

Strings and Lists: Learn about two sequential data types
 in Python.

Functions: Creating reusable code blocks and understanding how
functions work.


Technologies:

Python
HTML

CSS
Git
Command Line Interface


AI for Cybersecurity, technologies and frameworks:

OWASP Top 10 for
LLM Applications

Large Language
Models (LLMs)
Perplexity
MITRE ATLAS
OpenRouter
ChatGPT, Claude, Gemini
LangChain
Microsoft Copilot for S
ecurity
Prompt engineering
Gradio and Streamlit


Concepts covered:

Foundations of AI in Cybersecurity: Introduction to
 AI and ML in cybersecurity,
LLM fundamentals, MITRE ATLAS, OWASP Top 10 for LLM Applications, ENISA
AI Resources, NIST 
AI Risk Management Framework, and ethical considerations.

Threat Detection and Management: AI for anomaly detection and
 pattern
recognition, AI-powered intrusion detection systems.

Security Operations: AI-driven SIEM and log analysis, aut
omated incident
response using AI, and AI for threat hunting and intelligence.

Risk Assessment and Compliance: AI for s
ecurity compliance automation, risk
assessment and analysis using machine learning, and AI in policy enforcement
and mon
itoring.

Advanced Prompt Engineering for IT Security: Prompt engineering
fundamentals, LLM settings optimization, zero-
shot and few-shot prompting
techniques, meta prompting and prompt chaining strategies, Tree of Thoughts
methodology, and
 security-specific prompt examples.

AI for User Support and Problem-Solving: Implementing AI for IT support,
AI-driven 
troubleshooting and diagnostics, and automated problem resolution
using machine learning.

AI Tools and Platforms for Cy
bersecurity: Microsoft Copilot for Security,
Perplexity.ai for research and analysis, capabilities and use cases of Clau
de,
ChatGPT, and Gemini, and custom GPT creation for specialized security tasks.

Data Analysis and Insights: Anomaly de
tection in large datasets and predictive
analytics for threat forecasting.

AI Application Development for Cybersecurity
: Python programming for AI
security applications, LangChain Functions, Tools, and Agents), Gradio and
Streamlit for bui
lding AI security dashboards, and semantic search
implementation.

Advanced LLM Techniques: RAG Retrieval-Augmented Gene
ration), prompt
caching, embeddings, fine-tuning, and function calling in LLMs.

Security Automation: Developing AI-powe
red security scripts, command line
AI completions for security tasks, and automating vulnerability management
with AI.



If you’ve read this far, I thank you for your time and I'd appreciate any advice/suggestion.
```
---

     
 
all -  [ How to scrape URLs faster with WebbaseLoader/SeleniumURLLoader? ](https://www.reddit.com/r/LangChain/comments/1gcfd6g/how_to_scrape_urls_faster_with/) , 2024-10-29-0913
```
I have designed a multi agents RAG using Langgraph, based on query it diverts question to RAG or Web search, using Googl
e Serper, I'm getting the metadata and extracting links from there and those links (for example 5 links) I'm passing to 
SeleniumURLLoader(urls).load after that I'm storing the scraped content into vectorstore and retrieving relevant content
 based on query

But this process takes 1-2 minutes, I debug each step, and found that most of the time is going on in s
craping the websites, embeddings are being created within 1-2s

How can I speed up the process?

async functions will he
lp? Or parallel processing of each link to a different agent (creating 5 different agents and each of them handling 1 li
nk simultaneously)

(I'm aware of the approach, instead of scraping the website, use metadata from Google Serper, but it
 doesn't have detailed snippets that I'm looking for, so scraping is the only option I have)

(Also I can't use paid cra
wlers, if any of you have better scraper/crawler which is free, kindly suggest those)
```
---

     
 
all -  [ A Simple Implementation of Automatic Prompt Generation using DSPy ](https://www.reddit.com/r/LangChain/comments/1gc3oo2/a_simple_implementation_of_automatic_prompt/) , 2024-10-29-0913
```
A simplified implementation of 'automatic prompt generation' using the techniques used in DSPy's MIPROv2 optimizer. This
 program uses the gsm8k dataset consisting of math problems and is made up of 3 modules: This program is made up of 3 mo
dules:

1. Module 1 generates demos for the prompt
2. Module 2 generates an instruction for the prompt
3. Module 3 uses 
the outputs of module 1 & 2 to generate the final prompt.

**Module 1**

This module takes a labeled training data set a
nd generates 2 (`NUM_SETS`) sets of 10 demos each:

* 5 demos are directly sampled from the dataset
* 5 demos are genera
ted using the model and satisfies the metric i.e. generated output = expected output

https://preview.redd.it/fqda7rjwmy
wd1.png?width=1462&format=png&auto=webp&s=c2f86e2e7aaf03f6b40a0a6808dc4197f3d8d475

**Module 2**

This module takes the 
2 sets of 10 demos generated in Step 1 along with a string representation of the application code i.e. the code of this 
program and generates 2(`NUM_INSTRUCTIONS`) different instructions by

* Identifying the class of problems using the dem
os
* Identifying the intent of user using the program semantics

https://preview.redd.it/b846jbrymywd1.png?width=1390&fo
rmat=png&auto=webp&s=f919bad00b2a91eb953798e40ef896a21374c54b

**Module 3**

In this final step, it takes the outputs fr
om the previous steps as inputs and generates two different final prompts (since we have 2 sets of 10 demos from step 1 
and 2 instructions from step 2).

https://preview.redd.it/unyb65l0nywd1.png?width=1490&format=png&auto=webp&s=2d6bc6122a
15141ec51646f6affec47ecfa777c5

**Conclusion**

That's how you can generate prompt candidates using DSPy. Note that we s
tarted purely with a bunch of labeled datasets and nothing else. If you are curious to dive deep and understand more abo
ut this prompt optimization technique, check out the research paper [here](https://arxiv.org/pdf/2406.11695). If you wou
ld like to start using this optimizer, check out the dspy docs [here](https://dspy-docs.vercel.app/deep-dive/optimizers/
miprov2/).

**Source Code**

You can find the full source code for this example - https://github.com/Scale3-Labs/dspy-ex
amples/tree/main/src/simple_miprov2

# Additional Notes

1. Each one of the 3 modules are built using the ChainOfThought
 optimizer and Signature hints to guide the program to do what we want to do.
2. I am building an open source observabil
ity tool called Langtrace which you can use to understand what goes in and out of the LLM and trace every step within ea
ch module deeply.
3. The final prompts can be further optimized using a metric and you can technically generate 4 prompt
s with 2 demos and 2 instructions (2 x 2 permutation). These are left out for the sake of simplicity.
4. Since module 2 
uses the program code to identify the intent, re-structuring your code or adding comments can affect the outputs.
```
---

     
 
all -  [ Getting messages from within a tool in LangGraph ](https://www.reddit.com/r/LangChain/comments/1gc06vn/getting_messages_from_within_a_tool_in_langgraph/) , 2024-10-29-0913
```
Hello,

I have a graph with subggraphs, in one subgraph I call the tools inside of a node. Inside the tool itself I'm ta
king input from the user after I print to him what to enter and I also invoke the LLM. 

1. What's the usual way of prom
pting the user for input? I'm a bit confused here. Let's say in production, does the print statement get shown to the us
er? As far as I know it's the list of messages.

2. How can I access the state from within a tool in order to update the
 list of messages? I'm not using a ToolNode.

The first question might seem stupid, but I really don't know. I've been s
tuck for a while thinking through these. No clear thoughts yet. 

Thanks!
```
---

     
 
all -  [ Community/Network around AI Agents ](https://www.reddit.com/r/LangChain/comments/1gbz805/communitynetwork_around_ai_agents/) , 2024-10-29-0913
```
We just launched our community focused on AI Agents! Here it is: [https://discord.gg/qEfQVwg2](https://discord.gg/qEfQVw
g2)

We're going to have

* Constantly Updated News
* Learning Resources
* Hackathons and Investment Resources (getting 
your ideas funded)
* AI Agent Marketplace (Trading post for AI Agent buyers and sellers)
* Ongoing agent experiments tha
t the community can get involved in
* and much more as we grow

Me and my partner truly believe that AI will soon enable
 people to start enterprise level businesses on their own. Imagine you want to build a one-person software company run a
lmost entirely by agents. We're not there yet but we're getting closer, and we want to build platforms to make it insane
ly easy to build and manage these projects using AI Agents.

If you're excited for AI Agents and what they will help us 
create, consider joining!
```
---

     
 
all -  [ Agent runs on loop, sometimes time out, sometimes giving incorrect answer, sometimes proper answer. ](https://www.reddit.com/r/LangChain/comments/1gbz54x/agent_runs_on_loop_sometimes_time_out_sometimes/) , 2024-10-29-0913
```
Trying to build one text to sql project. I'm using ollama llama3.2 locally. But my model is slow and it keep on running 
sometimes without giving the result. Sometimes it generates query but not able to extract the query result. Could someon
e help me with this? Thanks in advance!! 

    from langchain_community.llms import Ollama
    from db import get_schema
, db
    from langchain_community.agent_toolkits import create_sql_agent
    from langchain_community.agent_toolkits.sql
.toolkit import SQLDatabaseToolkit
    from langchain_ollama import OllamaLLM
    from langchain.agents import AgentType

    from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool, InfoSQLDatabaseTool, ListSQLDatabaseT
ool, QuerySQLCheckerTool
    from langchain_community.embeddings import OllamaEmbeddings
    from langchain_community.ve
ctorstores import FAISS
    from langchain_core.example_selectors import SemanticSimilarityExampleSelector
    from lang
chain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate
    from langchain_core.prompts impo
rt SystemMessagePromptTemplate
    from langchain.agents import AgentExecutor, create_react_agent
    
    
    llm = Ol
lamaLLM(model='llama3.2')
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)
    
    embeddings = (
        OllamaEmbeddi
ngs(model = 'llama3.2')
    )
    
    examples = [
        {   'input': 'List all actors.', 
            'query': 'SELE
CT * FROM Actor;'
        },
        {
            'input': 'Find all movies of Ed Chase',
            'query': 'SELECT 
film.title, concat(actor.first_name, ' ', actor.last_name) as actorname from film LEFT JOIN filmactor on film.film_id=fi
lmactor.film_id LEFT JOIN actor on actor.actor_id=filmactor.actor_id WHERE concat(actor.first_name, ' ', actor.last_name
) LIKE '%Ed Chase%''
        },
        {
            'input': 'Find all customers for the postal code 35200.',
        
    'query': 'SELECT first_name,last_name,address_id FROM customer WHERE address_id = (SELECT address_id FROM address WH
ERE postal_code = '35200');',
        },
        {
            'input': 'Find full address of Mary Smith.',
            
'query': 'SELECT address, address2, district, postal_code from address where address_id = (select address_id from custom
er where concat(first_name,' ', last_name) LIKE '%Mary Smith%');',
        },
        {
            'input': 'How many c
ustomers are there',
            'query': 'SELECT COUNT(*) FROM customer',
        },
        {
            'input': 'Fi
nd the total number of actors.',
            'query': 'SELECT COUNT(DISTINT(actor_id)) FROM Actor;',
        },
        
{
            'input': 'Who are the top 5 customers by total purchase?',
            'query': 'SELECT customer.customer_
id AS customer_id, concat(customer.first_name, ' ', customer.last_name) as customer_name, SUM(payment.amount) AS TotalPu
rchase FROM payment LEFT JOIN customer on customer.customer_id=payment.customer_id GROUP BY customer.customer_id ORDER B
Y TotalPurchase DESC LIMIT 5;',
        },
    ]
    
    example_selector = SemanticSimilarityExampleSelector.from_exam
ples(
        examples,
        embeddings,
        FAISS,
        k=3,
        input_keys=['input'],
    )
    
    sql
_db_query =  QuerySQLDataBaseTool(db = db)
    sql_db_schema =  InfoSQLDatabaseTool(db = db)
    sql_db_list_tables =  L
istSQLDatabaseTool(db = db)
    sql_db_query_checker = QuerySQLCheckerTool(db = db, llm = llm)
    
    
    tools = [sq
l_db_query, sql_db_schema, sql_db_list_tables, sql_db_query_checker]
    
    # matched_queries = example_selector.vecto
rstore.search('How many actors are there?', search_type = 'mmr')
    
    
    # for tool in tools:
    #     print(tool
.name + ' - ' + tool.description.strip() + '\n')
    
    system_prefix = '''
    Answer the following questions as best
 you can. You have access to the following tools:
    
    {tools}
    
    Use the following format:
    
    Question:
 the input question you must answer
    Thought: you should always think about what to do
    Action: the action to take
, should be one of [{tool_names}]
    Action Input: the input to the action 
    Observation:
    the result of the acti
on 
    ... (this Thought/Action/Action Input/Observation can repeat N times)
    Thought: I now know the final answer
 
   Final Answer: the final answer to the original input question 
    
    Here are some examples of user inputs and the
ir corresponding SQL queries:
    '''
    
    suffix = '''
    Begin!
    
    Question: {input} 
    Thought:{agent_sc
ratchpad}
    '''
    
    dynamic_few_shot_prompt_template = FewShotPromptTemplate(
        example_selector = example_
selector,
        example_prompt=PromptTemplate.from_template(
            'User input: {input}\nSQL query: {query}'
   
     ),
        input_variables=['input'],
        prefix=system_prefix,
        suffix=suffix
    )
    
    
    full_
prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate(prompt=dynamic_few_shot_pro
mpt_template),
        ]
    )
    
    # prompt_val = full_prompt.invoke(
    #     {
    #         'input': 'How many 
actors are there?',
    #         'tool_names' : [tool.name for tool in tools],
    #         'tools' : [tool.name + ' -
 ' + tool.description.strip() for tool in tools],
    #         'agent_scratchpad': [],
    #     }
    # )
    # print(
prompt_val.to_string())
    
    
    agent = create_react_agent(llm, tools, full_prompt)
    agent_executor = AgentExec
utor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)
    des = agent_executor.invoke({'input': 'How 
many actors are there?'})
    
    
    print(des)

Terminal: **Runs on loop and not extracting output \[In this case '2
00' is the answer\], sometimes not even correct answer.**

    Entering new AgentExecutor chain...
    
    Action: sql_
db_query
    Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor;[(200,)]Question: How many actors are there? 
    
Thought: Action: sql_db_query
    Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many act
ors are there? 
    Thought: I need to execute a query that counts the number of distinct actor IDs.
    Action: sql_db_
query
    Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many actors are there?
    Thoug
ht: I need to count the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SELECT COUNT(DISTINCT a
ctor_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: I want to execute a SQL query that counts 
the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor[
(200,)]Question: How many actors are there? 
    Thought: I should use the sql_db_query tool to execute a SQL query that
 counts the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SELECT COUNT(DISTINCT actor_id) FRO
M Actor[(200,)]Question: How many actors are there? 
    Thought: I need to use the sql_db_query tool to execute a SQL q
uery that counts the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SELECT COUNT(DISTINCT acto
r_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: I need to count the number of distinct actor 
IDs.
    Action: sql_db_query
    Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many act
ors are there?
    Thought: Execute a SQL query that counts the number of distinct actor IDs.
    Action: sql_db_query
 
   Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: I 
want to execute a SQL query that counts the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SEL
ECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many actors are there?
    Thought: I need to use the sql_d
b_query tool to execute a SQL query that counts the number of distinct actor IDs.
    Action: sql_db_query
    Action In
put: SELECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: I need to use 
the sql_db_query tool to execute a SQL query that counts the number of distinct actor IDs.
    Action: sql_db_query
    
Action Input: SELECT COUNT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: Execu
te a SQL query to count the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SELECT COUNT(DISTIN
CT actor_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: I need to use the sql_db_query tool to
 execute a SQL query that counts the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SELECT COU
NT(DISTINCT actor_id) FROM Actor[(200,)]Question: How many actors are there? 
    Thought: I should use the sql_db_query
 tool to execute a SQL query that counts the number of distinct actor IDs.
    Action: sql_db_query
    Action Input: SE
LECT COUNT(DISTINCT actor_id) FROM Actor[(200,)] 
    
    
    {'input': 'How many actors are there?', 'output': 'Agent
 stopped due to iteration limit or time limit.'} 
```
---

     
 
all -  [ CopilotKit: Build Agent-Native Applications with CoAgents & LangGraph ](https://www.reddit.com/r/LangChain/comments/1gbxilp/copilotkit_build_agentnative_applications_with/) , 2024-10-29-0913
```
We are excited to release CoAgents + LangGraph - your new open-source tool for embedding powerful AI agents in your in-a
pp chatbot! With CoAgents, you can:

* Shared State (Agent `↔` Application) with support for intermediate state streamin
g
* Agentic Generative UI
* Human-in-the-Loop
* Realtime frontend actions
* Agent Steering (LangGraph checkpoints)

Reso
urces:

* Check it out here: [CopilotKit on GitHub](https://go.copilotkit.ai/copilot)
* And dive into our walkthrough fo
r a full guide: [Everything You Need to Build Agent-Native Applications](https://go.copilotkit.ai/coagent-blog)

Webinar
:

* Build Agent-Native Applications with CoAgents & LangGraph(Oct. 28th): [Register Here](https://lu.ma/2kxspzl4)
```
---

     
 
all -  [ Title: Urgent Help Needed with PDF Table Extraction in Langchain Project

 ](https://www.reddit.com/r/LangChain/comments/1gbx399/title_urgent_help_needed_with_pdf_table/) , 2024-10-29-0913
```
I'm currently working on a project utilizing Langchain for a large language model (LLM) RAG retriever. Despite having mi
llions of PDF files stored in Supabase, I'm achieving over 90% efficiency with structured data extraction. However, I'm 
facing a significant challenge with my PDFs, as they often contain complex multi-dimensional tables.

I've experimented 
with various parsers and libraries, including Camelot, OpenParser, Tabula, PDFMiner, PyMuPDF, and many others. Unfortuna
tely, none have effectively resolved the issues I'm encountering. The extracted data lacks a coherent structure, making 
it difficult to connect the dots between different pieces of information.

The complexity of the layouts in my PDFs is s
uch that even advanced and paid solutions (like AskYourPDF, OpenAI's 4.0, and Petal) seem to rely on similar underlying 
parsers, leading to the same parsing errors.

I would greatly appreciate any suggestions or insights on how to tackle th
is problem. Thank you!
```
---

     
 
all -  [ RAG-Enhanced Chatbot Application | AI-Powered Document Retrieval & Chatbot Demo | LangChain & OpenAI ](https://www.reddit.com/r/django/comments/1gbtw0l/ragenhanced_chatbot_application_aipowered/) , 2024-10-29-0913
```
I’m excited to share my latest project, an AI-driven chatbot built with LangChain, OpenAI’s GPT-4, ChromaDB, and Streaml
it. By leveraging Retrieval-Augmented Generation (RAG) this application delivers data-backed, contextually rich response
s, perfect for high-impact customer support and knowledge-based applications.

📽️ Watch the Demo - [https://youtu.be/MZD
iMMai6zo?si=xN6hJ-Zj0S627Sj0](https://youtu.be/MZDiMMai6zo?si=xN6hJ-Zj0S627Sj0)  
💻 Explore the Project - [https://githu
b.com/abdurrahimcs50/RAG\_Chatbot\_Project.git](https://github.com/abdurrahimcs50/RAG_Chatbot_Project.git)

🟢 Key Featur
es:

✅ Real-Time Chat Interface: Chat with AI models like OpenAI’s GPT-4 in a responsive interface.  
✅ Document Uploads
 for RAG: Improve chatbot responses by uploading your own documents (PDF, TXT, DOCX, MD).  
✅ URL-Based RAG: Integrate r
eal-time web content into your chat interactions for up-to-date responses.   
✅ Model Selection: Switch easily between O
penAI models, including GPT-4, to suit your needs.  
✅ Interaction Logging: Automatically logs chats for tracking insigh
ts and refining user experiences.

💼 Perfect For: Customer support, research assistants, and knowledge-based application
s that require reliable, accurate responses. This demo shows how the chatbot processes user inputs, retrieves document a
nd web data, and combines it with AI capabilities to deliver comprehensive answers.  
🟢 Tech Stack:

✅ LangChain  
✅ Ope
nAI (GPT-4)  
✅ Streamlit  
✅ ChromaDB  
✅ Docker

If you’re looking to bring AI-powered solutions to your business, fee
l free to connect! I’m a Freelance Python Developer & Generative AI Specialist ready to take on projects that demand cut
ting-edge AI solutions with Django, Docker, LangChain, OpenAI, and more.
```
---

     
 
MachineLearning -  [ [D] How are folks building conversational Retrieval Augmented Generation apps ](https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/) , 2024-10-29-0913
```
I've read through various resources such as:  
- [https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/](htt
ps://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/)  
- [https://python.langchain.com/docs/tutorials/qa\_cha
t\_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)  
- [https://langchain-ai.github.io/langgraph
/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) 
 
- [https://docs.llamaindex.ai/en/stable/module\_guides/deploying/chat\_engines/](https://docs.llamaindex.ai/en/stable/
module_guides/deploying/chat_engines/)  
- [https://huggingface.co/datasets/nvidia/ChatRAG-Bench](https://huggingface.co
/datasets/nvidia/ChatRAG-Bench) 

But these feel overly reductive, since they don't address complexities like:  
1) when
 to retrieve vs. just respond immediately to reduce latency  
2) rely on existing context previously retrieved in the co
nversation instead of retrieving again at the current turn  
3) partition LLM context between retrieved information and 
past conversation history.

I'm sure some teams already have good systems for this, would appreciate pointers!
```
---

     
 
deeplearning -  [ Fast AI's deep learning for coders by jeremy howard for begginer?  ](https://www.reddit.com/r/deeplearning/comments/1gb2k3p/fast_ais_deep_learning_for_coders_by_jeremy/) , 2024-10-29-0913
```
I am a full stack python developer who do web dev in django

I am now starting deep learning,i am a compelete begginer


(Have worked with pandas,numpy,matplotlib,langchain only)

I wanna ask,should i do this course,will i understand what he
 is coding and code myslef

I just dont want to do blind coding,i wanna learn what is the purpose,how it works and how t
o do it

Will this course teach me that or not?

Thanks in advance
```
---

     
