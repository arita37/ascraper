 
all -  [ How do I get really good at RAG? ](https://www.reddit.com/r/LangChain/comments/1hre9pq/how_do_i_get_really_good_at_rag/) , 2025-01-02-0913
```
I want to learn as much as I can about RAG, so that I can build product ready RAG for a new job I'm joining. How can I b
ecome an expert? I'm a full stack dev with decent experience building AI agents
```
---

     
 
all -  [ Looking for AI solutions in this industry that would integrate with my platform? ](https://www.reddit.com/r/LangChain/comments/1hrcwjy/looking_for_ai_solutions_in_this_industry_that/) , 2025-01-02-0913
```




Im currently putting together a startup, Analytics Depot, that will be a one-stop AI solution for businesses. Like H
ome Depot, but it will have AI chatbots in Legal, Finance, Insurance, Real Estate, Oil and Gas, Ecommerce, etc. The end 
clients will be freelancers and small businesses that could benefit from such resources. Later would like to offer solut
ions to the Fortune 500 companies etc. 



If you are building such domain specific AI chatbots, I would love to discuss
 integrating your solution into my marketplace/platform. That would enable my teams to focus on marketing and frontend, 
and I can pay based on subscriber usage/traffic etc. Seems like a win-win.



Dm me if this sounds interesting.
```
---

     
 
all -  [ How do I install this on my laptop? ](https://www.reddit.com/r/crewai/comments/1hrb4li/how_do_i_install_this_on_my_laptop/) , 2025-01-02-0913
```
Im currently taking the multi agents CrewAI class on Deeplearning AI. In one of the classes, it says that I can install 
the following on my own machine. Can you give me guidance on how i can do that? (I do have python 3.12 on my laptop) 

h
ttps://preview.redd.it/f6gehd2xufae1.png?width=1417&format=png&auto=webp&s=86f6e2f0ee192de9b8ebe85bf07bae8619ea2fb6


```
---

     
 
all -  [ I think networking is the only key to land a job in this market. Here's my experience so far. ](https://www.reddit.com/r/developersIndia/comments/1hr8ahr/i_think_networking_is_the_only_key_to_land_a_job/) , 2025-01-02-0913
```
I am in my final year of BE. I am constantly applying on job platforms, even after getting shortlisted (which itself is 
tough and very luck based) the recruiter will ghost you.
Here is what's gonna happen:
Your profile will get shortlisted.

You're gonna recieve an assignment link.
You'll submit your best work within the deadline.
In 95% of the case you'll ge
t ghosted here (even if your assignment/work is as per industry standards).
Let's say you pass the first round, then com
es interview round.
The interview round has always been good for me, what's not good is post interview part (in one case
 the recruiter forgot he interviewed me, I had to call him and he gave me another assignment and later ghosted me).
Afte
r all this, the founder will hire someone they already know.

I spoke with many experienced people and my friends who ar
e working, everyone says application on job sites are purely luck based.
Everyone suggests me to 'Network' on Twitter or
 Reddit, saying this is how they landed a job.

To all the people who landed a successful refferal or got a job purely b
y reaching out or networking, any tips on 'How to actually Network?' will be helpful.
Anyone with extra position in thei
r company for a developer/intern, can you consider a refferal?
(Tech stack: Next.js/ React, Python, Flask/Django, RAG im
plemention, Langchain and development of AI agents using CrewAI or LangGraph)

Thank you.
```
---

     
 
all -  [ Is Langchain useful for non developers? ](https://www.reddit.com/r/LangChain/comments/1hr52m5/is_langchain_useful_for_non_developers/) , 2025-01-02-0913
```
As someone who is very passionate about ai tech and would like to create my own app using private database and images, I
 am in search of the best way to achieve this as I have no developer experience. Langchain has been mentioned in YouTube
 videos so here I am. I have experience training my own Loras through stable diffusion and such but would like to take s
teps further. Lately, I have been seeing people using Bolt to create websites and such. It seems like you need some codi
ng knowledge and experience to work with Langchain.
```
---

     
 
all -  [ Beginner Vision rag with ColQwen in pure python ](https://www.reddit.com/r/LangChain/comments/1hr43hf/beginner_vision_rag_with_colqwen_in_pure_python/) , 2025-01-02-0913
```
I made a beginner Vision rag project without using langchain or llamaindex or any framework. This is how project works -
 first we convert the pdf to images using pymupdf. Then embeddings are generated for these images using jina clip v2 and
 ColQwen. Images and along with vectors are indexed to qdrant. Then based on user query we perform search on jina embedd
ings and rerank using ColQwen. Gemini flash is used to answer the user queries based on retrieved images. Entire ColQwen
 work is inspired from Qdrant youtube video on ColPali. I would definitely recommend watching that video. 

GitHub repo

https://github.com/Lokesh-Chimakurthi/vision-rag

Qdrant video
https://www.youtube.com/live/_h6SN1WwnLs?si=YzTBY_vhYVkiy
uNH
```
---

     
 
all -  [ 🚀 Enhancing Mathematical Problem Solving with Large Language Models: A Divide and Conquer Approach ](https://www.reddit.com/r/LangChain/comments/1hqz8fa/enhancing_mathematical_problem_solving_with_large/) , 2025-01-02-0913
```
Hi everyone!

I'm excited to share our latest project: **Enhancing Mathematical Problem Solving with Large Language Mode
ls (LLMs)**. Our team has developed a novel approach that utilizes a divide and conquer strategy to improve the accuracy
 of LLMs in mathematical applications.

# Key Highlights:

* Focuses on computational challenges rather than proof-based
 problems.
* Achieves state-of-the-art performance in various tests.
* Open-source code available for anyone to explore 
and contribute!

Check out our GitHub repository here: [DaC-LLM](https://github.com/JasonAlbertEinstien/DaC-LLM)

We’re 
looking for feedback and potential collaborators who are interested in advancing research in this area. Feel free to rea
ch out or comment with any questions!

Thanks for your support!
```
---

     
 
all -  [ 'Why' isn't Langchain/Langgraph  production-ready? ](https://www.reddit.com/r/LangChain/comments/1hqufg2/why_isnt_langchainlanggraph_productionready/) , 2025-01-02-0913
```
Please help me understand. I've come across many comments stating that 'Langchain isn't production-ready,' but none expl
ain why. For my use case - developing serverless AI call agents with customizable features (like calendar booking and ad
ding another human to calls mid-conversation) - would it be okay to proceed? I'm using Langraph, but if this approach (s
erverless + Langgraph ) will cause problems, how should I proceed? Should I build from scratch? If so, where should I st
art learning that?

Thanks for reading! Any answers would be greatly appreciated.'
```
---

     
 
all -  [ Fast Multi-turn (follow-up questions) Intent detection and smart information extraction. ](https://www.reddit.com/r/LangChain/comments/1hqtqgi/fast_multiturn_followup_questions_intent/) , 2025-01-02-0913
```
There several posts and threads on reddit like this [one](https://www.reddit.com/r/LocalLLaMA/comments/18mqwg6/best_prac
tice_for_rag_with_followup_chat/) and this [one](https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_ra
g_how_to_search_for_follow_up/) that highlight challenges with effectively handling follow-up questions from a user, esp
ecially in RAG scenarios. These scenarios include adjusting retrieval (e.g. what are the benefits of renewable energy ->
 *include cost considerations)*, clarifying a response (e.g. tell me about the history of the internet -> *now focus on 
how ARPANET worked*), switching intent (e.g. What are the symptoms of diabetes? -> *How is it diagnosed*?)*,* etc. All o
f these are multi-turn scenarios.

Handling multi-turn scenarios requires carefully crafting, editing and optimizing a p
rompt to an LLM to first rewrite the follow-up query, extract relevant contextual information and then trigger retrieval
 to answer the question. The whole process is slow, error prone and adds significant latency.

We built a 2M LoRA LLM ca
lled Arch-Intent and packaged it in [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw) \- the inte
lligent gateway for agents - which offers fast and accurate detection of multi-turn prompts (default 4K context window) 
and can call downstream APIs in <500 ms (via [Arch-Function](https://huggingface.co/katanemo/Arch-Function-3B), the fast
est and leading OSS function calling LLM ) with required and optional parameters so that developers can write simple API
s.

Below is simple example code on how you can easily support multi-turn scenarios in RAG, and let Arch handle all the 
complexity ahead in the request lifecycle around intent detection, information extraction, and function calling - so tha
t developers can focus on the stuff that matters the most.

    import os
    import gradio as gr
    
    from fastapi 
import FastAPI, HTTPException
    from pydantic import BaseModel
    from typing import Optional
    from openai import 
OpenAI
    
    app = FastAPI()
    
    # Define the request model
    class EnergySourceRequest(BaseModel):
        en
ergy_source: str
        consideration: Optional[str] = None
    
    class EnergySourceResponse(BaseModel):
        ene
rgy_source: str
        consideration: Optional[str] = None
    
    # Post method for device summary
    app.post('/age
nt/energy_source_info')
    def get_energy_information(request: EnergySourceRequest):
        '''
        Endpoint to ge
t details about energy source
        '''
        considertion = 'You don't have any specific consideration. Feel free t
o talk in a more open ended fashion'
    
        if request.consideration is not None:
            considertion = f'Add
 specific focus on the following consideration when you summarize the content for the energy source: {request.considerat
ion}'
    
        response = {
            'energy_source': request.energy_source,
            'consideration': conside
rtion,
        }
        return response

And this is what the user experience looks like when the above APIs are config
ured with Arch.

https://preview.redd.it/b6m2qrv9n19e1.png?width=1666&format=png&auto=webp&s=e7c41be36d381041352f3f11e68
dcb389b72d936
```
---

     
 
all -  [ In the final semester, looking for AI/ML or Data Science positions ](https://www.reddit.com/r/learnmachinelearning/comments/1hqt9p8/in_the_final_semester_looking_for_aiml_or_data/) , 2025-01-02-0913
```
Hello guys, I'm in my final semester of the masters and now looking for a full-time roles. I want to improve my resume. 
Please rate it and give feedback.

https://preview.redd.it/03arzufciaae1.jpg?width=2479&format=pjpg&auto=webp&s=68954afa
718edf280175b563fe1cf6ed30435b7d


```
---

     
 
all -  [ I also would be thinking about how to increase my skills immediately after suffering brain damage. / ](https://i.redd.it/qwkxlviio9ae1.jpeg) , 2025-01-02-0913
```
This is wild way to start a LinkedIn post. I’ve seen some stuff but wow.
```
---

     
 
all -  [ LangGraph Conceptual Guide ](https://www.reddit.com/r/copilotkit/comments/1hqla7z/langgraph_conceptual_guide/) , 2025-01-02-0913
```
Check out LangGraph's Conceptual guide.  
A perfect pair with [CoAgents](https://docs.copilotkit.ai/coagents)  
[https:/
/langchain-ai.github.io/langgraph/concepts/](https://langchain-ai.github.io/langgraph/concepts/)
```
---

     
 
all -  [ How do I Display what the Agent is doing on the backend to the frontend ](https://www.reddit.com/r/LangChain/comments/1hqjvf3/how_do_i_display_what_the_agent_is_doing_on_the/) , 2025-01-02-0913
```
I’ve been playing around with the LangGraph tutorials to get myself familiar. I’m trying to figure out how to display wh
at the Agent is doing on the frontend.   


Here's a snippet from the LangGraph docs:  
[Streaming](https://langchain-ai
.github.io/langgraph/concepts/streaming/): Streaming is crucial for enhancing the responsiveness of applications built o
n LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves u
ser experience (UX), particularly when dealing with the latency of LLMs.

I'd like to be able to stream between nodes fo
r better UX of my app, but since LangGraph is a backend solution, is there a good solution for the frontend so I don't h
ave to build this from scratch? 

My stack is Next.js with some Python backend, and I want to add LangGraph(Agents)

Tha
nks in advance!
```
---

     
 
all -  [ Seeking Feedback on My RAG Architecture for Context Retrieval and Lightweight Optimization ](https://www.reddit.com/r/LangChain/comments/1hqhmdx/seeking_feedback_on_my_rag_architecture_for/) , 2025-01-02-0913
```
I am currently developing a Retrieval-Augmented Generation (RAG) system designed to answer user questions by extracting 
context from two sources: a database or CSV documents. To guide the RAG in selecting the optimal source for context extr
action, I have outlined the following architecture. How do you find it? Is it efficient, or are there bottlenecks I shou
ld address to make it more lightweight?

# Architecture Outline

# 1. Agent Node

* Processes the user question and deci
des the retrieval source or falls back to the LLM's internal knowledge.

# 2. Conditional Edge: Determine Retrieval Sour
ce

* **Check Database**: Verifies if the relevant context exists in the SQLite database.
* **Check CSV Documents**: Ver
ifies if the relevant context exists in the CSV documents.
* **Fallback**: If neither source is applicable, defaults to 
the LLM's internal knowledge.

# 3. SQLite Path

* **Schema Prompt Node**: The LLM receives the database schema and the 
user query to identify relevant fields.
* **Query Construction Node**: The LLM generates an SQL query based on the schem
a and fields.
* **Execute Query Node**: Executes the SQL query and retrieves the results.
* **Answer Formulation Node**:
 Formats the retrieved data into a natural language response using the LLM.

# 4. CSV Path

* **Retrieve CSV Data Node**
: Fetches relevant rows from the CSV documents.
* **Answer Formulation Node**: Summarizes and formats the retrieved data
 using the LLM.

# 5. Fallback Path

* **Generate Using Internal Knowledge Node**: Produces a response solely based on t
he LLM's internal knowledge.

This structure aims to balance flexibility and efficiency. Do you see any potential bottle
necks, or have suggestions for optimization to ensure the system remains lightweight and responsive?


```
---

     
 
all -  [ Graph Structure vs Others  ](https://www.reddit.com/r/LangChain/comments/1hqf270/graph_structure_vs_others/) , 2025-01-02-0913
```
Hi all. I'm looking to see if there's any empirical work done that validates the graph agent structure (e.g., LangGraph)
 as more reliable than other agent/Multi-Agent architectures (non-graph, event-driven). 
```
---

     
 
all -  [ An Agent that creates memes for you ](https://open.substack.com/pub/diamantai/p/viral-marketing-made-easy-unlocking?r=336pe4&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false) , 2025-01-02-0913
```
Memes are the internet’s universal language, but creating ones that truly align with your brand and actually connect wit
h your audience? That’s no small task.

During the hackathon that I ran with LangChain, a talented participant worked on
 a system designed to solve this challenge. It uses AI to analyze a brand’s tone, audience, and personality and then tra
nsforms that data into memes that feel authentic and relevant.

Here’s what makes it exciting:

- It simplifies complex 
brand messaging into relatable humor.
- It adapts to internet trends in real-time.
- It creates memes that aren’t just f
unny—they’re actually effective.
If you’re curious about how it all works, I’ve broken it down in a blog post attached w
ith examples and insights into the process.
```
---

     
 
all -  [ Converting PDFs to Markdown for Higher Quality Embeddings with Langchain.js ](https://www.reddit.com/r/LangChain/comments/1hq8d6c/converting_pdfs_to_markdown_for_higher_quality/) , 2025-01-02-0913
```
I am working on RAG LLM projects with Langchain.js using Node.js. Most of the data I retrieve are PDFs and a bit of JSON
.

For higher quality, I would like to convert my PDFs into Markdown before embedding starts. This involves deleting hea
ders and footers, extracting tables and pictures, and converting the text into Markdown so that it is clear to the LLM w
hether it is body text or an important title.

Example: **This is Title** and < p >just text

I want to ensure that pict
ures and tables are clearly identified in Markdown, and that unnecessary elements are removed. I have found some Python 
libraries that people use for this purpose, but I need something for Node.js. Has anyone experienced with this?

And of 
course, any other tips for better RAG LLM are also welcome! :)

Already tried 'markdown-it' library but not really happy
 with result. The only thing it does is sometimes adding < p >. (Which is still better than nothing but i want everythin
g to be very clear) It's not clear :

* if this text is Title or Body,
* Can't delete Footer Information with this
* Can
't export Table/Picture etc.
```
---

     
 
all -  [ langchain_experimental  SQLDatabaseChain.invoke() question ](https://www.reddit.com/r/LangChain/comments/1hq5px2/langchain_experimental_sqldatabasechaininvoke/) , 2025-01-02-0913
```
I am learning another langchain video  with LLM interacting with SQL.

Here is the  [Jupyter notebook link](https://gith
ub.com/codebasics/langchain/blob/main/4_sqldb_tshirts/t_shirt_sales_llm.ipynb) and its accompanying [YOUTUBE  Video](htt
ps://www.youtube.com/watch?v=4wtrl4hnPT8&t=354s) between the `15:35` `and the 15:39 mark`.

My question is regarding the
 following chunks of code - the issue is specifically executing the second line of code.

    db_chain = SQLDatabaseChai
n.from_llm(llm, db, verbose=True)
    qns1 = db_chain('How many t-shirts do we have left for nike in extra small size an
d white color?')

Update - please note that the second line of code shown above can actually  be executed using `'db_cha
in.invoke'` as shown below, and it yields the same result as the original code :

    qns1 = db_chain.invoke('How many t
-shirts do we have left for nike in extra small size and white color?')

The `Jupyter notebook  output`  upon execution 
of  the above two lines of code is shown as follows:

    > Entering new SQLDatabaseChain chain...
    How many t-shirts
 do we have left for nike in extra small size and white color?
    SQLQuery:SELECT stock_quantity FROM t_shirts WHERE br
and = 'Nike' AND color = 'White' AND size = 'XS'
    SQLResult: [(91,)]
    Answer:91
    > Finished chain.

And here is
 my output :

    > Entering new SQLDatabaseChain chain...
    How many t-shirts do we have left for nike in extra small
 size and white color?
    SQLQuery:SQLQuery: SELECT `stock_quantity` FROM `t_shirts` WHERE `brand` = 'Nike' AND `size` 
= 'XS' AND `color` = 'White';
    SQLResult: [(91,)]
    Answer:Question: How many t-shirts do we have left for nike in 
extra small size and white color?
    SQLQuery: SELECT `stock_quantity` FROM `t_shirts` WHERE `brand` = 'Nike' AND `size
` = 'XS' AND `color` = 'White';
    > Finished chain.

So the difference between the two output  are the 'SQLQuery' and 
'Answer'.   But we can SKIP the 'SQLQuery' differences as this looks like just a minor formatting issue - the SQL query 
generated may not be 'exactly' the same word-for-word but they are identical.

My main issue is my 'Answer' output is sh
owing a string representation of a dictionary keys `'Question'`  and `'SQLQuery'` with their respective values.   Why is
 the  'Answer' not showing  `91` ?

Both the 'SQLResult' outputs (from the tutorial and my output)  are identical.

I di
d a little further digging and executed the following lines

    from pprint import pprint 
    pprint(qns1)

The output
 shows the  `qns1` variable  is actually a dictionary with the following output (on my end):

    {'query': 'How many t-
shirts do we have left for nike in extra small size and '
              'red color?',
     'result': 'Question: How many
 t-shirts do we have left for nike in extra '
               'small size and red color?\n'
               'SQLQuery: SEL
ECT `stock_quantity` FROM `t_shirts` WHERE `brand` = '
               ''Nike' AND `size` = 'XS' AND `color` = 'Red';'}


What code change do I need to do to make the `'result'` show  as `'91'` ?
```
---

     
 
all -  [ StepsTrack: A small Typescript library that helps Logging and Optimizing Pipeline Steps ](https://www.reddit.com/r/Rag/comments/1hq2571/stepstrack_a_small_typescript_library_that_helps/) , 2025-01-02-0913
```
Hello everyone 👋,

I have been working on optimizing an RAG pipeline (mainly on improving speed and reducing token usage
) deployed in production. I found debugging and optimizing very challenging, as there can be a bunch of sub-steps in the
 pipeline, depending on user's query, each step may involves dynamic LLM response and random data retrieval, making the 
response time and runtime flow very non-deterministic. 

So I created **StepsTrack** [https://github.com/lokwkin/steps-t
rack](https://github.com/lokwkin/steps-track) which is a small and simple Typescript library to help me track what was h
appening inside each RAG pipeline runs. It:

* ***Track the results and latency*** of each steps, allow me to export for
 further debug.
* Visualize them into ***Gantt Chart*** & ***Execution Graph*** (I found it very useful when explaining 
the bottlenecks and flow issues to boss and other teammates).
* ***Emit events*** hooks to allow integrating (for furthe
r frontend or external integration like SSE / websocket)

Upcoming planned features:

* Generate execution stats aggrega
ted from multiple pipeline runs (useful on Prod environments to see the behavior from different user inputs)
* Add Redis
 support for pub/sub events and data storage (as an adapter for integrations)
* Implement real-time execution monitoring
. (Probably an internal dashboard frontend to monitor what steps in-progress) 

Note: while StepsTrack focuses on speed 
improvement and debugging logical flow, it doesn’t help address RAG accuracy. I also tried to write this tool non-LLM fo
cused so it can possibility used in other types of applications that has pipeline-like chains of steps.

\---

I’m sure 
there would be similar or better libraries out there, and this library probably won’t work with popular RAG frameworks l
ike LangChain etc. But if you are building pipelines in Typescript and without using any frameworks, you might find Step
sTrack as helpful as I did. 

Feel free to check it out at [https://github.com/lokwkin/steps-track](https://github.com/l
okwkin/steps-track)

Welcome any thoughts, comments, or suggestions! Thanks! 😊 
```
---

     
 
all -  [ What is actually sent to the LLM to decide whether or not to call a tool? ](https://www.reddit.com/r/LangChain/comments/1hpypmj/what_is_actually_sent_to_the_llm_to_decide/) , 2025-01-02-0913
```
I'm working through the 'Build a Retrieval Augmented Generation (RAG) App: Part 2' tutorial where you bind the retrieve 
function as a tool, and then add a conditional edge for when it is executed.

I've been digging through the documentatio
n, but I cannot seem to find an answer for the actual message sent to the LLM to instruct it to either use the retrieve 
tool or respond (as an example: 'If needed, use the retrieve tool to retrieve information related to a query. Otherwise 
respond directly').

This tutorial also had the LLM rephrase the user queries when calling the retrieve tool.

Although 
it doesn't stop me from proceeding, I would really like a look behind the hood as to what LangChain is sending when tool
s are binded to a ChatModel.

  
EDIT: Thank you for your help. I was unable to see this information because it is passe
d as a separate argument in the API (which is exactly the understanding I was trying to obtain). 
```
---

     
 
all -  [ Chat memory  ](https://www.reddit.com/r/LangChain/comments/1hpx94c/chat_memory/) , 2025-01-02-0913
```
Let me ask you an absolute beginner doubt. I have created a simple react agentic architecture using langraph. I have use
d 'MessagesState' as State and passed it to the graph while compiling it.
But for multiple sequential invocations, it se
ems like the state is refreshed. 

For example if I invoke the graph with 
'hi, from now on your name is 'Groot'', 

The
 response will be like 
'Hi, I'm Groot, how can I help you?' 

But for the next execution if I ask the name, it's helple
ss about it.

Is there any other way than using checkpointers to tackle this task?
```
---

     
 
all -  [ How to Add System Message to a multimodal Prompt? ](https://www.reddit.com/r/LangChain/comments/1hpto5u/how_to_add_system_message_to_a_multimodal_prompt/) , 2025-01-02-0913
```
Hi guys, Am using Langchain Python. I have created a sorta Prompt Template to pass my prompt and an Image. But I want to
 add System Message to the Prompt. How do I do it? The code is below

````
import base64

from langchain_core.messages i
mport HumanMessage, SystemMessage

def ga_template(image_path, prompt):

with open(image_path, 'rb') as image_file:

dec
oded_string = base64.b64encode(image_file.read()).decode('utf-8')

message HumanMessage(

content=[

{'type': 'text', 't
ext': prompt},

{

'type': 'image_url',

'image_url': {'url': f'data:image/jpeg;base64, {decoded_string}'},

},

],

ret
urn message
```


```
---

     
 
all -  [ Hot take: Just use Langchain ](https://i.redd.it/v0t3q1xpu0ae1.jpeg) , 2025-01-02-0913
```

```
---

     
 
all -  [ Handle return of Multiple tool request ](https://www.reddit.com/r/LangChain/comments/1hprrzi/handle_return_of_multiple_tool_request/) , 2025-01-02-0913
```
Hi guys,I need some help working with LangChain and LangGraph.

I have an agent, a supervisor, and a tool associated wit
h my agent (this tool is an API request that returns a JSON). Basically, my problem is:When my agent calls the tool mult
iple times, before returning to the supervisor, the agent compacts the multiple returns into  single text. 

https://pre
view.redd.it/9q98tldpo0ae1.png?width=537&format=png&auto=webp&s=54a254df5970b98856644aa391b5304fe76062ef

However, when 
the tool is requested a single time, this doesn't happen, and I don't know how to resolve this efficiently.

https://pre
view.redd.it/2y42wavqo0ae1.png?width=356&format=png&auto=webp&s=0c446845a5e18ff85a81df697c569e4d5dbc6765

Can someone he
lp me?  

```
---

     
 
all -  [ NestJS + LangChain “Langgraphs”: Embed or Deploy Separately? ](https://www.reddit.com/r/LangChain/comments/1hpmxt9/nestjs_langchain_langgraphs_embed_or_deploy/) , 2025-01-02-0913
```
Hey folks, I’m experimenting with LangChain’s Langgraphs in a NestJS server.

Thank you for amazing work, Langchain team
.  All these days I thought 'good prompt building' enough, only until I discovered 'Langgraph'

I’m trying to decide if 
I should embed the langgraphs directly in my NestJS app or set them up as a separate service (or even use their Langgrap
h platform, maybe eventually). 

Would love any pointers about performance, scalability, or best practices.

Thanks! 🙏
```
---

     
 
all -  [ How to Handle Token Limit Exceeded Error in OpenAI API ](https://www.reddit.com/r/LangChain/comments/1hpjms4/how_to_handle_token_limit_exceeded_error_in/) , 2025-01-02-0913
```
I'm getting an error from the OpenAI API stating that the context length exceeds the model's limit, even though I'm only
 passing the last four messages to the prompt. **I’ve verified that each interaction is using around 1056 tokens**, but 
I’m still encountering the error when sending the prompt to the model and not sure why I'm still exceeding the token lim
it.

Full error message:

openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model's maximum context 
length is 8192 tokens. However, your messages resulted in 8452 tokens (8415 in the messages, 37 in the functions). Pleas
e reduce the length of the messages or functions.', 'type': 'invalid\_request\_error', 'param': 'messages', 'code': 'con
text\_length\_exceeded'}}

Here is my code:



`tool(response_format='content_and_artifact')`  
`def retrieve(query: str
):`  
`'''Retrivieving function'''`  
`try:`  
`vector_store = document_embeddings.get_vectorstore()`  
`retrieved_docs 
= vector_store.similarity_search(query, k=4, max_tokens_limit=4000)`  
`serialized = '\n\n'.join(`  
`(f'Source: {doc.me
tadata}\n' f'Content: {doc.page_content}')`  
`for doc in retrieved_docs`  
`)`  
`return serialized, retrieved_docs`  

`except Exception as e:`  
`print(f'Error during retrieval: {e}')`  
`raise e`

    def filter_messages(messages: list):

        # This is very simple helper function which only ever uses the 4 last messages to prevent context limit error
 
       return messages[-4:]
    
    def query_or_respond(state: MessagesState):
        llm_with_tools = llm.bind_tools
([retrieve])
        response = llm_with_tools.invoke(state['messages'])
        return {'messages': [response]}
    
  
  tools = ToolNode([retrieve])
    from langchain_community.callbacks.manager import get_openai_callback
    def generat
e(state: MessagesState):
        messages = filter_messages(state['messages'])
        recent_tool_messages = []
       
 for message in reversed(messages):
            if message.type == 'tool':
                print('Tool')
               
 recent_tool_messages.append(message)
            else:
                break
        tool_messages = recent_tool_messag
es[::-1]
    
        docs_content = '\n\n'.join(doc.content for doc in tool_messages)
        system_message_content = 
(
            'You are an assistant for question-answering tasks. '
            'Use the following pieces of retrieved c
ontext to answer '
            'the question. If you don't know the answer, say that you '
            'don't know. Use 
three sentences maximum and keep the '
            'answer concise.'
            '\n\n'
            f'{docs_content}'
  
          'Use documents/context . ')
    
        conversation_messages = [
            message
            for message
 in messages
            if message.type in ('human', 'system')
            or (message.type == 'ai' and not message.too
l_calls)
        ]
        prompt = [SystemMessage(system_message_content)] + conversation_messages
        print(f'prom
pt: {prompt}')
            # Run
        with get_openai_callback() as cb:
            response = llm.invoke(prompt)
   
         print(f'Total Tokens: {cb.total_tokens}')
            print(f'Prompt Tokens: {cb.prompt_tokens}')
            p
rint(f'Completion Tokens: {cb.completion_tokens}')
            print(f'Total Cost (USD): ${cb.total_cost}')
    
       
 return {'messages': [response]}
    
    
    memory = MemorySaver()
    
     graph_builder = StateGraph(MessagesState
)
     graph_builder.add_node(query_or_respond)
     graph_builder.add_node(tools)
     graph_builder.add_node(generate)

     graph_builder.set_entry_point('query_or_respond')
     graph_builder.add_conditional_edges(
         'query_or_res
pond',
         tools_condition,
         {END: END, 'tools': 'tools'},
     )
     graph_builder.add_edge('tools','gene
rate')
     graph_builder.add_edge('generate', END)
     graph = graph_builder.compile(checkpointer=memory)

For the emb
edding i am using Openai embedding, chunk size = 1000, overlap = 200, parsin with Llamaparse and Unstructured for Makrdo
wnLoader\`

Any advice or solutions would be greatly appreciated!
```
---

     
 
all -  [ Need Feedback on Custom Reducer to Summarize Conversations ](https://www.reddit.com/r/LangChain/comments/1hpjm65/need_feedback_on_custom_reducer_to_summarize/) , 2025-01-02-0913
```
I’m newbie to Langchain and experimenting with LangGraph to build an SQL analysis workflow. I’ve come up with a pattern 
for maintaining conversation context and would love feedback:

* At the end of each query, I summarize the Q&A using a n
ode.
* A custom state reducer takes the previous summary, combines it with the new one, updates the state, and stores it
 as `previous_convo`.

The goal is to keep a condensed version of the entire conversation history accessible.

**Does th
is seem efficient? Any better approaches I should consider?**
```
---

     
 
all -  [ Resume Review: ML/AI Engineering Grad Student Looking for Internships, not having much luck ](https://www.reddit.com/r/Resume/comments/1hp6pt3/resume_review_mlai_engineering_grad_student/) , 2025-01-02-0913
```
I'm a current Computer Engineering Master's student focusing on AI/Machine Learning, and I've been applying to internshi
ps, but the only replies I ever get are if I have a referral. I got an interview with Salesforce for an AI internship be
cause of a referral and the it went well. So even though I don't know if I'll get the position I'm wondering why I can't
 get companies lower on the totem pole than Salesforce to give me an interview or OA.

I'm wondering if my resume format
 is bad and getting auto rejected or if I just don't have enough experience to make it in a competitive market (It could
 be a combination of both). I get the sense that my resume isn't even making it past recruiters. I see other people at l
east get interviews with these places, I'm wondering what they're doing that I'm not.

Any advice or tips would be great
ly appreciated. I haven't had much luck getting advice in other subs or having my resume refined by someone on Fiver for
 $60. For reference I am a US citizen (I know that can affect what recruiters do with applications). 

https://preview.r
edd.it/wkkxxgb1yu9e1.jpg?width=1252&format=pjpg&auto=webp&s=f09d4baa91524c95784f175dce6b085689404595


```
---

     
 
all -  [ Is there any simpler way to implement memory in a chat app? ](https://www.reddit.com/r/OpenAI/comments/1hp6h5g/is_there_any_simpler_way_to_implement_memory_in_a/) , 2025-01-02-0913
```
I am creating a simple chat app with Haystack but like Langchain, Haystack is too much abstraction for a simple task.

S
o I was wondering if anyone knows of a way to implement memory for the chat app without using Haystack or Langchain?

Al
so, this is a simple chat app and not a document based chat app.
```
---

     
 
all -  [ Need advice regarding job search  ](https://i.redd.it/4v2brxtzkt9e1.jpeg) , 2025-01-02-0913
```
I'm a Master's student pursuing my degree in EE, my bachelor's was in EE as well.  I was originally interested in roboti
cs design.
However, opportunities have been limited to say the least. I figure I can't be selective anymore and have bee
n applying to broader roles in EE as well, but have had no luck with the applications. Tried internships but no luck the
re either.
I need advice on what roles would be the best fit given my skills. And what approach I should take to improve
 my resume.
Any feedback on the resume itself would be greatly appreciated as well.
Thank you for your time.

```
---

     
 
all -  [ LinkedIn tool integration - how to implement ](https://www.reddit.com/r/AI_Agents/comments/1hp028i/linkedin_tool_integration_how_to_implement/) , 2025-01-02-0913
```
Hi there,  
I am currently working on an AI agent project and need to scrape through my clients Linkedin contacts, to id
entify potential leads. As far as I am aware, there is no tool integration for LinkedIn. I have checked the LangChain an
d crewAI documentation, and could not find a tool there. I saw composio has a LinkedIn connector, but then you have to i
mplement your own tool logic (which is fine for me, I am a senior dev, but I have concerns about getting blocked on Link
edIn for scraping). 

Do you know of any other libraries or frameworks I can use? I am looking for free solutions, no lo
w code. Thanks in advance. 
```
---

     
 
all -  [ Web browser example is giving me - Error: Failed to parse. Text: '```json... is not valid JSON'  ](https://www.reddit.com/r/flowise/comments/1hos4vp/web_browser_example_is_giving_me_error_failed_to/) , 2025-01-02-0913
```
After following this example - [https://www.youtube.com/watch?v=yEHC7\_x2x4U](https://www.youtube.com/watch?v=yEHC7_x2x4
U) \- I'm getting an error while working on Flowise and I'm hoping someone can help me troubleshoot. I'm getting a 'Fail
ed to parse. Text: \`\`\`json... is not valid JSON' error.

Here's the full error message:

Failed to parse. Text: '\`\`
\`json { 'action': 'Final Answer', 'action\_input': | { 'definition\_of\_word\_of\_the\_day': 'A word of the day is a re
gularly occurring piece of content, such as a quote, fact, or definition, that is shared with users on a particular plat
form.', 'example\_use\_case\_for\_perspicacious': 'The detective was known for being perspicacious and could often solve
 cases by observing small details that others had missed.' } }

{

  '... is not valid JSON



Troubleshooting URL: [htt
ps://js.langchain.com/docs/troubleshooting/errors/OUTPUT\_PARSING\_FAILURE/](https://js.langchain.com/docs/troubleshooti
ng/errors/OUTPUT_PARSING_FAILURE/)

*Processing img 2i89l4mshn9e1...*


```
---

     
 
all -  [ Building a Production-Ready RAG Application: Need Advice ](https://www.reddit.com/r/LangChain/comments/1horktb/building_a_productionready_rag_application_need/) , 2025-01-02-0913
```

Hi everyone,

I’m currently working on building a production-ready RAG application. My primary is Azure OpenAI, but I’m
 open to other LLM integrations if necessary.

Key Requirements:
- Handling a large volume of internal documents.
- Feat
ures like question reformulation and reranking to improve retrieval accuracy.

Frameworks I’m Considering:
- LangChain
-
 Haystack
- Semantic Kernel

Do you have any recommendations on which framework is best suited for this use case? Am I m
issing any other frameworks that I should evaluate?

I’d appreciate your insights and experiences
```
---

     
 
all -  [ Monopoly - beat a human collective ](https://www.reddit.com/r/ollama/comments/1hoi9rm/monopoly_beat_a_human_collective/) , 2025-01-02-0913
```
So its christmas and the thought turns to the family board games,

Is there a model or app (python) or gui site where i 
can basicaly kick the families backsides by using ai to process the permutations for me?

Ditto many other popular board
 games such as cludeo, sorry, stratego,, ( ignoring knowledge based such as trivial pursuits) 

I have a decent enough l
aptop with a 4070 that runs models happily locally using ollama and i am comfortable with python and langchain and local
 class modules.. 

Just a random thought to use what i mess with in the day job to kick the families arses round the din
ner table 🤣
```
---

     
 
all -  [ Best Way to Chunk Large-ish Text Documents for Make.com ](https://www.reddit.com/r/vectordatabase/comments/1hoeees/best_way_to_chunk_largeish_text_documents_for/) , 2025-01-02-0913
```
I'm looking around for the best way to chunk large documents in a [Make.com](http://Make.com) scenario before sending th
e data into Pinecone. Within Make, there is CustomJS and 0CodeUtil. LangChain apparently is an option as well. Honestly,
 I would love to be able to deploy a custom function somewhere in the cloud and then call it using an API. Really trying
 to avoid complex setups and want quick and dirty.
```
---

     
 
all -  [ Underlying tech of Cursor? ](https://www.reddit.com/r/ycombinator/comments/1hod5iv/underlying_tech_of_cursor/) , 2025-01-02-0913
```
Very curious about how the Cursor team is approaching their solution technically

1. How do they handle context across t
he project?
2. Do they use raw APIs or something like LangChain or LangGraph?
3. Is it proprietary tech or just a 'LLM w
rapper'? (I don't think so)

Is there info about this?
```
---

     
 
all -  [ Top 5 Hacker News Posts on RAG This Week ](https://www.reddit.com/r/Rag/comments/1hobxkl/top_5_hacker_news_posts_on_rag_this_week/) , 2025-01-02-0913
```
Curated the top 5 most insightful posts on RAG — highlighting key discussions and practical takeaways:

1️⃣ 𝗧𝗶𝘁𝗹𝗲: RAG L
ogger: An Open-Source Alternative to LangSmith  
𝗨𝗽𝘃𝗼𝘁𝗲𝘀: 95  
𝗟𝗶𝗻𝗸: [https://news.ycombinator.com/item?id=42485113](htt
ps://news.ycombinator.com/item?id=42485113)  
𝗪𝗵𝗮𝘁 𝗶𝘀 𝗶𝘁 𝗮𝗯𝗼𝘂𝘁: RAG Logger is a simple, open-source RAG pipeline logging
 tool with suggested enhancements like visualization, OpenTelemetry support, and replay features.

2️⃣ 𝗧𝗶𝘁𝗹𝗲: Collab Not
ebook – RAG on Your Unstructured Data  
𝗨𝗽𝘃𝗼𝘁𝗲𝘀: 14  
𝗟𝗶𝗻𝗸: [https://news.ycombinator.com/item?id=42517745](https://news
.ycombinator.com/item?id=42517745)  
𝗪𝗵𝗮𝘁 𝗶𝘀 𝗶𝘁 𝗮𝗯𝗼𝘂𝘁: The post outlines using LangChain and Unstructured IO to address 
unstructured data challenges in RAG with FAISS, LLMs, and Athina AI evaluation.

3️⃣ 𝗧𝗶𝘁𝗹𝗲: Web RAG to generate perplexi
ty like answers from your docs in browser  
𝗨𝗽𝘃𝗼𝘁𝗲𝘀: 5  
𝗟𝗶𝗻𝗸: [https://news.ycombinator.com/item?id=42516981](https://n
ews.ycombinator.com/item?id=42516981)  
𝗪𝗵𝗮𝘁 𝗶𝘀 𝗶𝘁 𝗮𝗯𝗼𝘂𝘁: The system offers a private, browser-based solution for indexi
ng, searching, and generating responses using GTE-small, SQLite, and WebLLM, with zero API costs 👩‍💻

4️⃣ 𝗧𝗶𝘁𝗹𝗲: LLM app
s, AI Agents, and RAG tutorials with step-by-step instructions  
𝗨𝗽𝘃𝗼𝘁𝗲𝘀: 3  
𝗟𝗶𝗻𝗸: [https://news.ycombinator.com/item?i
d=42510073](https://news.ycombinator.com/item?id=42510073)  
𝗪𝗵𝗮𝘁 𝗶𝘀 𝗶𝘁 𝗮𝗯𝗼𝘂𝘁: A curated repository of RAG-powered LLM a
pplications, showcasing models from OpenAI, Anthropic, Google, and open-source options like LLaMA.

5️⃣ 𝗧𝗶𝘁𝗹𝗲: GraphRAG 
SDK 0.4.0: Simplify RAG with Graph Databases  
𝗨𝗽𝘃𝗼𝘁𝗲𝘀: 2  
𝗟𝗶𝗻𝗸: [https://news.ycombinator.com/item?id=42496411](https:
//news.ycombinator.com/item?id=42496411)  
𝗪𝗵𝗮𝘁 𝗶𝘀 𝗶𝘁 𝗮𝗯𝗼𝘂𝘁: The module simplifies RAG application development with grap
h databases, multi-LLM support, smarter queries, LiteLLM integration, and cost-effective deployment 🚀
```
---

     
 
all -  [ [Student] Looking for feedback on my resume and general advise regarding job search ](https://www.reddit.com/r/EngineeringResumes/comments/1hoa3j9/student_looking_for_feedback_on_my_resume_and/) , 2025-01-02-0913
```
My last post didn't get much traction so I hope this one does better. I believe this is my first time posting here, but 
I have posted on the resumes subreddit before. Looking for criticism/feedback on the latest iteration of my resume. 

I 
am looking for robotics/EE roles, and have been applying since the start of the fall semester, but no luck regarding get
ting interviews. No luck with internships either. A couple of pre-recorded video interviews but they didn't go anywhere.
 I am not sure what I am doing wrong, is it the fact that I am an international student, lack of experience, my resume, 
or all three.

I would also like advice on what roles in EE/robotics would best fit me, given my skills. I figure being 
specific will not get me anywhere hence the desperation.

Thanks again for your time.

https://preview.redd.it/zcn91jq9g
m9e1.jpg?width=4967&format=pjpg&auto=webp&s=75cd4e61fcc35fd21d160ef084daa48f029f30cc




```
---

     
 
all -  [ An Open Source Computer/Browser Tool for your Langgraph AI Agents ](https://www.reddit.com/r/LangChain/comments/1ho8m91/an_open_source_computerbrowser_tool_for_your/) , 2025-01-02-0913
```
MarinaBox is an open-source toolkit for creating browser/computer sandboxes for AI Agents. If you ever wanted your Langg
raph agents to use a computer using Claude Computer-Use, you can check this out,  
[https://medium.com/@bayllama/a-compu
ter-tool-for-your-langgraph-agents-using-marinabox-b48e0db1379c](https://medium.com/@bayllama/a-computer-tool-for-your-l
anggraph-agents-using-marinabox-b48e0db1379c)

We also support creating just a browser sandbox if having access to a des
ktop environment is not necessary.

Documentation:[https://marinabox.mintlify.app/get-started/introduction](https://mari
nabox.mintlify.app/get-started/introduction)   
Main Repo: [https://github.com/marinabox/marinabox](https://github.com/m
arinabox/marinabox)   
Infra Repo: [https://github.com/marinabox/marinabox-sandbox](https://github.com/marinabox/marinab
ox-sandbox)

PS: We currently only support running locally. Will soon add the ability to self-host on your own cloud.
```
---

     
 
all -  [ Supabase and Open AI Realtime with langchain powered App to interact with your PDFs ](https://www.reddit.com/r/Supabase/comments/1ho40ha/supabase_and_open_ai_realtime_with_langchain/) , 2025-01-02-0913
```
Hi Everyone, we are proud to share the release of our open source voice-to-voice Proof of concept where you can upload y
our documents and ask questions related to them.

You can upload your documents and interact with them through our dashb
oard.📊.

Based on OpenAI Realtime AND langchain

Powered by [Supabase](https://www.linkedin.com/company/supabase/)  \+ [
Qdrant](https://www.linkedin.com/company/qdrant/)  \+ [NextJs](https://www.linkedin.com/company/nextjs/)

Github repo: [
https://github.com/actualize-ae/voice-chat-pdf](https://github.com/actualize-ae/voice-chat-pdf)

**If you like the conce
pt or have feedback please feel free to contribute a star and share feedback :)**

Video: [https://vimeo.com/1039742928?
share=copy](https://vimeo.com/1039742928?share=copy)
```
---

     
 
all -  [ Open AI Realtime with langchain powered RAG POC ](https://www.reddit.com/r/OpenAIDev/comments/1ho3sxb/open_ai_realtime_with_langchain_powered_rag_poc/) , 2025-01-02-0913
```
Hi Everyone, we are proud to share the release of our open source voice-to-voice Proof of concept where you can upload y
our documents and ask questions related to them.

You can upload your documents and interact with them through our dashb
oard.📊.

Based on OpenAI Realtime AND langchain

Powered by [Supabase](https://www.linkedin.com/company/supabase/)  \+ [
Qdrant](https://www.linkedin.com/company/qdrant/)  \+ [NextJs](https://www.linkedin.com/company/nextjs/)

Github repo: [
https://github.com/actualize-ae/voice-chat-pdf](https://github.com/actualize-ae/voice-chat-pdf)

**If you like the conce
pt or have feedback please feel free to contribute a star and share feedback :)**

Video: [https://vimeo.com/1039742928?
share=copy](https://vimeo.com/1039742928?share=copy)


```
---

     
 
all -  [ Invite Emails for EU Region are not being Sent ](https://www.reddit.com/r/LangChain/comments/1ho3a66/invite_emails_for_eu_region_are_not_being_sent/) , 2025-01-02-0913
```
I've been trying to sign up via https://eu.smith.langchain.com/. It keeps saying I should check my email for a confirmat
ion link, which is not arriving (no, not in my spam either). Please advise.
```
---

     
 
all -  [ Langchain and embeddings in voice pipeline? ](https://www.reddit.com/r/homeassistant/comments/1hnwy4q/langchain_and_embeddings_in_voice_pipeline/) , 2025-01-02-0913
```
Has anyone thought about adding a vertex DB to the voice pipeline rather than adding all entities exported in the prompt
 every time? Maybe we could have a plug-in that allows calling a langchain pipeline rather than an LLM directly, this wo
uld allow querying entities rather than having them in the prompt and could open up to a more complex multi agent setup 

```
---

     
