 
all -  [ Daily struggles with my LLM based chatbot in production ](https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/) , 2024-03-22-0909
```
What are some challenges you face after deploying your LLM based application in production? 

My only goal is to improve
 the accuracy of my chatbot. It seems like everything boils down to this unless there are any other special usecases you
 are using the LLMs for. Basically. I try to monitor for all the responses of my chatbot and measure them objectively so
 I can tweak and improve the accuracy. This seems pretty basic. But, what are some of the other levers that I can pull t
o improve the accuracy of my RAG based chat application?

&#x200B;

I am also building a tooling for tracing and monitor
ing the responses with higher cardinality compared to the ones that are in the market. Plan to open source it pretty soo
n.
```
---

     
 
all -  [ chunking strategies for code? ](https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/) , 2024-03-22-0909
```
I'm looking to build a RAG app on our github repositories but wanted to ask if anyone has done something like this and w
hat chunking strategies worked and didn't work. I've been looking at semantic chunking but unsure how this would work wi
th code? 
```
---

     
 
all -  [ Okahu AI Observability is now in preview! ](https://www.reddit.com/r/AIObservability/comments/1bkknxl/okahu_ai_observability_is_now_in_preview/) , 2024-03-22-0909
```
Observe your AI apps and cloud infra they run on with [Okahu](https://www.okahu.ai/) to understand how to make them work
 better - more reliable, performant and cost-effective.

Check out [What’s Okahu you ask?](https://youtu.be/JOno9MXenps)
 on Youtube

Check out [this Github repo](https://github.com/okahu/okahu-demo-openai) to try Okahu yourself with a sampl
e chatbot that uses OpenAI and Langchain right from your Github Codespaces and Okahu developer API. 

Give us feedback o
n what observations and systems you want use to support next! 

Add a comment to this discussion or reach out to us at [
dx@okahu.ai](mailto:dx@okahu.ai). 
```
---

     
 
all -  [ Need advice for structuring multimodal data for RAG ](https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/) , 2024-03-22-0909
```
Hey folks,  
I am in the process of building my first custom GPT and have some questions regarding how to work properly 
with multimodal data. Let me explain what I am trying to achieve.  
I am creating a helper tool that will assist me in a
nalyzing various pricing strategies of different SaaS tools. I have a dataset of 100k SaaS companies that have been labe
led in some way, so I can cluster them based on their industry, category, etc.

Here is what I have as an input for my G
PT so far:

1. I have collected screenshots of their pricing pages, which are stored in S3.
2. I have collected the HTML
 for the pricing pages, which is stored in MongoDB.
3. I have a table of the companies with enriched data.

I would like
 to build RAG on top of these documents, but I am a bit concerned about the next steps. My plan is to start with a simpl
e one and use LlamaIndex. Here are the steps I have in mind:

1. Connect the data to the LlamaHub and pick the proper da
tabase. I want to keep the connection between the three mediums. and thus, I am not sure which database is best for my c
ase. Should I use a vector database, graph database, or key-value database here?
2. Query the data and come up with eval
uation metrics based on expert knowledge.

I have some questions along the way:

1. Should I parse the data from the scr
eenshots and HTML structure beforehand, or can I put it into storage as it is? Will it help with the quality of RAG?
```
---

     
 
all -  [ text-embedding-3-large chunking question for RAG ](https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/) , 2024-03-22-0909
```
We know 3-large has a 8191 token context window. I have text articles that are anywhere from 2500-4500 tokens each. Is t
here any advantage to chunking these? Or will I lose some of the context splitting articles into pieces?

Is it better t
o just get embeddings on whole articles or is it still a good idea to split them up into paragraphs? Or both? Feed it wh
ole articles and paragraphs?

Thanks in advance for your insight.
```
---

     
 
all -  [ LangChain Functionality in Node.js and Python for Text Processing ](https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/) , 2024-03-22-0909
```
Does LangChain for Node.js offer the same level of functionality as its Python counterpart when it comes to functions an
d features? 

If not, is there an alternative framework ?

Context : I am familiar with Node.js. I am looking to interac
t with an LLM for text extraction, NER, and coherence. I aim to take the response to create nodes, relationships, and la
bels in a Neo4j graph database.
```
---

     
 
all -  [ Struggling to get an interview for entry level Machine Learning Engineer/Data Science roles ](https://www.reddit.com/r/resumes/comments/1bkcno9/struggling_to_get_an_interview_for_entry_level/) , 2024-03-22-0909
```
As mentioned in the title, I'm a new grad looking to break into the ML space but am struggling to get even a single inte
rview. Please review my resume and let me know of any suggestions. Feel free to be harsh. Thanks!

https://preview.redd.
it/h8uy6n247qpc1.png?width=913&format=png&auto=webp&s=9c3aa991ae69e54de74ec1eda3ef537c89dde477
```
---

     
 
all -  [ Rule Based LLM Chatbot ](https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/) , 2024-03-22-0909
```
I want to build a LLM Chatbot that can follow a particular flow the one we build in intent based chatbot frameworks. I w
ant the llm to collect some information from user based on it fetch some data handle fallback queries and it should not 
deviate from the flow handling multi turn conversations.Any idea or open source framework that does this? Basically I wa
nt to use RASA stories and feed it to LLM so that it can follow a particular conversational flow. 
```
---

     
 
all -  [ 22 New Data Science, Data Engineering and Machine Learning jobs ](https://www.reddit.com/r/jobbit/comments/1bkbtai/22_new_data_science_data_engineering_and_machine/) , 2024-03-22-0909
```
|Job Title|Company|Location|Country|Skills|
|:-|:-|:-|:-|:-|
|[Data Scientist](https://datayoshi.com/offer/575107/data-s
cientist)|[CodeUp](https://www.datayoshi.com/company/codeup-jobs)|[Los Angeles](https://datayoshi.com/offer/575107/data-
scientist)|[United States](https://datayoshi.com/offer/575107/data-scientist)|[Machine Learning, Data Visualization](htt
ps://datayoshi.com/offer/575107/data-scientist)|
|[(Senior) Data Analyst – Digital Analytics (m/f/d)](https://datayoshi.
com/offer/945939/senior-data-analyst-digita)|[L'Oréal](https://www.datayoshi.com/company/l'oréal-jobs)|[Düsseldorf](http
s://datayoshi.com/offer/945939/senior-data-analyst-digita)|[Germany](https://datayoshi.com/offer/945939/senior-data-anal
yst-digita)|[SQL, Power BI, Python](https://datayoshi.com/offer/945939/senior-data-analyst-digita)|
|[Senior Data Analys
t - Demand & Assortment Planning...](https://datayoshi.com/offer/660763/senior-data-analyst-demand)|[Decathlon Digital](
https://www.datayoshi.com/company/decathlon-digital-jobs)|[Paris](https://datayoshi.com/offer/660763/senior-data-analyst
-demand)|[France](https://datayoshi.com/offer/660763/senior-data-analyst-demand)|[Modeling, Spark, Python](https://datay
oshi.com/offer/660763/senior-data-analyst-demand)|
|[Développeur Big Data / Data analyst H/F](https://datayoshi.com/offe
r/254174/developpeur-big-data-data-an)|[CNAM](https://www.datayoshi.com/company/cnam-jobs)|[Paris](https://datayoshi.com
/offer/254174/developpeur-big-data-data-an)|[France](https://datayoshi.com/offer/254174/developpeur-big-data-data-an)|[S
QL, Java, Tableau](https://datayoshi.com/offer/254174/developpeur-big-data-data-an)|
|[Senior Data Analyst - Value Chain
 (f/m/d)](https://datayoshi.com/offer/469469/senior-data-analyst-value-ch)|[Decathlon Digital](https://www.datayoshi.com
/company/decathlon-digital-jobs)|[Croix](https://datayoshi.com/offer/469469/senior-data-analyst-value-ch)|[France](https
://datayoshi.com/offer/469469/senior-data-analyst-value-ch)|[Tableau, scikit-learn, Spark](https://datayoshi.com/offer/4
69469/senior-data-analyst-value-ch)|
|[Stagiaire Data Engineer (6 mois)](https://datayoshi.com/offer/149005/stagiaire-da
ta-engineer-6-moi)|[AXA Partners](https://www.datayoshi.com/company/axa-partners-jobs)|[Malakoff](https://datayoshi.com/
offer/149005/stagiaire-data-engineer-6-moi)|[France](https://datayoshi.com/offer/149005/stagiaire-data-engineer-6-moi)|[
Spark](https://datayoshi.com/offer/149005/stagiaire-data-engineer-6-moi)|
|[Data Analyst](https://datayoshi.com/offer/10
7630/data-analyst)|[Hatch](https://www.datayoshi.com/company/hatch-jobs)|[Sydney](https://datayoshi.com/offer/107630/dat
a-analyst)|[Australia](https://datayoshi.com/offer/107630/data-analyst)|[](https://datayoshi.com/offer/107630/data-analy
st)|
|[Data Engineer](https://datayoshi.com/offer/394607/data-engineer)|[BCT Resourcing](https://www.datayoshi.com/compa
ny/bct-resourcing-jobs)|[London](https://datayoshi.com/offer/394607/data-engineer)|[United Kingdom](https://datayoshi.co
m/offer/394607/data-engineer)|[Python, Scala](https://datayoshi.com/offer/394607/data-engineer)|
|[Machine Learning Engi
neer](https://datayoshi.com/offer/355609/machine-learning-engineer)|[IQVIA](https://www.datayoshi.com/company/iqvia-jobs
)|[Warsaw](https://datayoshi.com/offer/355609/machine-learning-engineer)|[Poland](https://datayoshi.com/offer/355609/mac
hine-learning-engineer)|[Python, Machine Learning](https://datayoshi.com/offer/355609/machine-learning-engineer)|
|[Data
 Engineer](https://datayoshi.com/offer/614299/data-engineer)|[9am](https://www.datayoshi.com/company/9am-jobs)|[Austria]
(https://datayoshi.com/offer/614299/data-engineer)|[Austria](https://datayoshi.com/offer/614299/data-engineer)|[ETL, SQL
, Python](https://datayoshi.com/offer/614299/data-engineer)|
|[AI Engineer (Python, Langchain)](https://datayoshi.com/of
fer/240666/ai-engineer-python-langchain)|[JUPUS](https://www.datayoshi.com/company/jupus-jobs)|[Germany](https://datayos
hi.com/offer/240666/ai-engineer-python-langchain)|[Germany](https://datayoshi.com/offer/240666/ai-engineer-python-langch
ain)|[Python](https://datayoshi.com/offer/240666/ai-engineer-python-langchain)|
|[Staff/Senior Data Engineer](https://da
tayoshi.com/offer/793093/staff-senior-data-engineer)|[Glia](https://www.datayoshi.com/company/glia-jobs)|[Spain](https:/
/datayoshi.com/offer/793093/staff-senior-data-engineer)|[Spain](https://datayoshi.com/offer/793093/staff-senior-data-eng
ineer)|[SQL, Kafka](https://datayoshi.com/offer/793093/staff-senior-data-engineer)|
|[Data Analyst (m/w/d)](https://data
yoshi.com/offer/270696/data-analyst-m-w-d)|[Pink Personalmanagement GmbH](https://www.datayoshi.com/company/pink-persona
lmanagement-gmbh-jobs)|[Herford](https://datayoshi.com/offer/270696/data-analyst-m-w-d)|[Germany](https://datayoshi.com/
offer/270696/data-analyst-m-w-d)|[Power BI](https://datayoshi.com/offer/270696/data-analyst-m-w-d)|
|[Master Data Analys
t](https://datayoshi.com/offer/155622/master-data-analyst)|[Hach](https://www.datayoshi.com/company/hach-jobs)|[Berlin](
https://datayoshi.com/offer/155622/master-data-analyst)|[Germany](https://datayoshi.com/offer/155622/master-data-analyst
)|[SQL](https://datayoshi.com/offer/155622/master-data-analyst)|
|[Junior Data Engineer](https://datayoshi.com/offer/323
784/junior-data-engineer)|[Team Remotely Inc](https://www.datayoshi.com/company/team-remotely-inc-jobs)|[New Haven](http
s://datayoshi.com/offer/323784/junior-data-engineer)|[United States](https://datayoshi.com/offer/323784/junior-data-engi
neer)|[Modeling, Scala](https://datayoshi.com/offer/323784/junior-data-engineer)|
|[Data Scientist 1](https://datayoshi.
com/offer/248159/data-scientist-1)|[PayPal](https://www.datayoshi.com/company/paypal-jobs)|[Bengaluru](https://datayoshi
.com/offer/248159/data-scientist-1)|[India](https://datayoshi.com/offer/248159/data-scientist-1)|[SQL, Python, Machine L
earning](https://datayoshi.com/offer/248159/data-scientist-1)|
|[Machine Learning Engineer II](https://datayoshi.com/off
er/367087/machine-learning-engineer-ii)|[The Shoprite Group of Companies](https://www.datayoshi.com/company/the-shoprite
-group-of-companies-jobs)|[Brackenfell](https://datayoshi.com/offer/367087/machine-learning-engineer-ii)|[South Africa](
https://datayoshi.com/offer/367087/machine-learning-engineer-ii)|[Machine Learning](https://datayoshi.com/offer/367087/m
achine-learning-engineer-ii)|
|[AI Product Manager](https://datayoshi.com/offer/234947/ai-product-manager)|[Match Health
](https://www.datayoshi.com/company/match-health-jobs)|[Birmingham](https://datayoshi.com/offer/234947/ai-product-manage
r)|[United Kingdom](https://datayoshi.com/offer/234947/ai-product-manager)|[Machine Learning](https://datayoshi.com/offe
r/234947/ai-product-manager)|
|[Senior  Data Analyst](https://datayoshi.com/offer/950284/senior-data-analyst)|[Optum](ht
tps://www.datayoshi.com/company/optum-jobs)|[Dublin](https://datayoshi.com/offer/950284/senior-data-analyst)|[Ireland](h
ttps://datayoshi.com/offer/950284/senior-data-analyst)|[Power BI, AWS, Data Visualization](https://datayoshi.com/offer/9
50284/senior-data-analyst)|
|[Data Engineer](https://datayoshi.com/offer/981999/data-engineer)|[MindPal](https://www.dat
ayoshi.com/company/mindpal-jobs)|[Lyon](https://datayoshi.com/offer/981999/data-engineer)|[France](https://datayoshi.com
/offer/981999/data-engineer)|[AWS](https://datayoshi.com/offer/981999/data-engineer)|
|[Junior Machine Learning Engineer
](https://datayoshi.com/offer/830779/junior-machine-learning-engine)|[Patterned Learning Career](https://www.datayoshi.c
om/company/patterned-learning-career-jobs)|[Enterprise](https://datayoshi.com/offer/830779/junior-machine-learning-engin
e)|[United States](https://datayoshi.com/offer/830779/junior-machine-learning-engine)|[Machine Learning, Deep Learning](
https://datayoshi.com/offer/830779/junior-machine-learning-engine)|
|[Data Engineer H/F](https://datayoshi.com/offer/849
434/data-engineer-h-f)|[LesJeudis](https://www.datayoshi.com/company/lesjeudis-jobs)|[Clichy](https://datayoshi.com/offe
r/849434/data-engineer-h-f)|[France](https://datayoshi.com/offer/849434/data-engineer-h-f)|[SQL](https://datayoshi.com/o
ffer/849434/data-engineer-h-f)|
                        
 Hey, here are 22 New Data Science, Data Engineering and Machin
e Learning jobs. 

 For more, check our Google sheet with more opportunities in Data Science and Machine Learning (updat
ed each week) [here](https://docs.google.com/spreadsheets/d/1Vsg1Jmc0ZIDc_tPqZTzhbgxGIeTDQkUsBySMNbbCFI4/) 

 If you wan
t to take some Data and ML courses, click [here](https://learncrunch.com/courses?utm_source=reddit&source=reddit) 

  Le
t me know if you have any questions. Cheers!
```
---

     
 
all -  [ Suggestions on working agents and base LLMs? ](https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/) , 2024-03-22-0909
```
Hi. I’m testing a variety of LLaMa2 7b and 13b (Hermes2Pro, MistralInstruct0.2, Chat, Solar10) as base for the React age
nt, but I can’t get outputs consistently as I’m encountering these issues:

1. After providing the final answer, the LLM
 keeps running other actions and it gets off track with eg. Non relevant questions or even random programming code.
2. S
ometimes it calls the tools incorrectly, especially if I switch to the structured chat agent (non existing arguments or 
swapped args between functions).

What I’m guessing is that I need larger models. I would appreciate someone else’s expe
rience and takes on this. Thank you very much!

```
---

     
 
all -  [ In need of data scientist in ATLANTA/GA ](https://www.reddit.com/r/DataScientist/comments/1bk8wsd/in_need_of_data_scientist_in_atlantaga/) , 2024-03-22-0909
```
  

# data scientist ( long term role)

**Role - Data Scientist**

**Skills Required** \-Proficiency in SQL, Natural Lan
guage Processing (NLP) using technologies like PyTorch, TensorFlow, and Apache Spark is essential. A strong foundation i
n Python programming and familiarity with Databricks and pandas for data manipulation is required.  
Machine Learning En
gineer Comprehensive understanding of Machine Learning models and algorithms is necessary. The ability to script in Pyth
on for data analysis, and experience with Python libraries such as HuggingFace, Langchain, scikit-learn and Keras is exp
ected.

send your resume to [kiruthick.murali@mazosol.com](mailto:kiruthick.murali@mazosol.com)
```
---

     
 
all -  [ data scientist ( long term role) ](https://www.reddit.com/r/atlantajobs/comments/1bk8uqr/data_scientist_long_term_role/) , 2024-03-22-0909
```
**Role - Data Scientist**	

  **Skills Required** \-Proficiency in SQL,  Natural Language Processing (NLP) using technol
ogies like PyTorch, TensorFlow, and Apache Spark is essential. A strong foundation in Python programming and familiarity
 with Databricks and pandas for data manipulation is required.  
Machine Learning Engineer	Comprehensive understanding o
f Machine Learning models and algorithms is necessary. The ability to script in Python for data analysis, and experience
 with Python libraries such as HuggingFace, Langchain, scikit-learn and Keras is expected.

send your resume to [kiruthi
ck.murali@mazosol.com](mailto:kiruthick.murali@mazosol.com)

&#x200B;
```
---

     
 
all -  [ Need input on Software Project ](https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/) , 2024-03-22-0909
```
I'm building a software application. I want to use a LLM as the basis. I will finetune the model with about 20,000 pages
 of legal text. Specifically laws all around the country. I will then use the trained model to answer help companies cre
ate compliant products and services. At the moment I am unsure as to the best way to go about it. My initial thought was
 to use gpt 3.5 and then further train it with the 20,000 pages of text. The text will be broken down by state and feder
al agencies. This text will be added to as new laws and regulations are passed.

&#x200B;

The goal is for the model to 
only answer based on the data set it's finetuned with. The data will be broken down by state. For example if they want i
nformation about loan requirements, they will ask the system and it will return with an outline of the requirements for 
loans by state. It will respond in a way that's easy to understand.

&#x200B;

My thinking is once I have all the data c
ollected, using Langchain to fine tune the LLM. Am I on the right path here?
```
---

     
 
all -  [ Planning the development for a new app for a client with 2M+ existing users ](https://www.reddit.com/r/softwarearchitecture/comments/1bk83el/planning_the_development_for_a_new_app_for_a/) , 2024-03-22-0909
```
**TLDR;**

Built a GPT wrapper app with LangChain+Vector storage using Flowai + NextJS + Clerk + Firebase. Should I cont
inue building with this for a scalable app (2M+ users) or write my own services/APIs using Node.js, Langchain, Postgres 
(pgvector)?

**Why thinking of scale from the start?**  
I run a software outsourcing business. Recently got a project f
rom a client who is running a B2C app with 2M+ users. They're planning to launch a new app, the development has to be do
ne from scratch. The only 2 business requirements I have been give is that they need faster iterations and scalable appl
ication. The reason they need scale from the start it because they've planned massive PR activities around the launch an
d are also looking at moving their existing users to the new app, thereby shutting down the existing app.

**App Synopsi
s**

1. The app is largely a wrapper on GPT with very few REST APIs:
2. Auth Module
3. Uploading content to store in a V
ector DB
4. Storing/Retrieving conversation history from DB
5. Payment Module
6. GPT communication layer with Langchain 
+ prompt templates

**Work Done**

As part of my pitch, I had created a quick demo using NextJS + Clerk + Pinecone + Flo
wAI + Firebase. It took me a total of 5 days to create this working Demo MVP.

**My Confusion**

1. Should I continue wi
th this stack itself, and work to refine the frontend and additional minor changes?
2. If yes, will that be scalable eno
ugh?
3. Should I write code the traditional way, wherein I create my own services for communicating with GPT, APIs, etc.
 using Node.js (Nest.js), mongoDB + Prisma, and use self hosted vector storage (Postgres + pgvector)?
```
---

     
 
all -  [ Best way to create an AI browsing agent ](https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/) , 2024-03-22-0909
```
Hi folks!

I have been interested in AI for some time, but now the time has come for I want to create a self-browsing ag
ent that answers a certain question.

Couple of examples:

1️⃣ Prompt: what is the company [www.segment.com](https://www
.segment.com) about?

\- Should execute a Google search  
\- Navigate to the about-us page  
\- Read the page and return
 the result

&#x200B;

2️⃣ Prompt: what is the difference between [www.segment.com](https://www.segment.com) and [www.in
tercom.com](https://www.intercom.com)?

\- Should split this into multiple tasks, doing something similar to the workflo
w above.  
\- Returns a detailed comparison based on the scanned pages

&#x200B;

What are the best ways of implementing
 this? Are there any open-source frameworks that I might get inspired by?
```
---

     
 
all -  [ My debut book on GenAI is a  International Bestseller now ! ](https://i.redd.it/j9xhh57o9opc1.png) , 2024-03-22-0909
```
I'm happy to share that my debut book 'LangChain in your Pocket: Beginners guide to building Generative AI applications 
using LLMs' that was released in late Jan'24 is now amongst International Bestsellers (AI category) on amazon.com . A bi
g thanks to everyone for the support !!

You can check out the book here :  https://amzn.in/d/iWh7a7Y


```
---

     
 
all -  [ Store metadata in faiss and retrieve along with embedding? ](https://www.reddit.com/r/LocalLLaMA/comments/1bk3tkw/store_metadata_in_faiss_and_retrieve_along_with/) , 2024-03-22-0909
```
Hey everyone, just looking for some opinions/suggestions or maybe an example if you have one to help me move forward on 
this project.

To give some background,

- using GPT I’ve summarized about 500 different docs, each about 6000 words ori
ginally now condensed down to around 300

- with those summaries, I intend to create embeddings using langchain faiss an
d store them in a vector database

- along with each embedding set I want to attach a metadata tag that will link back t
o the original full text doc

- when the similarity search returns the most relevant embeddings (based on the summaries)
, I will pull the metadata tag that links to the full docs for each relevant summary, and pass all of the full docs to G
PT to provide a thorough answer 

I’m just having trouble figuring out how to tag the meta data with my embeddings and h
ow to capture it in the results from the similarity search. Does anyone have any examples similar to this that they coul
d share? 
```
---

     
 
all -  [ Talk with your Data! (LangChain + Streamlit) ](https://www.reddit.com/r/MediumApp/comments/1bk2nio/talk_with_your_data_langchain_streamlit/) , 2024-03-22-0909
```
Read my article to learn how to build a simple GenAI bot, with LangChain and Streamlit in Python.

Give it a dataset and
 get straight answers on it!

[https://medium.com/python-in-plain-english/talk-with-your-data-c26aac1836b2](https://medi
um.com/python-in-plain-english/talk-with-your-data-c26aac1836b2)
```
---

     
 
all -  [ How to build a RAG on structed data? ](https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/) , 2024-03-22-0909
```
I have a word doc and an excel file whose information is interconnected? The excel file outlines the process steps and t
he word file has process specifics. 

I want to build a RAG by leveraging these two files to generate a document based o
n some prompts. What is the best strategy to do it?
```
---

     
 
all -  [ Creating chatbot of characters using RAG ](https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/) , 2024-03-22-0909
```
I have the biography of a fictional character. It is about 160 pages long. How do I create a chatbot of this character w
ith memory using RAG? I am using Gemini btw. 
```
---

     
 
all -  [ Ideal Toolchain for Embedding Employee Training Documents ](/r/LLMDevs/comments/1bjzhja/ideal_toolchain_for_embedding_employee_training/) , 2024-03-22-0909
```

```
---

     
 
all -  [ [For Hire] Ex-Booking[dot]com Data Analyst and GenAI specialist at a Startup [35 USD/hr] ](https://www.reddit.com/r/SaaS/comments/1bjzsxo/for_hire_exbookingdotcom_data_analyst_and_genai/) , 2024-03-22-0909
```
Hey, I have been using this subreddit to get clients and have completed 3 jobs here, all with good reviews.

Here are my
 skills and experience:

1) Technical Skills: Python, SQL, R 

2) Academics: Bs Econometrics, Mathematics and Computer S
cience

3) Specialty: GenAI, Statistics, Data Visualization.

4) Sectors: Databases, Hospitality, ecommerce and Telecom



Here are some of my projects (I write extensively on Medium):

Link: https://arslanshahid-1997.medium.com/using-langch
ain-to-teach-an-llm-to-write-like-you-aab394d54792

Link: https://towardsdatascience.com/money-balling-cricket-probabili
ty-of-100-using-repeated-conditioning-2fc8dbceb42e


I have over 4+ years experience in Data Science. 


Please do reach
 out, fair rate of 35 USD/hr. 
```
---

     
 
all -  [ Langchain in Production (& Alternatives) ](https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/) , 2024-03-22-0909
```
Has anyone here succesfully deployed LangChain in production? If yes, what were the main issues enountered and how did y
ou approach them?

If not, what alternatives did you use or considering (e.g. Haystack etc.) ?
```
---

     
 
all -  [ what is the advantage of overlapping in chunking strategy ](https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/) , 2024-03-22-0909
```
I am implementing RAG and trying to understand what's the advantage of Overlapping.  
Consider this text:  
`'One of the
 most important things I didn't understand about the world when I was a child is the degree to which the returns for per
formance are superlinear.'`  
 which is chunked and overlapped as  using Naive or any strategy :  
`chunk 1 : One of the
 most important things`  
`Chunk 2 : things I didn't understand about`  
`chunk 3: about the world when I was a child`  

and so on..  
As you can see there is a word overlap with the chunks.   
What advantage does LLM get when you feed over
lapping  `chunk2` and `chunk3` to execute RAG prompt against a user query. 
```
---

     
 
all -  [ Best Search Tool in Langchain  ](https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/) , 2024-03-22-0909
```
Hi all, was going through the search tools available via langchain. Just wanted to check which is the best one to use? W
hat are the key aspects to consider other than cost? If anyone who has used/compared these APIs that would be a great va
lue add to my research 
```
---

     
 
all -  [ Best Search Tool in Langchain  ](https://www.reddit.com/r/Langchaindev/comments/1bjsw3d/best_search_tool_in_langchain/) , 2024-03-22-0909
```
Hi all, was going through the search tools available via langchain. Just wanted to check which is the best one to use
```
---

     
 
all -  [ Human intervention in agent workflows ](https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/) , 2024-03-22-0909
```
When building LLM workflows with LangChain/LangGraph what's the best way to build a node in the workflow **where a human
 can validate/approve/reject** a flow? I know there is a Human-in-the-loop component in LangGraph that will prompt the u
ser for input. But what if I'm not creating a user-initiated chat conversation, but a flow that reacts to e.g. incoming 
emails?

I guess I'd have to design my UI so that it's not only a simple single-threaded chat interface, but some sort o
f inbox, right? Or is there any standard way that comes to mind?
```
---

     
 
all -  [ The Perils of trying to build a RAG using TinyLlama on CPU ](https://www.reddit.com/r/LLMDevs/comments/1bjn91u/the_perils_of_trying_to_build_a_rag_using/) , 2024-03-22-0909
```
Hi everyone. I needed some help understanding, how to properly build a RAG.

Disclaimer: Everything here is only a week 
worth of effort. There is a lot of theoritical part about neural networks or ML in general I do not understand. However 
I understand matrices, ranking.

I was intrigued about the whole LLM wave, and wanted to give this a try. Most open to w
ebsites, do not really allow me to play with csvs, pdfs, code from rocksdb's lru\_cache.cc etc. Atleast not the free ver
sions.

Source Code: [https://github.com/ikouchiha47/llama-experiments/tree/master](https://github.com/ikouchiha47/llama
-experiments/tree/master)

So I started off with a small simple project where I could index the *titles.basic.tsv* from 
IMDB and ask it questions like:

1. Which year The Bourne Identity was released?
2. Suggest 5 movies in the same genre.


*And boy, was it not simple.*

There are certain choices I have made:

1. Relying on CPU instead of GPU. Google collab 
is not for me, I had to do a bunch of trial and error, and runtime keeps crashing, google drive goes out of space.
2. Us
ing **llama-cpp-python**, instead of transformers or ctransformers, it seemed simple, also wouldn't need a GPU, I could 
use a **GGUF** format.
3. I used the [TinyLlama-1.1B-Chat-v1.0-GGUF](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat
-v1.0-GGUF)  file.
4. Trying to run the word embedding locally, I just don't know if a GPU is required, or would speed u
p the process
5. Used **FAISS** as vector-database. And I just question my life choices.

Now, I have some understanding
 of using the low ranked matrices. Where the delta of the weights (which we trained) is represented using two low rank m
atrices. But while doing word embedding using sentence-transformer, I wasn't sure if the multiplication can happen on my
 Mac M1 Silicon (aplty named, useless thing).

I also noticed, that during preparing the training data, when I trained u
sing a normal language, like,

`f'The movie {movie_name} was released in the year {year}, and belonged to {genre} genres
'` was producing better results, than something like:

    <|user|>
    Which year was {movie_name} released?</s>
    <|
assistant|>
    {year} </s>
    <|user|>
    What genre {movie_name} belongs to?</s>
    <|assistant|>
    Move {movie_n
ame} belongs to {genre}</s>



So, my `first question` is, **Does the text data for training benefit better from keeping
 the text simple, instead of trying to template it in the way the user query prompt will be formatted?**

The docs: [htt
ps://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/blob/main/README.md](https://huggingface.co/TheBloke/TinyLlam
a-1.1B-Chat-v1.0-GGUF/blob/main/README.md)

    prompt_template: |
      <|system|>
      {system_message}</s>
      <|u
ser|>
      {prompt}</s>
      <|assistant|>

The first hurdle I faced was with loading the data. The [IMDB titles.basic
.tsv dataset](https://developer.imdb.com/non-commercial-datasets/) , it becomes nearly 1GB unziped.

Most tutorials, tha
t claimed that they build an RAG,

1. they either took only 10-20% of any big dataset, or just used a bunch of text in a
rray.
2. Used TextSplitting or CSVLoader from **llangchain** , And then feed the chunks directly into a vector dataset
3
. Varying prompt templates, and maybe a query, and everything worked out fan-fugging-tastic.

Except, it's not real. I f
irst had the idea of using **chunksize** in pandas, and then trying to parallelize it. And in trying to do so, I stumble
d upon **dask,** with a blocksize of **128MB** . There were 7 partitions.

For each partition, it would use the **embedd
ings** from [sentence-transformers](https://huggingface.co/sentence-transformers) / [all-MiniLM-L6-v2](https://huggingfa
ce.co/sentence-transformers/all-MiniLM-L6-v2)  and do

    def __index_parition(self, partition):
            db_ids = n
p.arange(len(partition)) + partition.index.start
    
            result = partition.apply(self.cfg.format_row, axis=1)

            encoded_data = self.transfomer.encode(
                result.tolist(),
                normalize_embeddings
=False,
                show_progress_bar=True,
            )
    
            index = faiss.IndexIDMap(
               
 faiss.IndexFlatL2(self.model.get_sentence_embedding_dimension())
            )
            
            faiss.normalize
_L2(encoded_data)
            index.add(encoded_data, db_ids)

and then call \`index.wirte\_index(filepath)\` . But ther
e is a problem with this.  The **langchain**'s FAISS , while creating index, also create a pickle file. when you use the
 regular methods, like `FAISS.from_texts or FAISS.from_documents`, This is important because, while loading back with `F
AISS.load_local` it would not find the pickle file.

The code is here in changelog: [https://github.com/ikouchiha47/llam
a-experiments/commit/0a52168b3b8405ed641b7b6d142d809982aee2ad](https://github.com/ikouchiha47/llama-experiments/commit/0
a52168b3b8405ed641b7b6d142d809982aee2ad)

So, I had to switch to using `FAISS.from_texts` and in doing so, I now lost th
e ability to see the god damn progress bar.

So comes the second problem. The whole thing, it took like 5-6 hours and di
dn't complete. I am not sure why this is taking this much time. But it certainly has something to do with the way I am e
mbedding. This is how I am doing it now:

        self.merged_index = FAISS.from_texts([''], self.model)
        
      
  def __index_parition(self, partition):
            result = partition.apply(self.cfg.format_row, axis=1)
            i
ndex = FAISS.from_texts(result.tolist(), self.model)
    
            self.merged_index.merge_from(index)
            re
turn result
    
        def index_db(self, meta_keys={}):
            if self.df is None:
                raise Excepti
on('UninitializedDataframeException')
    
            if self.model is None:
                raise Exception('Uninitial
izedModelExeception')
    
            if self.merged_index is None:
                raise Exception('UninitializedIndex
Exception')
    
            partitions = self.df.map_partitions(
                self.__index_parition
                
# , meta={'text': 'str'}
            )
    
            # partitions.visualize()
            partitions.compute()
      
      self.merged_index.save_local(self.vectorstore_path)

source: [https://github.com/ikouchiha47/llama-experiments/blo
b/master/src/mammal.py](https://github.com/ikouchiha47/llama-experiments/blob/master/src/mammal.py)

The idea I have is 
to save the index to individual files, my changing the `index_name` parameter in `save_local` . But I just hate the fact
 that there is no progress-bar.



So, my `second question`, **How do I make this embedding and saving to vector store f
aster?** **Its only 1Gig.**

And my `third and final question`, **Why does this thing so difficult to work with?**

Beca
use this file was so huge, I wanted to take a smaller file and atleast see **RAG** at work. So I took dataset from kaggl
e for [IPL 2023](https://www.kaggle.com/datasets/sankha1998/ipl2023) . And as usual, I wanted to ask question like:

1. 
Who won the IPL 2023?
2. How many matches did they play?

And the answers vary vastly differently depending on how I use
 the training text. Right now the training template looks like this:

    template = (
            'Match {match_number}
 was played between two teams '
            '{team1} and {team2} at {venue}. It was a {match_type} match. '
            
'{toss_won} won the toss and decided to {decision}. '
            '{winner} won the match and {loser} lost the match.'
 
       )

And the PromptTemplate for the user query looks like this:

    prompt_template = '''
    <|system|>
    You a
re given the results of cricket matches played in the Indian
    Premier League(IPL) in the year 2023. Given the chat hi
story \
    delimited by(<hs></hs>) and question which might \
    reference context (delimited by <ctx></ctx>) \
    in
 chat history (delimited by <hs></hs>), formulate an answer.\
    Do NOT print the question.
    
    When answering to 
user, if you do NOT know, just say that you do NOT know.
    Do NOT make up answers from outside the context.
    
    A
void mentioning that you obtained the information from the context.
    And answer according to the language of the user
's question.
    
    <ctx>
    {context}
    </ctx>
    
    <hs>
    {chat_history}
    </hs>
    
    <|user|>
    {q
uestion}</s>
    <|assistant|>

source: [https://github.com/ikouchiha47/llama-experiments/blob/master/src/templates.py](
https://github.com/ikouchiha47/llama-experiments/blob/master/src/templates.py)

The problem is when I query. I am going 
to paste the results:

    Input Prompt: Who won IPL 2023 final match?
     Chennai Super Kings won IPL 2023 final match
.
    Input Prompt: Whom did they play against?
     Did they play against Chennai Super Kings or any other team? No, th
ey did not play against Chennai Super Kings, as they were playing against Gujarat Titans in the Final match.
    Input P
rompt: How many matches were played in total in IPL 2023?
     How many matches did Chennai Super Kings play against Guj
arat Titans in the Final match of IPL 2023? 2
    Input Prompt: exit

Here, the answer is definitely wrong. I checked ag
ainst the **index.pkl** file as well. They did not play only 2 matches.

Secondly, sometimes, the the output also has **
wrong spellings** , I am not sure why, the csv file doesn't have those spellings in them.

Thirdly, the output looks all
 weird, with **rephrasing** and subsequent answers showing in 1 line.

    self.model = LlamaCpp(
                    mo
del_path=self.model_path,
                    stop=['</s>'],
                    context_window=2048,
                  
  n_ctx=1024,
                    n_batch=100,
                    n_threads=8,
                    n_gpu_layers=0,
    
                callbacks=[StreamingStdOutCallbackHandler()],
                    temperature=0,
                    # r
epeat_penalty=1.2,
                    verbose=self.verbose,
                )

this is how the model has been configure
d.



Please haaaaalp.
```
---

     
 
all -  [ RAG chain with HF model works fine for first quest, then OOM for subsequent chain. No OOM issue when ](https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/) , 2024-03-22-0909
```
I have built a simple RAG chain with message history using Mistral-7b model with 4bit quantization. 

Whenever I build t
his chain using a model from the dockerized Ollama, everything works fine and I can have a long conversation with the ch
ain. 

However, as soon as I switch to HF model, only the first message goes through, everything else gets the OOM memor
y. In fact, the memory usage seems to increase with each subsequent invoke. 

In both cases, I am using the Mistral-7b m
odel with quantization. So I am confused as to where the memory issue comes from.   


Here are the code snippets:  


U
sing HF model:

    model_name = 'mistralai/Mistral-7B-Instruct-v0.2'
    bnb_config = BitsAndBytesConfig(
        load_
in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.bfloat16
    )
   
 
    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)
    tokenizer = AutoToken
izer.from_pretrained(model_name)
    
    text_generation_pipeline = pipeline(
        model=model,
        tokenizer=to
kenizer,
        task='text-generation',
        temperature=0.2,
        do_sample=True,
        repetition_penalty=1.1
,
        max_new_tokens=400,
    )
    
    chat_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

Using Ol
lama model:

    chat_llm = ChatOllama(model='mistral:7b')

Overall chain setup

    chatbot_conversation_with_context_c
hain = 
   RunnablePassthrough.assign(standalone_message=standalone_message_chain).assign(context= 
   itemgetter('stand
alone_message') | retriever).assign(output= question_answering_prompt | 
   chat_llm | StrOutputParser())
    
    chatb
ot = RunnableWithMessageHistory(
       chatbot_conversation_with_context_chain,
       get_session_history=get_session_
history,
       input_messages_key='messages',
       history_messages_key='chat_history',
       history_factory_config
=[
       ConfigurableFieldSpec(
             id='user_id',
             annotation=str,
             name='User ID',
  
           description='Unique identifier for the user.',
             default='',
             is_shared=True,
        
),
       ConfigurableFieldSpec(
             id='conversation_id',
             annotation=str,
             name='Conv
ersation ID',
             description='Unique identifier for the conversation.',
             default='',
             
is_shared=True,
        ),
    ],
)
    response = chatbot.invoke(
    {'messages': 'Can you give me the basic Java code
 for reading a CSV file?'},
       config={
       'configurable': {'user_id': 'test', 'conversation_id': 'dummy'}
     
   },
)

print(response.keys())
for key in response.keys():
       print(key+': ', end='')
       print(response[key])


response = chatbot.invoke(
      {'messages': 'Can you elaborate on the first function?'},
      config={'configurable':
 {'user_id': 'test', 'conversation_id': 'dummy'}
  },
)

print(response.keys())
for key in response.keys():
       print
(key+': ', end='')
       print(response[key])
    

&#x200B;
```
---

     
 
all -  [ Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions. ](https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/) , 2024-03-22-0909
```
* Adding function definitions in the system prompt of functions (Clickup's API calls).
* Flattening the Schema of the fu
nction
* Adding system prompts
* Adding function definitions in system prompt
* Adding individual parameter examples
* A
dding function examples

Wrote a nice blog with an [Indepth explanation](https://blog.composio.dev/improving-function-ca
lling-accuracy-for-agentic-integrations/) here.

https://preview.redd.it/rmxgt35zfjpc1.png?width=816&format=png&auto=web
p&s=934eddf839e17f2324c590157943a92ebbdedffa
```
---

     
 
all -  [ Do researchers like langchain? ](https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/) , 2024-03-22-0909
```
I’m a Ph.D. student who recently try to switch from hugging face to langchain. It feels like huggingface organize their 
libraries     the research way (or the PyTorch way? It just feel like I can use them the same way I use research papers’
 code), but langchain is more like something developed by JavaScript engineers and designed with no research user cases.
 

For example, all the “batch inference “ requirements on GitHub are ignored. The interface for customized functions (e
.g., chat history post processing) are ill-designed. 

I chose langchain in the beginning because the LLMs hosted by lan
gchain responds faster than my local ones. But it seems that it’s really hard to customize the functionalities for resea
rch purposes.
```
---

     
 
all -  [ Can anyone suggest a idea to implement RAG with LLm.Like if the searched query not in RAG data then  ](https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/) , 2024-03-22-0909
```
If any Colab notebook or github repo available then it will be helpful
```
---

     
 
all -  [ is it possible to connect 2 GeForce RTX 4090 to a Laptop? ](https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/) , 2024-03-22-0909
```
Hi,

i have built a Langchain RAG app with a local model and now want to be able to run it on a Laptop. I am using a qua
ntized Mixtral Model (Q5\_0) and for this I want to conntect 2 GeoForce RTX 4090 to my laptop. As I am a newby (and noob
y) in the Hardware topic, is it even possible to connect 2 RTX 4090 to a more or less 'normal' Laptop?

The use case wou
ld be that the customer tries the (local) application on a standalone device and if he is happy with it he buys more Har
dware to host it for production.

At the moment I am running everything on my Macbook with 64GB RAM but I need a solutio
n for a customer with a Windows PC.

One other option would be that the customer just buys a Macbook, but the 2 GeForece
 RTX 4090 would be a better investment I think because these could further be used for a prodcution setting.

&#x200B;


Thanks for you suggestions!
```
---

     
 
all -  [ Tengyu Ma on Voyage AI - Weaviate Podcast #91! ](https://www.reddit.com/r/deeplearning/comments/1bjft8i/tengyu_ma_on_voyage_ai_weaviate_podcast_91/) , 2024-03-22-0909
```
**Voyage AI** is the newest giant in the embedding, reranking, and search model game!

I am SUPER excited to publish our
 latest Weaviate podcast with Tengyu Ma, Co-Founder of Voyage AI and Assistant Professor at Stanford University!

We beg
an the podcast with a deep dive into everything embedding model training and contrastive learning theory. Tengyu deliver
ed a **masterclass** in everything from scaling laws to multi-vector representations, neural architectures, representati
on collapse, data augmentation, semantic similarity, and more! I am beyond impressed with Tengyu's extensive knowledge a
nd explanations of all these topics.

The next chapter dives into a case study Voyage AI did **fine-tuning an embedding 
model for the LangChain documentation.** This is an absolutely fascinating example of the role of continual fine-tuning 
with very new concepts (for example, very few people were talking about chaining together LLM calls 2 years ago), as wel
l as the data efficiency advances in fine-tuning.

We concluded by discussing ML systems challenges in serving an embedd
ings API. Particularly the challenge of detecting if a request is for batch or query inference and the optimizations tha
t go into either say \~100ms latency for a query embedding or maximizing throughput for batch embeddings.

I hope you fi
nd the podcast interesting, more than happy to discuss any of these topics with you or answer any questions about the co
ntent in the podcast! Thank you so much!

YouTube: [https://www.youtube.com/watch?v=xPdyivfheqI](https://www.youtube.com
/watch?v=xPdyivfheqI)

Spotify: [https://spotifyanchor-web.app.link/e/u6XPLYfF7Hb](https://spotifyanchor-web.app.link/e/
u6XPLYfF7Hb)
```
---

     
 
all -  [ Seeking the Ideal Stack for Natural Language Database Interactions ](https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/) , 2024-03-22-0909
```
Hello everyone,

I'm embarking on a project that requires a fresh start, and I find myself at a crossroads trying to dec
ide on the optimal technology stack. The core objective is to enable conversations with a database using natural languag
e, aiming for precise outcomes. This involves working with tabular data, applying filters, and conducting semantic searc
hes.

Given the plethora of options out there, from graph databases and SQLCoder models to Retrieval-Augmented Generatio
n (RAG) techniques, making a choice feels overwhelming. Each of these technologies brings something unique to the table,
 but I'm looking for a solution that balances ease of integration, scalability, and, most importantly, the ability to un
derstand and process natural language queries effectively.

I would greatly appreciate your insights, experiences, or an
y advice you could share on this matter. Which stack or combination of technologies have you found to be the most effect
ive for interacting with databases through natural language? Any pitfalls or success stories you could share would also 
be incredibly helpful as I navigate through these options.

Thank you in advance for your time and help!
```
---

     
 
all -  [ How to randomize output for same input in local LLM with langchain. ](https://www.reddit.com/r/LocalLLaMA/comments/1bjewg3/how_to_randomize_output_for_same_input_in_local/) , 2024-03-22-0909
```
I am relatively new to working with LLMs and I encountered this issue. The thing is that if I use langchain with OpenAI 
with any of their GPT models for the same input whenever I rerun the code I will get a different output. However while u
sing langchain with Pygmalion (notstoic/pygmalion-13b-4bit-128g) running with Oobabooga web-ui, For each rerun of the sa
me input I get the same output. This is confusing because when regenerating the output directly in the webui or via Sill
yTavern I get a different output every time. So can someone help me resolve this.  


Main code [https://pastebin.com/Ge
pu93Av](https://pastebin.com/Gepu93Av)  
CustomLLM wrapper (Not my code found it on github) [https://pastebin.com/D4Baub
Vv](https://pastebin.com/D4BaubVv)  

```
---

     
 
all -  [ Understanding JSONDecodeError when using JsonOutputParser ](https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/) , 2024-03-22-0909
```
Hi everyone, I hope it is fine to post questions here. 

I am just getting started with output-parsers and I'm impressed
 with their usefulness when they work properly. I have, however, run into a case where every now and then, a chain retur
ns an error that seems to be related to the JsonOutputParser that I use, as indicated by the following (condensed) error
 message:

`JSONDecodeError`                             
`JsonOutputParser.parse_result(self, result, partial)`   
`156
 # Parse the JSON string into a Python dictionary`  
`--> 157 parsed = parser(json_str)`  
 `159 return parsed`  
`122 #
 If we got here, we ran out of characters to remove`  
`123 # and still couldn't parse the string as JSON, so return the
 parse error`  
`124 # for the original string.`  
`--> 125 return json.loads(s, strict=strict)`

According to [this pos
t here](https://www.reddit.com/r/LangChain/comments/17hep0o/comment/k6na6nd/?utm_source=share&utm_medium=web3x&utm_name=
web3xcss&utm_term=1&utm_content=share_button) this could be related to there not being 'enough tokens left to fully gene
rate my output', which seems to be in line with the error message above:

\>`122 # If we got here, we ran out of charact
ers to remove` 

although I am not fully sure what that means or how it can be fixed. 

Has anybody encountered this pro
blem before and could offer some guidance? I must admit that I'm feeling kind of stumped, especially since the error can
't be reproduced reliably and only occurs every other time I run my script. 
```
---

     
 
all -  [ Has anyone used dspy for RAG? how does it compare to langchain/llama-index? and how does it 'train'  ](https://www.reddit.com/r/LLMDevs/comments/1bjctuz/has_anyone_used_dspy_for_rag_how_does_it_compare/) , 2024-03-22-0909
```
I have a few questions I am not able to understand about dspy

1. How is it training LLM? 
2. How is it writing prompts?
 (like I currently tell chatgpt never do this, how can I do this in dspy?)
3. How it is modifying and making pipelines b
etter? (without actively changing the code itself)
4. How is it able to use  models like T5 along with ChatGPT for bette
r RAG?
5. Is it possible to censor or stop it from getting off-topic?

if you have any experience in this it would be ve
ry helpful. I am specifically looking for text to sql application and really interested in how is it able to give ref da
ta to improve sql generation.
```
---

     
 
all -  [ Chatbot development with Gemini 1.0-pro API [Help need] ](https://www.reddit.com/r/GoogleGeminiAI/comments/1bja3jn/chatbot_development_with_gemini_10pro_api_help/) , 2024-03-22-0909
```
I am developing a chatbot with Gemini. Currently I am struggling with the conversation history issue.

**Problem**

I am
 trying to get the AI response for a user message but the AI response is not matching with the previous message. i.e 

u
ser - Hi I need to know about your company.

ai - we are a management consultant company. We provide the following servi
ces. Is there anything else I can help you with today?

user - no

ai - Great Is there anything else I can help you with
 today?

The last AI response is not correct. I have tried different ways and different prompts but not worked.

**Tech 
stack**

Node js

Langchain js (I have tried raw Google Gen ai studio SDK but did not work)

**Source code**

    const 
{ FewShotChatMessagePromptTemplate , ChatPromptTemplate } = require('@langchain/core/prompts')
    const examplePrompt =
 ChatPromptTemplate.fromTemplate(`Human: {human}\nAI: {ai}`);
    const gemini = require('./services/gemini')
    
    a
sync function build(){
        const fewShotPrompt = new FewShotChatMessagePromptTemplate({
            examplePrompt,
 
           examples: [],
            prefix:'You are an assistant of {org_name} company. You can schedule/reschedule/can
cel appointments , provide company information. reply to human conversation. You are emphatic, polite and friendly assis
tant. You always consider about human's feelings and respond to sad or bad situations with a wise message. If the human 
rejects your support never ask anything there for your help explain that you are available anytime. You don't know any o
ther area information other than your company information. If the human asks for anything non related to your knowledge 
base explain why you can't reply to that. Always your reply should be very short and straight and perfectly match with t
he conversation history. Your company details: {details}.',
            inputVariables: ['details', 'msg' , 'org_name' ,
 'examples'],
            suffix:'This human message is the latest message of the conversation. Always your response for
 this message should strictly match with the conversation. Human message: {msg}',
        });
        return fewShotProm
pt.format({
            orgName: 'Langchain',
            details: 'Langchain is a company that provides AI services. We
 accepts bookings by company website.',
            msg: 'Ok',
            org_name: 'Langchain',
        })
    }
    

    build().then(async res=>{
        console.log(res)
        console.log('============================================
========\n\n\n')
        let history = [
            ['human','What services does Langchain provide?'],
            ['ai
','Langchain is a company that provides AI services. Can I place a booking for you?'],
            ['human','ok'],
     
       ['ai','Sure! You can book an appointment through our website. Is there anything else I can help you with today?']
,
            ['human','No'],
            ['ai','Great!! If there anything feel free to drop a message'],
            ['
human',res]
        ]
        let geminiRes = await gemini.invoke(history)
        console.log(geminiRes.content)
    })


**Generated Prompt (res var value)**

You are an assistant of Langchain company. You can schedule/reschedule/cancel ap
pointments, and provide company information. reply to human conversation. You are an emphatic, polite, and friendly assi
stant. You always consider human feelings and respond to sad or bad situations with a wise message. If the human rejects
 your support never ask anything there for your help explain that you are available anytime. You don't know any other ar
ea information other than your company information. If the human asks for anything non-related to your knowledge base ex
plain why you can't reply to that. Always your reply should be very short and straight and perfectly match the conversat
ion history. Your company details: Langchain is a company that provides AI services. We accept bookings by company websi
te..

&#x200B;

This human message is the latest message of the conversation. Always your response to this message shoul
d strictly match with the conversation. Human message: OK

&#x200B;

If anyone can support on this It will be a great su
pport for me. 🙏 
```
---

     
 
all -  [ Project University Chat With PDF ](https://www.reddit.com/r/csharp/comments/1bj8gze/project_university_chat_with_pdf/) , 2024-03-22-0909
```
Hello! I want to create a CHAT app with pdf documents without OpenAI, but i don’t know how to use Langchain in c#. Could
 you help me please with some tutorials? Or some tips?


```
---

     
 
all -  [ Langchain Usage doubt for document generation ](https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/) , 2024-03-22-0909
```
I am trying to build an application that takes templates of things like a cover letter , resume , medical research docum
ent. Now based on this template I will upload another document containing information to be used to fill the template. H
owever after the model generates a new document following the template and information , the whole alignment of the docu
ment is wrong and it doesnt bold the necessary parts. Is there any way to ensure that a model can follow the format for 
a template like center allignment , bolding the headers , etc. 
```
---

     
 
all -  [ The glass ceiling I’m hitting is made up by my brain. ](https://www.reddit.com/r/SaaS/comments/1bj5808/the_glass_ceiling_im_hitting_is_made_up_by_my/) , 2024-03-22-0909
```
I quit all I had in France a year ago. Software engineer job, flat in Lyon (France), sold most of my belongings and went
 in Asia with a one-way ticket and my 50 backpack ~ 9kg inc. the MacBook.
Freedom drove me: the freedom to go wherever I
 want, whenever I want, doing whatever I want. Leaving France was motivated by the willing to unleash my entrepreneurial
 spirit. 

I wasn’t sure what I was gonna do: freelancing, remote work, indie hacking, etc
The first few months of trave
l got me thinking a lot, and at a point I met this French entrepreneur (owning 3 digital agencies), during our discussio
n one point got me thinking for weeks, after I asked him what brought him to his current position: “I kept doing what I 
loved to do until people where willing to pay me for it”
That triggered this question within me: wtf can I do that I act
ually love to do? 
My life has so many different phases, from bartender in NYC, to carpenter in New-Zealand, to Engineer
 in the aluminum industry with international customers, to software eng in a French startup. I was like: fk, what’s the 
common element in this sh*t. What do I love to do. After weeks of background thinking it was cristal clear: I love to so
lve problems, and I love to build solutions for it. 

That’s where I started my indie hacker journey, one year ago. Then
, long story short: 
- challenged myself for the first saas: created a WhatsApp bot integrating ai in couple of weeks (f
ocusing on audio transcription as pain point)
- Then seeing the potential of AI, created another WhatsApp bot that would
 integrste everything (text, audio, image, etc) - which didn’t solve any problem, except leveraging WhatsApp to make AI 
more accessible
- Then created couple of free stuff, worldll•e (a bot posting daily pics representing the world based on
 the previous day’s news), and another stuff to interact with Karl Marx books
All my projects were drivent by technologi
cal curiosity, each of them allowed me to go deeper into using and applying AI (for instance, the “Karl Marx” was my way
 to use and understand langchain, , embedding, vector databases  / pinecone). 

First first project was making ~$150/ mo
, from AI directories and couple of newsletters it’s been published in. 

The second WhatsApp bot was killed, it got spa
mmed in Facebook group in North Africa and India and got me into issues with Meta, ending up with my business account be
ing banned. 

Then I came a cross this AI influencer tweet lot of people have seen “we made $70k by selling 1$/min audio
 of an influencer on telegram”.
I was like: “f*ck META and WhatsApp, I need to try Telegram. Let’s explore the text-to-s
peech and custom LLMs and build an AI girlfriend”. I focused on the erotic part of it to differentiate it a bit. First m
onth it made $800. The lots of ups and downs. Most of my traffic is coming from AI directories, since i was one of the f
irst ones to register it on it (did this saas over the week end at first, it was 6 month ago). 
It went to $2k, up to $4
k at a point, now in between $2.2k and $3k. But I never did marketing, thinking that I don’t like this industry. I alway
s hide myself in technological / features / experience improvement. Always avoiding the marketing. Why? There’s actually
 a lot of potential in this business. Health, wealth and relationship are 3 axis that will always make money. I had LOAD
S of times of introspections, hips of downs, some ups of course, here’s what I realised: 
- I’ve been through the poores
t phase of my life while being in Bali, had no savings left, needed to live on a ~15-30$ / day revenue (still had to pay
 some services and APIs with it)-> I had to deconstruct my occidental vision or financial security. Learn to live with a
lmost nothing, with max 3-4 days of financial vision (the time Stripe takes to forward incomes to bank account). It tota
lly changed my way of thinking. 
- I was missing a goal: I was seeking “financial freedom”. But what is it? How much do 
I need to generate? Not having a number on it was the best way to not achieve this goal. 
- I became dependent of a proj
ect I didn’t like: the ai girlfriend. It was (and is) keeping me alive, and somehow blocked myself from pushing it for e
thical reasons, and afraid of being labelled as the “ai girlfriend” guy. but really: who gives a sh*t? Relationships & s
ex is taboo even though part of our daily life. Still, I went into the tech rabbit hole with always avoiding any marketi
ng and trying to fix/improve the product, that even brought me to another thought: 
- Am I afraid of success? Am I afrai
d of making money? As weird as it sounds, I believe the last 6 months were actually a transition of the employee version
 of me, dreaming of entrepreneurship, to be an actual entrepreneur. Believing in myself. I’m always harsh on myself, whi
ch makes it hard to consider wins, even the smallest ones, but as well pushes me to keep going. In December I had 3 week
s of holiday in Vietnam with a friend, the first self-paid holiday. Not touching the laptop, and money was still flowing
 in. That’s a huge step. 

So now, here I am, got two products with market fit: $150 MRR, and $2.2k MRR. Did the strict 
minimum for it marketing-wise. 
Im in Bangkok since 3 weeks, heading back to Bali next week, and a clearer vision, after
 months of evolutions, learnings, ups and down. And my goal is simple: Break. This. Fkg. Glass. Ceiling. My. Mind. Creat
ed. 

How? 
1. Growing these two products
- Reduce churn = improve experience
- Increase conversion = rework LP and offe
rs
- Increase traffic = MARKETING

2. Then going back to what I actually miss and love: Building new stuff. Fkg miss it.
 We should focus on what work yes, but as well focus on what drives us, and for me it’s the curiosity and the challenges
 a new product brings. But first, I need to hack and unlock marketing for me. 

This AI girlfriend stuff made me ashamed
 of Building in Public, so stopped to do it. Now I’m gonna be back and share my insights and learnings for growing my cu
rrent products and creating new stuff. 

This post is a way for me to be accountable to ANNIHILATE this mind-made glass 
ceiling. 

I’ll be sharing stuff again on twitter, [@maelus_](https://x.com/maelus_) - I’d love to connect with other in
diehackers - could even Meetup if you’re in Bali / Bangkok / France, world is small when travelling & indiehacking. 

Al
so, AMA

maelus
```
---

     
 
MachineLearning -  [ [D] : Scale PDF Q&A App to 10K Users with GPUs – <$250/Mo ](https://www.reddit.com/r/MachineLearning/comments/1b6jv56/d_scale_pdf_qa_app_to_10k_users_with_gpus_250mo/) , 2024-03-22-0909
```
Hello everyone,

Check out this step-by-step detailed tutorial on building and scaling a PDF Q&A Application using Pinec
one, Langchain and Inferless

&#x200B;

[Architecture](https://preview.redd.it/zfta52cbddmc1.png?width=1301&format=png&a
uto=webp&s=440399212d3feb03e861759a31602e2cde0dc7fb)

Alongside, the detailed quick deploy guide, it also includes cost 
analysis on how you can save upto 84% cost with an example of processing 3000 documents and nearly 10,000 queries every 
month, all while dramatically cutting your costs from $1800 ( AWS) to just $250 a month on Inferless.

Here is the tutor
ial - [https://cookbook.inferless.com/](https://cookbook.inferless.com/)

If you resonate, join the discussion on Hacker
news here - [https://news.ycombinator.com/item?id=39594588](https://news.ycombinator.com/item?id=39594588)
```
---

     
 
MachineLearning -  [ [D] What Is Your LLM Tech Stack in Production? ](https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/) , 2024-03-22-0909
```
Curious what everybody is using to implement LLM powered apps for production usage and your experience with these toolin
gs and advice. 

This is what I am using for some RAG prototypes I have been building for users in finance and capital m
arkets.

**Pre-processing\ETL:**
Unstructured.io + Spark, Airflow

**Embedding model:**
Cohere Embed v3
Previously using
 OpenAI Ada but Cohere has significantly better retrieval recall and precision for my use case. Also exploring other ope
n weights embedding models

**Vector Database:**
Elasticsearch previously but now using Pinecone

**LLM:**
Gone through 
quite a few including hosted and self-hosted options. Went with gpt4 early during prototyping then switched to gpt3.5-tu
rbo for more manageable costs and eventually open weights models. 

Now using a fine-tuned Llama2 70B model self hosted 
with vLLM 

**LLM Framework:**
Started with Langchain initially but found it cumbersome to extend as the app became more
 complex. Tried implementing it in LlamaIndex at some point just to learn and found it just as bad. Went back to Langcha
in and now I am in the midst of replacing it with my own logic

What is everyone else using?

Edit: correct model Llama2
 70B
```
---

     
 
MachineLearning -  [ [D] Graphs + vectordbs? Need your input: Cognee.ai . AI Data Pipelines for Real-World Production (Pa ](https://www.reddit.com/r/MachineLearning/comments/1aweo71/d_graphs_vectordbs_need_your_input_cogneeai_ai/) , 2024-03-22-0909
```
Hey there, Redditors!

I'm back with the latest installment on creating dependable AI data pipelines for real-world prod
uction.

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://top
oteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba40a
ab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines.

After a few months of work
, we integrated cognitive architecture with [keepi.ai](https://www.keepi.ai) 

We aim to explore with our demo:

**1. Co
ntext sanitization**  
The world of AI is fast-moving, and we've realized that the context is becoming a building block 
we refer to as a crucial part of future cognitive architecture.  
**2. Best Practices for AI Memory**  
In this rapidly 
evolving landscape, there are no established best practices. You'll need to make educated bets on tools and processes, k
nowing that things will change. We assume that having traditional data engineering practices + frameworks + classifiers 
and other AI solutions can solve a lot of standard hurdles  
**3. AI Frameworks**  
They are trying to do too much, too 
fast, too broad. We want to find a pattern and a correct layer of abstraction for the AI memory to fit new industry.  



&#x200B;

How does it work? 

The Github repo is l:

  


[How cognee works](https://preview.redd.it/yuiabmyihyjc1.png?
width=1633&format=png&auto=webp&s=4384c4441b615f72caf1e0591c5ab23aee735fab)

Github repo is [here](https://github.com/to
poteretes/cognee)

Next steps:  
I have questions for you:

1. Is context sanitization relevant for you?
2. How do you m
anage metadata? 
3. How do you prepare data for LLMs?
4. Are there any data enrichment steps you perform?

Check out the
 blog post:

[Link to part 4](https://topoteretes.notion.site/Going-beyond-Langchain-Weaviate-Level-4-towards-production
-fe90ff40e56e44c4a49f1492d360173c?pvs=4)

*Remember to give this post an upvote if you found it insightful!*  
*And also
 star our* [Github repo](https://github.com/topoteretes/cognee)
```
---

     
