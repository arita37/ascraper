 
all -  [ How are you maintaining existing access control permissions for a RAG LLM chatbot? ](https://www.reddit.com/r/LangChain/comments/1dtyunx/how_are_you_maintaining_existing_access_control/) , 2024-07-03-0911
```
We’re building an internal RAG LLM chatbot where the queries are augmented by a large volume of files we have in SharePo
int and some pages in Confluence. How would you maintain the existing RBAC of these files in the results from the chatbo
t when different employees are querying? Is there a tool that would allow the results to adhere to the existing permissi
ons of these data sources?
```
---

     
 
all -  [ Resume Chatbot ](https://www.reddit.com/r/LangChain/comments/1dtwsfv/resume_chatbot/) , 2024-07-03-0911
```
Hey Everyone, I made a Resume Chatbot, I'd Love for you'll to read my article linked below. Thank you!

https://medium.c
om/@aaronphilip2003/r%C3%A9sum%C3%A9-chatbot-abccc89de23b

```
---

     
 
all -  [ Understanding AI Hallucinations: What They Are and Why They Matter ](https://www.reddit.com/r/pythia/comments/1dtwfqy/understanding_ai_hallucinations_what_they_are_and/) , 2024-07-03-0911
```
Imagine this: You rely on an artificial intelligence (AI) assistant for a quick health check, and it misdiagnoses a mino
r rash as a life-threatening condition. Or, you skim through an article online, only to discover that AI fabricated it l
ater. While unsettling, these scenarios highlight a real challenge: AI hallucinations.

Large Language Models (LLMs) are
 a powerful type of AI that's rapidly changing how we interact with technology. They can generate creative text formats,
 like poems or code, and translate languages with impressive accuracy. LLMs are even being used in financial markets to 
automate trading strategies. However, it's important to remember that LLMs are still under development and can sometimes
 generate inaccurate or misleading information. 

As our dependence on AI grows, so do the potential consequences of unt
rustworthy AI. Fortunately, solutions are emerging to combat this challenge. [Wisecube's Pythia](https://askpythia.ai/bl
og/introducing-pythia-the-ai-oracle-eradicating-llm-hallucinations) is a prime example, designed to ensure the reliabili
ty and trustworthiness of AI outputs. 

This blog post dives into AI hallucinations and explores how solutions like Pyth
ia can empower a future built on dependable AI. 

# What are AI Hallucinations?

Consider a seemingly helpful AI analyzi
ng a skin lesion. It might incorrectly diagnose a harmless mole as cancerous. This is an illustration of an AI hallucina
tion in medical care. In essence, AI hallucinations are outputs generated by AI systems that are factually incorrect, il
logical, or simply nonsensical.

LLMs are trained on vast datasets of text and code. While impressive in their ability t
o learn patterns and generate human-like text, this very strength can be a weakness. LLMs may identify patterns that are
n't there or struggle with incomplete information. To fill these gaps , they could develop subtleties or make misleading
 associations, prompting hallucinations.

This highlights the importance of AI hallucination detection, especially in he
althcare, where decisions can have significant consequences. This is just one example. AI hallucinations can manifest in
 various ways, including:

* **Factual Inconsistencies:** LLMs might weave together factual elements from different cont
exts, creating an internally inconsistent narrative.
* **Logical Fallacies:** AI outputs might contain faulty reasoning 
or illogical arguments, despite appearing superficially coherent.
* **Nonsensical Outputs:** In some cases, LLMs might g
enerate entirely nonsensical text that bears no resemblance to the intended task or input data.

# Why Do AI Hallucinati
ons Matter?

AI hallucinations are far from harmless. They pose [significant risks ](https://askpythia.ai/blog/navigatin
g-risks-associated-with-unreliable-ai-and-trustworthiness-in-llms)across various fields that rely on accurate informatio
n and sound judgment. For example, a financial advisor using an AI tool that hallucinates about future market trends, le
ading to disastrous investment decisions. In the healthcare domain, [a medical diagnosis tool fed by inaccurate data cou
ld recommend the wrong treatment for a patient](https://www.wisecube.ai/blog/battling-llm-hallucinations-in-biomedicine-
the-role-of-knowledge-graphs-in-knowledge-injection-techniques/). The potential consequences of AI hallucinations are pa
rticularly worrisome in these high-stakes fields, including:

* **Finance:** AI-powered trading algorithms misled by hal
lucinations could cause significant financial losses for individuals and institutions.
* **Healthcare:** Misdiagnosis or
 improper treatment recommendations based on inaccurate AI outputs could have severe health consequences for patients.
*
 **Legal:** AI-driven legal research tools that fabricate information could lead to flawed legal arguments and miscarria
ges of justice.

Beyond these immediate risks, AI hallucinations can also erode public trust in AI technology as a whole
. When people encounter demonstrably false information generated by AI, they become more hesitant to rely on AI systems.
 

# The Impact of AI Hallucinations in Biomedicine

In the domain of biomedicine, AI hallucinations pose a significant 
threat. These hallucinations, where AI generates incorrect or misleading information, can have devastating consequences.
 They could lead to wasted research efforts, delayed breakthroughs, or even the development of unsafe medical treatments
.  Here's how they can negatively impact healthcare organizations and research institutions:

* **Misdiagnosis and Mistr
eatment:** AI-powered diagnostic tools misinterpreting medical scans or patient data could delay or prevent fitting trea
tment, endangering patient outcomes. 
* **Wasted Resources and Delayed Research:** Biomedical research heavily relies on
 accurate data analysis. AI hallucinations in research tools can lead to wasted resources and time spent pursuing false 
leads or ineffective treatment pathways based on fabricated information.
* **Loss of Patient Trust and Reputational Dama
ge:** When patients discover their diagnoses or treatment plans were influenced by AI errors, it can erode trust in the 
healthcare system and damage the reputation of institutions relying on such technology.
* **Legal Issues and Regulatory 
Challenges:** Biomedical AI with flawed outputs could lead to regulatory non-compliance or even legal repercussions if i
naccurate diagnoses or treatment decisions cause harm to patients.

The potential financial impact of AI hallucinations 
is also significant. Misdiagnoses and improper treatments can increase healthcare costs, while lawsuits arising from AI 
errors could result in hefty settlements.

Biomedical foundations can ensure patient well-being and advance medical prog
ress by prioritizing the development of reliable AI systems. This focus on dependable simulated intelligence solutions d
irectly addresses the threat of AI hallucinations, leading to more trustworthy AI-powered healthcare. 

# The Importance
 of AI Hallucination Detection in Biomedicine

Because AI can deliver results in biomedicine that could significantly im
pact a person's health, the development of reliable methods to identify AI hallucinations becomes a critical safeguard. 


A reliable AI hallucination detection system would recognize inconsistencies and alert researchers for further investi
gation. This crucial intervention could prevent the development of a harmful drug combination and potentially save a lif
e. In essence, reliable AI hallucination detection, facilitated by tools like Wisecube's Pythia, acts as a safety net wi
thin the biomedical research process. Let's explore how this safeguards the development of life-saving treatments.  

* 
**Enhanced Patient Safety:** AI hallucination detection directly contributes to improved patient care. Identifying and f
iltering out unreliable AI outputs allows for early detection of potential errors in diagnosis or treatment plans. This 
enables medical experts to mediate and guarantee patients get the absolute most exact consideration.
* **Boosted Researc
h Efficiency:** In the fast-paced world of biomedical research, robust data analysis is crucial. AI hallucination detect
ion helps scientists avoid wasting valuable time and resources pursuing false leads or ineffective treatment pathways du
e to AI errors.
* **Reliable Results:** Detection tools ensure data integrity in AI-driven research and clinical trials.
 This fosters trust in the overall research process and the validity of the results obtained using artificial intelligen
ce.
* **Building Trustworthy AI in Healthcare:** By mitigating the risk of AI errors, AI hallucination detection enables
 healthcare professionals to have a higher level of trust in AI-generated insights. This allows them to make more inform
ed decisions about diagnosis, care, and treatment approaches. 

Ensuring the reliability of AI outputs through robust hu
man oversight is key to unlocking a future where AI becomes a trusted partner in healthcare and biomedical research. Wis
ecube's Pythia exemplifies this approach. Reliable AI allows us to move beyond simply analyzing data to tackling proacti
ve tasks like predicting disease outbreaks, forecasting patient outcomes, and even personalizing drug treatments.  

# I
ntroducing Pythia: The AI Hallucination Firewall

Wisecube's Pythia is a powerful tool designed to combat AI hallucinati
ons, a significant challenge in the biomedical field. It goes beyond basic anomaly detection to provide a comprehensive 
AI hallucination identification and mitigation system. This means Pythia can not only identify potentially misleading AI
 outputs but also help researchers understand and address them.   

Here's how Pythia safeguards against unreliable AI o
utputs:

* **Knowledge-Based Verification:** Pythia tackles AI hallucinations by leveraging a powerful technique called 
[knowledge triplets](https://www.wisecube.ai/blog/the-wisecube-approach-to-enhancing-ai-reliability/). These triplets ca
pture factual relationships between entities, acting as building blocks for comparisons. When an LLM generates an output
, Pythia extracts claims as knowledge triplets from both the output and Wisecube's knowledge graph (or a custom dataset 
you provide). By comparing these triplets, Pythia can identify inconsistencies or factual impossibilities that flag pote
ntial hallucinations in the LLM's output. 
* **Multi-Layered Analysis:** Pythia's analysis goes beyond just checking a k
nowledge graph. It employs a multifaceted approach involving claim extraction and categorization. First, Pythia meticulo
usly extracts specific factual assertions from the LLM output. These claims are then compared against scientific databas
es and medical literature. By analyzing the alignment, Pythia categorizes them as Entailment, Contradiction, Neutral, or
 Missing Facts. Finally, it generates a report highlighting inconsistencies and areas requiring human review. This multi
-layered analysis ensures a higher level of trust in LLM outputs. 
* **Seamless Integration:** Pythia integrates seamles
sly within existing LLM workflows, acting as a real-time safety net. This ensures continuous monitoring and immediate fl
agging of potential hallucinations without disrupting ongoing AI processes.

By deploying Pythia, biomedical institution
s gain a multitude of advantages:

* **Advanced Hallucination Detection:** Pythia's multi-layered approach surpasses bas
ic anomaly detection, offering a robust and reliable solution for identifying even subtle AI hallucinations.
* **Privacy
 Protection:** It uses input/ output validators to examine artificial intelligence yields while never expecting admittan
ce to delicate patient information. This ensures patient confidentiality remains protected throughout the process. 
* **
Enhanced Trust:** Pythia fosters trust in AI-driven research and clinical decision-making by mitigating the risk of erro
rs and ensuring the reliability of AI outputs.
* **Customizable Detection:** Pythia allows for customizing detection par
ameters to cater to the specific needs of different research areas and applications within biomedicine.
* [**LangChain I
ntegration**](https://askpythia.ai/blog/a-guide-to-integrating-the-pythia-api-using-wisecube-python-sdk)**:** Pythia sea
mlessly integrates with the LangChain ecosystem, a unified environment for building and deploying trustworthy AI solutio
ns in healthcare.
* **Real-Time Analysis:** Pythia's real-time analysis capabilities enable immediate identification and
 flagging of potential hallucinations, allowing for swift intervention and course correction.

Pythia enables the biomed
ical field to tackle the maximum capacity of simulated intelligence while defending against hallucinations. With its adv
anced detection methods, seamless integration, and commitment to data privacy, Pythia paves the way for a new era of dep
endable AI in biomedicine. 

[Get started](https://askpythia.ai/#get-started) today to develop reliable LLMs with Pythia
 and embark on your journey towards trustworthy AI-powered healthcare. 

  
*The article was originally published on* [*
Pythia's website.*](https://askpythia.ai/)


```
---

     
 
all -  [ Is it possible to use LangGraph to orchestrate LangChain chains? ](https://www.reddit.com/r/LangChain/comments/1dtw8rr/is_it_possible_to_use_langgraph_to_orchestrate/) , 2024-07-03-0911
```
Suppose I use langchain’s ChatOpenAI as a model, ChatPromptTemplate to build a prompt and build myself a chain like that
:

chain = prompt | model

I’m trying to use LangGraph along with it’s memory capabilities to orchestrate chains built l
ike these. Is this the right way? Or should I be starting to build the chain with langgraph?

Looks like my chain has so
me kind of amnesia between messages.






```
---

     
 
all -  [ Langchain async for document loader and character splitter ](https://www.reddit.com/r/LangChain/comments/1dtvnw0/langchain_async_for_document_loader_and_character/) , 2024-07-03-0911
```
Should I use Async or not? 

I am building a Fast api RAG app that uses LLMs to generate responses using langchain but I
 am confused if it needs async. 

The user flow goes something like this:
1. User provides a link/ links to a text/pdf d
ocument in the form of a url.  
2. Langchain document loaders are used to load text or pdf from the remote public url.
3
. Character splitter is used to split and chunk the documents which is saved into a vector db. 
4. Langchain chain libra
ry is used to invoke LLMs via the asynchronous ainvoke(). 

My question is whether the document loading step via langcha
in and character splitting text via langchain  would need to be made async? Langchain doesn’t support async for the libr
aries I am using. Would I need to implement them myself? If so, what are some options to implement? 
```
---

     
 
all -  [ Should I use Async or not?  ](https://www.reddit.com/r/learnpython/comments/1dtvk3n/should_i_use_async_or_not/) , 2024-07-03-0911
```
I am building a Fast api RAG app that uses LLMs to generate responses using langchain but I am confused if it needs asyn
c. 

The user flow goes something like this:
1. User provides a link/ links to a text/pdf document in the form of a url.
  
2. Langchain document loaders are used to load text or pdf from the remote public url.
3. Character splitter is used 
to split and chunk the documents which is saved into a vector db. 
4. Langchain chain library is used to invoke LLMs via
 the asynchronous ainvoke(). 

My question is whether the document loading step via langchain and character splitting te
xt via langchain  would need to be made async? Langchain doesn’t support async for the libraries I am using. Would I nee
d to implement them myself? If so, what are some options to implement? 
```
---

     
 
all -  [ Frontend Software Engineer Resume Review ](https://www.reddit.com/r/resumes/comments/1dtvjsb/frontend_software_engineer_resume_review/) , 2024-07-03-0911
```
Hi Everyone! I'm looking for reviews on my resume. I have been a frontend developer for around 3 years and I'm looking f
or work. Been getting some hits (but not too great), I'm trying to see if there's anything that can be improved to get m
ore callbacks.

Would love to get some feedback. Thanks!

https://preview.redd.it/y55jtl1by5ad1.png?width=1275&format=pn
g&auto=webp&s=5ba6191fda5e8b6c7950a77c5198bd3a0659f724

  

```
---

     
 
all -  [ How do I upload a file and display the output in the langserve playground? ](https://www.reddit.com/r/LangChain/comments/1dtugcl/how_do_i_upload_a_file_and_display_the_output_in/) , 2024-07-03-0911
```
Hello everyone, I need your help with my project. I have two problems:  
1 - I'd like the user to be able to upload a cs
v file directly instead of putting the route to the file   
2 - I don't know why, but in csv/playground/, there's no out
put displayed  
I'm trying to make an askyourcsv project but in this project, the user has to set the file path instead 
of uploading the file. What can I do to enable the user to upload the file and how can I improve my code? And what's wro
ng with the code so that the output isn't displayed in the playground?

    #chain.py
    def loaddata(filepath: str) ->
 List[Document]:
        loader = CSVLoader(filepath=filepath)
        documents = loader.load()
        return document
s
    
    class QuestionRequest(BaseModel):
        file_path: str
        question: str
    
    class Rag_chain(Runna
ble):
        def invoke(self, input: Any, config: Dict[str, Any] = None) :
    
            if isinstance(input, dict):

                input = QuestionRequest(**input)
            question = input.question
            file_path = input.fi
le_path
            data = load_data(file_path)
    
            text_splitter = RecursiveCharacterTextSplitter(chunk_si
ze=1000, chunk_overlap=200)
            documents = text_splitter.split_documents(data)
    
            vectorstore = C
hroma.from_documents(
                documents,
                embedding=OpenAIEmbeddings(model='text-embedding-ada-00
2'),
            )
    
            retriever = RunnableLambda(vectorstore.similarity_search).bind(k=5)
    
           
 llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.6)
    
            template = '''Answer the question based only 
on the following context:
            {context}
    
            Question: {question}
            '''
            prompt
 = ChatPromptTemplate.from_template(template)
            chain = (
                # Extract the question text from the
 QuestionRequest object
                RunnableParallel({'context': retriever, 'question': RunnablePassthrough()})
    
            | prompt
                | llm
                | StrOutputParser()
            )
    
            response =
 chain.invoke(question)
            return response
    '''
    '''
    #server.py
    app = FastAPI()
    chain_instanc
e = Rag_chain()
    
    add_routes(app,
               chain_instance,
               path='/csv',
               input
_type=QuestionRequest)
    
    if __name == '__main':
        import uvicorn
        uvicorn.run(app, host='0.0.0.0', p
ort=8000)
    '''
```
---

     
 
all -  [ Verify ChatGPT Statement Truth Using Anthropic Claude Model ](https://www.reddit.com/r/LangChain/comments/1dtuell/verify_chatgpt_statement_truth_using_anthropic/) , 2024-07-03-0911
```
[https://youtu.be/18zTQv25qlk](https://youtu.be/18zTQv25qlk)

I built this in like 5 minutes using [https://visualagents
.ai](https://visualagents.ai) fully event driven data flow RAG graph built on top of js.langchain.

https://preview.redd
.it/qcp1brqap5ad1.png?width=1920&format=png&auto=webp&s=abae1244e70defe0c1301286cbdd5dfddc8ef8d2


```
---

     
 
all -  [ Not getting interviews. Is it my resume or the fact that I’m international student (US)  ](https://i.redd.it/s3z49ofsj5ad1.jpeg) , 2024-07-03-0911
```

```
---

     
 
all -  [ Roast Beno - Find Clients with AI-Powered Replies in Relevant Discussions Automatically ](https://www.reddit.com/r/Entrepreneur/comments/1dtt4uv/roast_beno_find_clients_with_aipowered_replies_in/) , 2024-07-03-0911
```
Roast my project - Find Clients with AI-Powered Replies in Relevant Discussions Automatically

Hi, community! I’ve creat
e a Beno, it tracks relevant conversations and mentions your business in a natural, human-like way.

While developing Be
no and talking to other founders, I was inspired by their excitement and the 'take my money' attitude. Now that we're la
unching Beno, I realize how important feedback is to make the product even better.

Imagine a small company with two emp
loyees who spend all their time monitoring online platforms, jumping into relevant discussions to organically mention th
e company and attract new clients. Or a startup with no marketing budget and limited resources, struggling to find clien
ts.

Now, imagine a tool, Beno, that does exactly what those employees do. It automatically finds relevant discussions o
nline and uses AI to craft human-like replies mentioning your product.

Just to be clear, all comments are posted by our
 team manually. We’ve optimized the process of finding and crafting these replies, making sure we follow platform rules 
and deliver great results.


Target Audience

Originally, I thought most of our users would be B2C, but after talking to
 many founders, it turns out most are B2B. The main things they have in common are:

- Small businesses, mostly indie fo
unders.
- They may do it manually now and want to automate this process.
- They may have developed their product and are
 now focusing on client acquisition.
- They are looking for client acquisition at a lower cost than other methods like t
argeting and influencer marketing.
- They appreciate the easy and fast setup process.


Current Stage
We have built a pr
oduct and are now in beta.


Customer Conversion Strategy
Most of our users come from our cold emails.

Why Me?
We start
ed as a small team with no marketing budget, manually replying to Reddit discussions to find clients for other products 
we were working on. So we decided to automate it and build Beno. We have some results to share based on our products tha
t we used Beno for: on Reddit, one of our products reached 700,000 people in a week—all automated, no manual effort. You
 can set up in just 5 minutes and spend only $50, instead of $1800 a month on manual labor.

Tech Stack
Svelte for the f
rontend, Pocketbase for the backend, and Python for AI processes. We use a complicated LLM pipeline with ChatGPT and Gro
q, and we do not use Langchain.

Website: beno(.)one


Roast my project and help me to create my first successful compan
y. Any feedback or questions are welcome
```
---

     
 
all -  [ Agent RAG (Parallel Quotes) - How we built RAG on 10,000's of docs with extremely high accuracy ](https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/) , 2024-07-03-0911
```
Edit - for some reason the prompts weren't showing up. Added them.

Hey all -

Today I want to walk through how we've be
en able to get extremely high accuracy recall on thousands of documents by taking advantage of splitting retrieval into 
an 'Agent' approach.

**Why?**

As we built RAG, we continued to notice hallucinations or incorrect answers. we realized
 three key issues:

1. There wasn't enough data in the vector to provide a coherent answer. i.e. vector was 2 sentences,
 but the answer was the entire paragraph or multiple paragraphs.
2. LLM's try to merge an answer from multiple different
 vectors which made an answer that looked right but wasn't.
3. End users couldn't figure out where the doc came from and
 if it was accurate.

We solved this problem by doing the following:

* Figure out document layout ([we posted about it 
a few days ago.](https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/)) This
 will make issue one much less common.
* Split each 'chunk' into separate prompts (Agent approach) to find exact quotes 
that may be important to answering the question. This fixes issue 2.
* Ask the LLM to only give direct quotes with refer
ences to the document it came from, both in step one and step two of the LLM answer generation. This solves issue 3.

**
What does it look like?**

https://preview.redd.it/otf9dbau05ad1.png?width=1625&format=png&auto=webp&s=72a7fe9349a9499a3
2ff5f6b6a245623e1d91102

We found these improvements, along with our prompt give us extremely high retrieval even on com
plex questions, or large corpuses of data.

**Why do we believe it works so well?** - LLM's still seem better to deal wi
th a single task at a time, and LLM's still struggle with large token counts on random data glued together with a prompt
 (i.e. a ton of random chunks). Because we are only providing a single Chunk, or relevant information, we found huge imp
rovements in recall and accuracy.

**Workflow:**

https://preview.redd.it/8aso1i7y05ad1.png?width=1109&format=png&auto=w
ebp&s=e115ee50b70fbf790e78af365ce96f7d2e16a020

**Step by step with example on above workflow**

1. **Query:** What are 
the recent advancements in self-supervised object detection technique
2. **Reconstruct document.** (highlighted would be
 the vector that came back) Then we'd reconstruct the doc until we get to a header.

https://preview.redd.it/ztj8q1f415a
d1.png?width=1122&format=png&auto=webp&s=ab3805273bf3320d7478c3619c3c0df8dcdc98d9

3. Input the reconstructed document c
hunk into the LLM. **(Parallel Quotes)**

**Prompt #1:**

**\_\_\_\_\_\_\_**

You are an expert research assistant. Here
 is a document you will find relevant quotes to the question asked:

  <doc>

  ${chunk}

  </doc>

Find the quotes from
 the document that are most relevant to answering the question, and then print them in numbered order. Quotes should be 
relatively short.

The format of your overall response should look like what's shown below. Make sure to follow the form
atting and spacing exactly.

  Example:

  \[1\] 'Company X reported revenue of $12 million in 2021.'

  \[2\] 'Almost 9
0% of revenue came from widget sales, with gadget sales making up the remaining 10%.'

  Do not write anything that's no
t a quote direct quote.

  If there are no quotes, please only print, 'N/a'

**\_\_\_\_\_\_\_**

4. **Response** from th
e LLM:

\[1.0\]'Recent advancements have seen the development of end-to-end self-supervised object detection models like
 UP-DETR and DETReg, as well as backbone pre-training strategies such as Self-EMD and Odin .'

\[1.1\] 'Despite the rema
rkable success of supervised object detection techniques such as Mask RCNN , Yolo , Retinanet , and DETR , their self-su
pervised alternatives have been somewhat limited in scope until recently.

Notes:

I deleted the internal references to 
make it less confusing

If there's more than 1 doc/chunk we start each new one with a new number i.e. \[2.0\] which make
s it easier to find which quote relates to which doc.

We put the query in the user prompt and the above in the system p
rompt

5. Give the LLM that will be generating the answer the **document name & quotes**.

**Prompt #2:**

**\_\_\_\_\_\
_\_**

All quotes are relevant to the question, please use them to answer the question:

When answering questions:

1. M
ake references to quotes relevant to each section of the answer solely by adding their bracketed numbers at the end of r
elevant sentences.
2.   Feel free to shorten quotes or merge quotes together as long as you reference them.
3.   Focus o
n making short, readable answers.
4.   Bold headers, bold general topics, bullet point, list, etc. if needed to make it 
easier to read.



DocName: UnSupDLA: Towards Unsupervised Document Layout Analysis

Quotes:

\[1.0\]'Recent advancement
s have seen the development of end-to-end self-supervised object detection models like UP-DETR and DETReg, as well as ba
ckbone pre-training strategies such as Self-EMD and Odin .'

\[1.1\] 'Despite the remarkable success of supervised objec
t detection techniques such as Mask RCNN , Yolo , Retinanet , and DETR , their self-supervised alternatives have been so
mewhat limited in scope until recently.'

DocName: 'doc2'

Quotes:

\[2.0\]'example quote2'

\[2.1\]'example'

If there 
isn't information to answer this question, please write:

I'm sorry, but there is no documentation available.

\_\_\_\_\
_\_\_\_

6. Final LLM **Response**:

**End-to-End Self-Supervised Models**

* **UP-DETR**: This model is a recent develo
pment in self-supervised object detection, aiming to enhance detection capabilities without relying on labeled data \[1.
0\].
* **DETReg**: Another advanced model, DETReg, also focuses on self-supervised object detection, providing improveme
nts in training and detection accuracy without requiring extensive labeled datasets \[1.0\].

**Backbone Pre-Training St
rategies**

* **Self-EMD**: This strategy is utilized to pre-train the backbone of detection models, helping in learning
 robust representations from unlabeled data \[1.0\].
* **Odin**: Similar to Self-EMD, Odin is a pre-training strategy th
at enhances the performance of detection models through self-supervised learning techniques \[1.0\].

**Comparison with 
Supervised Techniques**

**Scope and Limitations**: Despite the success of supervised techniques like Mask RCNN, Yolo, R
etinanet, and DETR, their self-supervised alternatives have only recently started to expand in scope and effectiveness \
[1.1\].

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Real world examples of where this comes i
nto use:**

* A lot of internal company documents are made with human workflows in mind only. For example, often see a d
ocument named 'integrations' or 'partners' and then just a list of 500 companies they integrate/partner with. If a vecto
r came back from within that document, the LLM would not be able to know it was regarding integrations or partnership be
cause it's only the document name.
* Some documents will talk about the product, idea, or topic in the header. Then not 
discuss it by that name again. Meaning if you only get the relevant chunk back, you will not know which product it's ref
erencing.

Based on our experience with internal documents, about 15% of queries fall into one of the above scenarios.


Notes - Yes, we plan on open sourcing this at some point but don't currently have the bandwidth (we built it as a produc
tion product first so we have to rip out some things before doing so)

Happy to answer any questions!

**Video:**

https
://reddit.com/link/1dtr49t/video/o196uuch15ad1/player
```
---

     
 
all -  [ Building Dynamic RAG Apps with LangChain + Pathway ](https://www.reddit.com/r/learnmachinelearning/comments/1dtpglu/building_dynamic_rag_apps_with_langchain_pathway/) , 2024-07-03-0911
```
Hi r/learnmachinelearning  
  
Here’s a straightforward approach to build Dynamic RAG Apps using LangChain.   
  
LangCh
ain is a widely used framework for RAG (Retrieval-Augmented Generation) applications, but changes in data sources can pr
esent significant challenges. As data evolves, ETL (Extract, Transform, Load) pipelines often become complex and difficu
lt to maintain, making it hard to keep applications up-to-date.  
  
Using Pathway with LangChain provides a solution to
 this problem by ensuring that applications always provide up-to-date knowledge. Key benefits of Pathway’s incremental u
pdates include:

* Easy monitoring of data source changes (insertions, deletions, changes)
* Instant syncing of RAG apps
 with these changes
* Simplified ETL adjustments from the beginning

By using this app template within Colab, you can st
reamline your RAG solutions and make them more efficient for production environments. Pathway is also available natively
 as a vector store within the LangChain ecosystem, offering additional integration options.   
  
Learn how to get start
ed with a dynamic RAG app in Google Colab using your own data in minutes: [https://pathway.com/developers/templates/lang
chain-integration](https://pathway.com/developers/templates/langchain-integration)
```
---

     
 
all -  [ App Template to build Dynamic RAG Apps with Langchain and Pathway ](https://www.reddit.com/r/datascienceproject/comments/1dtpccn/app_template_to_build_dynamic_rag_apps_with/) , 2024-07-03-0911
```
Hey r/datascienceproject, here's an App Template to build Dynamic RAG projects within Colab in minutes: [https://pathway
.com/developers/templates/langchain-integration](https://pathway.com/developers/templates/langchain-integration)

LangCh
ain is a popular framework for working on RAG applications. However, as changes occur in data sources, developers often 
face significant challenges. ETL pipelines can become messy, and keeping up with these changes can be a headache. Using 
Pathway with LangChain solves this problem by ensuring your applications always provide up-to-date knowledge. With this 
you get incremental indexing pipelines to:

* Easily monitor several data sources for any data changes (insertions/delet
ions/changes)
* Instantly sync your RAG apps 
* Avoid complex ETL adjustments from Day 1 

You can try this app template
 within Google Colab and streamline your RAG solutions for production. Pathway is also available natively as a vector st
ore within the LangChain ecosystem.
```
---

     
 
all -  [ LangSmith: configuring HF pipeline to display output as Chat? ](https://www.reddit.com/r/LangChain/comments/1dtnri2/langsmith_configuring_hf_pipeline_to_display/) , 2024-07-03-0911
```
I might be overlooking something quite simple but I'm trying to follow the RAG from Scratch tutorial by Lance Martin but
 by using a local LLM (Mistral 7B Instruct v0.3). The Runnable Sequence has a HuggingFacePipeline section which has a 'P
rompt & Completion' header and has the entire prompt, context and query, then the answer. The answer itself has the prom
pt, context, query and answer embedded within it:

```
...

metadata={'source': 'https://lilianweng.github.io/posts/2023
-06-23-agent/'})\]   
Answer: \[/INST\]  
  
<s> \[INST\] You are an assistant for question-answering tasks. Use the fol
lowing pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. U
se three sentences maximum and keep the answer concise. \[/INST\] </s> 

... 

  
'https://lilianweng.github.io/posts/20
23-06-23-agent/'})\]   
Answer: \[/INST\]  
Task Decomposition is a method used in autonomous agent systems where a comp
lex task is broken down into smaller, manageable steps. This is often achieved by instructing a Language Model (LLM) to 
'think step by step' or by using specific instructions tailored to the task at hand. For example, it could involve askin
g 'What are the subgoals for achieving XYZ?' or 'Steps for XYZ.' The goal is to simplify complex tasks and provide insig
hts into the model's thought process.
```

Instead, his -- using OpenAI -- has a proper chat with input and output heade
rs that are easily readable. Is there something straightforward I can change to get the same output on LangSmith?

My ra
g chain looks like this:

```
# Prompt
prompt = hub.pull('rlm/rag-prompt-mistral')

# Chain
rag_chain = (
    {'context'
: retriever, 'question': RunnablePassthrough()}
    | prompt
    | mistral_llm
    | StrOutputParser()
)

# Question
res
ponse = rag_chain.invoke('What is Task Decomposition?')
```
```
---

     
 
all -  [ Deep Reinforcement Learning ](https://www.reddit.com/r/LangChain/comments/1dtly3h/deep_reinforcement_learning/) , 2024-07-03-0911
```
Published in ICML 2024

Paper: [https://huggingface.co/papers/2406.16979](https://huggingface.co/papers/2406.16979)
```
---

     
 
all -  [ Resume Review ](https://www.reddit.com/r/resumes/comments/1dtlsp2/resume_review/) , 2024-07-03-0911
```
https://preview.redd.it/86m8k2v6x3ad1.png?width=625&format=png&auto=webp&s=57af9a65365355468d51709733c9e5385743a1e4

  

  
  
Hi, Im a student graduating next year with a CS degree. I am looking for entry jobs in AI/ML or any SDE roles.   

Im having trouble getting shortlisted. I feel like my resume lacks something but i cant pinpoint what...
```
---

     
 
all -  [ LangChain vs LlamaIndex vs txtai for vector search ](https://www.reddit.com/r/LocalLLaMA/comments/1dtlfex/langchain_vs_llamaindex_vs_txtai_for_vector_search/) , 2024-07-03-0911
```
https://preview.redd.it/d6fg0ixet3ad1.png?width=1613&format=png&auto=webp&s=8f45839401fe3818c6a7b781339d6e32c7ff994a

ht
tps://preview.redd.it/k26mczuft3ad1.png?width=1535&format=png&auto=webp&s=a9e79f8d111eae6c76fcc48bc0ba644af2f265ce

The 
comparison above shows the differences between the 3 libraries for building a vector search application backed by Faiss.


Faiss is a great vector indexing library. It has so many features past just a flat index. txtai automatically creates 
a performant Faiss index scaled by the size of the incoming data. The index type can also be fully customized with confi
guration.

[txtai](https://github.com/neuml/txtai) vector indexes use SQLite + Faiss by default. This enables search wit
h SQL and dynamic columns. Results are standard Python dictionaries and that allows direct integration with Pandas/Polar
s DataFrames.

This comparison demonstrates the power of a deep and meaningful integration vs supporting as many vector 
stores as possible (i.e. box checking).

Case in point, [LlamaIndex supports txtai](https://docs.llamaindex.ai/en/stable
/api_reference/storage/vector_store/txtai/) but it really doesn't integrate it the way it was intended.

Links to code:


[https://gist.github.com/davidmezzetti/e91a648732a4935cd551b06e9c8aaed7](https://gist.github.com/davidmezzetti/e91a6487
32a4935cd551b06e9c8aaed7)  
[https://gist.github.com/davidmezzetti/43a60af6e135a031c0bfb4de02bc11f4](https://gist.github
.com/davidmezzetti/43a60af6e135a031c0bfb4de02bc11f4)
```
---

     
 
all -  [ node-edge based GUI editor for LangGraph ](https://www.reddit.com/r/LangChain/comments/1dtklxf/nodeedge_based_gui_editor_for_langgraph/) , 2024-07-03-0911
```
I’m excited to share that I’ve created a node-edge based GUI editor for LangGraph!

https://preview.redd.it/1uf35sfym3ad
1.jpg?width=1850&format=pjpg&auto=webp&s=59f0728b3381b8a68d83f27137075587f13bb033

This tool provides an intuitive inter
face for creating and managing workflows, making it easier than ever to visualize and execute tasks. Whether you're work
ing with complex workflows or just getting started, LangGraph-GUI simplifies the process.

Check it out here: [LangGraph
-GUI on GitHub](https://github.com/LangGraph-GUI/LangGraph-GUI)

Some key features include:

* **User-Friendly Interface
:** Easily create and edit workflows with a visual editor.
* **Seamless Integration:** Supports local execution with lan
guage models like Mistral.
* **JSON Support:** Read and write JSON files for your workflows, ensuring compatibility and 
easy sharing.

To get started, follow the setup instructions in the repository. I’ve also included a guide on how to bui
ld the front-end GUI into a standalone executable.

If you want to learn LangGraph, we have LangGraph for dummy learning
: [LangGraph-learn](https://github.com/LangGraph-GUI/LangGraph-learn)

I’d love to hear your feedback and see how you’re
 using LangGraph-GUI in your projects. Feel free to contribute or raise issues on GitHub.

Happy graphing!
```
---

     
 
all -  [ Python vs Javascript for langchain ](https://www.reddit.com/r/LangChain/comments/1dtjqwi/python_vs_javascript_for_langchain/) , 2024-07-03-0911
```
I'm currently in the process of making a web app using next.js and I want to use LangChain on the backend. It would be m
uch simpler for me to use Javascript. I'm wondering if it's functionality lacking compared to the python sdk.
```
---

     
 
all -  [ Adding observability to Langchain based AI agents with just 3 lines of code ](https://www.reddit.com/r/Langchaindev/comments/1dtjmm9/adding_observability_to_langchain_based_ai_agents/) , 2024-07-03-0911
```
Hey folks,

I've been building AI assistants using Phidata which uses Langchain Knowledge Base. I needed better observab
ility for my assistants to track metrics such as costs, response latency, caching, etc.

To solve these issues I integra
ted Porktey with Phidata. I decided to create a simple cookbook. It integrates Phidata seamlessly with Portkey's AI gate
way in 3 lines of code, giving me clear insights into how my assistants perform.

Here's the link to the Google Collab n
otebook- [https://git.new/Phidata-Portkey](https://git.new/Phidata-Portkey)

Let me know what you think or if you have a
ny tips to improve!

Cheers!
```
---

     
 
all -  [ How I built observability for my AI assistant ](https://www.reddit.com/r/LangChain/comments/1dtjipf/how_i_built_observability_for_my_ai_assistant/) , 2024-07-03-0911
```
Hey folks,

I've been building AI assistants using Phidata which uses Langchain Knowledge Base. I needed better observab
ility for my assistants to track metrics such as costs, response latency, caching, etc.

To solve these issues I integra
ted Porktey with Phidata. I decided to create a simple cookbook. It integrates Phidata seamlessly with Portkey's AI gate
way in 3 lines of code, giving me clear insights into how my assistants perform.

Here's the link to the google collab n
otebook- [https://git.new/Phidata-Portkey](https://git.new/Phidata-Portkey)

Let me know what you think or if you have a
ny tips to improve!

Cheers!
```
---

     
 
all -  [ Text-to-sql system using LLM ](https://www.reddit.com/r/LangChain/comments/1dtjafd/texttosql_system_using_llm/) , 2024-07-03-0911
```
I am currently working on a project that involves developing a text-to-SQL system. The goal is to allow users to input n
atural language, and the system will generate SQL queries. I have reference documents (PDFs) containing SQL queries and 
their descriptions (of schemas as well). I am considering using a Retrieval-Augmented Generation (RAG) system for this t
ask. The idea is to use the provided documents to help generate the SQL queries. However, I am unsure if this is the bes
t approach. Alternatively, I am thinking about fine-tuning a model specifically for this purpose.  Also, is sqlcoder2-7b
 the sql code generator out there?

I would greatly appreciate guidance on the following:

* Is the RAG system the best 
solution for this problem?
* Would fine-tuning a model be more effective for generating accurate SQL queries?
* Is there
 a better approach that I should consider?

All your insights and suggestions will be invaluable in helping me determine
 the best course of action for this project. Thank you
```
---

     
 
all -  [ Aspiring AI Engineer ](https://www.reddit.com/r/LangChain/comments/1dtio2u/aspiring_ai_engineer/) , 2024-07-03-0911
```
Can you suggest some good projects to build and good open source repo to contribute for getting AI Engineer Job. I am fr
esher and I have 1 year to build my portfolio
```
---

     
 
all -  [ Building an Agentic Workflow with Langchain CrewAI and Groq ](https://www.reddit.com/r/LangChain/comments/1dth36y/building_an_agentic_workflow_with_langchain/) , 2024-07-03-0911
```
With the growing popularity of large language models, Agents are becoming a topic of discussion. In this article, we wil
l explore Autonomous Agents, cover the components of building an Agentic workflow, and discuss the practical implementat
ion of a Content creation agent using Langhchain Groq and crewAI.

Code Implementation and article in comment:
```
---

     
 
all -  [ 500+ applications for the past month, just as many rejections. What is going on? ](https://www.reddit.com/r/resumes/comments/1dt9nm6/500_applications_for_the_past_month_just_as_many/) , 2024-07-03-0911
```
Please pick apart my resume!

https://preview.redd.it/dsp1o5hlb0ad1.png?width=795&format=png&auto=webp&s=85ffac01c61b495
c41c180139496ba8e50684b66

https://preview.redd.it/3kriya7ob0ad1.png?width=793&format=png&auto=webp&s=6bf655a465a17a0d27
5257100f932fc93328afeb


```
---

     
 
all -  [ Paid Work Opportunity- Full Stack + Langchain/AI ](https://www.reddit.com/r/iitmadras/comments/1dt9fz6/paid_work_opportunity_full_stack_langchainai/) , 2024-07-03-0911
```
Location: Remote. We will do daily standups.

IIT grads, I'm seeking a junior/senior year student majoring in computer s
cience or software engineering who wants to work and study. Im the cofounder of the startup company(well just starting) 
and what better than giving opportunity to one of the smartest and finest students in the world.

Our stack:  
Frontend:
 React, Redux  
Backend: Fastapi(Python),

Technologies(preferred but not required)  
Redis, celery, CI/CD, GCP, Langcha
in

\*\*Definetely need to have frontend development skill.\*\*

Salary: $400/month

As long as you do the work and are 
communicative, I have no issues.

If interested, DM me your resume. If you dont have one, hitme up with github handle or
 describe the projects you did.
```
---

     
 
all -  [ How to Invoke RunnableSequence chain with multiple inputs ](https://www.reddit.com/r/LangChain/comments/1dt57k9/how_to_invoke_runnablesequence_chain_with/) , 2024-07-03-0911
```
I have a PromptTemplate with three inputs: cause, symptom, and action. My LLM should use the inputs to produce a 'theme'
 it gathers from the inputs. However, I am getting an error because the chain.invoke method expects one input instead of
 multiple.

This is the error that is thrown: ValueError: Invalid input type <class 'dict'>. Must be a PromptValue, str,
 or list of BaseMessages.

All the examples of invoking a chain in the langchain documentation only have one input. What
 am I supposed to do when my prompt template contains three inputs?

For reference, this is the code:

`llm = AzureChatO
penAI()`

`prompt_template = PromptTemplate(`

`input_variables=['cause', 'symptom', 'action'],`

`template='''`

`Root 
Cause:`

`{cause}`

`Complaint Symptom:`

`{symptom}`

`Corrective Action:`

`{action}`

`Based on the information provi
ded above, generate a concise 'Failure Theme' encapulating the reason for the failure.`

`Failure Theme:`

`'''`

`)`

`
chain = llm | prompt_template | StrOutputParser()`

`root_cause = df.iloc[0]['Root Cause']`

`complaint_symptom = df.ilo
c[0]['Complaint Symptom']`

`corrective_action = df.iloc[0]['Corrective Action']`

`chain.invoke({'cause': root_cause, '
symptom': complaint_symptom, 'action': corrective_action})`

 
```
---

     
 
all -  [ Resume for Machine Learning roles. Feel free to roast and suggest changes  ](https://www.reddit.com/r/developersIndia/comments/1dt2p6n/resume_for_machine_learning_roles_feel_free_to/) , 2024-07-03-0911
```
https://preview.redd.it/dbb8f6zvry9d1.png?width=1188&format=png&auto=webp&s=88152f72bc2fdbd90d066f5a314cfbb085ceaa53


```
---

     
 
all -  [ Automate Slide decks and presentations ](https://www.reddit.com/r/LangChain/comments/1dt0zzz/automate_slide_decks_and_presentations/) , 2024-07-03-0911
```
https://medium.com/genai-agents-unleashed/generate-powerpoint-presentation-with-openai-the-future-of-slide-decks-ce1a7c9
28986
```
---

     
 
all -  [ How to generate Cypher Query using LLM? ](https://www.reddit.com/r/LangChain/comments/1dt089d/how_to_generate_cypher_query_using_llm/) , 2024-07-03-0911
```
I have a huge schema in the neo4j database.

I'm using the LangChain function to generate a cypher query

chain = GraphC
ypherQAChain.from_llm(
ChatOpenAI(temperature=0), graph=graph, verbose=True
)

chain.invoke(query)

It's returning an er
ror saying that the model supports 16k tokens and I'm passing 15M+ tokens

How can I limit these tokens? I tried setting
 ChatOpenAI(temperature=0, max_tokens=1000) and it's still giving the same error.

I think it's passing the whole schema
 at once, how can I set a limit on that?
```
---

     
 
all -  [ HuggingFace models on multiple GPUs ](https://www.reddit.com/r/huggingface/comments/1dszyy3/huggingface_models_on_multiple_gpus/) , 2024-07-03-0911
```
I made a RAG app that basically answers user questions based on provided data, it works fine on GPU and a single GPU. I 
want to deploy it on multiple GPUs (4 T4s) but I always get CUDA out of Memory error on pipeline.



I tried using 'auto
' keyword too but Langchain does not let me use it as keyword.

I used Langchain as main framework, my code looks like t
his:



    from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings
    MODEL_NAME
='mistralai/Mistral-7B-Instruct-v0.3'
    pipe = HuggingFacePipeline.from_model_id(
                               model
_id=MODEL_NAME,
                               device=0,
                               model_kwargs={'torch_dtype':torc
h.float16},
                               task='text-generation')
    llm = ChatHuggingFace(llm=pipe)
    
    embeddin
g = HuggingFaceEmbeddings(model_name=MODEL_NAME,
                                      model_kwargs={'device':'cuda:1'},

                                      multi_process=True,
                                      )






```
---

     
 
all -  [ RAG app on multiple GPUs ](https://www.reddit.com/r/LangChain/comments/1dszxdk/rag_app_on_multiple_gpus/) , 2024-07-03-0911
```
I made a RAG app that basically answers user questions based on provided data, it works fine on GPU and a single GPU. I 
want to deploy it on multiple GPUs (4 T4s) but I always get CUDA out of Memory error on pipeline.

I tried using 'auto' 
keyword too but Langchain does not let me use it as keyword.

I used Langchain as main framework, my code looks like thi
s:

    from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings
    MODEL_NAME='mi
stralai/Mistral-7B-Instruct-v0.3'
    pipe = HuggingFacePipeline.from_model_id(
    model_id=MODEL_NAME,
    device=0,
 
   model_kwargs={'torch_dtype':torch.float16},
    task='text-generation')
    llm = ChatHuggingFace(llm=pipe)
    embed
ding = HuggingFaceEmbeddings(model_name=MODEL_NAME,
    model_kwargs={'device':'cuda:1'},
    multi_process=True,
    )
```
---

     
 
all -  [ What's wrong with Langchain React Agent ](https://www.reddit.com/r/LangChain/comments/1dsyf46/whats_wrong_with_langchain_react_agent/) , 2024-07-03-0911
```
    i asked react agent about last name of johnny it's not showing in it's final output why ?
    
    I think I have fo
und the table that contains the name 'JOHNNY'. It's the 'actor' table. Now, I need to write a query to retrieve the last
 name of the actor with the first name 'JOHNNY'.
    
    Action: sql_db_query_checker
    Action Input: SELECT last_nam
e FROM actor WHERE first_name = 'JOHNNY'SELECT last_name FROM actor WHERE first_name = 'JOHNNY'Action: sql_db_query
    
Action Input: SELECT last_name FROM actor WHERE first_name = 'JOHNNY'[('LOLLOBRIGIDA',), ('CAGE',)]I now know the final 
answer
    Final Answer: The last name of JOHNNY is not found in the database, but there are actors with the last names 
LOLLOBRIGIDA and CAGE who have first names that are not JOHNNY. It's possible that the actor with the first name JOHNNY 
does not exist in the database.
    
    > Finished chain.
```
---

     
 
all -  [ Built a PR Agent with Langchain and 3 other frameworks - A Comparison ](https://www.reddit.com/r/LangChain/comments/1dsych9/built_a_pr_agent_with_langchain_and_3_other/) , 2024-07-03-0911
```
The goal was to create an agent that would:

1. Monitor a GitHub repository for new PRs
2. Perform a code review on each
 PR
3. Post a summary of the review to a Slack channel

https://preview.redd.it/0yf542afwx9d1.png?width=1442&format=png&
auto=webp&s=791bd8618af7c67f8e20d928705639366bea065b

|Framework|Why langchain is better|Why it can be better|
|:-|:-|:-
|
|CrewAI|Langchain provides a more comprehensive toolkit for building various AI applications, including but not limite
d to multi-agent scenarios.|CrewAI may be easier to get started with for specific multi-agent tasks|
|Llama Index|Langch
ain offers more general-purpose capabilities|For projects primarily focused on data indexing and retrieval, llama index 
is better|
|Autogen|Langchain provides more structured components for building AI applications|For projects requiring hi
ghly dynamic multi-agent interactions, Autogen's specialized focus might provide a more intuitive framework|

The agent 
works great!

here's the link for the project: [https://git.new/pr-agent-langchain](https://git.new/pr-agent-langchain)
```
---

     
 
all -  [ I built a PR Agent with all the Agentic Frameworks - Here's my experience ](https://www.reddit.com/r/LocalLLaMA/comments/1dsxros/i_built_a_pr_agent_with_all_the_agentic/) , 2024-07-03-0911
```
Note: PR stands for Pull Request

I've intensively been building and experimenting with agents using all the famous libr
aries mentioned above. I've experimented with both Closed and Open Source LLMs and i want to share my experience.

**Qui
ck Framework Review**

|Framework|Strengths|Considerations|
|:-|:-|:-|
|CrewAI|High-level abstraction, good for beginner
s|Less mature ecosystem|
|Autogen|Easy agent setup, multi-agent interactions|Limited external tool integration|
|Langcha
in|Versatile, extensive features|Steeper learning curve|
|LlamaIndex|Specialized in data handling|Fewer agent interactio
n features|
|OpenAI|Direct model access, frequent updates|Requires more custom code for complex systems|

**Quick Toolin
g Review**

|Tools Framework|Strengths|
|:-|:-|
|Langchain Tools|High flexibility but increased complexity|
|Composio|Ex
ceptionally easy third-party app integration (e.g., GitHub, Slack)|

# Detailed Review

For developers with experience i
n LLM-based application development, adopting any of these frameworks is relatively straightforward. Each framework offe
rs unique opportunities to create innovative applications.

1. CrewAI and Autogen:
   * Recommended for beginners
   * H
igher level of abstraction
   * Easier entry point for AI agent development
2. Langchain:
   * Next step after gaining e
xperience
   * Offers more refined outputs
   * Greater flexibility and control
3. LlamaIndex:
   * Currently less featu
re-rich compared to Langchain
   * Rapidly evolving and closing the feature gap
4. OpenAI:
   * Best framework for using
 ChatGPT in terms of performance
   * You have a good support for the framework

# Detailed Review for Tooling

# Compos
io

* Exceptionally easy third-party app integration (e.g., GitHub, Slack)
* Minimal setup time required

# Langchain To
ols

* Comprehensive toolset with extensive integration options
* Requires more manual configuration (auth tokens, API s
etup)
* High flexibility but increased complexity

These insights come from my project that monitors PRs, reviews it and
 pings the review on slack. here's the link if you want to try it: [https://git.new/pr-agent](https://git.new/pr-agent)


# Challenges and Considerations

* Frequent updates lead to potential code obsolescence
* Syntax and function parameter
s often change
* Regular maintenance required to keep applications functional

# Open Source vs. Closed Source Performan
ce

* AI agents: Open source models competitive with closed-source alternatives
* Would recommend inference APIs over lo
cally hosted ones, just on the basis of convenience and not performance although with weaker models you would get weaker
 outputs.

edit: the frameworks ive used are CrewAI, Langchain, Llama Index and Autogen mainly, im sure there are multip
le frameworks that i'm unaware of, but for the sake of the post i chose these.
```
---

     
 
all -  [ Chat With Any WebPage or Application using Visual Agents & OpenAI ](https://www.reddit.com/r/OpenAI/comments/1dswf67/chat_with_any_webpage_or_application_using_visual/) , 2024-07-03-0911
```
I made a TL;DR video about using our browser extension to run your chat agents alongside any web page or app! Visually d
esign your LangChain RAG +  OpenAI LLM + Agents app, add a chat UI to it and use it instantly, all from your browser. No
 code!

[https://youtu.be/5-QV3lVI8uo](https://youtu.be/5-QV3lVI8uo)  


[https://visualagents.ai](https://visualagents.
ai)
```
---

     
 
all -  [ Chat With Any WebPage or Application using Visual Agents & LangChain ](https://www.reddit.com/r/LangChain/comments/1dswds9/chat_with_any_webpage_or_application_using_visual/) , 2024-07-03-0911
```
I made a TL;DR video about using our browser extension to run your chat agents alongside any web page or app! Visually d
esign your LangChain RAG + Agents app, add a chat UI to it and use it instantly, all from your browser. No code!

[https
://youtu.be/5-QV3lVI8uo](https://youtu.be/5-QV3lVI8uo)
```
---

     
 
all -  [ choosing multi-tool in agent ](https://www.reddit.com/r/LangChain/comments/1dsvy6s/choosing_multitool_in_agent/) , 2024-07-03-0911
```
I haven't seen the sample in Langchain 0.2, so I just referenced [v0.1](https://python.langchain.com/v0.1/docs/use_cases
/tool_use/multiple_tools/)

I use local model with 'taide-7b-a.2-q4\_k\_m.gguf', so there are some different

          
  llm = ChatLlamaCpp(
                model_path= str(model_path),
                n_gpu_layers=100,
                n_b
atch=512,
                n_ctx=2048,
                f16_kv=True,
                callback_manager=CallbackManager([Str
eamingStdOutCallbackHandler()]),
                verbose=True,
            ) 

after invoking the question, I can't get 
the tools information. like

    [{'name': 'multiply',
      'args': {'first_int': 23, 'second_int': 7},
      'id': 'to
olu_01Wf8kUs36kxRKLDL8vs7G8q',
      'output': 161}]

Can anyone tell me what's wrong? here's the code

        tools = 
[multiply, exponentiate, add]
        llm_with_tools = llm.bind_tools(tools)
    
        def call_tools(msg: AIMessage)
 -> Runnable:
            '''Simple tool calling helper.'''
            tool_map = {tool.name: tool for tool in tools}
 
           tool_calls = msg.tool_calls.copy()
            for tool_call in tool_calls:
                tool_call['output
']  = tool_map[tool_call['name']].invoke(tool_call['args'])
            return tool_calls
    
        chain = llm_with_
tools | call_tools
        chain.invoke('23 times 7')


```
---

     
 
all -  [ Streaming responses have words split ](https://www.reddit.com/r/LangChain/comments/1dsvmg8/streaming_responses_have_words_split/) , 2024-07-03-0911
```
Using astream, the response from the LLM has words that are split for example the word 'hippopotamus' comes as 2 chunks 
'hippo' and 'potamus'. When creating an app, how to recognize and combine the 2 split parts into a single word for front
-end?
```
---

     
 
all -  [ Integrate multiple AWS bedrock LLMs seamlessly with Langchain ](https://www.reddit.com/r/LangChain/comments/1dsrfca/integrate_multiple_aws_bedrock_llms_seamlessly/) , 2024-07-03-0911
```
Hello everyone,

Here's a cool post that shows how to integrate multiple AWS Bedrock LLMs in your LangChain apps and cho
osing which one used with only one configuration parameter.

Here's the link:  [link](https://www.metadocs.co/2024/04/11
/handle-multiple-llm-models-in-langchain-and-aws-bedrock-seamlessly/).

  
Have a nice read.
```
---

     
 
MachineLearning -  [ [P] Seeking Feedback on My GenAI Job Fit Project - New to LangChain/LangGraph ](https://www.reddit.com/r/MachineLearning/comments/1dgns9p/p_seeking_feedback_on_my_genai_job_fit_project/) , 2024-07-03-0911
```
Hi all,

Soo, i have been working on a a projectcalled [GenAI Job Fit](https://github.com/DAVEinside/GenAI_Job_Fit). It'
s an AI-driven system designed to enhance job applications by providing tailored recommendations based on individual pro
files.

I'm relatively new to LangChain and LangGraph, and I've incorporated them into this project. I would greatly app
reciate it if you could check out the repository and provide any feedback or suggestions for improvement.

Your insights
 on how I can better implement LangChain/LangGraph, or any other aspect of the project, would be incredibly valuable. I'
m eager to learn and make this project as robust as possible.

Thank you in advance for your time and feedback!

Repo Li
nk : [https://github.com/DAVEinside/GenAI\_Job\_Fit](https://github.com/DAVEinside/GenAI_Job_Fit)
```
---

     
 
MachineLearning -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-07-03-0911
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
deeplearning -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-07-03-0911
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
 
deeplearning -  [ What is ReAct Prompting? the most important piece in agentic frameworks ](https://www.reddit.com/gallery/1djk4nk) , 2024-07-03-0911
```
“What is ReAct Prompting? the most important piece in agentic frameworks” - A quick read from Mastering LLM (Large Langu
age Model) 'Coffee Break Concepts' Vol.6

This document deeps dive into the ReAct Prompting method and why it's importan
t:
1. Limitations of LLM
2. Why ReAct prompting matters?
3. How ReAct Works?
4. LangChain Implementation
5. Why Prompt w
ithin agentic frameworks Matters?

Comment below on which topic you want to understand next in this 'Coffee Break Concep
ts' series and we will include those topics in upcoming weeks.
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-07-03-0911
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
