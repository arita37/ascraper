 
all -  [ Canadian citizen and computer science student seeking US new grad software engineering roles. Did no ](https://www.reddit.com/r/resumes/comments/1dw8nfo/canadian_citizen_and_computer_science_student/) , 2024-07-06-0910
```
https://preview.redd.it/13roat0cnrad1.png?width=3060&format=png&auto=webp&s=4680e80c8c40ece87625c3c6357f659f65ff6d7f

To
 answer some questions:

* Have not applied at all, so I do not know the current strength of the resume. When I do apply
, I want my resume to be as strong as possible. If I apply to Microsoft now for example, then I wouldn't be able to appl
y with a stronger resume later on (I think at least?)
* I do need work authorization to work in the US, but I do not nee
d sponsorship (can work on TN visa)
* My university is a target school for Amazon and Microsoft where I live, but I'm sp
ecifically seeking for US jobs (my girlfriend works in the US)
* My target is big tech preferably of course, and I do ha
ve connections with people and recruiters in US (and Canadian) offices of several FAANGs. However, I would really like t
o be able to make the resume strong enough that I do not need a referral, even though I will of course ask
* Not a diver
sity hire (neurodivergent yes (autism + ADHD), but gender and race no)
* My bullet points are accurate, but I am aware t
hey may seem not believable. I am considering dumbing them down intentionally to make them more believable. But I believ
e I would be able to explain any of them during an interview (e.g. 60% performance increase was because I was optimizing
 the work of a first-year intern who left prior to me joining). However, I am scared that the bullet points are so unbel
ievable that I wouldn't even be given the chance to explain myself
* The reason why I have technologies listed in bullet
 points or projects but not in technical skills is because I did use them in the job/project, but I wouldn't consider my
self strong enough to talk about the intricacies of that technology during an interview (e.g. Golang, Rust, Django which
 are mentioned in bullet points or projecs, but not in technical skills)
* I put it on there but the internship dates th
at started before university was during high school
* While doing all of the internships, I've been taking classes part-
time online. So half of my credits are from my university and the other half are from other universities. I didn't put t
his on there of course, but that is why my expected graduation date is still in 2025 despite interning since the start o
f 2023
* The reason why the project under the name 'Personal project' has so many technologies is because the backend co
nsists of a fork of a Ruby on Rails app which I had to modify, communicating with a Golang GraphQL API I am making, Fire
base is only for auth. gRPC is used to communicate between the Rails app and Golang app. GraphQL is used between the Rea
ct Native frontend and Golang app
* I did not develop or start the 'Popular app among students' project myself, and I am
 also not the lead developer. I would consider myself a contributor/maintainer of the open source project
```
---

     
 
all -  [  LangChain vs txtai for BM25  ](https://i.redd.it/vnzx1bb9hrad1.png) , 2024-07-06-0910
```

```
---

     
 
all -  [ Django AI Assistant - Open-source Lib Launch ](https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/) , 2024-07-06-0910
```
Hey folks, weâ€™ve just launched an open-source library calledÂ Django AI Assistant, and weâ€™d love your feedback!

What It 
Does:

* **Function/Tool Calling**: Simplifies complex AI implementations with easy-to-use Python classes
* **Retrieval-
Augmented Generation**: Enhance AI functionalities efficiently.
* **Full Django Integration**: AI can access databases, 
check permissions, send emails, manage media files, and call external APIs effortlessly.

How You Can Help:

1. Try It:Â 
[https://github.com/vintasoftware/django-ai-assistant/](https://github.com/vintasoftware/django-ai-assistant/)
2. â–¶ï¸Â [Wa
tch the Demo](https://www.youtube.com/watch?v=bSJv4OIKLog&ab_channel=VintaSoftware)
3. ðŸ“–Â [Read the Docs](https://vintaso
ftware.github.io/django-ai-assistant/latest/get-started/)
4. Test It & Break Things: Integrate it, experiment, and see w
hat works (and what doesnâ€™t).
5. Give Feedback: Drop your thoughts here or on our GitHub issues page.

Your input will h
elp us make this lib better for everyone. Thanks!
```
---

     
 
all -  [ YouTube comments feature ](https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/) , 2024-07-06-0910
```
YouTube has a new feature where it organizes comments by. It it possible to organize a list of chat by topic with langch
ain?
```
---

     
 
all -  [ Canadian citizen and computer science student seeking US new grad software engineering roles. Did no ](https://www.reddit.com/r/csMajors/comments/1dw2y4v/canadian_citizen_and_computer_science_student/) , 2024-07-06-0910
```
https://preview.redd.it/b0d6uvnbfqad1.png?width=3060&format=png&auto=webp&s=a5b889e42b8851edb80e22fa36a9681265738b30

To
 answer some questions:

* Have not applied at all, so I do not know the current strength of the resume. When I do apply
, I want my resume to be as strong as possible. If I apply to Microsoft now for example, then I wouldn't be able to appl
y with a stronger resume later on (I think at least?)
* I do need work authorization to work in the US, but I do not nee
d sponsorship (can work on TN visa)
* My university is a target school for Amazon and Microsoft where I live, but I'm sp
ecifically seeking for US jobs (my girlfriend works in the US)
* My target is big tech preferably of course, and I do ha
ve connections with people and recruiters in US (and Canadian) offices of several FAANGs. However, I would really like t
o be able to make the resume strong enough that I do not need a referral, even though I will of course ask
* Not a diver
sity hire (neurodivergent yes (autism + ADHD), but gender and race no)
* My bullet points are accurate, but I am aware t
hey may seem not believable. I am considering dumbing them down intentionally to make them more believable. But I believ
e I would be able to explain any of them during an interview (e.g. 60% performance increase was because I was optimizing
 the work of a first-year intern who left prior to me joining). However, I am scared that the bullet points are so unbel
ievable that I wouldn't even be given the chance to explain myself
* The reason why I have technologies listed in bullet
 points or projects but not in technical skills is because I did use them in the job/project, but I wouldn't consider my
self strong enough to talk about the intricacies of that technology during an interview (e.g. Golang, Rust, Django which
 are mentioned in bullet points or projecs, but not in technical skills)
* I put it on there but the internship dates th
at started before university was during high school
* While doing all of the internships, I've been taking classes part-
time online. So half of my credits are from my university and the other half are from other universities. I didn't put t
his on there of course, but that is why my expected graduation date is still in 2025 despite interning since the start o
f 2023
* The reason why the project under the name 'Personal project' has so many technologies is because the backend co
nsists of a fork of a Ruby on Rails app which I had to modify, communicating with a Golang GraphQL API I am making, Fire
base is only for auth. gRPC is used to communicate between the Rails app and Golang app. GraphQL is used between the Rea
ct Native frontend and Golang app
* I did not develop or start the 'Popular app among students' project myself, and I am
 also not the lead developer. I would consider myself a contributor/maintainer of the open source project
```
---

     
 
all -  [ How to get into ML/AI domain?  ](https://www.reddit.com/r/careerguidance/comments/1dw24nk/how_to_get_into_mlai_domain/) , 2024-07-06-0910
```
Hi, I'm a software developer with 5 years of experience. The languages and technologies that I have used at work are Per
l and Oracle SQL at backend, React.js and Typescript for front-end with most of the work at backend. I did not care abou
t the tech stack at first as the job was paying me well. However, now I'm stuck at applying to other companies and start
ed to upskill myself. 

I'm interested in Machine learning recently and completed ML, DL and AI courses in Udemy. I have
 also started learning about Gen AI using Langchain.

Colleagues at office are suggesting to do AWS certifications if pl
anning to stay in the same domain and PG or MS in Machine Learning to get into AI/ML domain.

On going through sites lik
e quora and reddit, many have suggested to improve the skills instead of spending lakhs on getting a degree or certifica
tion.
Can anyone suggest me how to improve my ML/AI skills and get a job in this domain? Is PG/MS needed? 
```
---

     
 
all -  [ Is there a way to save a RAG after it has read its documents? ](https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/) , 2024-07-06-0910
```
Potentially dumb question lol. Basically when I run my RAG, it takes a long time to process all the documents that it wi
ll then retrieve. Is there a way to just save off the model after it is done reading the documents so that when you run 
it again, it can skip that step? Similar to how a fine-tuned model would work? It doesn't really make sense in my head, 
but I haven't been able to find a concrete answer to this so I want to be sure.
```
---

     
 
all -  [ Deploy Hugging Face model in Sagemaker ](https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/) , 2024-07-06-0910
```
I want to deploy a Huggingface model in Sagemaker with a context size of around 25-32k. I am having trouble finding a su
itable model that performs well with this context size. The model's task will be to map raw data to a target framework. 

```
---

     
 
all -  [ Any good resource/guide about how to do RAG on a codebase? (e.g. Github repo) ](https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/) , 2024-07-06-0910
```
like title. Thanks in advance!
```
---

     
 
all -  [ Struggling to Use Tools in an Agent-Based System with Ollama and Langgraph â€“ Any Success Stories? ](https://www.reddit.com/r/ollama/comments/1dvrlvc/struggling_to_use_tools_in_an_agentbased_system/) , 2024-07-06-0910
```
Has anyone successfully used tools in an agent-based system using Ollama and \`langgraph\`?   
  
I attempted to use \`T
AVILY\` for search within a \`ChatOpenAI\` wrapper but haven't succeeded yet.   
  
My goal is to achieve this on a **lo
cal LLM**. Any help would be highly appreciated.

Here is my code:  

[code snippet](https://preview.redd.it/mvzz5r8zbna
d1.png?width=4096&format=png&auto=webp&s=e7ea85e9ab88e29b030c3e74b4d25ed1e6e464cd)

>from langgraph.graph import StateGr
aph, END

>from typing import TypedDict, Annotated

>import operator

>from langchain\_core.messages import AnyMessage, 
SystemMessage, HumanMessage, ToolMessage, AIMessage

>from langchain\_openai import ChatOpenAI

>from langchain\_communi
ty.tools.tavily\_search import TavilySearchResults

>import re

>

>import os

>os.environ\['OPENAI\_API\_KEY'\] = 'NA'


>os.environ\['TAVILY\_API\_KEY'\] = 'tvly-XXXXXXXXXXXXXXXXXXXXXXX'

>

>tool = TavilySearchResults(max\_results=2)

>pr
int(type(tool))

>print(tool.name)

>

>class AgentState(TypedDict):

>messages: Annotated\[list\[AnyMessage\], operator
.add\]

>

>class Agent:

>

>def \_\_init\_\_(self, model, tools, system=''):

>self.system = system

>graph = StateGra
ph(AgentState)

>graph.add\_node('llm', self.call\_openai)

>graph.add\_node('action', self.take\_action)

>graph.add\_c
onditional\_edges(

>'llm',

>self.exists\_action,

>{True: 'action', False: END}

>)

>graph.add\_edge('action', 'llm')


>graph.set\_entry\_point('llm')

>self.graph = graph.compile()

>self.tools = {t.name: t for t in tools}

>self.model 
= model.bind\_tools(tools)

>

>def exists\_action(self, state: AgentState):

>result = state\['messages'\]\[-1\]

>tool
\_calls = getattr(result, 'tool\_calls', \[\])

>print(f'Tool calls detected: {tool\_calls}')

>return len(tool\_calls) 
> 0

>

>def call\_openai(self, state: AgentState):

>messages = state\['messages'\]

>if self.system:

>messages = \[Sy
stemMessage(content=self.system)\] + messages

>print(f'Messages sent to LLM: {messages}')

>message = self.model.invoke
(messages)

>print(f'LLM response: {message}')

>state\['messages'\].append(message)

>

># Check if the response mentio
ns a search but does not trigger a tool call

>if self.response\_mentions\_search(message.content) and not self.exists\_
action(state):

>print('Search mentioned in response but no tool call detected. Forcing tool call.')

># Manually create
 a tool call based on detected search

>search\_query = self.extract\_search\_query(message.content)

>tool\_call\_messa
ge = ToolMessage(tool\_call\_id='manual\_tool\_call', [name=tool.name](http://name=tool.name), content=f'search('{search
\_query}')')

>state\['messages'\].append(tool\_call\_message)

>return self.take\_action(state)

>

>return state

>

>
def response\_mentions\_search(self, response\_content):

># Detect if the response mentions a search command

>return b
ool(re.search(r'\\\[search\\('.\*'\\)\\\]', response\_content))

>

>def extract\_search\_query(self, response\_content)
:

># Extract the search query from the response content

>return re.search(r'\\\[search\\('(.\*)'\\)\\\]', response\_co
ntent).group(1)

>

>def take\_action(self, state: AgentState):

>last\_message = state\['messages'\]\[-1\]

>tool\_call
s = \[last\_message\] if isinstance(last\_message, ToolMessage) else last\_message.tool\_calls

>results = \[\]

>for t 
in tool\_calls:

>print(f'Calling tool: {t}')

>if t.name not in self.tools:

>print('Bad tool name detected.')

>result
 = 'bad tool name, retry'

>else:

>result = self.tools\[t.name\].invoke(t.content)

>results.append(ToolMessage(tool\_c
all\_id=t.tool\_call\_id, [name=t.name](http://name=t.name), content=str(result)))

>print('Returning to model with tool
 results.')

>state\['messages'\].extend(results)

>return state

>

>prompt = '''You are a smart research assistant. Us
e the search engine to look up information. \\

>You are allowed to make multiple calls (either together or in sequence)
. \\

>Only look up information when you are sure of what you want. \\

>If you need to look up some information before 
asking a follow-up question, you are allowed to do that! \\

>When you don't have the information you need, use the tool
s. For example: search('current weather in Tokyo').

>

>Example:

>Human: What is the weather in Tokyo?

>Assistant: Le
t me check. \[search('current weather in Tokyo')\]

>'''

>

># Ensure model supports tool usage

>model = ChatOpenAI(


>#     model='llama3',

>model='gemma2',

>base\_url = 'http://localhost:11434/v1' # ollama's default port

>)

>abot = 
Agent(model, \[tool\], system=prompt)

>

>query='Which day is today? what is the current time at tokyo? find the weathe
r as well'

>messages = \[HumanMessage(content=query)\]

>result = abot.graph.invoke({'messages': messages})

>

>print(
f'Final result: {result}')

And this is my output:

    <class 'langchain_community.tools.tavily_search.tool.TavilySearc
hResults'>
    tavily_search_results_json
    Messages sent to LLM: [SystemMessage(content='You are a smart research ass
istant. Use the search engine to look up information. You are allowed to make multiple calls (either together or in sequ
ence). Only look up information when you are sure of what you want. If you need to look up some information before askin
g a follow-up question, you are allowed to do that! When you don\'t have the information you need, use the tools. For ex
ample: search('current weather in Tokyo').\n\nExample:\nHuman: What is the weather in Tokyo?\nAssistant: Let me check. [
search('current weather in Tokyo')]\n'), HumanMessage(content='Which day is today? what is the current time at tokyo? fi
nd the weather as well')]
    LLM response: content='Let me check.  \n\n[search('current date')]\n[search('current time 
in Tokyo')] \n[search('weather in Tokyo')] \n' response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_token
s': 100, 'total_tokens': 139}, 'model_name': 'gemma2', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logp
robs': None} id='run-702af47e-486c-400e-b0fb-45fde7aa0728-0'
    Tool calls detected: []
    Search mentioned in respons
e but no tool call detected. Forcing tool call.
    Calling tool: content='search('current date')' name='tavily_search_r
esults_json' tool_call_id='manual_tool_call'
    Returning to model with tool results.
    Tool calls detected: []
    F
inal result: {'messages': [HumanMessage(content='Which day is today? what is the current time at tokyo? find the weather
 as well'), AIMessage(content='Let me check.  \n\n[search('current date')]\n[search('current time in Tokyo')] \n[search(
'weather in Tokyo')] \n', response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 100, 'total_token
s': 139}, 'model_name': 'gemma2', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run
-702af47e-486c-400e-b0fb-45fde7aa0728-0'), ToolMessage(content='search('current date')', name='tavily_search_results_jso
n', tool_call_id='manual_tool_call'), ToolMessage(content='[{\'url\': \'https://www.w3schools.com/SQl/func_mysql_current
_date.asp\', \'content\': \'Tutorials\\nHTML and CSS\\nData Analytics\\nWeb Building\\nJavaScript\\nWeb Building\\nBacke
nd\\nData Analytics\\nWeb Building\\nExercises\\nHTML and CSS\\nData Analytics\\nJavaScript\\nBackend\\nData Analytics\\
nCertificates\\nHTML and CSS\\nData Analytics\\nPrograms\\nJavaScript\\nPrograms\\nPrograms\\nBackend\\nData Analytics\\
nAll Our Services\\nW3Schools offers a wide range of services and products for beginners and professionals,\\nhelping mi
llions of people everyday to learn and master new skills.\\n Enjoy our free tutorials like millions of other internet us
ers since 1999\\nExplore our selection of references covering all popular coding languages\\nCreate your own website wit
h\\nW3Schools Spaces\\n- no setup required\\nTest your skills with different exercises\\nTest yourself with multiple cho
ice questions\\nDocument your knowledge\\nCreate a\\nfree\\nW3Schools Account to Improve Your Learning Experience\\nTrac
k your learning progress at W3Schools and collect rewards\\nBecome a PRO user and unlock powerful features (ad-free, hos
ting, videos,..)\\n Help the lynx collect pine cones\\nGet personalized learning journey based on your current skills an
d goals\\nJoin our newsletter and get access to exclusive content every month\\nSQL Tutorial\\nSQL Database\\nSQL Refere
nces\\nSQL Examples\\nMySQL CURRENT_DATE() Function\\nExample\\nReturn the current date:\\nDefinition and Usage\\nThe CU
RRENT_DATE() function returns the current date.\\n Large collection of code snippets for HTML, CSS and JavaScript\\nBuil
d fast and responsive sites using our free\\nW3.CSS\\nframework\\nRead long term trends of browser usage\\nTest your typ
ing speed\\nLearn Amazon Web Services\\nUse our color picker to find different RGB, HEX and HSL colors.\\n Syntax\\nTech
nical Details\\nMore Examples\\nExample\\nReturn the current date + 1:\\nReport Error\\nIf you want to report an error, 
or if you want to make a suggestion, do not hesitate to send us an e-mail:\\nhelp@w3schools.com\'}, {\'url\': \'https://
www.sqltutorial.org/sql-date-functions/sql-current-date/\', \'content\': 'The CURRENT_DATE is SQL-standard date function
 supported by almost all database systems such as Firebird, DB2, MySQL 5.x+, MonetDB, Oracle 11.x+, PostgreSQL, and SQLi
te. Note that Oracle\'s CURRENT_DATE returns both date and time values, therefore, to get the date data, you use the TRU
NC function to truncate the time part:'}]', name='tavily_search_results_json', tool_call_id='manual_tool_call'), HumanMe
ssage(content='Which day is today? what is the current time at tokyo? find the weather as well'), AIMessage(content='Let
 me check.  \n\n[search('current date')]\n[search('current time in Tokyo')] \n[search('weather in Tokyo')] \n', response
_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 100, 'total_tokens': 139}, 'model_name': 'gemma2', 
'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-702af47e-486c-400e-b0fb-45fde7aa0
728-0'), ToolMessage(content='search('current date')', name='tavily_search_results_json', tool_call_id='manual_tool_call
'), ToolMessage(content='[{\'url\': \'https://www.w3schools.com/SQl/func_mysql_current_date.asp\', \'content\': \'Tutori
als\\nHTML and CSS\\nData Analytics\\nWeb Building\\nJavaScript\\nWeb Building\\nBackend\\nData Analytics\\nWeb Building
\\nExercises\\nHTML and CSS\\nData Analytics\\nJavaScript\\nBackend\\nData Analytics\\nCertificates\\nHTML and CSS\\nDat
a Analytics\\nPrograms\\nJavaScript\\nPrograms\\nPrograms\\nBackend\\nData Analytics\\nAll Our Services\\nW3Schools offe
rs a wide range of services and products for beginners and professionals,\\nhelping millions of people everyday to learn
 and master new skills.\\n Enjoy our free tutorials like millions of other internet users since 1999\\nExplore our selec
tion of references covering all popular coding languages\\nCreate your own website with\\nW3Schools Spaces\\n- no setup 
required\\nTest your skills with different exercises\\nTest yourself with multiple choice questions\\nDocument your know
ledge\\nCreate a\\nfree\\nW3Schools Account to Improve Your Learning Experience\\nTrack your learning progress at W3Scho
ols and collect rewards\\nBecome a PRO user and unlock powerful features (ad-free, hosting, videos,..)\\n Help the lynx 
collect pine cones\\nGet personalized learning journey based on your current skills and goals\\nJoin our newsletter and 
get access to exclusive content every month\\nSQL Tutorial\\nSQL Database\\nSQL References\\nSQL Examples\\nMySQL CURREN
T_DATE() Function\\nExample\\nReturn the current date:\\nDefinition and Usage\\nThe CURRENT_DATE() function returns the 
current date.\\n Large collection of code snippets for HTML, CSS and JavaScript\\nBuild fast and responsive sites using 
our free\\nW3.CSS\\nframework\\nRead long term trends of browser usage\\nTest your typing speed\\nLearn Amazon Web Servi
ces\\nUse our color picker to find different RGB, HEX and HSL colors.\\n Syntax\\nTechnical Details\\nMore Examples\\nEx
ample\\nReturn the current date + 1:\\nReport Error\\nIf you want to report an error, or if you want to make a suggestio
n, do not hesitate to send us an e-mail:\\nhelp@w3schools.com\'}, {\'url\': \'https://www.sqltutorial.org/sql-date-funct
ions/sql-current-date/\', \'content\': 'The CURRENT_DATE is SQL-standard date function supported by almost all database 
systems such as Firebird, DB2, MySQL 5.x+, MonetDB, Oracle 11.x+, PostgreSQL, and SQLite. Note that Oracle\'s CURRENT_DA
TE returns both date and time values, therefore, to get the date data, you use the TRUNC function to truncate the time p
art:'}]', name='tavily_search_results_json', tool_call_id='manual_tool_call')]}


```
---

     
 
all -  [ Concurrent/parallel requests with vLLM ](https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/) , 2024-07-06-0910
```
My question might be a bit basic, but Iâ€™m new to all of this and eager to learn.

I have a basic setup where I initializ
e an LLM using vLLM with Langchain RAG and the Llama model (specifically, llama2-13b-chat-hf). Hereâ€™s what I do:

* I de
fine a system prompt and an instruction f
* I create anÂ `llm_chain`
* I then run the chain withÂ `llm_chain.run(text)`Â , 
which works for a single input.

I have build an app with FastAPI. Previously I used asyncio method to handle multiple r
equest to llm, but with each new request it become slower in response. So I decide to use vLLM method, but I got a probl
em now how to provide parallel or concurrent requests to vLLM when I have dealing with dozen or more users. Is there a w
ay to callÂ `run`Â in parallel for several inputs and receive valid results for each input?
```
---

     
 
all -  [ What is the best approach to achieve a better performant RAG? ](https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/) , 2024-07-06-0910
```
Hi!

I'm working on a RAG system for my company where we can use it to search through our internal wiki page.  
My syste
m is nearly in a releasable state and finds the correct information 90% of the times, and I'm happy about it, but I'm co
nstantly thinking, can I make it better?

I've made a custom scraper for our wiki, we're using an older version of Media
Wiki.  
The scraper I've made is basically extracting all sections out into its own 'document' and then sending it into 
qdrant vector database.  
That means that in the vector database, it doesn't have a full wiki page but rather a cut up v
ersion to make it easier for the search query to hit something right. But I feel like this is kinda wrong?

Whenever you
 send in your query to the backend, it'll then search for the 10 documents matching and then reranking with BAAI/bge-rer
anker-large. Then the context is being sent to Llama3:8b with your question in mind.  
This means that Llama3 will never
 get a fully contextual article, since the vectors are only smaller sections from the full page.

What could be done do 
make this better in the end? The one thing I see as an issue here, is that it will never know anything about the rest of
 the full page, but if it has the full page, it feels like Llama3 get overwhelmed by the data and then craps out.

We ha
ve  \~258 articles and that's resulting in about 1488 points in qdrant.
```
---

     
 
all -  [ Young Doctor looking for a mentor/Unpaid remote opportunities to gain experience in private equity ](https://www.reddit.com/r/private_equity/comments/1dvmcbo/young_doctor_looking_for_a_mentorunpaid_remote/) , 2024-07-06-0910
```
Hey everyone,

I'm an MBBS medical doctor in the UK interested in transitioning into private equity down the line. I'm f
lexible with moving countries in the future.

Currently I'm completing my training, however, have dedicated 20 hours a w
eek to exploring this space.  
Relevant knowledge: 

- Cert: Advanced Valuation and Strategy - M&A, Private Equity, and 
VC.

---> easy to learn: Did Mathematics Extension 2 in schooling (Stats, Vectors, Matrices, Diff + Integration, Perms &
 Combs etc.) However need more weekly practice with DCF to solidify the knowledge to make it second nature.

- Good skil
ls with MS Office \~ Excel scripts needs solidification.

- Know Langchain, RAG, Python and familiar with open source LL
M works.

Familiar with crunchbase but don't have enough capital to fund it for longer unless justified.

Does anyone kn
ow where I can find remote, part time experiences or even mentors in private equity?

Really keen on learning and will w
ork for it.
```
---

     
 
all -  [ Beginner here: found something confusing ](https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/) , 2024-07-06-0910
```
I've been playing around with GPT4All and langchain, for which there is a minimal demo here:

https://python.langchain.c
om/v0.2/docs/integrations/llms/gpt4all/

In this demo, they invoke the following:

```from langchain_core.callbacks impo
rt StreamingStdOutCallbackHandler```

From the API, it states that this only works with LLMs that support streaming. Acc
ording to the integrations page:

https://python.langchain.com/v0.2/docs/integrations/llms/

gpt4all does NOT support st
reaming. So I'm confused - what gives with this demo?
```
---

     
 
all -  [ Hybrid search with Postgres ](https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/) , 2024-07-06-0910
```
I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.

Anyone using 
Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?

Would love to hear your e
xperiences.
```
---

     
 
all -  [ Need honest feedback on resume (Final year B.Tech student) ](https://www.reddit.com/r/developersIndia/comments/1dvct1d/need_honest_feedback_on_resume_final_year_btech/) , 2024-07-06-0910
```
Hello! I'm a final year CS student from a tier-3 college, and am going to start applying to companies soon (both on and 
off campus). The hiring scene seems to be really terrifying right now, so I want to make sure I put my best foot forward
. I'm looking for brutal feedback on my resume, and how to stand out as an applicant. Thank you!

https://preview.redd.i
t/8w24zb48hjad1.jpg?width=2550&format=pjpg&auto=webp&s=8ad25c3a55ea1db97cdddb41c0b9e22eec842c6f
```
---

     
 
all -  [ Tool for Comparing Outputs of Multiple LLMs from Single Prompts ](https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/) , 2024-07-06-0910
```
I'm searching for a tool that allows users to compare outputs generated by several LLMs using just one prompt. While I u
nderstand that LangChain could potentially enable building such a solution locally, I'm curious if any existing products
 offer this functionality.

I'm weary of manually inputting the same prompt across different models like GPT, Claude, Ba
rd, and Perplexity to cross-reference answers and verify accuracy. Any recommendations or insights would be greatly appr
eciated!     
```
---

     
 
all -  [ Issue with Ollama and Pydantic  ](https://www.reddit.com/r/ollama/comments/1dv947k/issue_with_ollama_and_pydantic/) , 2024-07-06-0910
```
Hi guys,

Are you also facing issues when using Ollama with Pydantic? It seems that the response from the LLM is often n
ot put in the output format as requested by Pydantic. Hence, the subsequent code in our program throws an error.

In our
 tests, we used Codestral-22B and the following function to call the LLM inference. We also tested the same with the API
 from Mistral, and this works. **So it seems there is a problem in the interface between Ollama and Pydantic.**



Do yo
u have good experience with Ollama and Pydantic? Do you know how we can solve this?

Many thanks for your help!

Pseudo 
code (not functional, just to show which functions we call)

    from langchain_experimental.llms.ollama_functions impor
t OllamaFunctions
    
    llm = OllamaFunctions(base_url=Config.HOST_LLM, temperature=0, model='codestral:latest', form
at='json')
    
    llm = llm.with_structured_output(MyPydanticOutputClass)
    
    llm.invoke({...}) # pass data requi
red by the LLM to perform inference 

---- 

We are using Pydantic through the call **with\_structured\_output:**

**llm
.with\_structured\_output(MyPydanticOutputClass)**

So, sometimes there are some fields of the Pydantic object that are 
not parsed, so the output of the LLM is incomplete, for example:

***Got: 3 validation errors for CodeGenerationOutput**
*

***dev\_mode***

***field required (type=value\_error.missing)***

***rtos***

***field required (type=value\_error.m
issing)***

***is\_func\_empty***

And sometimes producing a more general error:

***raise ValueError(***  
***ValueErro
r: Failed to parse a response from codestral-latest output: {}***

---
```
---

     
 
all -  [ Design decision for an LLM app ](https://www.reddit.com/r/StackoverReddit/comments/1dv936n/design_decision_for_an_llm_app/) , 2024-07-06-0910
```


Langchain async for document loader and character splitter

Should I use Async or not? 

I am building a Fast api RAG 
app that uses LLMs to generate responses using langchain but I am confused if it needs async. 

The user flow goes somet
hing like this:
1. User provides a link/ links to a text/pdf document in the form of a url.  
2. Langchain document load
ers are used to load text or pdf from the remote public url.
3. Character splitter is used to split and chunk the docume
nts which is saved into a vector db. 
4. Langchain chain library is used to invoke LLMs via the asynchronous ainvoke(). 


My question is whether the document loading step via langchain and character splitting text via langchain  would need 
to be made async? Langchain doesnâ€™t support async for the libraries I am using. Would I need to implement them myself? I
f so, what are some options to implement? 
```
---

     
 
all -  [ Passing Chat History to Langchain Tool Calling Agent ](https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/) , 2024-07-06-0910
```
I'm working with Langchain to build a tool that calls an agent. Currently, I'm passing the chat history as an input vari
able to the agent. However, I've encountered an issue where the agent doesn't always seem to utilize the history data to
 answer questions consistently. This is especially problematic when users have queries spaced out over 10â€“15 days.

Is t
here a more efficient way to ensure the agent consistently remembers all chat history and context over multiple sessions
? What approach or best practices should I follow to address this issue?

Thanks in advance for your guidance!
```
---

     
 
all -  [ Load LLM (Mixtral 8x22B) from Azure AI endpoint as Langchain Model ](https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/) , 2024-07-06-0910
```
Hi,

  
I set up Mixtral 8x22B on Azure AI/Machine Learning and now want to use it with Langchain. I have difficulties w
ith the format I am getting, e.g. a ChatOpenAI response looks like this:

    from langchain_openai import ChatOpenAI
  
  llmm = ChatOpenAI()
    llmm.invoke('Hallo')

  
`AIMessage(content='Hallo! Wie kann ich Ihnen helfen?', response_meta
data={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-3.5-turbo', 's
ystem_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='r')`

This is how it looks when I am loading M
ixtral 8x22B with AzureMLChatOnlineEndpoint:

    from langchain_community.chat_models.azureml_endpoint import AzureMLCh
atOnlineEndpoint
    
    from langchain_community.chat_models.azureml_endpoint import (
        AzureMLEndpointApiType,

        CustomOpenAIChatContentFormatter,
    )
    from langchain_core.messages import HumanMessage
    
    chat = Az
ureMLChatOnlineEndpoint(
        endpoint_url='...',
        endpoint_api_type=AzureMLEndpointApiType.dedicated,
       
 endpoint_api_key='...',
        content_formatter=CustomOpenAIChatContentFormatter(),
    )
    
    chat.invoke('Hallo
')

  
`BaseMessage(content='Hallo, ich bin ein deutscher Sprachassistent. Was kann ich fÃ¼r', type='assistant', id='run-
23')`



So with the Mixtral model the output seems to be truncated and also the format is different (BaseMessage vs. AI
Message). How can I change this to make it work just like an ChatOpenAI model?

  
In my application I want to easily sw
itch between these two models.

  
Thanks in advance!
```
---

     
 
all -  [ How do I add meta data to Pinecone documents, I want to maintain overlap between chunks so I donâ€™t w ](https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/) , 2024-07-06-0910
```
Iâ€™m developing an app that creates a knowledge base based on transcripts of YouTube videos. And I need a way to have the
 LLM recognize where the transcript came from, I have the data I just donâ€™t know how to implement it effectively 
```
---

     
 
all -  [ History Aware Agent in Langgraph ](https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/) , 2024-07-06-0910
```
Hi,

I built a CRAG application and now want to further improve it. As a fist step I would like to check if the given qu
estion is a followup question of a previous question or not. If it is, then I want to use my messages history to create 
a new question based on the context in the chat history and the actual question. This is similar to the 'history\_aware\
_retriever' from Langchain.

However I am not satisfied with the classification of my model, as it often returns 'False'
 even if it is a followup question. So is there maybe a more elegant way of doing this or would you improve my prompt?


Heres the function within my Langgraph app:

    class CheckFollowupQuestion(BaseModel):
        '''Bool Werte um zu bes
timmen, ob die Frage auf eine zuvor gestellte Frage aufbaut.'''
    
        score: str = Field(
            description
='Die Frage bezieht sich auf eine vorherige Antwort oder zuvor gestellte Frage, 'True' oder 'False''
        )

    def 
followup_question_classifier(state: AgentState):
        messages = state['messages'][-5:]
        print(f'MESSAGES (in 
follow up): {messages}')
        question = state['question']
        system = '''<s>[INST] You assess whether the user'
s question is a follow-up question or not. For this, you get questions from the chat history and assess whether the ques
tion builds on a question or answer from the chat history or not.\n
            Evaluate with 'True' if it is a typical 
follow-up question. Also evaluate with 'True' if it seems that the question refers to a previous answer. \n
            
Evaluate with 'False' if it is a normal question, or if the question has nothing to do with the chat history. Here is an
 example:\n\n
            
            Example of a 'False' evaluation:\n
            Question: 'How much does a kebab c
urrently cost?'\n
            Chat history: [HumanMessage(content='Hello, I am Max, who are you?', id='8'), HumanMessage
(content='What was my name?', id='7'), HumanMessage(content='Name exactly one advantage of Multicloud.', id='f')]\n
    
        Your evaluation: 'False'. Reason: The questions from the chat history have nothing to do with the question asked
.\n\n
            
            Example of a 'True' evaluation:
            Question: 'And how warm will it be there tomo
rrow?'
            Chat history: [HumanMessage(content='Where is Munich located?', id='8'), HumanMessage(content='Which 
dialect is spoken in Munich?', id='7')]
            Your evaluation: 'True'. Reason: The chat history is about the city 
of Munich. In the follow-up question, the user wants to know what the weather will be 'there' tomorrow. Since the previo
us discussion was about Munich, 'there' refers to the city of Munich.
            [/INST]'''
    
        grade_prompt =
 ChatPromptTemplate.from_messages(
            [
                ('system', system),
                (
                 
   'human',
                    'User's question: {question} \n\n Chat history: {chat_history}',
                ),
    
        ]
        )
        llm = ChatOpenAI()
        structured_llm = llm.with_structured_output(GradeQuestion)
      
  grader_llm = grade_prompt | structured_llm
        result = grader_llm.invoke({'question': question, 'chat_history': m
essages})
        state['is_followup_question'] = result.score
        return state
```
---

     
 
all -  [ Pedantic data parsing ](https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/) , 2024-07-06-0910
```
I was not able to find anything on the web, the cases for which the pedantic parsers fail to parse the data coming from 
LLMs. I tried looking under the hood working and say that they are using json parsing, if anyone has info about this ple
ase enlighten me.
```
---

     
 
all -  [ How to increase the inference speed of Map reduce chain in langchain ](https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/) , 2024-07-06-0910
```
I have a problem. It takes literally 30 mins for my map-reduce summarisation chain to produce it's final output. Current
 vRAM is 16GB. What should I do to increase the speed?

Model: Llama2
```
---

     
 
all -  [ Ai Ml. Engg ](https://www.reddit.com/r/ranchi/comments/1dv0nub/ai_ml_engg/) , 2024-07-06-0910
```
Hello everyone. I hope you all are doing good. Any engineer doing Ai ML work here from Ranchi? 
Just wanted to know the 
level of mathematics required. 
Lot of talented Indians, but I feel like we are lagging way behind China, in terms of in
novation and research. I really hope I am wrong here, and my thinking is wrong. Recently came across Langchain and curre
ntly exploring it. Have a great day ahead everyone. Cheers. 
```
---

     
 
all -  [ Is it possible to stream function calling with Gemini? ](https://www.reddit.com/r/GoogleGeminiAI/comments/1dv0kut/is_it_possible_to_stream_function_calling_with/) , 2024-07-06-0910
```
Function calling is supported by Gemini models, but my initial tests with the LangChain implementation seem like I can't
 stream the function call output. I don't actually want to make an API call or something, I just want to get a structure
d output from the language model, and I would like to stream this to my frontend. Does anyone have some experience with 
this? Is it maybe possible to stream function calling output via Google's own implementation, without using LangChain? I
f it is not currently possible, are there any news or rumors about it?
```
---

     
 
all -  [ Frontend Resume Help! ](https://i.redd.it/di5xwxpjvfad1.jpeg) , 2024-07-06-0910
```
Hi Everyone! I'm looking for reviews on my resume. I have been a frontend developer for around 3 years and I'm looking f
or work. Been getting some hits (but not too great), I'm trying to see if there's anything that can be improved to get m
ore callbacks.

Would love to get some feedback. 
Thanks!

```
---

     
 
all -  [ How to incorporate a knowledge graph in RAG ](https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/) , 2024-07-06-0910
```

Hi there,

I am currently working on building a chatbot for internal use in my company. I have developed a relatively c
lassic RAG framework and now want to include a knowledge graph. I have read several papers on this topic, but I am confu
sed about how to actually 'activate' a graph in the RAG flow.

So far, I have found different approaches based on extrac
ting entities and relationships from chunks to generate community summaries of the related entities. This process occurs
 in the offline stage. At least, that is what I have tried to do. I am wondering how to activate this correctly. Current
ly, I match entities from the input with summaries as the flow runs, but I have the impression that others use the graph
 aspect differently, possibly using a function to inject relevant context into the LLM.

Can you help me understand this
 better?
```
---

     
 
all -  [ Ollama Function Calling Example ](https://www.reddit.com/r/ollama/comments/1duxz59/ollama_function_calling_example/) , 2024-07-06-0910
```
Just putting this out there for anyone interested in setting  up simple function calls using Ollama.

[https://github.co
m/BassAzayda/ollama-function-calling](https://github.com/BassAzayda/ollama-function-calling)



  
EDIT:  
This is using
 Langchain Experimental library, also recently discovered another nice light library called 'llm-axe'  
[https://github.
com/emirsahin1/llm-axe](https://github.com/emirsahin1/llm-axe)  
Might want to give this a a try too!
```
---

     
 
all -  [ LangFlow/LangChain to production ](https://www.reddit.com/r/LangChain/comments/1duug75/langflowlangchain_to_production/) , 2024-07-06-0910
```
Hello,

I'veen using LangFlow/LangChain to test a few concepts to create my RAG and works flawless.

Now my question may
be too easy or too complex, how to deliver to production?  
I saw the 'code snippets' for each component, but i can't fi
gure out to deliver directly to production without the GUI interface of LangFlow.

Here is a draft from my project:

htt
ps://preview.redd.it/iukb1l8fjead1.png?width=2532&format=png&auto=webp&s=1356a500acda75770e04aec93a89a5c2d4acdd1c

  



Any help will be really appreciated.

Thx
```
---

     
 
all -  [ How to build a reusable chain that takes the output of another chain as a keyword argument for the t ](https://www.reddit.com/r/LangChain/comments/1dursrw/how_to_build_a_reusable_chain_that_takes_the/) , 2024-07-06-0910
```
I'm trying to build reusable chains and I'm having issues trying to set it up so it takes the output of the previous cha
in as one of the template format arguments.

In this example, I have a chain that generates a title based on a topic and
 a chain that translates the title into a different language. The topic and the language I want to provide them when I i
nvoke the final chain.

If my 2nd chain only has one single input for the template (because I hardcode the language), ev
erything works perfectly:

    # Title Chain
    title_template = PromptTemplate.from_template('Generate a title for a b
log post about the following topic: {topic}')
    generate_title = title_template | llm | StrOutputParser()
    
    # T
ranslate Chain
    translate_template = PromptTemplate.from_template('Translate the following text to spanish: {text}')

    translate_chain = translate_template | llm | StrOutputParser()
    
    # Combine and run
    combined_chain = gener
ate_title | translate_chain
    result = combined_chain.invoke({'topic': 'The benefits of exercise'})

But if I want to 
be able to provide the language when I invoke the final chain then, I dont know how to pass the output to the template d
ictionary. See the 'HOW\_TO\_PASS\_OUTPUT\_OF\_PREVIOUS\_CHAIN' in the code

    # Title Chain
    title_template = Prom
ptTemplate.from_template('Generate a title for a blog post about the following topic: {topic}')
    generate_title = tit
le_template | llm | StrOutputParser()
    
    # Translate Chain
    translate_template = PromptTemplate.from_template('
Translate the following text to {language}: {text}')
    translate_chain = {'language': itemgetter('language'), 'text': 
HOW_TO_PASS_OUTPUT_OF_PREVIOUS_CHAIN?} | translate_template | llm | StrOutputParser()
    
    # Combine and run
    com
bined_chain = generate_title | translate_chain
    result = combined_chain.invoke({'topic': 'The benefits of exercise', 
'language': 'Spanish'})

  
I also tried to use a lambda function but the itemgetter doesn't seem to work:

    translat
e_chain = (
        lambda text: {'language': itemgetter('language'), 'text': text}
        | translate_template
       
 | llm
        | StrOutputParser()
    )

  
If anyone knows the answer I would be much appreciated.
```
---

     
 
all -  [ Read and write to Google sheets ](https://www.reddit.com/r/LangChain/comments/1dursic/read_and_write_to_google_sheets/) , 2024-07-06-0910
```
Iâ€™m not a programmer, only got basic understanding of the concepts. Iâ€™m wanting to do the following (Iâ€™ll probably get s
omeone from Upwork to do it) read and write to the company daily operations Google sheet using natural language. Users c
an type requests and the agent will write and read the document. Iâ€™m thinking Iâ€™ll have python scripts to do actions lik
e insert new work orders with all formulas in place.

My concern is, what happens if someone edits the GS manually. The 
agent will not know exactly which row a certain data is if I want to update it. Or can the agent read first to ensure th
e data is in the target position and then update it? 
```
---

     
 
all -  [ Retrieve a single document ](https://www.reddit.com/r/LangChain/comments/1duoub8/retrieve_a_single_document/) , 2024-07-06-0910
```
I've created a RAG with document retriever and everything works fine. 

I want to add a feature: the user can specify a 
document to apply search to. 
The question the user ask is something related to the single document, OR something like '
resume it', 'translate it', whatever. 

What I though was use parent document retriever and inject the whole document (d
on't think about tokens limit at the moment) in the context. Is theoretically possible but my question is: how can I det
ect a 'manipulation' question (resume, translate, etc)?

In these cases I want to use parent document (or whatever strat
egy), and in other cases the default strategy. 

Should I put some ML model between question and retriever? I want to av
oid using AI tools to not consume another open Ai call to detect what retriever I should use. 

Thanks 
```
---

     
 
all -  [ Best way to summarize a book? ](https://www.reddit.com/r/LocalLLaMA/comments/1dulrgg/best_way_to_summarize_a_book/) , 2024-07-06-0910
```
So, I have a collection of ebooks on my PC, and I want to summarize them.

I see there are many methods for doing this, 
but what was the best for you? I thought of summarizing chunks of it and adding them up in a final summary, but this can
 get buggy with LLM's due to the lack of context, and passing them through Claude or Gemini 2 is too expensive.

Word cl
ouds, vector databases, reducemap like in langchain, etc., etc.

Any other ideas of how to approach this?

My goal is to
 create a summary of each book then vectorize that and create a semantic search engine for my books. Maybe I could make 
an online version if it's good enough.

Thanks.

--

EDIT: Using gemini-1.5-flash seems good enough, however using pro, 
chat-gpt or claude yields even better results, however not scalable too much. Will chet other free alternatives.

Thanks
 a lot for your help.

What I'm doing is summarizing the book in chapters, sending them to 1.5-flash which is free. Then
 for the next chapter sending the previous summary + the chapter, and so on. This yields a consistent result with good m
odels, but not perfect yet.
```
---

     
 
all -  [ Unable to use Langchain with LiteLLM proxy ](https://www.reddit.com/r/LangChain/comments/1dukp60/unable_to_use_langchain_with_litellm_proxy/) , 2024-07-06-0910
```
The LiteLLMProxy needs a special authentication header (see [LiteLLM docs](https://litellm.vercel.app/docs/proxy/user_ke
ys)) which needs to go into a special HTTP header field:

    curl --location 'http://litellm/chat/completions' \
      
     -H 'Content-Type: application/json' \ 
           -H 'Authorization: Bearer <redacted>' ...

 As far as I can tell,
 it's impossible to get `langchain ` 
 to stuff this authorization header into its requests, meaning that `langchain` 
 
cannot be used with `LiteLLM` proxy atm.

My co-workers and I looked deep into the code to try to find a solution, but t
here are so many layers from a model's `invoke` method to anything that makes an HTTP request that we were defeated. Or 
perhaps I'm just missing something?
```
---

     
 
MachineLearning -  [ [P] Seeking Feedback on My GenAI Job Fit Project - New to LangChain/LangGraph ](https://www.reddit.com/r/MachineLearning/comments/1dgns9p/p_seeking_feedback_on_my_genai_job_fit_project/) , 2024-07-06-0910
```
Hi all,

Soo, i have been working on a a projectcalled [GenAI Job Fit](https://github.com/DAVEinside/GenAI_Job_Fit). It'
s an AI-driven system designed to enhance job applications by providing tailored recommendations based on individual pro
files.

I'm relatively new to LangChain and LangGraph, and I've incorporated them into this project. I would greatly app
reciate it if you could check out the repository and provide any feedback or suggestions for improvement.

Your insights
 on how I can better implement LangChain/LangGraph, or any other aspect of the project, would be incredibly valuable. I'
m eager to learn and make this project as robust as possible.

Thank you in advance for your time and feedback!

Repo Li
nk : [https://github.com/DAVEinside/GenAI\_Job\_Fit](https://github.com/DAVEinside/GenAI_Job_Fit)
```
---

     
 
MachineLearning -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-07-06-0910
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
deeplearning -  [ Llama 3 not running on GPU ](https://www.reddit.com/r/deeplearning/comments/1dptxsr/llama_3_not_running_on_gpu/) , 2024-07-06-0910
```
I dont know much theory about RAG but i need to implement it for a project.  
**I want to run llama3 on my GPU to get fa
ster results.**

`from langchain_community.llms import Ollama`  
`llm = Ollama(model='llama3',num_gpu=1)`  
`def generat
e_response(prompt, similar_jobs):`  
`descriptions = '\n\n'.join([job['Description'] for job in similar_jobs])`  
`augme
nted_prompt = f'{prompt}\n\nHere are some job recommendations based on your query:\n{descriptions}'`  
`for chunks in ll
m.stream(augmented_prompt):`  
`print(chunks, end='')`

I am giving llama3 my *'user prompt'* and top 5 nearest *'simila
r\_jobs'* using cosine similarity.  
This code goes not use my GPU but my CPU and RAM usage is high.

**My gpu usage is 
0%** , i have a Nvidia GeForce RTX 3050 Laptop GPU GDDR6 @ 4GB (128 bits)
```
---

     
 
deeplearning -  [ What is ReAct Prompting? the most important piece in agentic frameworks ](https://www.reddit.com/gallery/1djk4nk) , 2024-07-06-0910
```
â€œWhat is ReAct Prompting? the most important piece in agentic frameworksâ€ - A quick read from Mastering LLM (Large Langu
age Model) 'Coffee Break Concepts' Vol.6

This document deeps dive into the ReAct Prompting method and why it's importan
t:
1. Limitations of LLM
2. Why ReAct prompting matters?
3. How ReAct Works?
4. LangChain Implementation
5. Why Prompt w
ithin agentic frameworks Matters?

Comment below on which topic you want to understand next in this 'Coffee Break Concep
ts' series and we will include those topics in upcoming weeks.
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-07-06-0910
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
