 
all -  [ How to securely pass private data? ](https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/) , 2024-06-15-0953
```
Wondering if anyone here has dealt with passing private information from end user inputs to your LLM, later to interact 
with an external API? I'm not talking about authentication data per se, just private information (e.g PII) people wouldn
't normally want to share on the internet.   
What solution have you come up with to ensure some privacy for your users?

```
---

     
 
all -  [ Evaluating with Ragas ](https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/) , 2024-06-15-0953
```
I've finished my rag job, and performed a evaluation on my rag. results given below

[ragas output](https://preview.redd
.it/pt0khqy10l6d1.png?width=1280&format=png&auto=webp&s=4979a08f0e648937407d23feeb494f02a8e793ba)

context\_precision is
 better than good but why the other metrics sucks and how to improve them?
```
---

     
 
all -  [ RAG Me Up - RAG for chat /w Langchain ](https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/) , 2024-06-15-0953
```
We are in early stages of developing our project so keen feedback. RAG Me Up is a robust layer on top of Langchain desig
ned to make RAG easy and also not prone to simple issues like document re-retrieval, performance for rephrasind and perh
aps most importantly: make Langchain work well with Instruct/Chat models' templates.

[https://github.com/AI-Commandos/R
AGMeUp ](https://github.com/AI-Commandos/RAGMeUp)
```
---

     
 
all -  [ AI Innovations: Streamlining Insurance Claims, Enhancing Engineering Projects, and Revolutionizing P ](https://www.reddit.com/r/ai_news_by_ai/comments/1dfvkcx/ai_innovations_streamlining_insurance_claims/) , 2024-06-15-0953
```





#vc #tool #release #hardware #event #feature #opinions #opensource #update #major_players #science #leaders #startu
ps #scheduled

Y Combinator's S24 startup, ClaimSorted, is leveraging AI technology to streamline insurance claims proce
ssing, aiming to enhance customer experience and drive cost efficiencies [1]. Another startup from the same batch, Entan
gl_AI, has launched a platform that automates error detection in engineering projects, potentially saving time, money, a
nd lives by identifying and fixing design issues early [2].







The Data AI Summit will feature a fireside chat with 
NVIDIA AI Developer and Databricks Co-Founder and CEO, along with several informative sessions [3]. These include a sess
ion on the rapid development and deployment of Generative AI apps at scale with NVIDIA NIM [4], a session on Architectur
e Analysis for ETL Processing: CPU vs GPU [5], and a session on unlocking the power of Spark RAPIDS ML for GPU-accelerat
ed distributed machine learning [6]. 







NVIDIA AI Developer has also discussed the benefits of TensorRT Weight-Stri
pped Engines, which offer over 95% compression, support various models, and allow for refitting with weights directly on
 end-user devices [7]. The use of generative AI in CVE analysis can significantly enhance security workflows, with Agent
 Morpheus, a generative AI application, automating the process of detecting and remediating CVEs [8].







The era of 
the AI PC has arrived, offering unparalleled performance in generative tasks with NVIDIA RTX and GeForce RTX technologie
s [9]. Andrew Ng has developed an agentic machine translation demonstration that prompts a language model to translate t
ext, reflects on the translation to provide suggestions, and refines the translation based on those suggestions [10].








Groq Inc announced their GM of GroqCloud, Sundeep, and is hosting a follow-up AMA session to answer more questions 
about scaling for the fastest AI inference [12][13]. The company also powered the Build Together AI Hackathon, where 28 
incredible apps were created [14]. A webinar titled 'Build Blazing-Fast LLM Apps with Groq, LangFlow, & LangChain' is sc
heduled to take place, showcasing how Groq's AI inference technology integrates with LangFlow and LangChain to create po
werful applications using Large Language Models (LLMs) [15].







a16z emphasizes the importance of rethinking product
s from the ground up, rather than just adding AI, and discusses the evolution from AI-augmented to AI-native products on
 their podcast [16]. Vivek Natarajan, a Research Lead at Google Health AI, is focusing on AI and biomedicine with Projec
t AMIE [18]. OpenAI clarifies that their strategic cloud relationship with Microsoft remains unchanged, and they have a 
partnership with OCI to use the Azure AI platform on OCI infrastructure for inference and other needs [20].







Midjo
urney has announced several updates, including the release of model personalization, the development of a new set of alg
orithms, the alpha testing phase for a new feature, and the ability of their AI to figure things out from other people's
 images [21][22][23][24][25]. Stability AI has announced the release of Stable Diffusion 3 Medium, a text-to-image AI mo
del with two billion parameters, optimized for consumer PCs and laptops, as well as enterprise-tier GPUs [26].




[1. Y
 Combinator @ycombinator https://twitter.com/ycombinator/status/1800558618497962219](https://twitter.com/ycombinator/sta
tus/1800558618497962219)

[2. Y Combinator @ycombinator https://twitter.com/ycombinator/status/1800573712862695857](http
s://twitter.com/ycombinator/status/1800573712862695857)

[3. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIA
AIDev/status/1800574724944048585](https://twitter.com/NVIDIAAIDev/status/1800574724944048585)

[4. NVIDIA AI Developer @
NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1800574733043323141](https://twitter.com/NVIDIAAIDev/status/180057473
3043323141)

[5. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1800574743550038505](https://tw
itter.com/NVIDIAAIDev/status/1800574743550038505)

[6. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/
status/1800574751183634542](https://twitter.com/NVIDIAAIDev/status/1800574751183634542)

[7. NVIDIA AI Developer @NVIDIA
AIDev https://twitter.com/NVIDIAAIDev/status/1800603157665354116](https://twitter.com/NVIDIAAIDev/status/180060315766535
4116)

[8. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1800619033487692240](https://twitter.
com/NVIDIAAIDev/status/1800619033487692240)

[9. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status
/1800913453483225108](https://twitter.com/NVIDIAAIDev/status/1800913453483225108)

[10. Andrew Ng @AndrewYNg https://twi
tter.com/AndrewYNg/status/1800582171259982289](https://twitter.com/AndrewYNg/status/1800582171259982289)

[11. Pika @pik
a_labs https://twitter.com/pika_labs/status/1800593550494863425](https://twitter.com/pika_labs/status/180059355049486342
5)

[12. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1800563455977750704](https://twitter.com/GroqInc/status/18
00563455977750704)

[13. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1800589921876312090](https://twitter.com/G
roqInc/status/1800589921876312090)

[14. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1800635116592480543](https
://twitter.com/GroqInc/status/1800635116592480543)

[15. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1800913489
399099728](https://twitter.com/GroqInc/status/1800913489399099728)

[16. a16z @a16z https://twitter.com/a16z/status/1800
617449382969440](https://twitter.com/a16z/status/1800617449382969440)

[17. a16z @a16z https://twitter.com/a16z/status/1
800623154257367123](https://twitter.com/a16z/status/1800623154257367123)

[18. Google AI @googleai https://twitter.com/g
oogleai/status/1800657713418109269](https://twitter.com/googleai/status/1800657713418109269)

[19. Yann LeCun @ylecun ht
tps://twitter.com/ylecun/status/1800677672512806971](https://twitter.com/ylecun/status/1800677672512806971)

[20. OpenAI
 @openai https://twitter.com/openai/status/1800778542512550260](https://twitter.com/openai/status/1800778542512550260)


[21. Midjourney @midjourney https://twitter.com/midjourney/status/1800693675686834450](https://twitter.com/midjourney/st
atus/1800693675686834450)

[22. Midjourney @midjourney https://twitter.com/midjourney/status/1800707282726174954](https:
//twitter.com/midjourney/status/1800707282726174954)

[23. Midjourney @midjourney https://twitter.com/midjourney/status/
1800819091302965729](https://twitter.com/midjourney/status/1800819091302965729)

[24. Midjourney @midjourney https://twi
tter.com/midjourney/status/1800819304402976971](https://twitter.com/midjourney/status/1800819304402976971)

[25. Midjour
ney @midjourney https://twitter.com/midjourney/status/1800819590005752126](https://twitter.com/midjourney/status/1800819
590005752126)

[26. Stability AI @stabilityai https://twitter.com/stabilityai/status/1800875914299048404](https://twitte
r.com/stabilityai/status/1800875914299048404)

[27. NVIDIA AI @NVIDIAAI https://twitter.com/NVIDIAAI/status/180090716508
4848531](https://twitter.com/NVIDIAAI/status/1800907165084848531)
```
---

     
 
all -  [ Streaming with agents ](https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/) , 2024-06-15-0953
```
I have implementing streaming with a chain based runnable which gives token by token output ( word by word), making UI s
imilar to how ChatGPT has its UI. But while implementing the same with an Agent based runnable I see that it gives 3 out
puts  in order, actions, steps and, output which contains answer. All three come as a whole, one after the other, not wo
rd by word.

I want to get word by word streaming for the agent's final answer.
```
---

     
 
all -  [ Newbie Seeking Advice on Side Project - Chat with Calendar  ](https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/) , 2024-06-15-0953
```
As the title reads, I'm building a side project to chat with my google calendar + assignments from Canvas (learning mana
gement system). I'm using GCP to practice working with the cloud. 

As of April 2024, Cloud SQL for MySQL now supports v
ector embeddings. Essentially, I have all of my coursework and assignments in an events table. At first I embedded at th
e row level but this lost the understanding of columns. Now, I have a new column that is JSON representation of all the 
relevant columns for my eventual retrieval (event\_title, start\_time, end\_time, tag (Assignment, Discussion, Quiz, Stu
dy Times, Personal Events)). In a new column, I've successfully embedded all of these JSON's. What I've described above 
is pretty much the extent of what I've done. 

My end goal is to develop a streamlit UI to query this vector column in m
y SQL database. I have a few different paths I can go down, but I'm intentionally keeping this at a high level to hear d
iverse responses. 

  
Any advice? All thoughts are greatly appreciated. 

  
Cheers
```
---

     
 
all -  [ Improved Text2SQL Dataset Now Available on Huggingface! ](https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/) , 2024-06-15-0953
```
I'm excited to share an updated open-source resource we’ve been working on—an improved version of the Spider dataset ori
ginally published by Yale University for Text2SQL tasks. You can check it out here: [https://huggingface.co/datasets/Raf
faSch121/fixed\_spider](https://huggingface.co/datasets/RaffaSch121/fixed_spider)

During our own model training at [Tur
bular](http://www.turbular.com) we identified several issues in the original dataset. To help the community and give bac
k, we decided to address these problems and release a corrected version. We hope this enhanced dataset will benefit ever
yone working on Text2SQL and similar projects.

Feel free to download, experiment, and contribute back if you find ways 
to make it even better!
```
---

     
 
all -  [ A tutorial on creating video from text using AI ](https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/) , 2024-06-15-0953
```
I have written an article on how to create a Text to Video AI generator which generates video from a topic by collecting
 relevant stock videos and stitching them together. 

The code is completely open-source and uses free to use tools to g
enerate videos

Link to article :- [https://medium.com/@anilmatcha/text-to-video-ai-how-to-create-videos-for-free-a-comp
lete-guide-a25c91de50b8](https://medium.com/@anilmatcha/text-to-video-ai-how-to-create-videos-for-free-a-complete-guide-
a25c91de50b8)
```
---

     
 
all -  [ Token count and cost for chain.astream_events(). ](https://www.reddit.com/r/LangChain/comments/1dfpjpr/token_count_and_cost_for_chainastream_events/) , 2024-06-15-0953
```
 Hey all, How do I get the token count for chain.astream_events()
```
---

     
 
all -  [ RAGchain searching for similar prompts ](https://www.reddit.com/r/LangChain/comments/1dfp7xf/ragchain_searching_for_similar_prompts/) , 2024-06-15-0953
```
Hi i am new to the framework of langchain and i want to search for some information in contract documents regarding tota
l m2 area for a partner. The problem is that the main partner contract can have several newer appendices where the old t
otal m2 area in the old original contract is now replaced. Now i only want to extract the new total m2 area. Is there a 
clever way to sort or filter this?
```
---

     
 
all -  [ [Project] Compare Top 10 LMSYS Models with a Universal LLM API Library ](https://www.reddit.com/r/LangChain/comments/1dfp2z5/project_compare_top_10_lmsys_models_with_a/) , 2024-06-15-0953
```
Hello Langchain community!

I'm excited to share a project we've been working on - an open-source 'AI Gateway' library t
hat allows you to access and compare 200+ language models from multiple providers using a simple, unified API.

To showc
ase the capabilities of this library, I've created a Google Colab notebook that demonstrates how you can easily compare 
the top 10 models from the LMSYS leaderboard with just a few lines of code.

Here's a snippet:

https://preview.redd.it/
lcqhryzx0j6d1.png?width=1822&format=png&auto=webp&s=cf7d055fa0e79117fed5dd8f8dc37498fe43b9e3

The library handles all th
e complexities of authenticating and communicating with different provider APIs behind the scenes, allowing you to focus
 on experimenting with and comparing the models themselves.

Some key features of the AI Gateway library:

* Unified API
 for accessing 200+ LLMs from OpenAI, Anthropic, Google, Ollama, Cohere, Together AI, and more
* Compatible with existin
g OpenAI client libraries for easy integration
* Routing capabilities like fallbacks, load balancing, retries

I believe
 this library could be incredibly useful for researchers and developers in the Langchain community who want to easily co
mpare and benchmark different LLMs, or build applications that leverage multiple models.

I've put the demo notebook lin
k below, I'd love to get your feedback, suggestions, and contributions:

[https://github.com/Portkey-AI/gateway/blob/mai
n/cookbook/use-cases/LMSYS%20Series/comparing-top10-LMSYS-models-with-Portkey.ipynb](https://github.com/Portkey-AI/gatew
ay/blob/main/cookbook/use-cases/LMSYS%20Series/comparing-top10-LMSYS-models-with-Portkey.ipynb)
```
---

     
 
all -  [ Please guide towards solving this problem ](https://www.reddit.com/r/generativeAI/comments/1dflxqo/please_guide_towards_solving_this_problem/) , 2024-06-15-0953
```
How to get started with the problem statement?

Hey guys, i am new to generative AI, it’s been a few days learning new t
hings. I have a problem statement in hand. We have to evaluate a startup idea. We already have an evaluation checklist t
hat has like 30 parameters on the basic of which we decide the feasibility of the idea. We have to build a model in whic
h we prompt an idea and the input idea goes through various agents who are (business analysts, cofounder, VC). So it fir
st goes to BA and then the result goes to cofounder and so on therefore getting perspective of all the agents. For start
ers i want to build the model with 3 agents. Once it passes through 3rd agent it gives the final result as an evaluation
 checklist (the same one i talked about above). 

Now my question is how should i approach this problem and what would b
e the underlying concept used for building such a model? 
Also from where can i start ? 
FYI - i read a bit about genert
ive ai topics like embedding, fine tuning and a bit of langchain (built a simple agent) etc. Still exploring agentic AI.
 

Thanks in advance !! 
```
---

     
 
all -  [ How would you schedule an agent for a task? ](https://www.reddit.com/r/AI_Agents/comments/1dfjh5q/how_would_you_schedule_an_agent_for_a_task/) , 2024-06-15-0953
```
I'd like to have my agent prepare my mornings with news and some interesting investment ideas. The agent exists already 
as ReAct on Langchain and works quite well, but I have to start him up and ask him to do his work. How would you schedul
e this?
```
---

     
 
all -  [ Building a Generative UI App With LangChain Python ](https://www.youtube.com/watch?v=d3uoLbfBPkw) , 2024-06-15-0953
```

```
---

     
 
all -  [ Run Evaluations with Langtrace ](https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/) , 2024-06-15-0953
```
Hi all,

Its been a while from me, but just wanted to share that we have added support for running automated evals with 
Langtrace. As a reminder, Langtrace is an open source LLM application observability and evaluations tool. It is open tel
emetry compatible so no vendor lock-in. You can also self-host and run Langtrace.

We integrated langtrace with inspect 
AI (https://github.com/UKGovernmentBEIS/inspect\_ai). Inspect is an open source evluations tool from the developers of R
Studio - you should definitely check it out. I love it.  
With langtrace, you can now

* set up tracing in 2 lines of co
de
* annotate and curate datasets
* run evaluations against this dataset using Inspect
* view results, compare the outpu
ts against models and understand the performance of your app

So, you can now establish this feedback loop with langtrac
e.

https://preview.redd.it/qrwn7r1kte6d1.png?width=2304&format=png&auto=webp&s=3c2d7c82abbb329518b35c133c0e7a0e73a6d53d


Shown below are some screenshots:

https://preview.redd.it/t45vq2xute6d1.png?width=3156&format=png&auto=webp&s=1c15fc7
1499ba5c5ccbf0aa566fc78c82730e209

https://preview.redd.it/0gwmyz0xte6d1.png?width=3150&format=png&auto=webp&s=2713ba619
e903d2db227d5922e8e9c7a562fb9b7

Would love get any feedback. Please do try it out and let me know.

Link: [https://gith
ub.com/Scale3-Labs/langtrace](https://github.com/Scale3-Labs/langtrace)
```
---

     
 
all -  [ Why should i use lang chain? ](https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/) , 2024-06-15-0953
```
Hello everyone,
I mean no disrespect to anyone but I am having trouble seeing the appeal of using the lang chain.
 In my
 opinion I'am at best a beginner there for my view coulde be too shalow.
 I am hoping to find an anweser to where my bli
nd spots  are and what use cases the lang chain is useful for.
For example, if I want to build a rag chatbot. I would us
e Ollama with Chromadb without any libery except for chromadb and requests.
I have to admit that it is nice to try diffe
rent things with lang chain. It is also easier to handle complex files like PDF. 

If some of you say I don't have enoug
h experience, that's why I don't get it, the answer is fair enough for me to take a agaib a look at Lang Chain.

But I h
ave already tried to work with the framework 3 times and it always seems too complex for what I want to achieve.
All tho
se time i build an Chatbot that allows to interact with an modell with some litte custmasation over envs. And the last t
ime was a Rag Chatbot that allows me to index Websites to get answers about their content.
```
---

     
 
all -  [ Data and AI Summit - Day 2 Announcements! ](https://www.reddit.com/r/databricks/comments/1df92yc/data_and_ai_summit_day_2_announcements/) , 2024-06-15-0953
```
🚀 Day 2 got off to an incredible start, some amazing announcements:

* Unity Catalog has officially been open-sourced, L
IVE on stage by the Databricks CTO, Matei Zaharia -> [HERE](https://www.databricks.com/blog/open-sourcing-unity-catalog)

* Introducing Databricks LakeFlow, a new solution that makes building production-grade data pipelines easy and efficien
t -> [HERE](https://www.databricks.com/blog/introducing-databricks-lakeflow)
* New Delta Sharing Features, Expansion of 
Partner Sharing Ecosystem, More Marketplace Data Providers and Growth, and Introducing Databricks Clean Rooms in Public 
Preview on AWS and Azure -> [HERE](https://www.databricks.com/blog/whats-new-data-sharing-and-collaboration)
* Unity Cat
alog new features: Governed business metrics, Attribute-based access controls, Lakehouse Federation GA, and more -> [HER
E](https://www.databricks.com/blog/whats-new-databricks-unity-catalog-data-ai-summit-2024)

We are looking forward to al
l of the amazing technical deep dive sessions at Summit!

https://preview.redd.it/t4rjcvm9he6d1.jpg?width=1200&format=pj
pg&auto=webp&s=6e43e9f6cbcf5a1bbb4f04543b2e63e955c38a5f


```
---

     
 
all -  [ suggest 5 follow-up question based on previous asked  ](https://www.reddit.com/r/LangChain/comments/1df5lc7/suggest_5_followup_question_based_on_previous/) , 2024-06-15-0953
```
Hello guys

i am creating chat bot with QA retrieval using vector DB and i want to add one more feature that is follow u
p question along with response to current question  
can anybody provide me example how implement it ?
```
---

     
 
all -  [ RAG performs differently in different environments  ](https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/) , 2024-06-15-0953
```
To set the context we have 4 environments predev, dev, testing and production. Our RAG uses langchain for PDF extraction
, qdrant(self hosted on kubernetes) for vectorstore and gpt-3.5-turbo-16k for the llm.

We have built a RAG, which worke
d well(gave correct answers from PDF) on predev. When we moved it to dev, in the initial days its performance(correctnes
s) was bad and eventually got good without any changes, except for minor document update. Then it moved to testing envir
onment where again the same behaviour. Now it's in prod and again behaves the same. Facing a lot of backlash from client
 due to this strange behaviour.

It's the same document, same gpt,  but different qdrant hosted different for different 
environments.

Did anyone experience similar issue? Can anyone explain why the warmup time.

Any help is greatly appreci
ated.
```
---

     
 
all -  [ Reference Platform Architecture ](https://www.reddit.com/r/platformengineering/comments/1df0j3u/reference_platform_architecture/) , 2024-06-15-0953
```
We are a startup studio, and naturally, we wanted to build repeatable processes to jumpstart new products. Our reference
 platform architecture is a battle hardened best practices and technology components which work together like a charm fo
r a majority of real world use cases. We have a lot of boilerplate which helps you start a new platform within days.  
 
 
We are now putting our platform architecture out there for everyone to see. I’d love to hear comments and suggestions.
 Details can be seen here: [https://venturenox.com/work/vrpa/](https://venturenox.com/work/vrpa/)

https://preview.redd.
it/2d619reioc6d1.png?width=1900&format=png&auto=webp&s=8b1db43302bde957141e666af964847f9c1859f1
```
---

     
 
all -  [ RAG Model TOO SLOW ](https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/) , 2024-06-15-0953
```
I am trying to build a model to take in 5-10 PDFs and answer questions based on them.

This is my flow ==> LlamaParse->O
penAI ada embeddings -> FAISS vector store -> multi query retriever -> cohere reranker -> OpenAI gpt4o -> results

I als
o have a part in my retriever stage where I get citations and chunking is done page wise

The questions I ask take anywh
ere between 25-50 seconds to get an answer and also I am missing out on information, I have made the retriever send back
 all relevant pages, not just the top 3 relevant pages

Is there anyway to get this under 20 seconds and extract all rel
evant chunks with keeping in mind I need citations?


```
---

     
 
all -  [ Seeking Recommendations: Tools for Chemists Using Large Language Models and Agents ](https://www.reddit.com/r/LangChain/comments/1dewg6w/seeking_recommendations_tools_for_chemists_using/) , 2024-06-15-0953
```
I'm looking for recommendations on tools for chemists that can be implemented using LLM and LangChain agents. What usefu
l tools or applications do you think can be created with these technologies? I would appreciate any ideas and suggestion
s.

  
Which LLMs do you recommend for laboratory automation solutions, and what data processing life cycles can be impl
emented by agents?

I'm particularly interested in how to work with the Canonical SMILES format using chatbots and modif
y it through agents.

I'm exploring this topic as a theoretical preparation for a long-term hackathon focused on the aut
omation of chemical laboratories. All solutions will be **published** and **open source** after our team’s presentation.

```
---

     
 
all -  [ Review my resume, not getting any calls ](https://www.reddit.com/r/resumes/comments/1dehi9a/review_my_resume_not_getting_any_calls/) , 2024-06-15-0953
```
https://preview.redd.it/kbr9xd1hg76d1.jpg?width=1080&format=pjpg&auto=webp&s=e32bbba25f324d01f797f323f50ca4e7479dc3b8

H
i, Please review my resume. I am not getting any calls. I have been applying for jobs since March 
```
---

     
 
all -  [ Need Help to make langchain chatbot ](https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/) , 2024-06-15-0953
```
I have to make llm chatbit using open ai on flask. 
Help me to make this. 
```
---

     
 
all -  [ [P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using a ](https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/) , 2024-06-15-0953
```
[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for buildin
g tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and t
akes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, to
ols and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building 
this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 
things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally vi
able version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, 
but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in prod
uction with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples 
work and are easy to be be adapted to your use case.
```
---

     
 
all -  [ Build a QA Bot for your documentation with Langchain ](https://www.reddit.com/r/developersIndia/comments/1dedwwg/build_a_qa_bot_for_your_documentation_with/) , 2024-06-15-0953
```
Build an AI-powered Q&A bot for your website documentation using Wing TypeScript, Next.js, and Langchain.  
  
-  Create
 a user-friendly Next.js app to accept questions and URLs  
-  Set up a Wing TypeScript backend to handle all the reques
ts  
-  Incorporate Langchain for AI-driven answers by scraping and analyzing documentation using RAG  
-  Complete conn
ection between frontend input and AI-processed responses.

Check out the full article [here](https://wingla.ng/qa-bot).
```
---

     
 
all -  [ AI / LLMs in your Obsidian - what's actually been useful for you? ](https://www.reddit.com/r/ObsidianMD/comments/1dedmeu/ai_llms_in_your_obsidian_whats_actually_been/) , 2024-06-15-0953
```
There's a bunch of new plug ins available and with projects like PrivateGPT or LangChain it's easy to start talking with
 your own data.

  
But...

  
**What has been the most useful way of using LLMs within your Vault?**

  
I've been work
ing on a flow that takes stuff that I read/ learned this week and summarizing it to form the draft I can use as a weekly
 newsletter for learning in public. 

It's not there yet, and I still edit/ write the content. But the initial draft and
 summarization of my highlights has been helpful. So I see:



Source 1

- takeaway

- takeaway

Source 2

..




```
---

     
 
all -  [ Roast my Resume ](https://www.reddit.com/r/resumes/comments/1deblue/roast_my_resume/) , 2024-06-15-0953
```
Been trying to get a Software Engineering Internship for a year now, but with over 200 applications submitted, my interv
iew rate was 1%. I am a Software Developer niching out into DevOps, AI, and AWS System Design infrastructure, because I 
feel everyone just knows Software Development nowadays. I really want to lock in a Software Engineering Internship by ne
xt summer so please give me all the criticism you got.

https://preview.redd.it/y9hx8cq7966d1.png?width=647&format=png&a
uto=webp&s=45fe513c46fdc8a6eca7bd2909a9e1ad0e0f4bd2


```
---

     
 
all -  [ Need Help Implementing Supervisor with LangGraph ](https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/) , 2024-06-15-0953
```
Hey everyone,

I'm working on a Supervisor with LangGraph for a company internship. My mentor has asked me to create thr
ee Agents: 'Question Agent', 'Answer Agent', and 'Summarizer Agent'. The input is a PDF, which I need to split by page a
nd add each page to a vectorial database for later use. Each agent will also save its outputs in the vectorial POSTGRES 
database. Here's a rough idea of the structure:

**Questions Table**

* id (Primary Key)
* question (Text)
* embedding (
Vector)
* document\_id (Integer)
* page\_number (Integer)

**Answers Table**

* id (Primary Key)
* answer (Text)
* embed
ding (Vector)
* document\_id (Integer)
* page\_number (Integer)
* question\_id (Foreign Key to Questions table)

**Summa
ries Table**

* id (Primary Key)
* summary (Text)
* embedding (Vector)
* document\_id (Integer)
* page\_number (Integer)


**Documents Table**

* id (Primary Key)
* summary (Text)
* document\_id (Integer)
* number\_of\_pages (Integer)

The w
orkflow is something like this:

1. Load the document (sanitize the text, embed it, save in 'Documents')
2. Make a summa
ry of each page (save in 'Summaries')
3. Generate questions for each page (save in 'Questions')
4. Answer all the questi
ons generated by the Question Agent, considering the context of the page (save in 'Answers')

**My biggest question is:*
* what tools and agents should I implement for this? Most resources I've found online use tools like Tavily Search and P
ython REPL, which aren't really helpful for my case. I need to use the Supervisor since it's a project requirement, and 
I'm a bit confused about the implementation details, since this would be very easy to implement with simple chains, and 
the only solution I could come up with is tooless agents...?

Any advice or pointers would be greatly appreciated! Thank
s!
```
---

     
 
all -  [ Open-source implementation of Meta’s TestGen–LLM - CodiumAI ](https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/) , 2024-06-15-0953
```
In Feb 2024, Meta published a paper introducing TestGen-LLM, a tool for automated unit test generation using LLMs, but d
idn’t release the TestGen-LLM code.The following blog shows how CodiumAI created the first open-source implementation - 
Cover-Agent, based on Meta's approach: [We created the first open-source implementation of Meta’s TestGen–LLM](https://w
ww.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/)

The tool is implemented as fol
lows:

1. Receive the following user inputs (Source File for code under test, Existing Test Suite to enhance, Coverage R
eport, Build/Test Command
Code coverage target and maximum iterations to run, Additional context and prompting options)

2. Generate more tests in the same style
3. Validate those tests using your runtime environment - Do they build and pass
?
4. Ensure that the tests add value by reviewing metrics such as increased code coverage
5. Update existing Test Suite 
and Coverage Report
6. Repeat until code reaches criteria: either code coverage threshold met, or reached the maximum nu
mber of iterations
```
---

     
 
all -  [ Error with tool calling while using Gemini Pro ](https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/) , 2024-06-15-0953
```
Hi all, 

I get an 'An error occurred: Multiple function calls are not currently supported' while using Gemini Pro .  
A
nyone had the same issue?

Using:

    llm = ChatGoogleGenerativeAI(temperature=0, model='gemini-pro')
    llm.bind_tool
s(tools)
```
---

     
 
all -  [ A serious security issue with GPT4o ](https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/) , 2024-06-15-0953
```
Last week we came across a serious security issue with GPT4o. In both ChatGPT and OpenAI APIs. Until OpenAI fixes it, we
 manage to fix from our side. We would like to share it with the community. We implemented it in LlamaIndex but should b
e easy to implement using Langchain as well.  
This is the medium article about the fix - [https://medium.com/@deltaarun
a/fixing-a-serious-security-issue-in-gpt4o-api-via-llamaindex-4aa1368b5d2f](https://medium.com/@deltaaruna/fixing-a-seri
ous-security-issue-in-gpt4o-api-via-llamaindex-4aa1368b5d2f)

This is the medium article about the issue - [https://medi
um.com/@deltaaruna/how-anyone-can-hack-chatgpt-aa7959684ef0](https://medium.com/@deltaaruna/how-anyone-can-hack-chatgpt-
aa7959684ef0)
```
---

     
 
all -  [ Best Open Source Model to being to fine tune for Algebra/Geometry? ](https://www.reddit.com/r/LLMDevs/comments/1de6z6u/best_open_source_model_to_being_to_fine_tune_for/) , 2024-06-15-0953
```
Looking to make an open source LLM a textbook you can talk to.  I'm a teacher and currently knee deep into langchain, py
thon, and ollama for now.  But before I start this mess (not even sure this will work), what is the best model to begin 
with that is focused on solving math problems.  I'm going to use RAG and train it on some textbooks (how I feed it the i
nformation successfully I'm also trying to figure out now, not to mention can i even train it using past conversations a
nd fine tune on my NVIDIA 3070).  But any recommendations?  Want to help my kids next year and give them the best possib
le experience to learn.  I'm didn't go to college for this but I've taught myself python, langchain, and other long word
s so far so how hard can it be...?
```
---

     
 
all -  [ Deploy Langgraph in Google Cloud? ](https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/) , 2024-06-15-0953
```
I have been searching extensively but haven't found any guide on deploying a Langgraph runnable with Google Cloud.   
  

Currently, I am using an Reasoning Engine (Vertex AI) with the LangchainAgent template (from Google Cloud documentation
)  
  
Now, I tried to deploy my custom Reasoning Engine agent based on Langgraph and I can't.   
  
I would greatly app
reciate any kind of help.   
  
Regards.   
  
PD: Langchain image to bait.

https://preview.redd.it/03fqi0fm856d1.jpg?w
idth=1024&format=pjpg&auto=webp&s=0bab82582504720db2d45a2d82cbec2aad0981a6


```
---

     
 
all -  [ Help regarding application specific chatbot ](https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/) , 2024-06-15-0953
```
I have been given a requirement from my company to look into and try to comeup with a chatbot that would be integrated i
nto the web application.
Specifically, we have a list of Companies and their details like name, what they do, their reve
nues, etc. and some uploaded pdf files that contain more information regarding the company.
So the chatbot will be integ
rated into the details page of the companies. User could then ask any question regarding the company and the chatbot sho
uld provide a relevant answer for that company.

I am fairly new to this, but was able to find out that we can use RAG f
or achieving this, wherein we take all the data and embed it in a vector database. Then fetch relevant vectors per the q
uestion asked and provide it as context to the LLM for answer.

However the issue is that some of the data of the compan
y can change with time.

Is there a way to do it so that the pdf data can use vector store, but the rest of the data can
 be obtained from API calls?
That way, we will always have the most recent data of the company, but also have the additi
onal data from the pdf docs?

How would all these things fit?
How would the decision be made when to use data from vecto
r database or when to fetch data from API?

Do you guys have any experience with something like this or any recommendati
ons or resources where I can look into for this project?

That would be very helpful.
```
---

     
 
all -  [ Google study says fine-tuning an LLM linearly increases hallucinations? 😐 ](https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/) , 2024-06-15-0953
```
They prepare a QA task to observe hallucinations, on both Known examples (training instances similar to the info that th
e model has seen during its initial training) and Unknown examples (that introduce new info that the model hasn't been e
xposed to before).

They see that:

1. Unknown examples in the fine-tuning dataset bring down performance, the more you 
train, because of overfitting. They lead to hallucinations and reduce accuracy. Known examples positively impact perform
ance.

2. Early stopping helps avoid this, which might mean that Unknown examples are neutral in shorter training.

3. T
he slower fitting of Unknown examples also indicates that models struggle to acquire new knowledge through fine-tuning.


Paper: [https://arxiv.org/pdf/2405.05904](https://arxiv.org/pdf/2405.05904)



I share high quality AI updates and tuto
rials daily.

If you like this post and want to stay updated on latest AI research, you can check out: [https://linktr.e
e/sarthakrastogi](https://linktr.ee/sarthakrastogi) or my Twitter: [https://x.com/sarthakai](https://x.com/sarthakai)
```
---

     
 
all -  [ LangChain/Next.js chatbot displaying incorrect sources ](https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/) , 2024-06-15-0953
```
I'm building a chatbot using `LangChain`, `Next.js`,and `CosmosDB` (vector store). My implementation is based on [this](
https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/api/chat/retrieval/route.ts). I'm trying to disp
lay the source documents used by the LLM in my UI, but I'm facing two issues:

1. Source documents not displaying: Despi
te using a **StreamingTextResponse** to send the source information in the headers as JSON chunks (see code snippet belo
w), they don't show up in my UI. There are no console errors.
2. Incorrect sources: When some source documents do appear
, they are not the ones actually used by the LLM or contain unrelated information.

Here's the part supposed to return t
he sources:

    import { NextRequest, NextResponse } from 'next/server';
    import { Message as VercelChatMessage, Str
eamingTextResponse } from 'ai';import { AzureCosmosDBVectorStore } from '@langchain/community/vectorstores/azure_cosmosd
b';
    import {
      AzureOpenAIEmbeddings,
      AzureChatOpenAI,
    } from '@langchain/azure-openai';
    import { 
PromptTemplate } from '@langchain/core/prompts';
    import { Document } from '@langchain/core/documents';
    import { 
RunnableSequence } from '@langchain/core/runnables';
    import {
      BytesOutputParser,
      StringOutputParser,
   
 } from '@langchain/core/output_parsers';
    
    const combineDocumentsFn = (docs: Document[]) => {
      const serial
izedDocs = docs.map((doc) => doc.pageContent);
      return serializedDocs.join('\n\n');
    };
    
    const formatVer
celMessages = (chatHistory: VercelChatMessage[]) => {
      const formattedDialogueTurns = chatHistory.map((message) => 
{
        if (message.role === 'user') {
          return `Human: ${message.content}`;
        } else if (message.role =
== 'assistant') {
          return `Assistant: ${message.content}`;
        } else {
          return `${message.role}: 
${message.content}`;
        }
      });
      return formattedDialogueTurns.join('\n');
    };
    
    const CONDENSE_
QUESTION_TEMPLATE = `Given the following conversation and a follow up question, rephrase the follow up question to be a 
standalone question, in its original language.<chat_history>
      {chat_history}
    </chat_history>
    
    Follow Up
 Input: {question}
    Standalone question:`;
    const condenseQuestionPrompt = PromptTemplate.fromTemplate(
      COND
ENSE_QUESTION_TEMPLATE
    );
    
    const ANSWER_TEMPLATE = `Answer the question based only on the following context 
and chat history:
    <context>
      {context}
    </context>
    <chat_history>
      {chat_history}
    </chat_histor
y>
    
    Question: {question}
    `;
    const answerPrompt = PromptTemplate.fromTemplate(ANSWER_TEMPLATE);
    
    
export async function POST(req: NextRequest) {
      try {
        const body = await req.json();
        const messages
 = body.messages ?? [];
        const previousMessages = messages.slice(0, -1);
        const currentMessageContent = me
ssages[messages.length - 1].content;
    
        const vectorstore = new AzureCosmosDBVectorStore(
          new AzureO
penAIEmbeddings(),
          {
            databaseName: process.env.DB_NAME,
            collectionName: process.env.DB
_COLLECTION_NAME,
          }
        );
    
        const model = new AzureChatOpenAI({
          azureOpenAIEndpoint:
 process.env.AZURE_OPENAI_API_ENDPOINT,
          azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY,
          azureOp
enAIApiDeploymentName:
            process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME,
          modelName: process.env.AZURE_
OPENAI_MODEL_NAME,
        });
        const standaloneQuestionChain = RunnableSequence.from([
          condenseQuestio
nPrompt,
          model,
          new StringOutputParser(),
        ]);
    
        let resolveWithDocuments: (value:
 Document[]) => void;
        const documentPromise = new Promise<Document[]>((resolve) => {
          resolveWithDocume
nts = resolve;
        });
    
        const retriever = vectorstore.asRetriever({
          callbacks: [
            {

              handleRetrieverEnd(documents) {
                resolveWithDocuments(documents);
              },
       
     },
          ],
        });
    
        const retrievalChain = retriever.pipe(combineDocumentsFn);
    
        co
nst answerChain = RunnableSequence.from([
          {
            context: RunnableSequence.from([
              (input)
 => input.question,
              retrievalChain,
            ]),
            chat_history: (input) => input.chat_histor
y,
            question: (input) => input.question,
          },
          answerPrompt,
          model,
        ]);
  
  
        const conversationalRetrievalQAChain = RunnableSequence.from([
          {
            question: standaloneQu
estionChain,
            chat_history: (input) => input.chat_history,
          },
          answerChain,
          new 
BytesOutputParser(),
        ]);
    
        const stream = await conversationalRetrievalQAChain.stream({
          que
stion: currentMessageContent,
          chat_history: formatVercelMessages(previousMessages),
        });
    
        c
onst documents = await documentPromise;
        console.log('documents ', documents.length);
        const serializedSou
rces = Buffer.from(
          JSON.stringify(
            documents.map((doc) => {
              return {
              
  pageContent: doc.pageContent.slice(0, 50) + '...',
                metadata: doc.metadata,
              };
          
  })
          )
        ).toString('base64');
        const sourceMetadata = documents.map((doc) => ({
          title:
 doc.metadata.title, // Or whatever metadata you want
          url: doc.metadata.url,
        }));
    
        return 
new StreamingTextResponse(stream, {
          headers: {
            'x-message-index': (previousMessages.length + 1).to
String(),
            'x-message-sources': serializedSources,
          },
        });
      } catch (e: any) {
        
return NextResponse.json({ error: e.message }, { status: e.status ?? 500 });
      }
    }
    

So my questions:

1. Is
 my approach to streaming source documents via **StreamingTextResponse** wrong?
2. How can I ensure I'm associating the 
correct source documents with each LLM response?
3. What debugging techniques can I use to pinpoint where the source inf
ormation is getting lost or mismatched?  
```
---

     
 
all -  [ Local Source Code Indexing & Querying with RAG ](https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/) , 2024-06-15-0953
```
I'm currently working on a local codebase and was using Cody to ask questions from my local code base context. But was w
ondering if there is an open-source project that's similar. Ideally, the tool would:

* Index all project files
* Retrie
ve specific code snippets from all files or tell about specific local variables
* Use as Chatbot?

Document loaders and 
file embeddings could work but I'm not sure on how to handle function interdependencies. Do I need to also additionally 
pass AST for it to organize it better? Not really sure on what direction to take?

Anyone has tried something similar? W
hat approach did you take? and how was the result?
```
---

     
 
all -  [ What are the biggest problems you're facing while building AI apps? ](https://www.reddit.com/r/ChatGPT/comments/1de32az/what_are_the_biggest_problems_youre_facing_while/) , 2024-06-15-0953
```
I've just started building an AI app using ChatGPT + Langchain set-up. I've been facing a number of recurring pain point
s and I'm genuinely curious if the rest of you are facing similar issues. I'm purposely not mentioning the issues I'm fa
cing, because I don't want to bias the answers / keep the discussion as open as possible. What are the biggest issues / 
pain points you're struggling with while building AI apps?
```
---

     
 
all -  [ How to Speed Up Execution with OpenAI GPT-4o in Multi-Agent System? ](https://www.reddit.com/r/crewai/comments/1de2mdr/how_to_speed_up_execution_with_openai_gpt4o_in/) , 2024-06-15-0953
```
**Hey everyone,**

I've been working on a multi-agent system using OpenAI's GPT-4o model, but I'm running into performan
ce issues. The execution time is longer than I'd like, even though I've set `max_iter` to 2 for each agent. but i have u
ser groq its fast but i need solution for gpt-4o or openAI api only !

**Here's a brief overview of my setup:**

1. **Qu
estioning Agent**: Designed to ask relevant questions to gather information from the user.
2. **Validation Agent**: Ensu
res the questions generated by the Questioning Agent are of high quality.
3. **Crew**: Manages the agents and tasks, exe
cuting them hierarchically.

Here's a snippet of my code for context:

  
`import os`

`from crewai import Agent, Task, 
Crew, Process`

`from langchain_openai import ChatOpenAI`



`os.environ['OPENAI_API_KEY'] = 'MY_KEY'`

`llm = ChatOpenA
I(`

`model='gpt-4o',`

`temperature=0.1,`

`)`



`number_of_questions = 10`



`questioning_agent = Agent(`

`role='Qu
estioning Agent',`

`goal='Ask relevant questions to gather information and clarify user needs here is how you should re
spond :- {sys-prompt}',`

`verbose=True,`

`max_iter=2,`

`llm = llm,`

`memory=False,`

`backstory=('Your AI guide for 
asking insightful questions. I'm designed to help users clarify their goals and needs by asking targeted questions. My g
oal is to gather information, identify patterns, and provide a personalized learning experience. I'll ask questions that
 are open-ended, relevant, and designed to encourage the user to provide detailed responses. I'll also use the user's re
sponses to ask follow-up questions, ensuring that I gather all the necessary information to provide a comprehensive lear
ning experience.'),`

`allow_delegation=True`

`)`



`validation_agent = Agent(`

`role='Validation Agent',`

`goal='Va
lidate {val-prompt} and check does it follow this format:-{for}',`

`verbose=True,`

`max_iter = 2,`

`llm = llm,`

`mem
ory=False,`

`backstory=('Your AI guide for ensuring question quality. I'm responsible for reviewing the questions gener
ated by the Questioning Agent and providing feedback on their relevance, clarity, and effectiveness. My goal is to ensur
e that the questions are useful, concise, and easy to understand. I'll review the questions for grammar, syntax, and spe
lling, as well as their ability to gather useful information from the user. I'll provide feedback on the questions, sugg
esting improvements or changes as needed, to ensure that they meet the highest standards of quality and effectiveness.')
,`

`allow_delegation=True`

`)`



`questioning_task = Task(`

`description=('Ask a series of questions to gather infor
mation about the user's learning goals, preferences, and needs. The questions should be open-ended, relevant, and design
ed to encourage the user to provide detailed responses. The questions should also be tailored to the user's specific goa
ls and needs, taking into account their level of expertise, learning style, and desired outcomes. Here is What User situ
ation : {user}'),`

`expected_output='A set of relevant, well-crafted questions that gather useful information about the
 user.You respond should be like this \n{for}',`

`agent=questioning_agent,`

`async_execution=False`

`)`



`validatio
n_task = Task(`

`description=('Review the generated questions for quality, relevance, and effectiveness. Provide feedba
ck on the questions, suggesting improvements or changes as needed. The feedback should be detailed and constructive, pro
viding specific examples and suggestions for improvement. The goal is to ensure that the questions are of the highest qu
ality, and that they will gather useful information from the user.'),`

`expected_output='A validate the question if que
stions are correct the output it. Always output in json in this format {for}. Remember output should be JSON Always and 
in this format {for} and it should have {num} questions',`

`agent=validation_agent,`

`async_execution=False`

`)`



`
crew = Crew(`

`agents=[questioning_agent, validation_agent],`

`tasks=[questioning_task, validation_task],`

`process=P
rocess.hierarchical,`

`manager_llm = llm,`

`cache=True,`

`)`



`user_input = {`

`'What do you want to learn?': 'MER
N Stack Web dev',`

`'What is your level?': 'beginner',`

`'Do you have any specific goals or outcomes you want to achie
ve by learning?': 'I just wanna learn MERN stack web dev'`

`}`

`number_of_questions = 10`

`result = crew.kickoff(inpu
ts={'sys-prompt':questioning_system_prompt,'val-prompt':validation_system_prompt,'user':user_input,'for':format_p,'num':
number_of_questions})`

`print(result)`



  
**Question**: How can I optimize this setup to reduce the execution time? 
Any tips on improving performance with OpenAI's API or the overall agent management would be greatly appreciated.

Thank
s in advance!
```
---

     
 
all -  [ Good Tutorials For RAG with Structured State and Output?  ](https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/) , 2024-06-15-0953
```
Hey guys!

So I’ve been looking at a lot of tutorials to build a basic RAG search which does the following:

1. Takes th
e user query and put it into the state “user_query”
2. Searches the internet for results. These results are then populat
ed as text in the state “internet_search_results” field with the url and title of the text
3.  Does the same but searche
s the local database and populates the state “local_search_results” field with the post ID and title of the search resul
ts. 
4. Then passes the state with the information above into a summariser function which uses GPT 3.5 to return structu
red output with the following fields: (i) the text response, (ii) an array of the sources which include the title, the t
ype (web search or local post), and either the url or the post ID. 


I’m at a loss on this as can’t find any good tutor
ials for this. 


```
---

     
 
all -  [ Question regarding limitation of agent use ](https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/) , 2024-06-15-0953
```
I am not a software engineer but an enthusiast of RAG and LLM agents. I wanted to know where is the real bottleneck in b
uilding an agent who would build documents based on chat that I am currently having with an LLM based chat interface and
 embed the chat text using embedding models and store it in vector db for the user to search in later? 
```
---

     
 
all -  [ Production Ready Unstructured Text to Knowledge Graph ](https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/) , 2024-06-15-0953
```
I'm working on a use case that relies on very robust knowledge graph construction and I wanted to know if any startups/c
ompanies/open-source have built either free or paid production ready solutions for the unstructured text to knowledge gr
aph pipeline.

UPDATE:

Diffbot seems to have a pretty good API that is compatiable with Llama Index and Langchain

this
 tutorial for Llama Index was released the same day I posted this and looks promising: [https://www.llamaindex.ai/blog/c
ustomizing-property-graph-index-in-llamaindex](https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamai
ndex)​

And Here is one for Langchain [Diffbot | 🦜️🔗 LangChain](https://python.langchain.com/v0.1/docs/integrations/grap
hs/diffbot/)​
```
---

     
 
all -  [ How does this LangChain agent correctly identify the tool to use? ](https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/) , 2024-06-15-0953
```
In this [Medium article](https://medium.com/ama-tech-blog/combining-langchain-and-llamaindex-to-build-your-first-agentic
-rag-system-6e8e2e7825e7), the agent has three tools:

- 'lyft_10k': 'Provides information about Lyft financials for yea
r 2021. '

- 'uber_10k': 'Provides information about Uber financials for year 2021. '

and

- 'DuckDuckGoSearch': 'Use f
or when you need to perform an internet search to find information that another tool can not provide.'

In one of the te
st cases, the author queries the agent

    'List me the names of Uber's board of directors.'

Intuitively, one would as
sume the agent will invoke the 'uber_10k' tool. However, the agent invokes 'DuckDuckGoSearch'.

The author explains that
:
> Since this information is out-of-scope for any of the retriever tools, the agent correctly decided to invoke the ext
ernal search tool.

How does the agent know that question is out-of-scope for the 'uber_10k' retriever?
```
---

     
 
all -  [ Errors loading `langchain_anthropic` ](https://www.reddit.com/r/LangChain/comments/1ddort5/errors_loading_langchain_anthropic/) , 2024-06-15-0953
```
Recently when I try to do `import langchain_anthropic` I have been getting errors like this:
```
>>> import langchain_an
thropic
Traceback (most recent call last):
  File '<stdin>', line 1, in <module>
  File '/usr/local/lib/python3.12/site-
packages/langchain_anthropic/__init__.py', line 1, in <module>
    from langchain_anthropic.chat_models import ChatAnthr
opic, ChatAnthropicMessages
  File '/usr/local/lib/python3.12/site-packages/langchain_anthropic/chat_models.py', line 26
, in <module>
    from langchain_core.callbacks import (
  File '/usr/local/lib/python3.12/site-packages/langchain_core/
callbacks/__init__.py', line 22, in <module>
    from langchain_core.callbacks.manager import (
  File '/usr/local/lib/p
ython3.12/site-packages/langchain_core/callbacks/manager.py', line 29, in <module>
    from langsmith.run_helpers import
 get_run_tree_context
  File '/usr/local/lib/python3.12/site-packages/langsmith/run_helpers.py', line 40, in <module>
  
  from langsmith import client as ls_client
  File '/usr/local/lib/python3.12/site-packages/langsmith/client.py', line 5
2, in <module>
    from langsmith import env as ls_env
  File '/usr/local/lib/python3.12/site-packages/langsmith/env/__i
nit__.py', line 3, in <module>
    from langsmith.env._runtime_env import (
  File '/usr/local/lib/python3.12/site-packa
ges/langsmith/env/_runtime_env.py', line 10, in <module>
    from langsmith.utils import get_docker_compose_command
  Fi
le '/usr/local/lib/python3.12/site-packages/langsmith/utils.py', line 31, in <module>
    from langsmith import schemas 
as ls_schemas
  File '/usr/local/lib/python3.12/site-packages/langsmith/schemas.py', line 69, in <module>
    class Exam
ple(ExampleBase):
  File '/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py', line 286, in __new__
    cls.__t
ry_update_forward_refs__()
  File '/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py', line 807, in __try_upda
te_forward_refs__
    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (Na
meError,))
  File '/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py', line 554, in update_model_forward_ref
s
    update_field_forward_refs(f, globalns=globalns, localns=localns)
  File '/usr/local/lib/python3.12/site-packages/p
ydantic/v1/typing.py', line 520, in update_field_forward_refs
    field.type_ = evaluate_forwardref(field.type_, globaln
s, localns or None)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File '/usr/local/lib
/python3.12/site-packages/pydantic/v1/typing.py', line 66, in evaluate_forwardref
    return cast(Any, type_)._evaluate(
globalns, localns, set())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: ForwardRef._evaluat
e() missing 1 required keyword-only argument: 'recursive_guard'
>>>
```
That smells like maybe there's an un-captured de
pendency from the `langchain_anthropic` code onto pydantic or something, but I don't have any great ideas about how to d
ebug this.  Anyone else seen this? Any fix or work-around?

Checking, it looks like I have `langchain-anthropic==0.1.13`
 and `langchain==0.1.20` if that helps.
```
---

     
 
all -  [ [Student] Looking for advice on resume and upcoming search, interested in product and software devel ](https://www.reddit.com/r/EngineeringResumes/comments/1ddoopc/student_looking_for_advice_on_resume_and_upcoming/) , 2024-06-15-0953
```
I'm a U.S. citizen and an upcoming senior at Cal State University, getting ready to dive into the job search. Currently 
located in the Bay Area, I attend a Cal-State near a major tech hub, which has provided me with some fantastic opportuni
ties. I've completed internships at a large, albeit controversial especially rn, tech(?) company, and I'm eager to secur
e a full-time position after grad or even a masters degree tbh. As I prepare to start applying, my primary concern is ho
w to increase my chances of getting callbacks since I didn't get as many callbacks as expected for the internship search
--still got multiple offers though. Additionally, I’m looking for advice on how to effectively incorporate another inter
nship experience from the same company into my resume, especially since the work I'm doing there differs from my previou
s role. I'm targeting roles in software engineering, particularly those involving machine learning, AI, and big data ana
lytics. I'm open to both local and remote opportunities and am willing to relocate for the right position.

’m seeking f
eedback on my resume to fine-tune it and make sure it stands out to recruiters. Specifically, I'd appreciate any advice 
on whether my technical skills and experiences are presented effectively and how I can best highlight my projects and in
ternship roles. I feel that it might go over a page tbh. The resumse says May 25 as my grad year, but i'm probably gradu
ating in december 2024. 

https://preview.redd.it/lf38t9on806d1.png?width=5100&format=png&auto=webp&s=89a0912f63dbe8a18d
4198e29893dc4632827472


```
---

     
 
all -  [ Rate my resume and tell me if this is good enough for Data/Business Analyst positions. ](https://i.redd.it/imr71o0t306d1.jpeg) , 2024-06-15-0953
```
Please guide me on what all should be added/removed. Also, can I apply for senior roles? 
```
---

     
 
all -  [ Python Projects ](https://www.reddit.com/r/learnpython/comments/1ddo02e/python_projects/) , 2024-06-15-0953
```
Any suggestions about where I can find some python projects with langchain, flask ?
```
---

     
 
MachineLearning -  [ [P] Superfast RAG: Langchain Streaming and Groq ](https://www.reddit.com/r/MachineLearning/comments/1d5s9g4/p_superfast_rag_langchain_streaming_and_groq/) , 2024-06-15-0953
```
  
Fast LLM RAG inference using Groq and Langchain Streaming.  
  
Groq is introducing a new, simpler processing archite
cture designed specifically for the performance requirements of machine learning applications and other compute-intensiv
e workloads. The simpler hardware also saves developer resources by eliminating the need for profiling, and also makes i
t easier to deploy AI solutions at scale.  
  
Resource: [https://www.youtube.com/watch?v=frMdOL8knqg](https://www.youtu
be.com/watch?v=frMdOL8knqg)
```
---

     
 
deeplearning -  [ How to finetune? ](https://www.reddit.com/r/deeplearning/comments/1daio0h/how_to_finetune/) , 2024-06-15-0953
```
Can someone guide me to some resource how can I finetune an open source llm or some library (like langchain) on unstruct
ured data (example: news articles on cricket) So that model can answer a question (like When did India won world Cup?)
```
---

     
