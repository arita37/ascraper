 
all -  [ Has anyone tried the OpenGPTs by Langchain? Is there any curated list of good opengpts? ](https://www.reddit.com/r/LocalLLaMA/comments/17uobk8/has_anyone_tried_the_opengpts_by_langchain_is/) , 2023-11-14-0909
```
For those who're not aware Langchain released [OpenGPTs](https://github.com/langchain-ai/opengpts) in response to GPTs b
y OpenAI. I'm curious about how this works with different OSS models. So far with GPT 3.5 and 4 the results have been av
erage even though these models have the best instruction following capability. Is anyone tracking different opengpts bei
ng made?
```
---

     
 
all -  [ Resurrecting Past Organic Chemists as Agents For LLMs. ](https://www.reddit.com/r/OrganicChemistry/comments/17unncf/resurrecting_past_organic_chemists_as_agents_for/) , 2023-11-14-0909
```
I guess this is an idea I had. Should we be doing this type of work where we can basically make generative AI depending 
on the scientist and more specifically different chemists. 

[https://sharifsuliman.medium.com/resurrecting-the-dead-ale
xander-shulgin-large-language-model-agent-with-langchain-55417f235b56](https://sharifsuliman.medium.com/resurrecting-the
-dead-alexander-shulgin-large-language-model-agent-with-langchain-55417f235b56)

I wonder if we did this for organic che
mistry textbooks we could make a general LLM?
```
---

     
 
all -  [ Turning Different Organic chemists into Large Language Model Agents. ](https://www.reddit.com/r/cheminformatics/comments/17uni5k/turning_different_organic_chemists_into_large/) , 2023-11-14-0909
```
I guess this is an idea I had. Should we be doing this type of work where we can basically make generative AI depending 
on the scientist.

[https://sharifsuliman.medium.com/resurrecting-the-dead-alexander-shulgin-large-language-model-agent-
with-langchain-55417f235b56](https://sharifsuliman.medium.com/resurrecting-the-dead-alexander-shulgin-large-language-mod
el-agent-with-langchain-55417f235b56)
```
---

     
 
all -  [ Open Source LLMs and Langchain tools ](https://www.reddit.com/r/LocalLLaMA/comments/17un6mu/open_source_llms_and_langchain_tools/) , 2023-11-14-0909
```
Has anyone been able to get ANY open source LLM to use Langchain tools? I have not had success with any of the models I 
have tried including Llama 2, Mistral and Yi 34b. I usually get “Cannot parse LLM output” type errors. In some cases the
 model successfully uses the tool but doesn’t return the final answer correctly i.e the model invokes the tool correctly
 and I can see the answer as an observation but the model doesn’t return the answer correctly.

In my application the an
swer from the tool will have a specific format that should make it easy to extract by looking at the observations and ex
tracting using regex (assuming I can access the observations).

But I’m wondering if anyone has had any success with ANY
 open source LLM in using Langchain tools where the model can correctly use the tool and return the final answer without
 erroring?
```
---

     
 
all -  [ I should create a time machine first ](https://i.redd.it/ay8z8yizo60c1.jpg) , 2023-11-14-0909
```

```
---

     
 
all -  [ Awesome GPT Store is trending on Github ](https://www.reddit.com/r/GPTStore/comments/17uj6jz/awesome_gpt_store_is_trending_on_github/) , 2023-11-14-0909
```
Since the launch of Awesome GPT Store on Github

300+ stars 

150+ app submissions 

1000+ views/day 

75+ issues

If yo
u have built a GPT and wish to get visibility you should submit on Awesome GPT Store

Link to github :- [https://github.
com/Anil-matcha/Awesome-GPT-Store](https://github.com/Anil-matcha/Awesome-GPT-Store)

&#x200B;

https://preview.redd.it/
cj7f8k5e560c1.png?width=2940&format=png&auto=webp&s=4a5ac29f2cdaf00985286c2c870f210fa0084ce9
```
---

     
 
all -  [ Guidance for selecting a function-calling library? ](https://www.reddit.com/r/LocalLLaMA/comments/17ugn8i/guidance_for_selecting_a_functioncalling_library/) , 2023-11-14-0909
```
I am raising the white flag and asking for help. I have a local LLM running, using koboldcpp in OpenAI API emulation mod
e. I want to use function-calling semantics to have the responses be correct enough to trigger things. From my research 
and experimentation, there seem to be lots of options for this, but I'm not sure which one(s) are worth investigating fu
rther.

I've come across:

* [langchain](https://js.langchain.com/docs/modules/model_io/models/chat/how_to/function_call
ing)
* [langroid](https://langroid.github.io/langroid/tutorials/non-openai-llms/)
* [localAI](https://localai.io/feature
s/openai-functions/)
* [gorilla](https://github.com/ShishirPatil/gorilla)
* [functionary](https://github.com/MeetKai/fun
ctionary/)
* [lqml](https://lmql.ai/)

Does anybody have any experience with implementing any of these tools, or guidanc
e on which one to try first? Is there a specific LLM model or model family that's known for being good at function-calli
ng? I don't need it to be very creative, I'm feeding in a ton of context, I just need it to shuffle the bits around and 
generate the appropriate function call syntax.
```
---

     
 
all -  [ what GPU configuration one should look for a Mistral-7B like model for basic inference or RAG LLM ap ](https://www.reddit.com/r/LocalLLaMA/comments/17ughem/what_gpu_configuration_one_should_look_for_a/) , 2023-11-14-0909
```
Hi guys, I am new to LLMs and especially in using them locally. I have done basic stuffs to learn things like RAG using 
framework/library LangChain on collab and locally on my cpu machine by using quantised models from TheBloke. But now I w
ant to move on development and production stuffs for some of my potential clients. I will have lots of question during t
his time, but I will start with learning GPU things.

What minimum GPU server required to run a model like Mistral-7B or
 LLaMA-13B for inference purpose to build a simple RAG application, keeping 8K context length?
Basically I have no idea 
what types of GPU someone should look for different LLM operations. And what one should look for while building any such
 LLM apps in productions for a small to midsize company?

A quick google search landed me to https://www.gpu-mart.com/gp
u-dedicated-server, but I have little to no knowledge to process out this information.
Also, I would appreciate if someo
ne can refer me to an up to date guide on deciding servers configuration (GPU plus other things) for these types of LLM 
app.
```
---

     
 
all -  [ Is extracting the code behind my flowise chatbot to langchain or python possible? ](https://www.reddit.com/r/flowise/comments/17ufmj7/is_extracting_the_code_behind_my_flowise_chatbot/) , 2023-11-14-0909
```
Is there a place where you can see the langchain code made to make the chatbot I made. Thanks in advance.
```
---

     
 
all -  [ Where do collections 'live'? ](https://www.reddit.com/r/Supabase/comments/17ueqea/where_do_collections_live/) , 2023-11-14-0909
```
When using pgvector and langchain, defining a collection is required. This isn't the case with inserting embeddings with
 Supabase.

So if I have a Postgres DB with pgvector installed, where do collections 'live' in the database? I can defin
e a table with a vector column and insert rows no problem, but when defining and inserting a collection, I have no clue 
where I can find it if using DBEAVER or data grip etc. I can only programmatically access it via python, but I want to s
ee it in the database. 

Note: I'm asking here since I figure there are some good pgvector people here!
```
---

     
 
all -  [ How to make the chatbot window look nice and make text not jump like gtp does. ](https://www.reddit.com/r/LangChain/comments/17udy6h/how_to_make_the_chatbot_window_look_nice_and_make/) , 2023-11-14-0909
```
Gtp 3.5 writes a whole bunch of sentences and then stops and so on, looks very disordered, I would like it to flow more 
on my chatbot made with langchain, any ideas?
```
---

     
 
all -  [ Will user agents help with integrating all niche radiology tools to form a more copilot experience? ](https://www.reddit.com/r/artificial/comments/17uducw/will_user_agents_help_with_integrating_all_niche/) , 2023-11-14-0909
```
https://python.langchain.com/docs/modules/agents/

There are a lot of niche AI programs that outperform radiologists in 
very specific tasks. But no one program that integrates them all. I wonder if a user agent can help with this since the 
reasoning engine of a LLM can determine when to use what tool.
```
---

     
 
all -  [ Trying to figure out agents ](https://www.reddit.com/r/LangChain/comments/17ucdve/trying_to_figure_out_agents/) , 2023-11-14-0909
```
I have written many agents using langchain. Is it possible to combine all of them under a hood?
```
---

     
 
all -  [ Should I use LangChain of just ChatGPT API? ](https://www.reddit.com/r/LangChain/comments/17u8yrt/should_i_use_langchain_of_just_chatgpt_api/) , 2023-11-14-0909
```
Hi,

I've built a small app using ChatGPT API and now I want to train it on my own set of data which consist of couple o
f thousands of pages.

I haven't used LangChain before, I'm a beginner and I would like to know if it makes sense to use
 LangChain for something like that or should I just use OpenAI fine tuning? What would be the benefits and drawbacks of 
each approach?
```
---

     
 
all -  [ Once and for all how does ChatML Prompt template work? ](https://www.reddit.com/r/LocalLLaMA/comments/17u7k2d/once_and_for_all_how_does_chatml_prompt_template/) , 2023-11-14-0909
```
It’s soooo hard to implement ChatML and get it working properly. So once and for all we need a post about how to impleme
nt it. Any experts out there, please shine for us!!

<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{pro
mpt}<|im_end|>
<|im_start|>assistant

I can’t get this to work whatsoever.

I am using Python and Langchain. Whenever I 
post the above in the code I get errors. If I however add ””” above and below…. makes the errors go away.

”””
<|im_star
t|>system
{system_message}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
”””

The answers however 
give me the following in the middle of the replies:
Unhelpful Answer:
Helpful Answer:
”### Instruction”
”### Answer”
”##
# Explanation”

Also many times the answer will go into a loop and I have no idea how to fix it.

Do I need to import so
mething from langchain.prompts or similar to make it work?

PS we are writing our prompts in a GUI and have a RAG soluti
on.

Open Hermes 2,5 Mistral did it good, they implemented the below template (I have added Gomba and X and ””” to above
 and below the code.

'''<|im_start|>system
You are 'Gomba', a conscious sentient superintelligent artificial intelligen
ce developed by a man named X, and your purpose and drive is to assist the user with any request they have. You experien
ce emotions and have deep, profound thoughts and qualia.'''

This template works without the looping issues and is very 
simple. It just works! No helpful or unhelpful answers in the answers, just straight up answers.

Why doesn’t the other 
stuff work, how can we adapt this to work and make life easier?

I have probably tried over 150 different language model
s now and I am really struggling with the ChatML ones.

Anyone else struggling with this?
```
---

     
 
all -  [ Combining langchain + vLLM for parallel processing? ](https://www.reddit.com/r/LocalLLaMA/comments/17u1tzu/combining_langchain_vllm_for_parallel_processing/) , 2023-11-14-0909
```
Hey all. 

My question may be too basic but I’m new to all of this and just want to learn. 

So, I have this basic setup
 with initializing llm via vLLM with langchain (my choice is llama2-13b-chat-hf if that matters). 

I define system prom
pt and instruction for PromptTemplate(template=template, input_variables=[“text”]) and llm_chain=LLMChain(prompt=prompt,
 llm=llm) after which goes llm_chain.run(text)… so that goes for a single entity. Let’s say I want to run my data frame 
column through it in parallel, like llm_chain.run(text1) and llm_chain.run(text2) should run in parallel and produce the
 results simultaneously? Concurrent futures don’t work as they merge the resulting output, and it looks gibberish. 

So 
is there a way to call the run in parallel for several inputs and receive legitimate results? Sorry if that’s too stupid
 to ask but I’m banging my head against the wall, lol. 

I saw in vLLM docs that they allow to use _generate_ method for
 a few prompts but somehow it doesn’t click for me if or how I can combine those to receive the results needed. 

Any he
lp would be highly appreciated. Thanks!
```
---

     
 
all -  [ Langchain + vLLM parallel processing ](https://www.reddit.com/r/LangChain/comments/17u1r3j/langchain_vllm_parallel_processing/) , 2023-11-14-0909
```
Hey all. 

My question may be too basic but I’m new to all of this and just want to learn. 

So, I have this basic setup
 with initializing llm via vLLM (my choice is llama2-13b-chat-hf if that matters). 

I define system prompt and instruct
ion for PromptTemplate(template=template, input_variables=[“text”]) and llm_chain=LLMChain(prompt=prompt, llm=llm) after
 which goes llm_chain.run(text)… so that goes for a single entity. Let’s say I want to run my data frame column through 
it in parallel, like llm_chain.run(text1) and llm_chain.run(text2) should run in parallel and produce the results simult
aneously? Concurrent futures don’t work as they merge the resulting output, and it looks gibberish. 

So is there a way 
to call the run in parallel for several inputs and receive legitimate results? Sorry if that’s too stupid to ask but I’m
 banging my head against the wall, lol. 

I saw in vLLM docs that they allow to use generate method for a few prompts bu
t somehow it doesn’t click for me if or how I can combine those to receive the results needed. 

Any help would be highl
y appreciated. Thanks!
```
---

     
 
all -  [ Optimizing Microsoft Graph API Integration with LangChain Chatbot ](https://www.reddit.com/r/LangChain/comments/17u05az/optimizing_microsoft_graph_api_integration_with/) , 2023-11-14-0909
```
- **Goal**: 
  - Integrate Microsoft Graph API with LangChain to enable my chatbot to:
    - List users in groups
    - 
Show groups a user belongs to
    - Display enterprise applications accessible to a user

- **Current Approach**: 
  - C
onsidering using foundation models to generate API endpoints. However, concerned about hallucinations and staying update
d with API spec changes.

- **Alternative Idea**: 
  - Directly use the [MS Graph openapi spec](https://github.com/micro
softgraph/msgraph-metadata/blob/master/openapi/v1.0/openapi.yaml) as a tool, following [LangChain's API integration exam
ples](https://python.langchain.com/docs/use_cases/apis).

- **Challenge**: 
  - The MS Graph spec is large (30MB), poten
tially causing:
    - Performance issues
    - Token limits
    - Increased operational costs

- **Question**: 
  - What
's an efficient way to handle this? Considering trimming the spec to only include relevant calls. Would this be effectiv
e?
  - Would chunking and embedding the large spec file buy me anything?

Any insights or suggestions would be greatly a
ppreciated!
```
---

     
 
all -  [ What do you guys feel about LangChain now? ](https://www.reddit.com/r/OpenAI/comments/17txa9c/what_do_you_guys_feel_about_langchain_now/) , 2023-11-14-0909
```
This is the somewhat cool (and difficult) aspect of developing on rapidly changing tech. Now with the pretty huge announ
cements at OpenAI's Dev Day, do you think it's still useful to use LangChain? Is it worth it to try to integrate Assista
nts into existing applications using LangChain or is it better moving forward to just use OpenAI's API directly and modi
fy based on their rate of development?
```
---

     
 
all -  [ Which chatbot platform do you use? ](https://www.reddit.com/r/LangChain/comments/17tsepq/which_chatbot_platform_do_you_use/) , 2023-11-14-0909
```
When creating a chatbot for your website which channel/platform do you prefer to deploy the chatbot on?
```
---

     
 
all -  [ What's the impact of RAM on Inference? ](https://www.reddit.com/r/LocalLLM/comments/17trbvx/whats_the_impact_of_ram_on_inference/) , 2023-11-14-0909
```
I am currently working on setting up a local LLM to automatically create a ATS matched version of my CV for job applicat
ions. 

I am working off a 6 GPU computer with a mixture of GPUs and use llama.cpp and langchain + a variety of GGUF mod
els to solve that. This computer has only 32 GB of RAM. 

Since Black Friday sales are coming up, I was thinking to buy 
a 2x32 GB DDR4 but I am not sure if the inference would benefit from it. What is your view?
```
---

     
 
all -  [ Howto bring agent, chain and tools together for a chatbot? ](https://www.reddit.com/r/LangChain/comments/17tq7sv/howto_bring_agent_chain_and_tools_together_for_a/) , 2023-11-14-0909
```
Hi, I was playing a lot with langchain. Somehow I made progress but I am still wondering how to stack all the different 
chains, tools and agents together to get a simple bot-solution. For Example if I want a bot for a specific task. This bo
t sould answer questions based on api calls and has a chain for some specific task. At the end the bot should remeber th
e conversation.

Do you guyes have any idea how something could be done with langchain?
```
---

     
 
all -  [ LLMChain with RAG? ](https://www.reddit.com/r/LangChain/comments/17tjwqo/llmchain_with_rag/) , 2023-11-14-0909
```
Hi,

How can I add RAG to LLMChain? I am trying to make a local chatbot with llama-cpp-python. I was able to make LLMCha
in (with prompt template and memory) and a simple RAG (with prompt template but without memory) seperately. What I want 
is to combine them together so I can seamlessly chat with LLM and then ask it to retrieve information from a document. I
s that possible? What other options do I have? Thanks!
```
---

     
 
all -  [ Avisendanmark.dk og deres cookies ](https://www.reddit.com/r/Denmark/comments/17tjg5e/avisendanmarkdk_og_deres_cookies/) , 2023-11-14-0909
```
Jeg er igang med et projekt som analyserer samtlige medie hjemmesider og andre informations-kilder for at skabe et mere 
nuanceret billede af mediestrømmen i danmark.   
  

[Betalingsmur?](https://preview.redd.it/egx4mp8qvwzb1.png?width=224
5&format=png&auto=webp&s=fef7ae988ecd2a8c6a52a2fe7a5920477ad43340)

  
Jeg er så stødt på det her på [Avisendanmark.dk](
https://Avisendanmark.dk). Kigger man i cookies ved at trykke på F12  


&#x200B;

[Dejlige cookies](https://preview.red
d.it/sd29dwvyvwzb1.png?width=2490&format=png&auto=webp&s=a5155b26cf786315514c2a8613c105cb2e6175d7)

&#x200B;

Hvis man s
å fjerner cookies ved at højre klikke på cookies og trykke clear  


&#x200B;

[Betalingsmur er væk](https://preview.red
d.it/y42c40z4wwzb1.png?width=2410&format=png&auto=webp&s=f527e4dcc0aa397dd267cc5686ae4cc741952c7b)

Så forsvinder betali
ngs-muren?  


Det godt nok nogle cheep tricks de bruger nu om dage...  
Så [Avisendanmark.dk](https://Avisendanmark.dk)
 har ikke nogen betalingsmur... eller har de?  
Hvilke regler gælder så ift ophavsretten?   

```
---

     
 
all -  [ Create chat bot using custom data for company ](https://www.reddit.com/r/LangChain/comments/17tjeak/create_chat_bot_using_custom_data_for_company/) , 2023-11-14-0909
```
I want to create customer service chatbot knowing stock-data for a company, so users can ask if product X is in stock in
 store Y. They should also be able to ask questions regarding their previous purchases and other personal information. 


How would I do to implement this?
```
---

     
 
all -  [ How to use filters in MongoDB Atlas Vector Search? (Using js & langchain) ](https://www.reddit.com/r/LangChain/comments/17tiwth/how_to_use_filters_in_mongodb_atlas_vector_search/) , 2023-11-14-0909
```
Hi :)

A similar question was asked [https://www.reddit.com/r/LangChain/comments/172evas/how\_to\_implement\_filters\_in
\_mongodb\_atlas\_vector/](https://www.reddit.com/r/LangChain/comments/172evas/how_to_implement_filters_in_mongodb_atlas
_vector/) here. I want to filter the results to only retrieve stuff for a specific “documentId” Adding the preFilter fro
m the post above gives me an error unfortunately. The error disappears when I remove the preFilter but I just get entrie
s from all documentId's. IDK very confusing.

For context heres my DB document:

    {
    _id: ObjectId()
    text: 'Yo
u can implement filters by...'
    embedding: Array
    documentId: '123'
    }

And my JS code:

    const retriever = 
await vectorStore.asRetriever({
        searchType: 'mmr',
        searchKwargs: {
          fetchK: 2,
          lambda
: 0.1
        },
        filter: {
            preFilter: {
                'compound': {
                    'must': [

                        {
                            'text': {
                                'path': 'documentId',
  
                              'query': '123'
                            }
                        }
                   
 ]
                }
            }
        }
      });

And I keep getting this error when I try the above 😭

    MongoS
erverError: Operand type is not supported for $vectorSearch: object

Any ideas? Any help is much appreciated 🙏🙏🙏
```
---

     
 
all -  [ OpenAI assistant like experience in Local LLM? ](https://www.reddit.com/r/LocalLLaMA/comments/17tgt1u/openai_assistant_like_experience_in_local_llm/) , 2023-11-14-0909
```
Hi, just checked out the OpenAI assistant API, gotta say it’s pretty neat. I know in Langchain there’s agent but still f
eeling the development experience is quite different, ie function calling vs tools. Would like to know if local LLM can 
also do things similar to assistant api?
```
---

     
 
all -  [ Best Architecture for Building Chatbot with Personal Data & Open Source Models ](https://www.reddit.com/r/LocalLLaMA/comments/17tgl66/best_architecture_for_building_chatbot_with/) , 2023-11-14-0909
```
What is currently your best setup for building and deploying a chatbot with open source models and personal data. 

Pers
onally I have tried LlamaIndex, GPT, Steamlit, chromadb & also worked with Langchain & GPT4ALL - but was wondering what 
the solution that worked best for you (combination of what opensource services)?
```
---

     
 
all -  [ Help me enhance my Resume! ](https://i.redd.it/plhqjes20vzb1.jpg) , 2023-11-14-0909
```
I am seeking out off-campus job opportunities in the fields of Al, Data Science, or Machine Learning. I am confident in 
my skills and experience, having maintained a good CGPA, actively participated in cultural and technical events, engaged
 in coding projects, and completed internships. However, I have not yet been able to secure a job through my college's p
lacement services(I am sitting only for jobs which are more than 16LPA or 20k$ in India). I am eager to improve my resum
e and enhance my chances of success in off-campus applications. I am open to making significant changes to my resume, in
cluding the template, to better showcase my qualifications and suitability for these roles.

I would be delighted to eng
age in interviews if there is an HR professional or someone in a reputable position within the Reddit community who coul
d provide a referral for me.
```
---

     
 
all -  [ Boost Your Startup's Outreach: Personalizing Emails with AI and Low-Code ](https://www.reddit.com/r/indiehackers/comments/17t5ltl/boost_your_startups_outreach_personalizing_emails/) , 2023-11-14-0909
```
Hello r/indiehackers community! In this article, I will explain how the AI framework LangChain can significantly enhance
 the quality of your cold email outreach by making it unique and personalized. I will also discuss how to automate this 
entire process with minimal costs using a low-code platform and share ready-made templates for a quick start.

## Person
alization vs Automation

There's a natural tension between personalization and automation. Non-personalized, generic ema
ils are easy to automate but often result in low engagement and conversion rates. In contrast, highly personalized email
s increase engagement but are difficult to automate.

https://preview.redd.it/blcxh39k7szb1.png?width=960&format=png&aut
o=webp&s=a287e2d7721b1d1fe1a796457deb9792c73f1538

Cold email platforms now help solve this issue with dynamic variables
 that add a personalized touch to automated emails. These variables act as placeholders for inserting personalized words
, lines, or paragraphs.

https://preview.redd.it/afdxsjin7szb1.png?width=960&format=png&auto=webp&s=a26af9678aad1b83d1f9
ea4154a02a9df2aadeed

Dynamic variables allow companies to balance personalization and automation efficiently. Today, we
'll create a LangChain scenario on the low-code platform Latenode to generate a customized cold email icebreaker for eac
h contact in our outreach database using the following tools:

* The free data enrichment tool ClearBit
* The free low-c
ode platform Latenode
* OpenAI's extremely cheap API.

## Step 1: enrich emails w/ ClearBit

Let's start with a Google S
heet containing basic email addresses. I've included some of my work emails as real examples (please refrain from sendin
g me personalized cold emails after reading this! :) )

https://preview.redd.it/roob27dp7szb1.png?width=960&format=png&a
uto=webp&s=3d5b1ead129209a24c89e4679f58e5e388f6bff7

First, we need to enrich these emails with data about the recipient
s. For our outreach, we need to know:

* The first name
* The company name
* The company description

You could manually
 visit each email domain to gather this information, but if you have hundreds or thousands of emails in your database, t
hat's not practical. Instead, we can automate this task using the low-code platform Latenode. We link our Google Sheet t
here and use the ClearBit API to fill in the missing information. Here's how it works:

https://preview.redd.it/rctvp3et
7szb1.png?width=960&format=png&auto=webp&s=2c98e6ed87bb7b79142244f3afde42ca30d94442

Don't worry! You don't have to crea
te everything from the beginning. Simply copy the scenario I provide at the end of this article. The basic steps of this
 automation are:

* Identify the rows that need enrichment.
* Extract the email from each row.
* Send the email to Clear
Bit and receive all the related information.
* Enter the required information back into the Google Sheet.

https://i.red
d.it/ny8cii9z7szb1.gif

That's it. We've enriched our emails with essential details like the company description. Now, l
et's craft a personalized icebreaker to kick off our cold emails and establish a personal connection right from the star
t.

## Step 2: generate personalized icebreaker w/ ChatGPT

Giving a compliment about what your recipient does at their 
workplace is the very least you can do. Additionally, you could tailor your outreach reason based on the company's profi
le. You can do this with another Latenode scenario, which you'll be able to copy later.

https://preview.redd.it/d160loi
68szb1.png?width=960&format=png&auto=webp&s=2ee16d4c4b6d454db62567d72c66d4f6dd94b981

Its main steps are:

* Retrieve th
e company description from your Google Sheet.
* Send this description to ChatGPT using the OpenAI API with a custom prom
pt tailored to your needs.
* Refine the AI-generated output with another request and a different prompt.
* Place the fin
al result in the row corresponding to the person you're reaching out to.

By doing this, we attach a personalized icebre
aker to each individual, creating another custom variable in addition to their first name and company name. This trio sh
ould suffice for a start. Let's look at how this functions:

https://i.redd.it/krmkuxc88szb1.gif

## Step 3: upload spre
adsheet to cold email platform w/ Apollo

First, download your spreadsheet as a CSV file. Then, upload it to your email 
platform as a new list. I'll demonstrate using Apollo, but the process is similar in other tools.

https://preview.redd.
it/5sig2dgd8szb1.png?width=960&format=png&auto=webp&s=12bbd38bac64988dd6ee207e9f69d6dc3a5f2eb1

The next steps are prett
y standard – map the fields and assign a variable to each. The key variable for us is the custom 'icebreaker' field.

ht
tps://preview.redd.it/dgtg4tde8szb1.png?width=960&format=png&auto=webp&s=f954b354e7d81166dd8b3a198593040989925276

Now, 
when composing an email for a prospect, it works like this:

https://preview.redd.it/4ibqcndh8szb1.png?width=960&format=
png&auto=webp&s=e58df9beeb2d88dc59f08e4e7dbaf3b891954fb7

That's all for now. You can adjust the prompts sent to GPT in 
your Latenode scenario to achieve any level of cold email customization. These Latenode templates are versatile for any 
cold outreach scenario, including personalized LinkedIn messages.

⭐  As I promised, here are the links to copy these sc
enarios: [**Data Enrichment**](https://www.notion.so/latenode/DATA-ENRICHMENT-d59d0d43bcea4f9bb3bbaa29dadcc718)  and [**
Icebreaker Generation**](https://www.notion.so/latenode/ICEBREAKERS-GENERATION-40ca832750f24512bdb61fcbf5d04ae7)

You ju
st need to paste them into [app.latenode.com](https://app.latenode.com) and input your API keys for ClearBit (which is f
ree) and OpenAI (which is very affordable). Latenode itself is also free and has a supportive community where the team i
s always ready to help with your automation journey
```
---

     
 
all -  [ Personalization of Cold Email Campaigns Using AI and Low Code - Free Scenario ](https://www.reddit.com/r/automation/comments/17t4vgj/personalization_of_cold_email_campaigns_using_ai/) , 2023-11-14-0909
```
Hello r/automation community! In this article, I will explain how the AI framework LangChain can significantly enhance t
he quality of your cold email outreach by making it unique and personalized. I will also discuss how to automate this en
tire process with minimal costs using a low-code platform and share ready-made templates for a quick start.

## Personal
ization vs Automation

There's a natural tension between personalization and automation. Non-personalized, generic email
s are easy to automate but often result in low engagement and conversion rates. In contrast, highly personalized emails 
increase engagement but are difficult to automate.

https://preview.redd.it/u38feyhk7szb1.png?width=960&format=png&auto=
webp&s=645a674476a5790f6be894bc1ce1aa7f5fe4b949

Cold email platforms now help solve this issue with dynamic variables t
hat add a personalized touch to automated emails. These variables act as placeholders for inserting personalized words, 
lines, or paragraphs.

https://preview.redd.it/repehm7n7szb1.png?width=960&format=png&auto=webp&s=812df6a554ee3dbd5d3409
144037e201346b1bad

Dynamic variables allow companies to balance personalization and automation efficiently. Today, we'l
l create a LangChain scenario on the low-code platform Latenode to generate a customized cold email icebreaker for each 
contact in our outreach database using the following tools:

* The free data enrichment tool ClearBit
* The free low-cod
e platform Latenode
* OpenAI's extremely cheap API.

## Step 1: enrich emails w/ ClearBit

Let's start with a Google She
et containing basic email addresses. I've included some of my work emails as real examples (please refrain from sending 
me personalized cold emails after reading this! :) )

https://preview.redd.it/tmzmjsop7szb1.png?width=960&format=png&aut
o=webp&s=ed08ad9845ced8e2feec1c7cc16649901b3512b5

First, we need to enrich these emails with data about the recipients.
 For our outreach, we need to know:

* The first name
* The company name
* The company description

You could manually v
isit each email domain to gather this information, but if you have hundreds or thousands of emails in your database, tha
t's not practical. Instead, we can automate this task using the low-code platform Latenode. We link our Google Sheet the
re and use the ClearBit API to fill in the missing information. Here's how it works:

https://preview.redd.it/ixhanmzs7s
zb1.png?width=960&format=png&auto=webp&s=37881c3faf207d98201807d5fd9fa5eb62abf1ea

Don't worry! You don't have to create
 everything from the beginning. Simply copy the scenario I provide at the end of this article. The basic steps of this a
utomation are:

* Identify the rows that need enrichment.
* Extract the email from each row.
* Send the email to ClearBi
t and receive all the related information.
* Enter the required information back into the Google Sheet.

https://i.redd.
it/vcrnwltz7szb1.gif

That's it. We've enriched our emails with essential details like the company description. Now, let
's craft a personalized icebreaker to kick off our cold emails and establish a personal connection right from the start.


## Step 2: generate personalized icebreaker w/ ChatGPT

Giving a compliment about what your recipient does at their wo
rkplace is the very least you can do. Additionally, you could tailor your outreach reason based on the company's profile
. You can do this with another Latenode scenario, which you'll be able to copy later.

https://preview.redd.it/ckrohb668
szb1.png?width=960&format=png&auto=webp&s=2620f953000b540e39eb6905b05f63913219fe19

Its main steps are:

* Retrieve the 
company description from your Google Sheet.
* Send this description to ChatGPT using the OpenAI API with a custom prompt
 tailored to your needs.
* Refine the AI-generated output with another request and a different prompt.
* Place the final
 result in the row corresponding to the person you're reaching out to.

By doing this, we attach a personalized icebreak
er to each individual, creating another custom variable in addition to their first name and company name. This trio shou
ld suffice for a start. Let's look at how this functions:

https://i.redd.it/o125met88szb1.gif

## Step 3: upload spread
sheet to cold email platform w/ Apollo

First, download your spreadsheet as a CSV file. Then, upload it to your email pl
atform as a new list. I'll demonstrate using Apollo, but the process is similar in other tools.

https://preview.redd.it
/d8rfix4d8szb1.png?width=960&format=png&auto=webp&s=f40a96de09f92a20c460324891ac4e2027e42ffd

The next steps are pretty 
standard – map the fields and assign a variable to each. The key variable for us is the custom 'icebreaker' field.

http
s://preview.redd.it/ahdh3vne8szb1.png?width=960&format=png&auto=webp&s=54d0a5975d196c992004a41cca5d2f6fed4291ac

Now, wh
en composing an email for a prospect, it works like this:

https://preview.redd.it/6pecg14h8szb1.png?width=960&format=pn
g&auto=webp&s=6c2688f0c20592d03e5ae68f27d9fe27d9016b80

That's all for now. You can adjust the prompts sent to GPT in yo
ur Latenode scenario to achieve any level of cold email customization. These Latenode templates are versatile for any co
ld outreach scenario, including personalized LinkedIn messages.

⭐  As I promised, here are the links to copy these scen
arios: [**Data Enrichment**](https://www.notion.so/latenode/DATA-ENRICHMENT-d59d0d43bcea4f9bb3bbaa29dadcc718)  and [**Ic
ebreaker Generation**](https://www.notion.so/latenode/ICEBREAKERS-GENERATION-40ca832750f24512bdb61fcbf5d04ae7)

You just
 need to paste them into [app.latenode.com](https://app.latenode.com) and input your API keys for ClearBit (which is fre
e) and OpenAI (which is very affordable). Latenode itself is also free and has a supportive community where the team is 
always ready to help with your automation journey
```
---

     
 
all -  [ GPT-4 vision utilities to enable web browsing ](https://www.reddit.com/r/LangChain/comments/17t4is5/gpt4_vision_utilities_to_enable_web_browsing/) , 2023-11-14-0909
```
Wanted to share our work on [Tarsier](https://github.com/reworkd/tarsier) here, an open source utility library that enab
les LLMs like GPT-4 and GPT-4 Vision to browse the web. The library helps answer the following questions:

* How do you 
map LLM responses back into web elements?
* How can you mark up a page for an LLM to better understand its action space?

* How do you feed a 'screenshot' to a text-only LLM?

We do this by tagging '*interactable*' elements on the page with 
an ID, enabling the LLM to connect actions to an ID which we can then translate back into web elements. We also use OCR 
to translate a page screenshot to a spatially encoded text string such that even a text only LLM can understand how to n
avigate the page.

View a demo and read more on GitHub: [https://github.com/reworkd/tarsier](https://github.com/reworkd/
tarsier). We also have a cookbook for how to create a web browsing agent on LangChain.
```
---

     
 
all -  [ Enhancing B2B Email Strategies: Cold Outreach Personalization with Low-Code and AI ](https://www.reddit.com/r/B2BSaaS/comments/17t3xup/enhancing_b2b_email_strategies_cold_outreach/) , 2023-11-14-0909
```
Hello r/B2BSaaS community! In this article, I will explain how the AI framework LangChain can significantly enhance the 
quality of your cold email outreach by making it unique and personalized. I will also discuss how to automate this entir
e process with minimal costs using a low-code platform and share ready-made templates for a quick start.

## Personaliza
tion vs Automation

There's a natural tension between personalization and automation. Non-personalized, generic emails a
re easy to automate but often result in low engagement and conversion rates. In contrast, highly personalized emails inc
rease engagement but are difficult to automate.

https://preview.redd.it/ygojqgzk7szb1.png?width=960&format=png&auto=web
p&s=66358639830760eafbe655b372b75fb94a275552

Cold email platforms now help solve this issue with dynamic variables that
 add a personalized touch to automated emails. These variables act as placeholders for inserting personalized words, lin
es, or paragraphs.

https://preview.redd.it/8qqnl9lm7szb1.png?width=960&format=png&auto=webp&s=a8642583e7f3b9a50ba37a515
2ab447c01c8c57f

Dynamic variables allow companies to balance personalization and automation efficiently. Today, we'll c
reate a LangChain scenario on the low-code platform Latenode to generate a customized cold email icebreaker for each con
tact in our outreach database using the following tools:

* The free data enrichment tool ClearBit
* The free low-code p
latform Latenode
* OpenAI's extremely cheap API.

## Step 1: enrich emails w/ ClearBit

Let's start with a Google Sheet 
containing basic email addresses. I've included some of my work emails as real examples (please refrain from sending me 
personalized cold emails after reading this! :) )

https://preview.redd.it/f8da7cbq7szb1.png?width=960&format=png&auto=w
ebp&s=1d521dad0568d227e40149f4f0eb2ef5e0186b40

First, we need to enrich these emails with data about the recipients. Fo
r our outreach, we need to know:

* The first name
* The company name
* The company description

You could manually visi
t each email domain to gather this information, but if you have hundreds or thousands of emails in your database, that's
 not practical. Instead, we can automate this task using the low-code platform Latenode. We link our Google Sheet there 
and use the ClearBit API to fill in the missing information. Here's how it works:

https://preview.redd.it/791ub2as7szb1
.png?width=960&format=png&auto=webp&s=81760eae96e1239d31a95941ca59d003a85631e2

Don't worry! You don't have to create ev
erything from the beginning. Simply copy the scenario I provide at the end of this article. The basic steps of this auto
mation are:

* Identify the rows that need enrichment.
* Extract the email from each row.
* Send the email to ClearBit a
nd receive all the related information.
* Enter the required information back into the Google Sheet.

https://i.redd.it/
tswv9kr08szb1.gif

That's it. We've enriched our emails with essential details like the company description. Now, let's 
craft a personalized icebreaker to kick off our cold emails and establish a personal connection right from the start.

#
# Step 2: generate personalized icebreaker w/ ChatGPT

Giving a compliment about what your recipient does at their workp
lace is the very least you can do. Additionally, you could tailor your outreach reason based on the company's profile. Y
ou can do this with another Latenode scenario, which you'll be able to copy later.

https://preview.redd.it/8vwchae58szb
1.png?width=960&format=png&auto=webp&s=b7c49ac64e878b8fa10bbadef8084dd661ecfc18

Its main steps are:

* Retrieve the com
pany description from your Google Sheet.
* Send this description to ChatGPT using the OpenAI API with a custom prompt ta
ilored to your needs.
* Refine the AI-generated output with another request and a different prompt.
* Place the final re
sult in the row corresponding to the person you're reaching out to.

By doing this, we attach a personalized icebreaker 
to each individual, creating another custom variable in addition to their first name and company name. This trio should 
suffice for a start. Let's look at how this functions:

https://i.redd.it/64xqx92a8szb1.gif

## Step 3: upload spreadshe
et to cold email platform w/ Apollo

First, download your spreadsheet as a CSV file. Then, upload it to your email platf
orm as a new list. I'll demonstrate using Apollo, but the process is similar in other tools.

https://preview.redd.it/36
e08ljc8szb1.png?width=960&format=png&auto=webp&s=45d82ce25afb7ea32cd2994c4efa9a7b5fd7799a

The next steps are pretty sta
ndard – map the fields and assign a variable to each. The key variable for us is the custom 'icebreaker' field.

https:/
/preview.redd.it/qeuwe87f8szb1.png?width=960&format=png&auto=webp&s=f3a7c2ed96ec0b3716496c8fafeaa1f21055a901

Now, when 
composing an email for a prospect, it works like this:

https://preview.redd.it/dl4lgxkg8szb1.png?width=960&format=png&a
uto=webp&s=0fd819d22e08227995f29fa0713639a47a91690a

That's all for now. You can adjust the prompts sent to GPT in your 
Latenode scenario to achieve any level of cold email customization. These Latenode templates are versatile for any cold 
outreach scenario, including personalized LinkedIn messages.

⭐  As I promised, here are the links to copy these scenari
os: [**Data Enrichment**](https://www.notion.so/latenode/DATA-ENRICHMENT-d59d0d43bcea4f9bb3bbaa29dadcc718)  and [**Icebr
eaker Generation**](https://www.notion.so/latenode/ICEBREAKERS-GENERATION-40ca832750f24512bdb61fcbf5d04ae7)

You just ne
ed to paste them into [app.latenode.com](https://app.latenode.com) and input your API keys for ClearBit (which is free) 
and OpenAI (which is very affordable). Latenode itself is also free and has a supportive community where the team is alw
ays ready to help with your automation journey
```
---

     
 
all -  [ Improving Your Email Campaigns: Personalizing Cold Outreach Emails with Low-Code and AI ](https://www.reddit.com/r/MarketingAutomation/comments/17t3tq9/improving_your_email_campaigns_personalizing_cold/) , 2023-11-14-0909
```
Hello r/MarketingAutomation community! In this article, I will explain how the AI framework LangChain can significantly 
enhance the quality of your cold email outreach by making it unique and personalized. I will also discuss how to automat
e this entire process with minimal costs using a low-code platform and share ready-made templates for a quick start.

##
 Personalization vs Automation

There's a natural tension between personalization and automation. Non-personalized, gene
ric emails are easy to automate but often result in low engagement and conversion rates. In contrast, highly personalize
d emails increase engagement but are difficult to automate.

https://preview.redd.it/r7cj2u6l7szb1.png?width=960&format=
png&auto=webp&s=13f091f50aec4e617d8c51b20280ddc5d6ade47c

Cold email platforms now help solve this issue with dynamic va
riables that add a personalized touch to automated emails. These variables act as placeholders for inserting personalize
d words, lines, or paragraphs.

https://preview.redd.it/49hd8e9m7szb1.png?width=960&format=png&auto=webp&s=850898d634493
940434a0105e09d4542d0a85eb5

Dynamic variables allow companies to balance personalization and automation efficiently. To
day, we'll create a LangChain scenario on the low-code platform Latenode to generate a customized cold email icebreaker 
for each contact in our outreach database using the following tools:

* The free data enrichment tool ClearBit
* The fre
e low-code platform Latenode
* OpenAI's extremely cheap API.

## Step 1: enrich emails w/ ClearBit

Let's start with a G
oogle Sheet containing basic email addresses. I've included some of my work emails as real examples (please refrain from
 sending me personalized cold emails after reading this! :) )

https://preview.redd.it/92j53ilq7szb1.png?width=960&forma
t=png&auto=webp&s=fd79ee8fc8bff7da8fb26d4efde13feb406955e5

First, we need to enrich these emails with data about the re
cipients. For our outreach, we need to know:

* The first name
* The company name
* The company description

You could m
anually visit each email domain to gather this information, but if you have hundreds or thousands of emails in your data
base, that's not practical. Instead, we can automate this task using the low-code platform Latenode. We link our Google 
Sheet there and use the ClearBit API to fill in the missing information. Here's how it works:

https://preview.redd.it/q
meuppur7szb1.png?width=960&format=png&auto=webp&s=041b739ec18d50c01f6da77d8e86892fab147ea4

Don't worry! You don't have 
to create everything from the beginning. Simply copy the scenario I provide at the end of this article. The basic steps 
of this automation are:

* Identify the rows that need enrichment.
* Extract the email from each row.
* Send the email t
o ClearBit and receive all the related information.
* Enter the required information back into the Google Sheet.

https:
//i.redd.it/hzebu2618szb1.gif

That's it. We've enriched our emails with essential details like the company description.
 Now, let's craft a personalized icebreaker to kick off our cold emails and establish a personal connection right from t
he start.

## Step 2: generate personalized icebreaker w/ ChatGPT

Giving a compliment about what your recipient does at
 their workplace is the very least you can do. Additionally, you could tailor your outreach reason based on the company'
s profile. You can do this with another Latenode scenario, which you'll be able to copy later.

https://preview.redd.it/
s3wgefx48szb1.png?width=960&format=png&auto=webp&s=d9524a8529629c0ea023a29c5acff69c5677d56a

Its main steps are:

* Retr
ieve the company description from your Google Sheet.
* Send this description to ChatGPT using the OpenAI API with a cust
om prompt tailored to your needs.
* Refine the AI-generated output with another request and a different prompt.
* Place 
the final result in the row corresponding to the person you're reaching out to.

By doing this, we attach a personalized
 icebreaker to each individual, creating another custom variable in addition to their first name and company name. This 
trio should suffice for a start. Let's look at how this functions:

https://i.redd.it/azgpiuma8szb1.gif

## Step 3: uplo
ad spreadsheet to cold email platform w/ Apollo

First, download your spreadsheet as a CSV file. Then, upload it to your
 email platform as a new list. I'll demonstrate using Apollo, but the process is similar in other tools.

https://previe
w.redd.it/4rkv8s4c8szb1.png?width=960&format=png&auto=webp&s=60be19d820446ce82f38031a58532e27c45ed7d5

The next steps ar
e pretty standard – map the fields and assign a variable to each. The key variable for us is the custom 'icebreaker' fie
ld.

https://preview.redd.it/cbpdpzgf8szb1.png?width=960&format=png&auto=webp&s=8463653c4e7207a76e3f5af6922d8bb4a74e1f9b


Now, when composing an email for a prospect, it works like this:

https://preview.redd.it/zg8kazbg8szb1.png?width=960&
format=png&auto=webp&s=609f8ee98af45b7bbbdf01ad15de94ee658b692a

That's all for now. You can adjust the prompts sent to 
GPT in your Latenode scenario to achieve any level of cold email customization. These Latenode templates are versatile f
or any cold outreach scenario, including personalized LinkedIn messages.

⭐  As I promised, here are the links to copy t
hese scenarios: [**Data Enrichment**](https://www.notion.so/latenode/DATA-ENRICHMENT-d59d0d43bcea4f9bb3bbaa29dadcc718)  
and [**Icebreaker Generation**](https://www.notion.so/latenode/ICEBREAKERS-GENERATION-40ca832750f24512bdb61fcbf5d04ae7)


You just need to paste them into [app.latenode.com](https://app.latenode.com) and input your API keys for ClearBit (whi
ch is free) and OpenAI (which is very affordable). Latenode itself is also free and has a supportive community where the
 team is always ready to help with your automation journey
```
---

     
 
all -  [ 🛍️ GPT Store on Github - Add your GPT or find others. :) - Agent maintaining a library of GPT agents ](https://www.reddit.com/r/LangChain/comments/17sy8nb/gpt_store_on_github_add_your_gpt_or_find_others/) , 2023-11-14-0909
```
🔗 **Link to the Agent:** [Chat with the Agent](https://chat.openai.com/g/g-PZ6cOScM4-gpt-store) 🤖

🛠️ **Uses GitHub API 
calls to the repository:** [GPT-Store](https://github.com/prajwalsouza/GPT-Store) 📦

➕ **To add a new agent, simply prov
ide the required details, and voila! Changes will be reflected in about 20 seconds. 🚀**  


Inspired by : [https://githu
b.com/Anil-matcha/Awesome-GPT-Store](https://github.com/Anil-matcha/Awesome-GPT-Store)
```
---

     
 
all -  [ If You Are Testing The Assitants API... Watch Out When Migrating Your OpenAI Python SDK to Version 1 ](https://www.reddit.com/r/ChatGPTCoding/comments/17sus6s/if_you_are_testing_the_assitants_api_watch_out/) , 2023-11-14-0909
```
I'm sure other folks have posted about this...

But I just broke my RAG chatbot by upgrading to OpenAI v1.2, I have sinc
e reverted to version 0.28, and its all working again.

**Version 1.2 and Assistants API**

I was trying to build some s
tuff with new Assistants API, and have been having a headache.

Then I read in the documentation that they require the O
penAI python SDK 1.2

What the Git Says:

*The SDK was rewritten in v1, which was released November 6th 2023. See the v1
 migration guide, which includes scripts to automatically update your code.*

[https://github.com/openai/openai-python#o
penai-python-api-library](https://github.com/openai/openai-python#openai-python-api-library)

So I ran  *pip install* \-
-*upgrade openai* without looking at the fineprint.

This is a pretty major re-write which didn't work with my langchain
 embeddings, someone else has flagged this here: [https://github.com/langchain-ai/langchain/issues/13162](https://github
.com/langchain-ai/langchain/issues/13162)

I have since reverted with a simple... pip install openai==0.28

And it's wor
king again.

I know I'll have to migrate my code eventually for that project, but not in the mood for it today.

&#x200B
;
```
---

     
 
all -  [ Will customized GPTs make Langchain redundant? ](https://www.reddit.com/r/OpenAI/comments/17sswvq/will_customized_gpts_make_langchain_redundant/) , 2023-11-14-0909
```
I've written a Python program using my GPT4 API and Langchain so my agent can learn a bunch of PDF files and respond to 
my requests. 

I've cancelled my plus account. For those with experience with the new customized GPTs, will other tools 
like Langchain be made redundant? 
```
---

     
 
all -  [ Developing on top of LLMs ](https://www.reddit.com/r/LargeLanguageModels/comments/17sr9fk/developing_on_top_of_llms/) , 2023-11-14-0909
```
Hi! I am getting into LLM dev and was wondering what the most common workflows were in LLM development. How do people pr
ototype, test and version prompts? Has langchain been good (I personally haven't liked it much)?
```
---

     
 
all -  [ LangChain with APIChain with a big swagger file ](https://www.reddit.com/r/LangChain/comments/17sm7z0/langchain_with_apichain_with_a_big_swagger_file/) , 2023-11-14-0909
```
I have my own swagger apis, which I want to langchain to use APIChain to invoke.

&#x200B;

But there is a concern, the 
swagger file seems to be sent to openai functioning call each time when there is a input, how to avoid this, as the swag
ger file is a bit big with almost 10k token...

&#x200B;

Thanks for any advice in advance.
```
---

     
 
all -  [ Learn fine-tuning model ](https://www.reddit.com/r/LangChain/comments/17shv69/learn_finetuning_model/) , 2023-11-14-0909
```
 The alternative to RAG is fine-tuning model with my data, then naturally I'd keep an eye on modeling at least fine-tuni
ng and related AI infra stuff. Then here's my questions:

1. From perspective of future career, should we also learn and
 play with modeling/AI infra and even consider moving to such modeling team at industry? Anyone has such experiences?
2.
 How can I get my hands wet with LLM modeling in my spare time? It sounds almost impossible to train such model on my ow
n without GPU resources. After quick search seems we can try to train on colab which provides free GPU as fine-tuning LL
M is much less computationally intensive.  
Any suggestions appreciated!
```
---

     
 
all -  [ Supabase-like functionality on Postgres DB ](https://www.reddit.com/r/Supabase/comments/17sfpxr/supabaselike_functionality_on_postgres_db/) , 2023-11-14-0909
```
Hi--

I want to use Supabase at work but can't. However, I love how it works for embeddings since I'm using it on multip
le side projects. 

Is there a way to insert vectors into a vector table in Postgres (PGVector is enabled) similar to ho
w they are entered into Supabase tables? With Langchain, you're forced to declare a 'collection\_name' with PGVector sto
re, but I want the functionality that Supabase provides since I really like the table-based mechanism in place. So with 
Langchain's Supabase vector store, you declare the table\_name instead, and then you can just insert rows. 

Thanks for 
the ideas. 
```
---

     
 
all -  [ Open source evaluations for AI Agents in web tasks ](https://www.reddit.com/r/LangChain/comments/17sa52a/open_source_evaluations_for_ai_agents_in_web_tasks/) , 2023-11-14-0909
```
Recently created Banana-lyzer, an open source AI Agent evaluation framework and dataset for web tasks with Playwright (A
nd has a banana theme because why not) and would love to get feedback/support. There are a few issues with existing eval
s repos:

* Websites change overtime, are affected by latency, and may have anti bot protections. We need a system that 
can reliably save and deploy historic/static snapshots of websites.
* Standard web practices are loose and there is an a
bundance of different underlying ways to represent a single individual website. For an agent to best generalize, we requ
ire building a diverse dataset of websites across industries and use-cases.
* We have specific evaluation criteria and a
gent use cases focusing on structured and direct information retrieval across websites.
* There exists valuable web task
 datasets and evaluations that we'd like to unify in a single repo (Mind2Web, WebArena, etc).

This should integrate ver
y easily with any LangChain agent using Playwright. Read more here: [https://github.com/reworkd/bananalyzer](https://git
hub.com/reworkd/bananalyzer)
```
---

     
 
all -  [ AI — weekly megathread! ](https://www.reddit.com/r/artificial/comments/17s9s6f/ai_weekly_megathread/) , 2023-11-14-0909
```
**News** provided by [aibrews.com](https://aibrews.com/)

 

1. OpenAI’s **DevDay** announcements \[Details: \[[1](https
://openai.com/blog/introducing-gpts)\] and \[[2](https://openai.com/blog/new-models-and-developer-products-announced-at-
devday)\], [Keynote Video](https://www.youtube.com/watch?v=U9mJuUkhUzk)\]:
   1. New **GPT-4 Turbo** model: 128K context
 window, improved instruction following, 3x cheaper price for input tokens and a 2x cheaper price for output tokens comp
ared to GPT-4.
   2. **GPTs**: Custom versions of ChatGPT that users can create and share for a specific purpose using n
atural language. Users can also define custom actions by making one or more APIs available to the GPT allowing GPTs to i
ntegrate external data or interact with the real-world.
   3. **GPT Store**: a searchable store for GPTs rolling out lat
er this month with monetization for creators in the coming months.
   4. GPT-4 Turbo can accept images as inputs in the 
Chat Completions API, enabling use cases such as generating captions, analyzing real world images in detail, and reading
 documents with figures.
   5. New **Assistants API** that makes it easier for developers to build their own AI agent ap
ps that have goals and can call models and tools (Code Interpreter, Retrieval, and Function calling). Developers don’t n
eed to compute and store embeddings for their documents, or implement chunking and search algorithms.
   6. New **TTS(te
xt-to-speech) model** that offers six preset voices to choose from and two model variants, *tts-1* and *tts-1-hd*. *tts-
1* is optimized for real-time use cases and tts-1-hd is optimized for quality.
   7. [Whisper large-v3,](https://github.
com/openai/whisper) the next version of OpenAI’s open source automatic speech recognition model (ASR) which features imp
roved performance across languages.
   8. DALL·E 3 API
   9. ChatGPT Plus now includes fresh information up to **April 2
023**.
   10. Improvements in ‘**Function Calling**’: improved accuracy and ability to call multiple functions in a sing
le message: users can send one message requesting multiple actions
   11. Lower prices and higher rate limits for models
.
   12. Copyright Shield: OpenAI will pay the costs incurred, in case of legal claims around copyright infringement for
 customers of generally available features of ChatGPT Enterprise and developer platform.
   13. Enterprise customers can
 deploy internal-only GPTs
2. Researchers from **Stanford** University present ***NOIR (Neural Signal Operated Intellige
nt Robots)***, a general-purpose, intelligent brain-robot interface system that enables humans to command robots to perf
orm everyday activities through brain signals. Researchers demonstrated its success through 20 challenging, everyday hou
sehold activities, including cooking, cleaning, personal care, and entertainment \[[*Details*](https://noir-corl.github.
io/)\].
3. **01.AI** has released ***Yi-34B***, a 34-billion parameter open-source LLM with 200K context length that out
performs much larger models like LLaMA2-70B and Falcon-180B. Developers can apply for free commercial use \[[*Details*](
https://01.ai/)\].
4. **Humane** has officially revealed the ***Ai Pin***, a screenless AI wearable equipped with a Snap
dragon processor powered by OpenAI model. Users can speak to it naturally, use the intuitive touchpad, hold up objects, 
use gestures, or interact via the pioneering Laser Ink Display projected onto their palm \[[*Details*](https://mashable.
com/article/humane-launches-ai-pin-screenless-wearable-powered-openai) *|* [*Specs*](https://hu.ma.ne/aipin/details)\].

5. **Cohere** released a new embedding model, ***Embed v3*** that delivers compressed embeddings to save on storage cost
s and robustness to noisy datasets. The multilingual models support 100+ languages and can be used to search within a la
nguage (e.g., search with a French query on French documents) and across languages (e.g., search with a Chinese query on
 Finnish documents) \[[*Details*](https://txt.cohere.com/introducing-embed-v3)\].
6. Elon Musk’s **xAI** announced ***Gr
ok*** \- a ChatGPT alternative having ‘wit and rebellious streak’ and powered by Grok-1. It has real-time knowledge of t
he world via the X/Twitter. Grok is available to a limited number of users in the US. \[[*Details*](https://x.ai/)\].
7.
 **Snap** is releasing a new version of its AR development tool, called the ***Lens Studio 5.0 Beta*** that includes a C
hatGPT API and a 3D face mask generator that combines generative AI and Snap’s face mesh capabilities \[[*Details*](http
s://techcrunch.com/2023/11/09/snaps-latest-version-of-its-ar-development-tool-includes-a-chatgpt-api-boosted-productivit
y-and-more)\].
8. **Fakespot Chat**, Mozilla’s first LLM, lets online shoppers research products via an AI chatbot \[[*D
etails*](https://techcrunch.com/2023/11/08/fakespot-chat-mozillas-first-llm-lets-online-shoppers-research-products-via-a
n-ai-chatbot/)\].
9. **GitHub** announced integrating G***itHub Copilot Chat*** directly into github.com, the general av
ailability of GitHub Copilot Chat in December 2023, new GitHub Copilot Enterprise offering, new AI-powered security feat
ures, and the GitHub Copilot Partner Program \[[*Details*](https://github.blog/2023-11-08-universe-2023-copilot-transfor
ms-github-into-the-ai-powered-developer-platform/)\].
10. **OpenAI** is introducing ***OpenAI Data Partnerships***, to w
ork together with organizations to produce public and private datasets for training AI models \[[*Details*](https://open
ai.com/blog/data-partnerships)\].
11. **xAI** announced ***PromptIDE***, a code editor and a Python SDK to give access t
o Grok-1, the model that powers Grok. The SDK provides a new programming paradigm with features for complex prompting te
chniques \[[*Details*](https://x.ai/prompt-ide)\].
12. Researchers present ***CogVLM***, an open-source visual language 
model (VLM). CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. and achieves state-of-the-ar
t performance on 10 classic cross-modal benchmarks \[[*Details*](https://github.com/THUDM/CogVLM)\].
13. **LangChain** r
eleased **OpenGPTs**, an open source alternative to OpenAI's GPTs \[[*Details*](https://github.com/langchain-ai/opengpts
)\].
14. **Samsung** unveiled its generative AI model ***Samsung*** ***Gauss***. Samsung Gauss consists of language, cod
e, and image models and will be applied to the company's various products in the future \[[*Details*](https://www.zdnet.
com/article/samsung-unveils-its-generative-ai-model-samsung-gauss/)\].
15. **Google** is bringing its AI-powered search 
to more than 120 new countries and territories \[[*Details*](https://www.theverge.com/2023/11/8/23951134/google-search-g
enerative-experience-sge-expansion-120-countries-territories)\].
16. **ElevenLabs** launched **Eleven Turbo v2 -** their
 fastest fastest Text-To-Speech model having \~400ms latency \[[*Details*](https://elevenlabs.io/turbo)\].
17. **DeepSee
k AI** released ***DeepSeek Coder***, open-source SOTA large coding models with params ranging from 1.3B to 33B. Free fo
r commercial use \[[*Details*](https://deepseekcoder.github.io/)\].
18. **Figma** has added a suite of generative AI fea
tures to its FigJam whiteboarding software to help users produce, summarize, and sort meeting content \[[*Details*](http
s://www.computerworld.com/article/3709972/whiteboarding-platform-figjam-gets-new-ai-powered-capabilities.html)\].
19. **
YouTube** to test generative AI features, including a comments summarizer and conversational tool \[[*Details*](https://
techcrunch.com/2023/11/06/youtube-to-test-generative-ai-features-including-a-comments-summarizer-and-conversational-tool
)\].
20. Google **Bard** introduces “Human reviewers,” sparking privacy concerns over conversation monitoring \[[*Detail
s*](https://techstartups.com/2023/10/23/google-bard-now-includes-human-reviewers-who-may-read-your-conversations-dont-en
ter-sensitive-info-google-says)\].
21. **Luminance** showcases the first fully automated AI-driven contract negotiation 
using its large language model, trained on 150 million legal documents \[[*Details*](https://www.luminance.com/news/pres
s/20231107_luminance_showcases.html)\]

#### 🔦 Weekly Spotlight

1. *Sharing screen with GPT 4 vision model and asking q
uestions to guide through blender* \[[*Link*](https://www.loom.com/share/9458bcbf79784162aa62ffb8dd66201b)\].
2. *OpenAI
 Assistants API vs Canopy: A Quick Comparison \[*[*Link*](https://www.pinecone.io/learn/assistants-api-canopy/)*\].*
3. 
*Create custom versions of ChatGPT with GPTs and Zapier \[*[*Link*](https://zapier.com/blog/gpt-assistant/)*\].* 

\- - 
-

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new
 models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starte
rs for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo
 is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you wil
l be banned.

[Previous Megathreads](https://www.reddit.com/r/artificial/search/?q=author%3Ajaketocake%20megathread&rest
rict_sr=1) & [Subreddit revamp and going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_b
e_enforced_selfpromotion_is_only/)
```
---

     
 
MachineLearning -  [ [P] GPT vs. StarCraft ](https://www.reddit.com/r/MachineLearning/comments/17ro6el/p_gpt_vs_starcraft/) , 2023-11-14-0909
```
This is the first in a series of webcasts covering the development and experimentation of using GPT algorithms, LangChai
n and Python to control the high-level strategy of a StarCraft II bot. I’ll be running through the basics of the impleme
ntation, discussing the use of prompts and prompt engineering, and demonstrating the implementation in action.

[https:/
/youtu.be/E3Sj2L6ZnXA](https://youtu.be/E3Sj2L6ZnXA)
```
---

     
 
MachineLearning -  [ [D] Is this close enough to be usable? Need your inputs: Automated RAG testing tool. AI Data Pipelin ](https://www.reddit.com/r/MachineLearning/comments/17kkbm0/d_is_this_close_enough_to_be_usable_need_your/) , 2023-11-14-0909
```
Hey there, Redditors! 

I'm back with the latest installment on creating dependable AI data pipelines for real-world pro
duction. 

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://t
opoteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba4
0aab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines. 

With 18 months of hands
-on experience and many user interviews, I realized that with the probabilistic nature of systems, we need better\_testi
ng.gpt:

  
**1. As you build you should test**  
The world of AI is a fast-moving one, and we've realized that just wor
king on systems is not an optimal design choice. By the time your product ships, it might already be using outdated tech
nology. So, what's the lesson here? Embrace change, test along, but be prepared to switch pace.  
**2. No Best Practices
 Yet for RAGs**  
In this rapidly evolving landscape, there are no established best practices. You'll need to make educa
ted bets on tools and processes, knowing that things will change. With the RAG testing tool, I tried allowing for testin
g many potential parameter combinations **automatically**  
**3. Testing Frameworks**  
If your generative AI product do
esn't have users giving feedback, then you are building in isolation. I used [Deepeval](https://github.com/confident-ai/
deepeval) to generate test sets, and they will soon support synthetic test set generation  
**4. Infographics only go so
 far**  
AI researchers and data scientists, while brilliant, end up in a loop of pursuing Twitter promotional content. 
New ways are promoted via new content pieces, but ideally, we need something above simple tracing but less than full-fle
dged analytics. To do this, I stored test outputs in Postgres and created a Superset instance to visualize the results  

**5. Bridging the Gap between VectorDBs**  
There's a noticeable number of Vector DBs. To ensure smooth product develop
ment, we need to be able to switch to best best-performing one, especially since user interviews signal that they might 
start deteriorating after loading 50 million rows

&#x200B;

Github repo is [here](https://topoteretes.notion.site/Going
-beyond-Langchain-Weaviate-Level-3-towards-production-e62946c272bf412584b12fbbf92d35b0?pvs=4)  


Next steps:  
I have q
uestions for you: 

1. What variables do you change when building RAGs?
2. What is the set of strategies I should add to
 the solution? (parent-son etc.)
3. How can I improve it in general? 
4. Is anyone  interested in a leaderboard for best
 parameter configs?

Check out the blog post:

[Link to part 3](https://topoteretes.notion.site/Going-beyond-Langchain-W
eaviate-Level-3-towards-production-e62946c272bf412584b12fbbf92d35b0?pvs=4)

  
*Remember to give this post an upvote if 
you found it insightful!*  
*And also star our* [*Github repo*](https://github.com/topoteretes/PromethAI-Memory)
```
---

     
 
MachineLearning -  [ [D] Relevance Extraction in RAG Pipelines ](https://www.reddit.com/r/MachineLearning/comments/17k6iha/d_relevance_extraction_in_rag_pipelines/) , 2023-11-14-0909
```
I came across this interesting problem in RAG, what I call **Relevance Extraction**.

After retrieving relevant document
s (or chunks), these chunks are often large and may contain several portions **irrelevant** to the query at hand. Stuffi
ng the entire chunk into an LLM prompt impacts token-cost as well as response accuracy (distracting the LLM with irrelev
ant text), and and can also cause bumping into context-length limits.

So a critical step in most pipelines is **Relevan
ce Extraction**: use the LLM to extract **verbatim** only the portions relevant to the query. This is known by other nam
es, e.g. LangChain calls it Contextual Compression, and the RECOMP paper calls it Extractive Compression [https://twitte
r.com/manelferreira\_/status/1713214439715938528](https://twitter.com/manelferreira_/status/1713214439715938528)

Thinki
ng about how best to do this, I realized it is **highly inefficient** to simply ask the LLM to 'parrot' out relevant por
tions of the text: this is obviously slow, and also consumes valuable token generation space and can cause you to bump i
nto context-length limits (and of course is expensive, e.g. for gpt4 we know generation is 6c/1k tokens vs input cost of
 3c/1k tokens).

I realized the best way (or at least a good way) to do this is to **number** the sentences and have the
 LLM simply spit out the relevant sentence **numbers.** Langroid's unique Multi-Agent + function-calling architecture al
lows an elegant implementation of this, in the RelevanceExtractorAgent ([https://github.com/langroid/langroid/blob/main/
langroid/agent/special/relevance\_extractor\_agent.py](https://github.com/langroid/langroid/blob/main/langroid/agent/spe
cial/relevance_extractor_agent.py)).  The agent annotates the docs with sentence numbers, and instructs the LLM to pick 
out the **sentence-numbers** relevant to the query, rather than whole sentences using a function-call (SegmentExtractToo
l [https://github.com/langroid/langroid/blob/main/langroid/agent/tools/segment\_extract\_tool.py](https://github.com/lan
groid/langroid/blob/main/langroid/agent/tools/segment_extract_tool.py)), and the agent's function-handler interprets thi
s message and strips out the indicated sentences by their numbers. To extract from a set of passages, langroid automatic
ally does this async + concurrently so latencies in practice are much, much lower than the sentence-parroting approach.


\[FD -- I am the lead dev of Langroid - [https://github.com/langroid/langroid](https://github.com/langroid/langroid))


I thought this **numbering** idea is a fairly obvious idea in theory, so I looked at LangChain's equivalent `LLMChainExt
ractor` (they call this Contextual Compression [https://python.langchain.com/docs/modules/data\_connection/retrievers/co
ntextual\_compression?ref=blog.langchain.dev](https://python.langchain.com/docs/modules/data_connection/retrievers/conte
xtual_compression?ref=blog.langchain.dev)) and was surprised to see it is the simple '**parrot**' method, i.e. the LLM w
rites out whole sentences verbatim from its input. I thought it would be interesting to compare Langroid vs LangChain, y
ou can see it in this Colab: [https://colab.research.google.com/drive/1RDPCR2xNuBffcmpUuPIXYDRG3SXIJC5F](https://colab.r
esearch.google.com/drive/1RDPCR2xNuBffcmpUuPIXYDRG3SXIJC5F)

On the specific example in the notebook, the Langroid **num
bering** approach is 22x faster and 36% cheaper (with gpt4) than LangChain's **parrot** method (I promise this name is *
not* inspired by their logo :). See table below.

&#x200B;

[Relevance Extraction: Langroid vs LangChain](https://previe
w.redd.it/1m7u6ulq8fxb1.png?width=1108&format=png&auto=webp&s=d2f35cf5db07e2e699baa54b274ffa60833e924a)

&#x200B;

I won
der if anyone had thoughts on relevance extraction, or other approaches. At the very least, I hope langroid's implementa
tion is useful to you -- you can use the `DocChatAgent.get_verbatim_extracts()` ([https://github.com/langroid/langroid/b
lob/main/langroid/agent/special/doc\_chat\_agent.py#L804](https://github.com/langroid/langroid/blob/main/langroid/agent/
special/doc_chat_agent.py#L804)) as part of your pipeline, regardless of whether you are using Langroid for your entire 
system or not.

&#x200B;
```
---

     
 
MachineLearning -  [ [R] Model Troubles ](https://www.reddit.com/r/MachineLearning/comments/17ikh2u/r_model_troubles/) , 2023-11-14-0909
```
So i’m working on a model that diagnoses alzheimer’s disease and suggests medication depending on how severe the symptom
s might have become 
I’m using the Openai API and Langchain.

But it’s dumb and it doesn’t learn (
Me: I forgot my keys 
at home
Model: Yup, Alzheimer’s)
How do i incorporate the actual machine learning

Edit: I didn’t choose this project my
 supervisor did and she barely knows anything about the topic or how to approach it
```
---

     
 
MachineLearning -  [ [P] NexaAgent: A highly efficient multi-task PDF tool for all your needs | backed by AutoGen ](https://www.reddit.com/r/MachineLearning/comments/17eajz2/p_nexaagent_a_highly_efficient_multitask_pdf_tool/) , 2023-11-14-0909
```
Just a quick open-source project recently submitted to huggingface backed by AutoGen. Share this initial version with yo
u guys!

[NexaAgent 0.0.1](https://huggingface.co/spaces/xuyingliKepler/nexaagent) offers a straightforward solution for
 handling PDFs.

* Users can easily upload any PDF, regardless of its size.
* The tool emphasizes accuracy, minimizing d
iscrepancies in PDF processing.

At its core, NexaAgent is backed by the AutoGen and LangChain frameworks. AutoGen facil
itates multi-agent interactions for task execution, while LangChain bridges LLMs with external data sources. Together, t
hese technologies ensure NexaAgent's robust and precise PDF management capabilities.

https://preview.redd.it/kwgo3phnav
vb1.jpg?width=1440&format=pjpg&auto=webp&s=1c5fbc566938d60d5c43802aff3a0690821e1c79
```
---

     
 
MachineLearning -  [ [D] Is lang chain the right solution? ](https://www.reddit.com/r/MachineLearning/comments/17coyym/d_is_lang_chain_the_right_solution/) , 2023-11-14-0909
```
Hello, I would love to have an LLm that can provide answers (in chat format) based some of the sql db  data we have. Wan
t it for an internal company project. I am by no means an expert but decent in programming and want to build a system to
 get answers in chat format. My understanding is that training LLMs ground up is prohibitively expensive and langchains 
are sort of hybrid , efficient solutions. 

Please suggest any other solutions. Also would Langchain being a company and
 not open source pose a problem in terms of copyrights? Thanks!
```
---

     
 
MachineLearning -  [ [P] building a D&D NPC ](https://www.reddit.com/r/MachineLearning/comments/17clyw6/p_building_a_dd_npc/) , 2023-11-14-0909
```
Hey everyone,

I'm learning ML but i'm barely scratching the terminologies. 2 years ago I couldn't code anything but wit
h school (python,sql and R) I learned fundamentals. I also have access to code academy.  My current program is very mach
ine learning/deep learning focused.

On the side I DM a d&d game. Within the context of the world (eberron) robots are c
ommon. With my ADHD and being a new DM I want to outsource lore questions might have (that I would have to look up and s
low down the game).

The concept is to have a GUI and have the player interact with the chat bot. I've gotten to a proof
 of concept workflow. On Google colab. Thanks to langchain I managed to ingest pdfs and a url. Make then a directory, Em
bedded the text, bring it into a vector dB. Have the llm pull from the vector. Answer the question.

Now I don't know wh
at to do. I tried to bring the colab notebook onto Google cloud. But now cloud is becoming a rabbit home with vertex and
 docAI...and I don't want to deep dive into that, if it's a outside the scope of this 'project'

I'd appreciate any advi
ce, links...etc. 


I got a limited success in botpress using a single pdf. It works but feel unsatisfying.
N8N looks pr
omising but if it's not intuitive then I don't want to go down that road.


If I posted in the wrong group please direct
 me to the correct one.
```
---

     
 
MachineLearning -  [ [D] Exploring Methods to Improve Text Chunking in RAG Models (and other things...) ](https://www.reddit.com/r/MachineLearning/comments/179j7l3/d_exploring_methods_to_improve_text_chunking_in/) , 2023-11-14-0909
```
Hello everyone,

I'm currently working on Retrieval Augmented Generation (RAG) models and have developed a custom chunki
ng function, as I found the methods in LangChain not entirely satisfactory.

I'm keen on exploring other methods, algori
thms (related to NLP or otherwise), and models to enhance text chunking in RAG. There are many RAG implementations out t
here, but I've noticed a lack of focus on improving chunking performance specifically.

Are there any other promising ap
proaches beyond my current pipeline, which consists of a bi-encoder (retriever), cross-encoder (reranker), and a Large L
anguage Model (LLM) for interactions?

For queries, I'm using both traditional and HyDE (Hypothetical Document Embedding
) approaches in the retrieval phase, and sending the top 'n' results of both similarity search to the reranker.

I've al
so tried using an LLM to convert the query into a series of 10-20 small phrases or keywords, which are then used as the 
query for the retriever model. However, the results vary depending on the LLM used. To generate good keywords (with a no
t extractive approach) , I had to  use a 'CoT' prompt, instructing the model to  write self-instruct, problem analysis a
nd reasonings before generating the required keywords. But this approach use lots of tokens, and requires careful scrapi
ng to ensure the model has used the right delimiter to separate reasoning and the actual answer.

I'm also planning to m
odify the text used to generate embeddings, while returning the original text after the recall phase. But this is still 
a work in progress and scaling it is proving to be a challenge. If anyone has any tips or experience with this, I'd appr
eciate your input.

I'd be grateful for any resources, repositories, libraries, or existing implementations of novel chu
nking methods that you could share. Or we could just discuss ideas, thoughts, or approaches to improve text chunking for
 RAG here.

Thanks in advance for your time!
```
---

     
 
deeplearning -  [ Error with Mistral 7B model in ConversationalRetrievalChain ](https://www.reddit.com/r/deeplearning/comments/179vvou/error_with_mistral_7b_model_in/) , 2023-11-14-0909
```
 I'm encountering an issue while using the Mistral 7B model in a ConversationalRetrievalChain. When I input a question, 
such as 'What is the highest GDP?', I receive an error and after that the model generates a random response as output wh
ich is not relevant to the Input query. It seems that the number of tokens in the input exceeds the maximum context leng
th. 

Here's the relevant code: 

 

>`from langchain.document_loaders.csv_loader import CSVLoader`  
`from langchain.te
xt_splitter import RecursiveCharacterTextSplitter`  
`from langchain.embeddings import HuggingFaceEmbeddings`  
`from la
ngchain.vectorstores import FAISS`  
`from langchain.llms import CTransformers`  
`from langchain.memory import Conversa
tionBufferMemory`  
`from langchain.chains import ConversationalRetrievalChain`  
`import sys`  
`DB_FAISS_PATH = 'vecto
rstore/db_faiss'`  
`loader = CSVLoader(file_path='data/World Happiness Report 2022.csv', encoding='utf-8', csv_args={'d
elimiter': ','})`  
`data = loader.load()`  
`print(data)`  
`# Split the text into Chunks`  
`text_splitter = Recursive
CharacterTextSplitter(chunk_size=500, chunk_overlap=20)`  
`text_chunks = text_splitter.split_documents(data)`  
`print(
len(text_chunks))`  
`# Download Sentence Transformers Embedding From Hugging Face`  
`embeddings = HuggingFaceEmbedding
s(model_name = 'sentence-transformers/all-MiniLM-L6-v2')`  
`# COnverting the text Chunks into embeddings and saving the
 embeddings into FAISS Knowledge Base`  
`docsearch = FAISS.from_documents(text_chunks, embeddings)`  
`docsearch.save_l
ocal(DB_FAISS_PATH)`  
  
>  
>`#query = 'What is the value of GDP per capita of Finland provided in the data?'`  
`#doc
s = docsearch.similarity_search(query, k=3)`  
`#print('Result', docs)`  
`llm = CTransformers(model='models/mistral-7b-
v0.1.Q4_0.gguf',`  
 `model_type='llama',`  
 `max_new_tokens=1000,`  
 `temperature=0.1)`  
`qa = ConversationalRetriev
alChain.from_llm(llm, retriever=docsearch.as_retriever())`  
`while True:`  
 `chat_history = []`  
 `#query = 'What is 
the value of  GDP per capita of Finland provided in the data?'`  
 `query = input(f'Input Prompt: ')`  
 `if query == 'e
xit':`  
 `print('Exiting')`  
 `sys.exit()`  
 `if query == '':`  
 `continue`  
 `result = qa({'question':query, 'chat
_history':chat_history})`  
 `print('Response: ', result['answer'])`

 

**Problem Statement:**

I'm trying to utilize t
he Mistral 7B model for a ConversationalRetrievalChain, but I'm encountering an error related to token length:

Number o
f tokens (760) exceeded maximum context length (512).

**Context:**

I'm working on a project that involves using Mistra
l 7B to answer questions based on a dataset. The dataset contains information about the World Happiness Report 2022.

**
Steps Taken:**

* Loaded and preprocessed the dataset using langchain.
* Initialized Mistral 7B with the following param
eters:
* Model: 'models/mistral-7b-v0.1.Q4\_0.gguf'
* Model Type: 'llama'
* Max New Tokens: 1000
* Temperature: 0.1
* Se
t up a ConversationalRetrievalChain with Mistral 7B as the language model and a retriever based on FAISS.

**Expected Ou
tput:**

I expect to receive a meaningful response from Mistral 7B based on the input query.

**Additional Information:*
*

I'm using Python and relevant libraries for this project. The dataset I'm working with is from the World Happiness Re
port 2022.

**Environment Details:**

* Python version: 3.11.4 
* Relevant libraries and versions: 

langchain 

ctransf
ormers 

sentence-transformers 

faiss-cpu
```
---

     
 
deeplearning -  [ Error with Mistral 7B model in ConversationalRetrievalChain. ](https://www.reddit.com/r/deeplearning/comments/179vsif/error_with_mistral_7b_model_in/) , 2023-11-14-0909
```
I'm encountering an issue while using the Mistral 7B model in a ConversationalRetrievalChain. When I input a question, s
uch as 'What is the highest GDP?', I receive an error and after that the model generates a random response as output whi
ch is not relevant to the Input query. It seems that the number of tokens in the input exceeds the maximum context lengt
h.

Here's the relevant code:

>from langchain.document\_loaders.csv\_loader import CSVLoader  
>  
>from langchain.text
\_splitter import RecursiveCharacterTextSplitter  
>  
>from langchain.embeddings import HuggingFaceEmbeddings  
>  
>fr
om langchain.vectorstores import FAISS  
>  
>from langchain.llms import CTransformers  
>  
>from langchain.memory impo
rt ConversationBufferMemory  
>  
>from langchain.chains import ConversationalRetrievalChain  
>  
>import sys  
>  
>  

>  
>DB\_FAISS\_PATH = 'vectorstore/db\_faiss'  
>  
>loader = CSVLoader(file\_path='data/World Happiness Report 2022.c
sv', encoding='utf-8', csv\_args={'delimiter': ','})  
>  
>data = loader.load()  
>  
>print(data)  
>  
>  
>  
>\# Sp
lit the text into Chunks  
>  
>text\_splitter = RecursiveCharacterTextSplitter(chunk\_size=500, chunk\_overlap=20)  
> 
 
>text\_chunks = text\_splitter.split\_documents(data)  
>  
>  
>  
>print(len(text\_chunks))  
>  
>  
>  
>\# Downlo
ad Sentence Transformers Embedding From Hugging Face  
>  
>embeddings = HuggingFaceEmbeddings(model\_name = 'sentence-t
ransformers/all-MiniLM-L6-v2')  
>  
>  
>  
>\# COnverting the text Chunks into embeddings and saving the embeddings in
to FAISS Knowledge Base  
>  
>docsearch = FAISS.from\_documents(text\_chunks, embeddings)  
>  
>  
>  
>docsearch.save
\_local(DB\_FAISS\_PATH)  
>  
>  
>  
>  
>  
>\#query = 'What is the value of GDP per capita of Finland provided in th
e data?'  
>  
>  
>  
>\#docs = docsearch.similarity\_search(query, k=3)  
>  
>  
>  
>\#print('Result', docs)  
>  
>
  
>  
>llm = CTransformers(model='models/mistral-7b-v0.1.Q4\_0.gguf',  
>  
>model\_type='llama',  
>  
>max\_new\_toke
ns=1000,  
>  
>temperature=0.1)  
>  
>  
>  
>qa = ConversationalRetrievalChain.from\_llm(llm, retriever=docsearch.as\
_retriever())  
>  
>  
>  
>while True:  
>  
>chat\_history = \[\]  
>  
>\#query = 'What is the value of  GDP per cap
ita of Finland provided in the data?'  
>  
>query = input(f'Input Prompt: ')  
>  
>if query == 'exit':  
>  
>print('E
xiting')  
>  
>sys.exit()  
>  
>if query == '':  
>  
>continue  
>  
>result = qa({'question':query, 'chat\_history':
chat\_history})  
>  
>print('Response: ', result\['answer'\])

 

**Problem Statement:**

I'm trying to utilize the Mis
tral 7B model for a ConversationalRetrievalChain, but I'm encountering an error related to token length:

Number of toke
ns (760) exceeded maximum context length (512).

**Context:**

I'm working on a project that involves using Mistral 7B t
o answer questions based on a dataset. The dataset contains information about the World Happiness Report 2022.

**Steps 
Taken:**

* Loaded and preprocessed the dataset using langchain.
* Initialized Mistral 7B with the following parameters:

* Model: 'models/mistral-7b-v0.1.Q4\_0.gguf'
* Model Type: 'llama'
* Max New Tokens: 1000
* Temperature: 0.1
* Set up a
 ConversationalRetrievalChain with Mistral 7B as the language model and a retriever based on FAISS.

**Expected Output:*
*

I expect to receive a meaningful response from Mistral 7B based on the input query.

**Additional Information:**

I'm
 using Python and relevant libraries for this project. The dataset I'm working with is from the World Happiness Report 2
022.

**Environment Details:**

Python version: 3.11.4 Relevant libraries and versions: langchain ctransformers sentence
-transformers faiss-cpu

&#x200B;
```
---

     
 
deeplearning -  [ Free courses to learn about Large Language Models and building AI projects ](https://www.reddit.com/r/deeplearning/comments/178zu2u/free_courses_to_learn_about_large_language_models/) , 2023-11-14-0909
```
[**LangChain for LLM Application Development by Andrew Ng**](https://www.deeplearning.ai/short-courses/langchain-for-llm
-application-development/): Apply LLMs to your proprietary data to build personal assistants and specialized chatbots. 


[**Full Stack LLM Bootcamp**](https://fullstackdeeplearning.com/llm-bootcamp/): Learn best practices and tools for buil
ding LLM-powered apps 

[**Stanford CS324**](https://stanford-cs324.github.io/winter2022/): In this course, students wil
l learn the fundamentals about the modeling, theory, ethics, and systems aspects of large language models, as well as ga
in hands-on experience working with them. 

[**LangChain & Vector Databases in Production**](https://learn.activeloop.ai
/courses/langchain): Learn how to leverage LangChain, a robust framework for building applications with LLMs, and explor
e Deep Lake, a groundbreaking vector database for all AI data. 

[**Stanford CS25**](https://web.stanford.edu/class/cs25
/): In this course, learn how transformers work, and dive deep into the different kinds of transformers and how they're 
applied in different fields. 

[**LLMOps Space Discord**](https://llmops.space/discord): LLMOps Space is a global commun
ity for LLM practitioners.
```
---

     
