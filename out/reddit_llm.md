 
all -  [ Gemini endpoint url ](https://www.reddit.com/r/LangChain/comments/1g5coqw/gemini_endpoint_url/) , 2024-10-17-0912
```
Where can we find the endpoint URL to use the Gemini API key? Has anyone else encountered a similar issue while using th
e Gemini API key?
```
---

     
 
all -  [ List of FREE and Best Selling Discounted Courses ](https://www.reddit.com/r/udemyfreebies/comments/1g5alz7/list_of_free_and_best_selling_discounted_courses/) , 2024-10-17-0912
```
# Udemy Free Courses for 17 October 2024

Note : Coupons might expire anytime, so enroll as soon as possible to get the 
courses for FREE.

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19234/)LangGraph Mastery: Develop LLM Agents with
 LangGraph
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19233/)LangChain Mastery:Develop LLM Apps with LangChain 
& Pinecone
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19232/)Microsoft Applied Skills: Gen AI solutions with Az
ure OpenAI
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19231/)Python 3: Fundamentals
* [REDEEM OFFER ](https://i
downloadcoupon.com/udemy/19230/)Python 3: Deep Dive (Part 3 – Dictionaries, Sets, JSON)
* [REDEEM OFFER ](https://idownl
oadcoupon.com/udemy/19229/)Python 3: Deep Dive (Part 2 – Iterators, Generators)
* [REDEEM OFFER ](https://idownloadcoupo
n.com/udemy/19228/)Python 3: Deep Dive (Part 1 – Functional)
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19227/)
Azure ChatGPT and OpenAI Service – The Complete Guide
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19226/)Compute
r Security: A Hands-on Approach
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19225/)Internet Security: A Hands-on
 Approach
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19224/)Web Security: A Hands-on Approach
* [REDEEM OFFER ]
(https://idownloadcoupon.com/udemy/19223/)Spring 6 & Spring Boot 3 for Beginners (Includes 6 Projects)
* [REDEEM OFFER ]
(https://idownloadcoupon.com/udemy/19222/)Introduction to Machine Learning Models (AI) Testing
* [REDEEM OFFER ](https:/
/idownloadcoupon.com/udemy/19221/)Core Java and Coding for Automation Testers – For Beginners
* [REDEEM OFFER ](https://
idownloadcoupon.com/udemy/19220/)Curso de Gmail 2024, ¡Desde Cero Hasta Experto!
* [REDEEM OFFER ](https://idownloadcoup
on.com/udemy/19219/)Cómo Ganar Dinero con YouTube 2024 | Curso de YouTube 2024
* Cómo Transmitir en Vivo Por las Redes S
ociales 2024
* [REDEEM OFFER](https://idownloadcoupon.com/udemy/19218/)
* [REDEEM OFFER ](https://idownloadcoupon.com/ud
emy/19217/)NVIDIA-Certified Associate: Generative AI LLMs – Mock Exams
* [REDEEM OFFER ](https://idownloadcoupon.com/ude
my/19216/)SQL Essentials – Thinking in SQL form Beginners to Pro
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/192
15/)300-815: Implementing Cisco Adv Call Control Mobilie 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19214/
)300-810: Implementing Cisco Collaboration Applications 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19213/)
300-730: Implementing Secure Solutions with VPN 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19212/)300-735:
 Automating Programming Cisco Security Solution 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19211/)300-725:
 Securing the Web with Cisco Web Security 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19210/)300-720: Secur
ing Email with Cisco Email Security 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19209/)2V0-21.23: Professio
nal VMware vSphere 8.x Practice Exam 24
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19208/)300-835: Automating C
isco Collaboration Solutions 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19207/)300-910: Implementing DevOp
s Solutions and Practices 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19206/)350-201: Implementing Cisco Cy
berOps Core Technologies 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19205/)350-401: Implementing Operating
 Cisco Network 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19204/)350-501: Implementing Operating Cisco Ser
vice Provider 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19203/)350-601: Implementing and Operating Cisco 
Data Center 2024
* 400-007: CCIE Security Written Exam 2024
* [REDEEM OFFER](https://idownloadcoupon.com/udemy/19202/)
*
 [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19201/)500-230: Implementing Cisco Business Resiliency 2024
* [REDEEM
 OFFER ](https://idownloadcoupon.com/udemy/19200/)500-210: Implementing Cisco Business Resiliency 2024
* [REDEEM OFFER ]
(https://idownloadcoupon.com/udemy/19199/)500-052: Implementing Cisco IP Telephony Video, Part 1 2024
* [REDEEM OFFER ](
https://idownloadcoupon.com/udemy/19198/)400-101: Implementing Cisco IP Routing 2024
* [REDEEM OFFER ](https://idownload
coupon.com/udemy/19197/)350-901: Developing Applications Using Cisco Core 2024
* [REDEEM OFFER ](https://idownloadcoupon
.com/udemy/19196/)350-701: Implementing and Operating Cisco Security Core 2024
* [REDEEM OFFER ](https://idownloadcoupon
.com/udemy/19195/)70-347: Enabling Office 365 Services Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/
udemy/19194/)500-490: Implementing Cisco Collaboration Applications
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/
19193/)500-560: Implementing Cisco Collaboration Applications
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19192/
)500-325: Implementing Cisco Collaboration Applications 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19191/)
500-285: Implementing Cisco Collaboration Applications 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19190/)5
00-275: Implementing Cisco Collaboration Applications 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19189/)50
0-240: Implementing Cisco Business Resiliency 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19188/)70-412: Co
nfig Adv Windows Server 2012 Service Practice Test
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19187/)70-414: Im
plementing Windows Server 2012 Practice Test 2024
* 70-417: Upgrade MCSA Windows Server 2012 Practice Test 2024
* [REDEE
M OFFER](https://idownloadcoupon.com/udemy/19186/)
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19185/)70-448: Mi
crosoft SQL Server 2008 R2 Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19184/)70-487: Develop
 Windows Azure Web Service Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19183/)70-489: Develop
ing Microsoft SharePoint Practice test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19182/)70-492: Adv Micro
soft SharePoint Server Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19181/)700-680: Cisco Data
 Center Unified Computing Sale Specialist
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19180/)700-651: Cisco Coll
aboration Cloud and Managed Services Sale
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19179/)700-551: Cisco Clou
d and Managed Services Sales Specialist
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19178/)70-687: Configuring W
indows 8 Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19177/)70-686: Windows Desktop Technicia
n Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19176/)70-528: TS: Microsoft .NET Framework 2.0
 Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19175/)Advanced Program in Product & CX Manageme
nt and Development
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19174/)Business development and sales processes –
 a bird’s eye view
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19173/)Breakthrough Practices for Safer Schools
*
 [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19172/)700-765: Cisco IoT Solutions Sales Specialist Exam
* [REDEEM O
FFER ](https://idownloadcoupon.com/udemy/19171/)700-905: Cisco DevNet Professional Exam
* 700-751: Cisco CyberOps Profes
sional Exam
* [REDEEM OFFER](https://idownloadcoupon.com/udemy/19170/)
* [REDEEM OFFER ](https://idownloadcoupon.com/ude
my/19169/)700-750: Cisco CyberOps Associate Exam
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19168/)C-level mana
gement: 20 models for business operations (3/5)
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19167/)C++ And PHP C
omplete Course for C++ and PHP Beginners
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19166/)CISA: Information Sy
stems Auditor Practice Test – 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19165/)Microsoft Azure: Hands On 
Training: AZ-900 AZ-104 and AZ-305
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19164/)AZ 104 – Manage Identities
 and Governance in Azure
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19163/)CLF-C02: AWS Certified Cloud Practit
ioner Practice test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19162/)Cloud Engineer (Google) Practice Tes
t – 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19161/)Google Cloud (GCP) MasterClass : GCP Live Projects 2
024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19160/)Fortinet (NSE-4): Network Security Practice Test – 2024
*
 [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19159/)Ethically Hack the Planet Part 4
* [REDEEM OFFER ](https://ido
wnloadcoupon.com/udemy/19158/)DA-104: Tableau Desktop Practice Test – 2024
* [REDEEM OFFER ](https://idownloadcoupon.com
/udemy/19157/)Complete Python For Absolute Beginners
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19156/)JN0-335:
 Juniper Networks Specialist Security Practice 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19155/)JN0-280: 
Juniper Networks Specialist Cloud Security Practice
* JN0-252: Juniper Networks Specialist Cloud Practice 2024
* [REDEEM
 OFFER](https://idownloadcoupon.com/udemy/19154/)
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19153/)JN0-231: Ju
niper Networks Specialist Security Ptactice 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19152/)JN0-230: Jun
iper Networks Associate Security Practice 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19151/)JN0-104: Junip
er Networks Internet Practice Test – 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19150/)How neuromarketing 
can influence buying behavior
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19149/)JN0-349: Juniper Network Profes
sional Security Practice 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19148/)JN0-351: Juniper Network Profes
sional Data Cente Practice 24
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19147/)JN0-648: Juniper Networks Exper
t Security Practice Test 2024
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19146/)Leadership – Leading a Communit
y
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19145/)PCCSA: Palo Alto Network Cyber Security Practice Test -2024

* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19144/)PCEP (30-02): Entry-Level Python Practice Test – 2024
* [RED
EEM OFFER ](https://idownloadcoupon.com/udemy/19143/)PCEP (30-02): Entry-Level Python Practice Test – 2024
* [REDEEM OFF
ER ](https://idownloadcoupon.com/udemy/19142/)VCP550: VMware Data Center Practice Test -2024
* [REDEEM OFFER ](https://i
downloadcoupon.com/udemy/19141/)SC-400: Microsoft Info Protection Admin Practice Test 2024
* [REDEEM OFFER ](https://ido
wnloadcoupon.com/udemy/19140/)SAA-C03 AWS Certified Solutions Architect Associate Practice
* [REDEEM OFFER ](https://ido
wnloadcoupon.com/udemy/19139/)Professional Scrum Master PSM 1 / PSM1 Mock Exams | 2023
* CentOS Linux and Ubuntu Linux: 
Managing Packages
* [REDEEM OFFER](https://idownloadcoupon.com/udemy/19138/)
* [REDEEM OFFER ](https://idownloadcoupon.c
om/udemy/19137/)Python Data Structures & Algorithms: Ace Coding Interviews
* [REDEEM OFFER ](https://idownloadcoupon.com
/udemy/19136/)Peak Performance: the 7 essentials for sales supremacy
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy
/19135/)Executive Diploma in Sales and Service Management
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19134/)Jav
aScript From Scratch ( Part 1 – Beginner Level)
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19133/)Discover Your
 Business Priority
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19132/)Master the Basics of Paleo Diet
* [REDEEM 
OFFER ](https://idownloadcoupon.com/udemy/19131/)Problem Solving with C++ programming language
* [REDEEM OFFER ](https:/
/idownloadcoupon.com/udemy/19130/)Python Programming Language (Practice Projects)
* [REDEEM OFFER ](https://idownloadcou
pon.com/udemy/19129/)Mastering HTML5 and CSS3 (Part 2 – Intermediate Level)
* [REDEEM OFFER ](https://idownloadcoupon.co
m/udemy/19128/)Google Analytics 4 (GA4) Certification. How to Pass the Exam
* [REDEEM OFFER ](https://idownloadcoupon.co
m/udemy/19127/)Reputation Management: Take Control of Your Company’s Image
* [REDEEM OFFER ](https://idownloadcoupon.com
/udemy/19126/)Instagram Marketing. How to Promote Your Business!
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/191
25/)Facebook Marketing 2024. Promote Your Business on Facebook!
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/1912
4/)Project Management Fundamentals: A Beginner’s Guide
* [REDEEM OFFER ](https://idownloadcoupon.com/udemy/19123/)B2B Ou
tbound Lead Generation Mastery

GET MORE FREE ONLINE COURSES WITH CERTIFICATE – [CLICK HERE](https://idownloadcoupon.com
/)
```
---

     
 
all -  [ Looking for some cool Project Ideas.  ](https://www.reddit.com/r/LangChain/comments/1g54rea/looking_for_some_cool_project_ideas/) , 2024-10-17-0912
```
I recently got my hands dirty on langchain and langgraph, so i was thinking of making a project to know how much I know 
and to practice what I learned. I was looking for some cool project ideas using langgraph and langchain, it should not h
ave to be much complex and not too easy to implement. So guys please share some of the cool project idea you guys have o
r you currently working on ✌🏻

Thank you in advance 🙌🙏🏻
```
---

     
 
all -  [ [FOR HIRE] I will develop custom Salesforce solutions for you at an affordable price ](https://www.reddit.com/r/hiring/comments/1g53d0x/for_hire_i_will_develop_custom_salesforce/) , 2024-10-17-0912
```
Hey there! I'm David,

Are you overwhelmed with repetitive tasks or dreaming up exciting Salesforce projects? Let me bri
ng my expertise in Salesforce development to help you out!

With over 7 years of experience, I've worked on a variety of
 projects, focusing on Flows, Lightning Web Components (LWC), and integrations.

What can I do for you?

* Build Custom 
LWC: Create responsive components tailored to your needs, following the LDS from SF.

* Automate Processes: Streamline y
our workflows and eliminate manual tasks.

* Integrate Systems: Seamlessly connect Salesforce with other platforms.

* D
evelop Innovative Solutions: From custom apps to unique features, let’s build something great together.

* Work with AI:
 Using cool stuff like Langchain or the Chat-GPT API.

If you believe you have something that I could do for you, please
 place a $bid and send me the details of the project. As always, the price depends, but It's usually from 50$ to 100$ fo
r simple projects.



```
---

     
 
all -  [ What's the best framework for building productiin levwl RAG application? ](https://www.reddit.com/r/u_Tech_8976/comments/1g5310o/whats_the_best_framework_for_building_productiin/) , 2024-10-17-0912
```
Langchain is good for prototyping but when it comes to production level RAG I found that langchain is not good enough. C
an someone suggest some framework to build production level RAG application? What about llmaindex?
```
---

     
 
all -  [ AI News this week ](https://www.reddit.com/r/brainscriblr/comments/1g52vqt/ai_news_this_week/) , 2024-10-17-0912
```
||
||
|OpenAI's new '[Swarm](https://link.mail.beehiiv.com/ss/c/u001.bCJQm5nTOkyoUpvPqxXWZMOsre0WB3hJUyVxQAm__4HIO4v9uY6
6cAXbEzVswcWf8v1uU7-0Yk4VHyNvJa2CUBdbS2s48OjErH6RXPwdb-Ka_sGzXtLyZ3Fi1JVKu2xr3hGZlUoJ4Lk-WeyosHoE4WobxtB1y2hhPAhCk-UGOX8
ZZXjr5xJzSEdn39bDeGmARGksreb_Tkv6O5_OPUZxqkB3Rv2n0Tt_zOxvWr0KAv4UwpMcXw1R8owHvt4xPsJ9/4am/9ENZbDoZQWeDK8jSHCurcg/h1/h001
.n4RBfuz2eOecxl7AFemrxExSrneQA5lYl5QRug9uQiE)' framework enables developers to create multi-agent AI systems. It orchest
rates collaboration between AI agents on complex tasks using 'Routines' (step-by-step instructions) and 'Handoffs' (task
 delegation). Swarm facilitates smooth communication and coordination between agents, making it easier to build, test, a
nd control sophisticated AI systems. It's primarily designed for developers and researchers working on AI assistants, ro
botics, and task automation.|
|[Amazon](https://link.mail.beehiiv.com/ss/c/u001.U0cWUjXPgPq-FBBCovcNs6m3fN7FkkcIdUfHhdbT
GRGOya7OT-s81EqCzBtGctnsroWa8BH3gaMco4YAhnKOX7Z_lBID49_leyRuXyFi0oQ7fZhEgvnBi2S7sg6pNcrrZ2SEnjRBdSYBQNAAmF7XjzOWKhYAb2KM
sx8ie5YaPpwWeqi8h569zdilxM0-NdjbE7N1Y3lyE8a5BFK2ChoSBjxOCerTMyrnR6feUXY_gmywThsews_jnMo4KEzT54lHx4ax--YTu3dMOkoYNmykbDSd
0JSpQyARo8p5NMF4emsWJxNXf2m9E32htiQg55spUGa3aoEZn8ZaHr-70UGwGEaaORKXEK3UDSZjbwn2iUo/4am/9ENZbDoZQWeDK8jSHCurcg/h2/h001.m
o4zp4aaiDj4dMljy_lzpRUXXdwp710T3mpV_SStdvA) has built an AI-shopping bot.|
|[Langchain](https://link.mail.beehiiv.com/ss
/c/u001.bCJQm5nTOkyoUpvPqxXWZPfVviZxL4uMVDMAsYwQj7-__3zLdejqOQMUCUmH-mnissFOZbrcdlmMfGjuqX7FsiIt0LZjhrt3pt84vyOdpmsxRdIb
W_d_eQx1SqaXWNeRHahRWwh0JJN1PGmKPTXTkA4HyxwnLXjHxCaG5y8w5GYR3vhf716qW2MdHz69lLWfuMrnpuk1QfWwL0FO1rLI9qb-3FdjNu9ELdE3B2PS
EfLL6V5vZ7VybBcZuAsoVrF_rK6Zm2WDcJSc2CjWIKyNuQ/4am/9ENZbDoZQWeDK8jSHCurcg/h3/h001.cV0EotQc92jLszSY9Wozvhk9u70EcspCaOXFZt
z6aH8) has revealed Kotaemon and open-source RAG UI for document chat.|
|In a recent [Tweet](https://link.mail.beehiiv.c
om/ss/c/u001.Hb21N8P-60x7GolUpHo-ktEi_mxjwW9ydRKeLuaiVHa6G7izolZ6TpwQo4wHe-NhwVHdrDcAk3n68PdlCgDPKdE0JMRmDCdccOqpOhpStsQ
IAv74Z3lBr_qqscckZa9W4FfLtdnaKaogfl1iZBhz5Tbg0hbmqrR_LAFbabW-gNR8fdRxLGDpfluDq7_p2yS3VuLmjleIJ0cima1BazL_pDAYeO2fdRkWgRu
jhZml_uwV-bTQ1_GNetg_eRAHT-g8ZoMQA_BOJDVjyXotDCRlDQ/4am/9ENZbDoZQWeDK8jSHCurcg/h4/h001.m6GiQ2Pok0dRDFHU4ebvUrwk4di-eidzS
CN6omvNfJA) former OpenAI board member Karpathy suggested there should be more diversity in model fine-tuning.|
```
---

     
 
all -  [ Challenges in Word Counting with Langchain and Qdrant ](https://www.reddit.com/r/Langchaindev/comments/1g51dlk/challenges_in_word_counting_with_langchain_and/) , 2024-10-17-0912
```
I am developing a chatbot using Langchain and Qdrant, and I'm encountering challenges with tasks involving word counts. 
For example, after vectorizing the book The Lord of the Rings, I ask the AI how many times the name 'Frodo' appears, or 
to list the main characters and how frequently their names are mentioned. I’ve read that word counting can be a limitati
on of AI systems, but I’m unsure if this is a conceptual misunderstanding on my part or if there is a way to accomplish 
this. Could someone clarify whether AI can reliably count words in vectorized documents, or if this is indeed a known li
mitation?

I'm not asking for a specific task to be done, but rather seeking a conceptual clarification of the issue. Ev
en though I have read the documentation, I still don't fully understand whether this functionality is actually feasible


I attempted to use the functions related to the vectorization process, particularly the similarity search method in Qdr
ant, but the responses remain uncertain. From what I understand, similarity search works by comparing vector representat
ions of data points and returning those that are most similar based on their distance in the vector space. In theory, th
is should allow for highly relevant results. However, I’m unsure if my setup or the nature of the task—such as counting 
occurrences of a specific word like 'Frodo'—is making the responses less reliable. Could this be a limitation of the met
hod, or might there be something I’m missing in how the search is applied?
```
---

     
 
all -  [ RAG Hut - Submit your RAG projects here. Discover, Upvote, and Comment on RAG Projects. ](/r/Rag/comments/1g4xzy3/rag_hut_submit_your_rag_projects_here_discover/) , 2024-10-17-0912
```

```
---

     
 
all -  [ Challenges in Word Counting with Langchain and Qdran ](https://www.reddit.com/r/LangChain/comments/1g517g7/challenges_in_word_counting_with_langchain_and/) , 2024-10-17-0912
```
I am developing a chatbot using Langchain and Qdrant, and I'm encountering challenges with tasks involving word counts. 
For example, after vectorizing the book *The Lord of the Rings*, I ask the AI how many times the name 'Frodo' appears, o
r to list the main characters and how frequently their names are mentioned. I’ve read that word counting can be a limita
tion of AI systems, but I’m unsure if this is a conceptual misunderstanding on my part or if there is a way to accomplis
h this. Could someone clarify whether AI can reliably count words in vectorized documents, or if this is indeed a known 
limitation?
```
---

     
 
all -  [ I built a Langchain Agent that can use any website as a custom tool ](https://www.reddit.com/r/AI_Agents/comments/1g4zlqr/i_built_a_langchain_agent_that_can_use_any/) , 2024-10-17-0912
```
Here is the repo if anyone is interested:

[https://github.com/dendrite-systems/langchain-dendrite-example/tree/main](ht
tps://github.com/dendrite-systems/langchain-dendrite-example/tree/main)

It can go get OpenAI's API status, send emails,
 help search for conflicting trademarks and a few other random things :) 
```
---

     
 
all -  [ Laravelpy concept. Would you use this? ](https://www.reddit.com/r/laravel/comments/1g4z0nc/laravelpy_concept_would_you_use_this/) , 2024-10-17-0912
```
Hey Laravel Devs

Been working on a concept for an opensource laravel package over the past few weeks (still in early st
ages) it's called Laravelpy

The concept is to be able to harness the power of Python in your Laravel Application, my go
al is to achieve the following

Run custom Python scripts using Laravel syntax e.g. Python::run('your/custom/pyton/scrip
t.py')

Have out of the box integrations with popular python libraries such as pandas, matplotlib, NumPy, LangChain just
 to name a few. (see work in progress for Pandas in the attached image)

Upon installation have a Python virtual environ
ment and manage pip install using artisan

I have more ideas on how I could take this further but I wanted to get some i
nitial feedback to see if anyone would use a package like this?

[Concept of usage ](https://preview.redd.it/44r5xsunk5v
d1.png?width=3768&format=png&auto=webp&s=a950057ea8f7ff0c3efc9143fd15358afd46898e)


```
---

     
 
all -  [ Realna plata?  ](https://www.reddit.com/r/programiranje/comments/1g4xe1e/realna_plata/) , 2024-10-17-0912
```
Pozdrav ljudi, zanima me realna plata za ovaj stack. Prave se vise SaaS projekata. 

  
1. React JS  
2. Express JS  
3.
 MongDB & PostgresSQL  
4. LangChain (OpenAI Embeddings, LLM Chains etc)  
5. Azure (Blog Storage, Embeddings Storage, I
ndexing service, search service, Cognitive Services, Azure AI Studio) - Very Important.  
6. And NPM Libraries in genera
l (There are many used in this project)
```
---

     
 
all -  [ How I Started Learning Machine Learning ](https://www.reddit.com/r/learnmachinelearning/comments/1g4x299/how_i_started_learning_machine_learning/) , 2024-10-17-0912
```
Hello, everyone. As promised, I'll write a longer post about how I entered the world of ML, hoping it will help someone 
shape their path. I'll include links to all the useful materials I used alongside the story, which you can use for learn
ing.

I like to call myself an AI Research Scientist who enjoys exploring new AI trends, delving deeper into understandi
ng their background, and applying them to real products. This way, I try to connect science and entrepreneurship because
 I believe everything that starts as scientific research ends up 'on the shelves' as a product that solves a specific us
er problem.

I began my journey in ML in 2016 when it wasn't such a popular field. Everyone had heard of it, but few wer
e applying it. I have several years of development experience and want to try my hand at ML. The first problem I encount
ered was where to start - whether to learn mathematics, statistics, or something else. That's when I came across a name 
and a course that completely changed my career.

# Let's start

You guessed it. It was Professor Andrew Ng and his globa
lly popular Machine Learning course available on Coursera (I still have the certificate, hehe). This was also my first o
fficial online course ever. Since that course no longer exists as it's been replaced by a new one, I recommend you check
 out:

1. [Machine Learning (Stanford CS229)](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

2. [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)

These two 
courses start from the basics of ML and all the necessary calculus you need to know. Many always ask questions like whet
her to learn linear algebra, statistics, or probability, but you don't need to know everything in depth. This knowledge 
helps if you're a scientist developing a new architecture, but as an engineer, not really. You need to know some basics 
to understand, such as how the backpropagation algorithm works.

I know that Machine Learning (Stanford CS229) is a very
 long and arduous course, but it's the right start if you want to be really good at ML. In my time, I filled two thick n
otebooks by hand while taking the course mentioned above.

# TensorFlow and Keras

After the course, I didn't know how t
o apply my knowledge because I hadn't learned specifically how to code things. Then, I was looking for ways to learn how
 to code it. That's when I came across a popular framework called Keras, now part of TensorFlow. I started with a new co
urse and acquiring practical knowledge:

1. [Deep Learning Specialization](https://www.coursera.org/specializations/deep
-learning)
2. [Deep Learning by Ian Goodfellow](https://www.deeplearningbook.org/)
3. [Machine Learning Yearning by Andr
ew Ng](https://info.deeplearning.ai/machine-learning-yearning-book)

These resources above were my next step. I must adm
it that I learned the most from that course and from the book Deep Learning by Ian Goodfellow because I like reading boo
ks (although this one is quite difficult to read).

# Learn by coding

To avoid just learning, I went through various Gi
tHub repositories that I manually retyped and learned that way. It may be an old-fashioned technique, but it helped me a
 lot. Now, most of those repositories don't exist, so I'll share some that I found to be good:

1. [Really good Jupyter 
notebooks that can teach you the basics of TensorFlow](https://github.com/mrdbourke/tensorflow-deep-learning)
2. [Anothe
r good repo for learning TF and Keras](https://github.com/codebasics/deep-learning-keras-tf-tutorial)

# Master the chal
lenge

After mastering the basics in terms of programming in TF/Keras, I wanted to try solving some real problems. There
's no better place for that challenge than Kaggle and the popular Titanic dataset. Here, you can really find a bunch of 
materials and simple examples of ML applications. Here are some of my favorites:

1. [Titanic - Machine Learning from Di
saster](https://www.kaggle.com/c/titanic/overview)
2. [Home Credit Default Risk](https://www.kaggle.com/competitions/hom
e-credit-default-risk/overview)
3. [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/h
ouse-prices-advanced-regression-techniques)
4. [Two Sigma: Using News to Predict Stock Movements](https://www.kaggle.com
/competitions/two-sigma-financial-news)

I then decided to further develop my career in the direction of applying ML to 
the stock market, first using predictions on time series and then using natural language processing. I've remained in th
is field until today and will defend my doctoral dissertation soon.

# How to deploy models

To continue, before I move 
on to the topic of specialization, we need to address the topic of deployment. Now that we've learned how to make some b
asic models in Keras and how to use them, there are many ways and services, but I'll only mention what I use today. For 
all my ML models, whether simple regression models or complex GPT models, I use FastAPI. It's a straightforward framewor
k, and you can quickly create API endpoints. I'll share a few older and useful tutorials for beginners:

1. [AI as an AP
I tutorial series](https://www.youtube.com/watch?v=56qQNcHJxyQ)
2. [A step-by-step guide](https://medium.com/@ganiyuabdu
lwajeed2002/a-step-by-step-approach-to-building-a-fast-api-for-deep-learning-classification-projects-cbd2ea6bc2f2)
3. [P
roductizing an ML Model with FastAPI and Cloud Run](https://medium.com/semantixbr/deploy-an-ml-model-with-fastapi-and-cl
oud-run-part-1-1a0b0f3b3a5d)

Personally, I've deployed on various cloud providers, of which I would highlight GCP and A
WS because they have everything needed for model deployment, and if you know how to use them, they can be quite cheap.


# Chose your specialization

The next step in developing my career, besides choosing finance as the primary area, was my
 specialization in the field of NLP. This happened in early 2020 when I started working with models based on the Transfo
rmer architecture. The first model I worked with was BERT, and the first tasks were related to classifications. My recom
mendations are to master the Transformer architecture well because 99% of today's LLM models are based on it. Here are s
ome resources:

1. [The legendary paper 'Attention Is All You Need'](https://proceedings.neurips.cc/paper_files/paper/20
17/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
2. [Hugging Face Course on Transformers](https://huggingface.co/lear
n/nlp-course/chapter1/1)
3. [Illustrated Guide to Transformers - Step by Step Explanation](https://towardsdatascience.co
m/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)
4. [Good repository](https://github.com/Niels
Rogge/Transformers-Tutorials)
5. [How large language models work, a visual intro to transformers](https://www.youtube.co
m/watch?v=wjZofJX0v4M)

After spending years using encoder-based Transformer models, I started learning GPT models. Good
 open-source models like Llama 2 then appear. Then, I started fine-tuning these models using the excellent Unsloth libra
ry:

1. [How to Finetune Llama-3 and Export to Ollama](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-exp
ort-to-ollama)
2. [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3)


After that, I focused on studying various RAG techniques and developing Agent AI systems. This is now called AI enginee
ring, and, as far as I can see, it has become quite popular. So I'll write more about that in another post, but here I'l
l leave what I consider to be the three most famous representatives, i.e., their tutorials:

1. [LangChain tutorial](htt
ps://python.langchain.com/docs/tutorials/)
2. [LangGraph tutorial](https://langchain-ai.github.io/langgraph/tutorials/)

3. [CrewAI examples](https://github.com/crewAIInc/crewAI-examples)

# Here I am today

Thanks to the knowledge I've gene
rated over all these years in the field of ML, I've developed and worked on numerous projects. The most significant publ
icly available project is developing an agent AI system for well-being support, which I turned into a [mobile applicatio
n](https://sintelly.com/download/). Also, my entire doctoral dissertation is related to applying ML to the stock market 
in combination with the development of GPT models and reinforcement learning (more on that in a separate post). After lo
ng 6 years, I've completed my dissertation, and now I'm just waiting for its defense. I'll share everything I'm working 
on for the dissertation publicly [on the project](https://primoinvesting.com/), and in tutorials I'm preparing to write.


If you're interested in these topics, I announce that I'll soon start with activities of publishing content on Medium 
and a blog, but I'll share all of that here on Reddit as well. Now that I've gathered years of experience and knowledge 
in this field, I'd like to share it with others and help as much as possible.

If you have any questions, feel free to a
sk them, and I'll try to answer all of them.

Thank you for reading.
```
---

     
 
all -  [ Relevant file retrieval  ](https://www.reddit.com/r/LangChain/comments/1g4tsbo/relevant_file_retrieval/) , 2024-10-17-0912
```
I’m trying to implement what OpenAI Assistant API is doing when I feed it with some documents. 
So if my question is rel
ated to something on the documents that I have in a directory, it should take that into account; if not, it can provide 
a general answer. Is embeddings only approach here? 
```
---

     
 
all -  [ [OFFER] I will develop custom Salesforce solutions for you at an affordable price ](https://www.reddit.com/r/slavelabour/comments/1g4tf5u/offer_i_will_develop_custom_salesforce_solutions/) , 2024-10-17-0912
```
Hey there! I'm David,

Are you overwhelmed with repetitive tasks or dreaming up exciting Salesforce projects? Let me bri
ng my expertise in Salesforce development to help you out!

With over 7 years of experience, I've worked on a variety of
 projects, focusing on Flows, Lightning Web Components (LWC), and integrations.

What can I do for you?

* Build Custom 
LWC: Create responsive components tailored to your needs, following the LDS from SF.
* Automate Processes: Streamline yo
ur workflows and eliminate manual tasks.
* Integrate Systems: Seamlessly connect Salesforce with other platforms.
* Deve
lop Innovative Solutions: From custom apps to unique features, let’s build something great together.
* Work with AI: Usi
ng cool stuff like Langchain or the Chat-GPT API.

If you believe you have something that I could do for you, please pla
ce a $bid and send me the details of the project. As always, the price depends, but It's usually from 25$ to 50$ for sim
ple projects.
```
---

     
 
all -  [ What is the best custom agent you made using langchain or langgraph? ](https://www.reddit.com/r/LangChain/comments/1g4sj37/what_is_the_best_custom_agent_you_made_using/) , 2024-10-17-0912
```
I just want to see it's effectiveness through your experience.

I plan on creating a tool that queries on tabular data a
nd tweak it to work fine on large open source models. If you have done something similar please let me know what it was.

```
---

     
 
all -  [ Technique beyond RecursiveCharacterTextSplitter for unstructured PDF RAG ](https://www.reddit.com/r/LocalLLaMA/comments/1g4rcji/technique_beyond_recursivecharactertextsplitter/) , 2024-10-17-0912
```
I built a local RAG using ollama, langchain and llama3.2, using a MultiQueryRetriever to parse through multiple PDF and 
ask a set of identical questions for each.

The accuracy is good but not perfect, and I have noticed that failures tend 
to be happen on data that looks like a table in the PDF.

For example:
```
----------------|----|
Population 2023 | x  |

Population 2024 | y  |
Population 2025 | z  |
----------------|----|
```

will show up in the chunks of RecursiveCharac
terTextSplitter as 

```
Population 2023
Population 2024
Population 2025
x
y
z
```

and the chat will either retrieve th
e wrong information or not find the information at all. I would like to improve my PDF ingestion (chunking/splitting, ve
ctor embeddings) to be able to more accurately retrieve the correct data. I have played around with the embedding model,
 chunk size and overlap, but those won't fix this issue.

What techniques could I use to improve the layout of the chunk
s stored in my vector database? Or should I look into a different technique altogether?

Here is the current code regard
ing indexing:

```
TEXT_EMBEDDING_MODEL = os.getenv('TEXT_EMBEDDING_MODEL', 'nomic-embed-text')

def create_vector_embed
dings_from_pdf(file):
  # Ingest the PDF
  loader = UnstructuredPDFLoader(file_path=file)
  document_data = loader.load(
)

  # Vector Embeddings of PDF (split and chunking)
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, ch
unk_overlap=50)
  splits = text_splitter.split_documents(document_data)

  # Add to vector database
  embeddings = Ollam
aEmbeddings(model=TEXT_EMBEDDING_MODEL, show_progress=False)
  vector_db = Chroma.from_documents(
    documents=splits,

    embedding=embeddings
  )
  return vector_db
```

Thank you!
```
---

     
 
all -  [ How to chang database when using vector_store? ](https://www.reddit.com/r/LangChain/comments/1g4qe4z/how_to_chang_database_when_using_vector_store/) , 2024-10-17-0912
```
I wanna use my test\_db database in my milvus, but i donnnot find the way to change database as langchain.Milvus.vector\
_store defaultly use the default database. 
```
---

     
 
all -  [ Using LangChain to manage visual models for editing 3D scenes ](https://www.reddit.com/r/LangChain/comments/1g4n12e/using_langchain_to_manage_visual_models_for/) , 2024-10-17-0912
```
An ECCV paper, Chat-Edit-3D, utilizes ChatGPT to drive (by LangChain) nearly 30 AI models and enable 3D scene editing.


[https://github.com/Fangkang515/CE3D](https://github.com/Fangkang515/CE3D)

https://reddit.com/link/1g4n12e/video/5j54cy
ufl0vd1/player


```
---

     
 
all -  [ Unable to get desired results with ChatPromptTemplate and Prompt Caching with Anthropic ](https://www.reddit.com/r/LangChain/comments/1g4ip6z/unable_to_get_desired_results_with/) , 2024-10-17-0912
```
I have a long prompt of instructions that performs as intended when I use PromptTemplate.  
After reading about Prompt C
aching, I tried to implement it with the ChatPromptTemplate, but it did not work as intended. The demo of prompt caching
 uses a book as its context. I have a smaller context but specific instructions.  
Tried fine-tuning the prompt, but the
 model hallucinates badly.

Example: When I ask a question, it does not use the same question to reason/generate the ans
wer.
```
---

     
 
all -  [ Cisco Use Cases, ITSI Best Practices, and More New Articles from Splunk Lantern ](https://www.reddit.com/r/Splunk/comments/1g4g5wh/cisco_use_cases_itsi_best_practices_and_more_new/) , 2024-10-17-0912
```
[Splunk Lantern](https://lantern.splunk.com/) is a Splunk customer success center that provides advice from Splunk exper
ts on valuable data insights, key use cases, and tips on managing Splunk more efficiently.

We also host Getting Started
 Guides for a range of Splunk products, a library of Product Tips, and [Data Descriptor](https://lantern.splunk.com/Data
_Descriptors) articles that help you see everything that’s possible with data sources and data types in Splunk.

This mo
nth, we’re excited to share some articles that show you new ways to get Cisco and AppDynamics integrated with Splunk. We
’ve also updated our  Definitive Guide to Best Practices for IT Service Intelligence (ITSI), and as usual, we’re sharing
 all the rest of the use case, product tip, and data articles that we’ve published over the past month. Read on to find 
out more.

# Splunking with Cisco and AppDynamics

  
Here on the Splunk Lantern team we’ve been busy working with exper
ts in Cisco, AppDynamics, and Spunk to develop articles that show how our products can work together. Here are some of t
he most recent articles we’ve published, and keep watching out for more Cisco and AppD articles over the coming months!


[Monitoring Cisco switches, routers, WLAN controllers and access points](https://lantern.splunk.com/Security/UCE/Founda
tional_Visibility/Security_monitoring/Monitoring_Cisco_switches%2C_routers%2C_WLAN_controllers_and_access_points) shows 
you how to create a comprehensive solution to monitor Cisco network devices in the Splunk platform or in Splunk Enterpri
se Security. Learn how to get set up, create visualizations, and troubleshoot common problems in this new use case artic
le.

https://preview.redd.it/psocr3wu1zud1.png?width=781&format=png&auto=webp&s=d1d3ae3bbf9a8a299deaabf57d011e3d20f95bbd


[Enabling Log Observer Connect for AppDynamics](https://lantern.splunk.com/Observability/Product_Tips/Log_Observer_Con
nect/Enabling_Log_Observer_Connect_for_AppDynamics) teaches you how to configure Log Observer Connect for AppDynamics, a
llowing you to access the right logs in Splunk Log Observer Connect with a single click, all while providing troubleshoo
ting context from AppDynamics.

Looking for more Cisco and AppDynamics use cases? Check out our [Cisco](https://lantern.
splunk.com/Data_Descriptors/Cisco) and [AppDynamics](https://lantern.splunk.com/Data_Descriptors/AppDynamics) data descr
iptor pages for more configuration information, use cases and product tips, and please let us know in the comments what 
other articles you’d like to see!

# ITSI Best Practices

  
The [Definitive Guide to Best Practices for IT Service Inte
lligence](https://lantern.splunk.com/Observability/Product_Tips/IT_Service_Intelligence/The_definitive_guide_to_best_pra
ctices_for_IT_Service_Intelligence) is a must-read resource for ITSI administrators, with essential guidelines that help
 you to unlock the full potential of ITSI. We’ve just updated this resource with fresh articles to help you ensure optim
al operations and exceptional end-user experiences.

[Using dynamic entity rule configurations](https://lantern.splunk.c
om/Observability/Product_Tips/IT_Service_Intelligence/Using_dynamic_entity_rule_configurations) is helpful for anyone wh
o often adds or removes entities from their configurations. Learn how to create a rule configuration that updates immedi
ately and without the need for service configuration changes, reducing the time and risk of error that can result from m
anually reconfiguring entity filter rules.

If you use the ITSI default aggregation policy, you might not know that you 
shouldn’t be using this as your primary aggregation policy. Learn why and how to build policies that better fit your nee
ds in [Utilizing policies other than the default policy](https://lantern.splunk.com/Observability/Product_Tips/IT_Servic
e_Intelligence/Utilizing_policies_other_than_the_default_policy).

[Building your own custom threshold templates](https:
//lantern.splunk.com/Observability/Product_Tips/IT_Service_Intelligence/Building_your_own_custom_threshold_templates) sh
ows you how to use and customize the 33 ITSI out-of-the-box thresholding templates with the ability to configure time po
licies, choose different thresholding algorithms, and adjust sensitivity configurations.

Finally, [Knowing proper adapt
ive threshold configurations](https://lantern.splunk.com/Observability/Product_Tips/IT_Service_Intelligence/Knowing_prop
er_adaptive_threshold_configurations) explains how to best use adaptive thresholding in the most effective way possible,
 helping you to avoid confusing or noisy configurations.

https://preview.redd.it/t1f068yv1zud1.png?width=783&format=png
&auto=webp&s=c37852bd50bd239249bd19ce886a3f83789cd254

These four new articles are just some of many articles in the [De
finitive Guide to Best Practices for IT Service Intelligence](https://lantern.splunk.com/Observability/Product_Tips/IT_S
ervice_Intelligence/The_definitive_guide_to_best_practices_for_IT_Service_Intelligence), so if you’re looking to improve
 how you work with ITSI then don’t miss this helpful resource.

# The Rest of This Month’s New Articles

  
Here’s every
thing else we’ve published over the month:

* [Maximizing performance with the latest Splunk platform capabilities](http
s://lantern.splunk.com/Splunk_Platform/UCE/Security/Applying_the_differentiated_capabilities_of_the_Splunk_platform)
* [
Monitoring LangChain LLM applications with Splunk](https://lantern.splunk.com/Observability/UCE/Unified_Workflows/Enable
_Self-Service_Observability/Monitoring_Langchain_LLM_Applications_with_Splunk_Observability_Cloud)
* [Introduction to th
e Splunk ACS Github Action CI/CD Starter](https://lantern.splunk.com/Splunk_Platform/Product_Tips/Administration/Introdu
ction_to_the_Splunk_ACS_Github_Action_CI%2F%2FCD_Starter)
* [Integrating Kubernetes and Splunk Observability Cloud](http
s://lantern.splunk.com/Observability/UCE/Foundational_visibility/Optimize_Cloud/Integrating_Kubernetes_and_Splunk_Observ
ability_Cloud)
* [Expanding AWS log ingestion capabilities with custom logs in Splunk Data Manager](https://lantern.splu
nk.com/Data_Descriptors/Amazon/Expanding_AWS_log_ingestion_capabilities_with_Splunk_Data_Manager_custom_logs)
* [Using I
ngest Processor to convert JSON logs into metrics](https://lantern.splunk.com/Splunk_Platform/Product_Tips/Extending_the
_Platform/Using_Ingest_Processor_to_convert_JSON_logs_into_metrics)
* [Using generative AI to write and explain SPL sear
ches](https://lantern.splunk.com/Splunk_Platform/Product_Tips/Extending_the_Platform/Using_generative_AI_to_write_and_ex
plain_SPL_searches)

We hope you’ve found this update helpful. Thanks for reading!
```
---

     
 
all -  [ Is RAG Eval Even Possible? ](https://www.reddit.com/r/LangChain/comments/1g4fegp/is_rag_eval_even_possible/) , 2024-10-17-0912
```
I'm asking for a friend.

Just kidding of course. I run an AI tools company, basically APIs for enterprise-grade RAG.  W
e've seen a lot of eval tools, but nothing that actually evals the RAG pipeline.  Most seem focused on the last mile: co
mparing completions to retrievals.

**But RAG breaks down much earlier than that.**   
Did we parse the doc correctly?  

Did we extract correctly?  
Did we chunk correctly?  
Did we add proper metadata to the chunk?  
How performant was the
 search? How about the rerank?

  
Even simple things like how do you generate a correct QA set against a set of documen
ts? That sounds simple. Just ask an LLM. But if you don't have the issues above done perfectly than your QA pairs can't 
be relied upon. 

For example, if your system doesn't perfectly extract a table from a document, then any QA pair built 
on that table will be built on false data.

If anyone is playing with tools that start to tackle these issues, would lov
e your POV.
```
---

     
 
all -  [ Drag and Drop Platform for Agents ](https://www.reddit.com/r/LangChain/comments/1g4fc8e/drag_and_drop_platform_for_agents/) , 2024-10-17-0912
```
Hello,

I've been using LangGraph as a library in python to try and build an agent, however my code got quite disorganiz
ed and hard to debug, so I've been looking into platforms that have the same functionality, but in a diagram/drag and dr
op interface.

I've tried autogpt, flowise, langflow and n8n. However, they've all dropped short in functionality.

Some
 features that I want to use are: read file from my system, write file to my system, use those files for custom prompts,
 display in chat an LLM response, wait for output from chat (so far, most of them had this), sequence controls (if/else,
 loops), run multiple branches concurrently, simple memory system (not memory for chat messages, but sort of like variab
les that you can save a message to, and later use it for something).

Anybody has any suggestions for which platform has
 most of these features and isn't that much of a pain to work with? It's very possible that one of the one I've tried ab
ove is able to do what I want, but I just didn't figure out how, so feel free to correct me.

Or, if you have any sugges
tions for ways to use LangGraph in a more organized matter, whilst being easy to debug every step, please tell. What I m
ean by debug every step, is to be able to see each LLM's response to figure out where a bad output happened.

Thanks for
 any input!
```
---

     
 
all -  [ OpenAI Realtime API with voice detection mode ](https://www.reddit.com/r/LangChain/comments/1g4dtc0/openai_realtime_api_with_voice_detection_mode/) , 2024-10-17-0912
```
Hi, has anyone implemented RealTime API with voice activation detection in langchain? Seems like we have to covert the i
nput into audio file and process it through the API which doesn't give the user experience as in ChatGPT 'Advanced Voice
 Mode'. 
```
---

     
 
all -  [ Does the PGVector integration work with SelfQueryRetriever? ](https://www.reddit.com/r/LangChain/comments/1g49k2m/does_the_pgvector_integration_work_with/) , 2024-10-17-0912
```
Hi all, I'm trying to make a self-querying retriever with my PGVector vector store, following the instructions found on 
[the documentation](https://python.langchain.com/docs/integrations/retrievers/self_query/pgvector_self_query/) (what lit
tle there is, unfortunately) but when I run the code, I can see on my LangSmith trace that the StructuredQueryOutputPars
er receives the json-formatted input with a 'filter', but the output shows no value in the 'filter' key (not even NO\_FI
LTER as it should be). Is this a known issue? I've seen around the web that there are a few months-old posts raising iss
ues with the implementation but the code doesn't throw me any errors, and the documentation gives no explanations other 
than the example code.
```
---

     
 
all -  [ need help creating binary file ](https://www.reddit.com/r/PythonLearning/comments/1g477gy/need_help_creating_binary_file/) , 2024-10-17-0912
```
I have created a python application which will help talk to databases but i am not able to create working binary file, t
here are two files main-file and test_sql_1(second file)

i have provided both here please help me create a binary file

the installed pakages are 


pip install qt-material PySide6 pandas cryptography langchain langchain-core langchain-comm
unity langchain-google-genai  psycopg2 mysqlclient cx_Oracle pyodbc matplotlib

all these are required
**PLEASE HELP**
e
very time create a binary file and it is giveing error about pydantic
leave it and help me create the binary file
file-1

```python
import sys
import json
import os
from PySide6.QtWidgets import (
    QApplication,
    QMainWindow,
    QVBox
Layout,
    QWidget,
    QListWidget,
    QLineEdit,
    QFormLayout,
    QSplitter,
    QPushButton,
    QLabel,
    QL
istWidgetItem,
    QHBoxLayout,
    QRadioButton,
    QButtonGroup,
    QMessageBox,
    QTextEdit,
)

import pandas as 
pd
from cryptography.fernet import Fernet
from PySide6.QtCore import Qt, QThread, Signal
from datetime import datetime
f
rom langchain_core.messages import AIMessage, HumanMessage
from qt_material import apply_stylesheet
import matplotlib
im
port test_sql_1 as langc

CONFIG_FILE = 'db_credentials.json'  # Configuration file for storing DB credentials
KEY_FILE 
= 'secret.key'
CHROMA_DB_FILE = 'chroma\chroma.sqlite3'


# Worker class for handling database connection in a separate 
thread
class DatabaseConnectionWorker(QThread):
    connection_result = Signal(
        bool, object, object, object, ob
ject, object, object, object
    )

    def __init__(self, host, dbname, user, password, port, api_key, db_type):
      
  super().__init__()
        self.host = host
        self.dbname = dbname
        self.user = user
        self.passwor
d = password
        self.port = port
        self.api_key = api_key
        self.db_type = db_type

    def run(self):

        try:
            db_connection = langc.init_database(
                self.user,
                self.password,

                self.host,
                self.port,
                self.dbname,
                self.db_type,
       
     )
            sql_model = langc.get_sql_chain(self.api_key)
            explain_model = langc.get_response(self.api
_key)
            visual_model = langc.generate_plotly_code(self.api_key)
            ehancer_model = langc.better_quest
ion(self.api_key)
            table_names = db_connection.get_usable_table_names()
            table_names_description =
 {
                name: db_connection.get_table_info([name]) for name in table_names
            }
            print(ta
ble_names)
            self.connection_result.emit(
                True,
                db_connection,
               
 sql_model,
                explain_model,
                table_names_description,
                visual_model,
      
          ehancer_model,
                table_names,
            )
        except Exception as e:
            print(f'E
rror: {e}')
            self.connection_result.emit(
                False, None, None, None, None, None, None, None
   
         )  # Emit failure


# Worker class for processing AI responses in a separate thread
class AIProcessingWorkerSQL
(QThread):
    response_generated = Signal(
        str, object, object
    )  # Signal to send generated responses back


    def __init__(self, sql_model, user_text, chat_history, db, schema, db_type):
        super().__init__()
        se
lf.sql_model = sql_model
        self.chat_history = chat_history
        self.user_text = user_text
        self.db = d
b
        self.data = None
        self.schema = schema
        self.sql_command = None
        self.db_type = db_type


    def db_run(self):
        try:
            self.data = self.db.run(self.sql_command, fetch='cursor').fetchall()
    
        return 'work done'
        except Exception as e:
            print(f'Error: {e}')
            self.data = 'the 
SQL query encountered an error'
            return e

    def run(self):
        query_result = ''
        for attempt i
n range(5):
            print('sql worker++++', attempt)
            ai_response = self.sql_model.invoke(
              
  {
                    'schema': self.schema,
                    'chat_history': self.chat_history,
                  
  'question': self.user_text,
                    'db_type': self.db_type,
                }
            )['text']
     
       ai_response = ai_response.replace('```sql', '').replace('```', '')
            self.sql_command = ai_response
   
         query_result = self.db_run()

            if query_result == 'work done':
                break

            ai
_response = self.sql_model.invoke(
                {
                    'schema': self.schema,
                    'cha
t_history': self.chat_history,
                    'question': 'For this question: '
                    + str(self.user
_text)
                    + f' the query '{self.sql_command}' encountered an error: {query_result}',
                  
  'db_type': self.db_type,
                }
            )['text']
            query_result = self.db_run()

           
 if query_result == 'work done':
                break
        print('sql worker++++', query_result)
        self.respon
se_generated.emit(ai_response, self.data, self.user_text)


class AIProcessingWorkerExplain(QThread):
    responce_gener
ated = Signal(str)

    def __init__(
        self, explain_model, user_text, sql_command, data, chat_history, schema
  
  ):
        super().__init__()
        self.chat_history = chat_history
        self.explain_model = explain_model
    
    self.user_text = user_text
        self.sql_command = sql_command
        self.data = data
        self.schema = sch
ema

    def run(self):
        formatted_string = self.explain_model.invoke(
            {
                'schema': se
lf.schema,
                'chat_history': self.chat_history,
                'query': self.sql_command,
               
 'question': self.user_text['enhanced_question'],
                'response': self.data,
            }
        )['text']

        self.responce_generated.emit(formatted_string)


class AIProcessingWorkerUserExpresstion(QThread):
    responce
_ehancer_ = Signal(object)

    def __init__(self, user_input, model, chat_history, schema):
        super().__init__()

        self.user_input = user_input
        self.model = model
        self.chat_history = chat_history
        self.ta
ble_names = schema

    def clean_json(self, text):
        return text.replace('```json', '').replace('```', '')

    d
ef run(self):
        try:
            for attempt in range(5):
                response = self.model.invoke(
          
          {
                        'user_input': self.user_input,
                        'chat_history': self.chat_his
tory,
                        'tables': self.table_names,
                    }
                )['text']
              
  cleaned_response = self.clean_json(response)
                print(cleaned_response)
                print('user worke
r')
                try:
                    result = json.loads(cleaned_response)
                    self.responce_eha
ncer_.emit(result)
                    break
                except json.JSONDecodeError as decode_error:
              
      # Retry with updated input if there's an error
                    retry_input = f'Question: {self.user_input}, Re
sponse: {cleaned_response}, Error: {decode_error}'
                    retry_response = self.model.invoke(
             
           {
                            'user_input': retry_input,
                            'chat_history': self.cha
t_history,
                            'tables': self.table_names,
                        }
                    )['text
']
                    print('user text')
                    cleaned_retry_response = self.clean_json(retry_response)
 
                   result = json.loads(cleaned_retry_response)
                    self.responce_ehancer_.emit(result)
 
                   break

        except Exception as e:
            print(f'Error: {e}')
            self.responce_ehan
cer_.emit('User API key is invalid')


class AIProcessingWorkerGraph(QThread):
    responce_generated = Signal(str)

   
 def __init__(self, visual_model, user_text, data: pd.DataFrame, sql, complate=''):
        super().__init__()
        s
elf.visual_model = visual_model
        self.user_text = user_text
        self.data = data
        self.complate = comp
late
        self.sql_command = sql

    def run(self):
        formatted_string = self.visual_model.invoke(
           
 {
                'question': str(self.user_text)
                if self.complate == ''
                else str(self.
user_text) + ' do not for get that ' + self.complate,
                'sql': self.sql_command,
                'df_metad
ata': self.data.dtypes,
                'sample_data': self.data.head(),
            }
        )['text']
        self.re
sponce_generated.emit(
            formatted_string.replace('```python', '').replace('```', '')
        )


class MainWi
ndow(QMainWindow):
    def __init__(self):
        super(MainWindow, self).__init__()
        self.setWindowTitle('GPT-l
ike Chat with Database Connection')
        self.sql_model = None
        self.explain_model = None
        self.mainLay
out = QVBoxLayout()
        self.message_used_for_sql = None
        self.popup = None
        self.db_schema = None
   
     self.db_connection = None
        self.visual_model = None
        self.sql_db_type = 'postgresql'
        self.tex
t_ehancer = None
        self.db_tables_details = None
        self.db_tables_names = None
        self.key = self.load_
key()
        self.cipher_suite = Fernet(self.key)
        self.db_document = None
        # List widget for chat
      
  self.listWidget = QListWidget()
        self.listWidget.setSelectionMode(QListWidget.NoSelection)
        self.chatHis
tory = []
        self.screen_geometry = QApplication.primaryScreen().geometry()
        # LineEdit for user input
     
   self.lineEdit = QLineEdit()
        self.lineEdit.returnPressed.connect(self.update_chat)

        self.mainLayout.ad
dWidget(self.listWidget)
        self.mainLayout.addWidget(self.lineEdit)

        # Sidebar layout for database connect
ion details
        self.sidebarLayout = QVBoxLayout()

        # Create a form layout for the database connection detai
ls
        formLayout = QFormLayout()
        self.dbTypeGroup = QButtonGroup(self)
        self.postgresRadio = QRadioB
utton('PostgreSQL')
        self.mysqlRadio = QRadioButton('MySQL')
        self.sqliteRadio = QRadioButton('SQLite')
  
      self.oracleRadio = QRadioButton('Oracle')
        self.mssqlRadio = QRadioButton('MSSQL')
        self.dbTypeGroup
.addButton(self.postgresRadio)
        self.dbTypeGroup.addButton(self.mysqlRadio)
        self.dbTypeGroup.addButton(se
lf.sqliteRadio)
        self.dbTypeGroup.addButton(self.oracleRadio)
        self.dbTypeGroup.addButton(self.mssqlRadio)

        self.postgresRadio.setChecked(True)

        # Add radio buttons to form layout
        db_first_layout = QHBox
Layout()
        db_first_layout.addWidget(self.postgresRadio)
        db_first_layout.addWidget(self.mysqlRadio)
      
  db_first_layout.addWidget(self.sqliteRadio)
        formLayout.addRow(QLabel('Database Type:'))
        formLayout.add
Row(db_first_layout)

        db_second_layout = QHBoxLayout()
        db_second_layout.addWidget(self.oracleRadio)
    
    db_second_layout.addWidget(self.mssqlRadio)
        formLayout.addRow(db_second_layout)

        # Connect radio but
ton signals
        self.postgresRadio.toggled.connect(
            lambda: self.on_db_type_changed('postgresql')
      
  )
        self.mysqlRadio.toggled.connect(lambda: self.on_db_type_changed('mysql'))
        self.sqliteRadio.toggled.c
onnect(lambda: self.on_db_type_changed('sqlite'))
        self.oracleRadio.toggled.connect(lambda: self.on_db_type_chang
ed('oracle'))

        # Creating inputs for the connection details with empty default values
        self.hostInput = Q
LineEdit()
        self.dbnameInput = QLineEdit()
        self.userInput = QLineEdit()
        self.passwordInput = QLin
eEdit()
        self.portInput = QLineEdit('5432')
        self.apiKeyInput = QLineEdit()

        # Adding input fields
 to form layout
        formLayout.addRow(QLabel('Host:'), self.hostInput)
        formLayout.addRow(QLabel('Database Na
me:'), self.dbnameInput)
        formLayout.addRow(QLabel('User:'), self.userInput)
        formLayout.addRow(QLabel('Pa
ssword:'), self.passwordInput)
        formLayout.addRow(QLabel('Port:'), self.portInput)
        formLayout.addRow(QLab
el('API Key:'), self.apiKeyInput)

        # 'Connect' button
        self.connectButton = QPushButton('Connect')
      
  self.connectButton.clicked.connect(self.connect_to_database)
        formLayout.addRow(self.connectButton)

        # 
'Load Credentials' button
        self.loadButton = QPushButton('Load Credentials')
        self.loadButton.clicked.conn
ect(self.load_credentials)
        formLayout.addRow(self.loadButton)

        # Status label to show connection result 
(success or not connected)
        self.statusLabel = QLabel('')
        formLayout.addRow(self.statusLabel)

        # 
Add form layout to sidebar
        self.sidebarLayout.addLayout(formLayout)

        # Spacer to push the button to the 
bottom
        self.sidebarLayout.addStretch()

        # Create Chat Button at the bottom
        self.clearChatButton 
= QPushButton('Clear Chat')
        self.clearChatButton.clicked.connect(self.show_clear_chat_confirmation)
        self
.sidebarLayout.addWidget(self.clearChatButton)
        # Sidebar widget
        self.sidebarWidget = QWidget()
        s
elf.sidebarWidget.setLayout(self.sidebarLayout)

        # Chat widget
        self.chatWidget = QWidget()
        self.
chatWidget.setLayout(self.mainLayout)

        # Using QSplitter to divide the sidebar and the main chat area
        se
lf.splitter = QSplitter(Qt.Horizontal)

        # Add sidebar (with a minimum width) and chat widget
        self.splitt
er.addWidget(self.sidebarWidget)
        self.splitter.addWidget(self.chatWidget)

        # Setting sidebar size limita
tions
        self.sidebarWidget.setMinimumWidth(200)
        self.sidebarWidget.setMaximumWidth(350)

        # Control
 the relative sizing - 1:4 means sidebar takes less space than chat
        self.splitter.setStretchFactor(0, 1)
       
 self.splitter.setStretchFactor(1, 4)

        # Create Toggle Button (Fixed on main layout, outside of sidebar)
       
 self.toggleSidebarButton = QPushButton('Close')
        self.toggleSidebarButton.clicked.connect(self.toggle_sidebar)


        # Create a layout to position the toggle button at the top-left corner
        self.toggleLayout = QVBoxLayout()

        self.toggleLayout.addWidget(self.toggleSidebarButton, alignment=Qt.AlignLeft)

        # Create a wrapper widge
t to hold the toggle button and the splitter
        self.wrapperWidget = QWidget()
        self.wrapperLayout = QVBoxLa
yout()
        self.wrapperLayout.addLayout(self.toggleLayout)  # Add toggle button layout
        self.wrapperLayout.ad
dWidget(self.splitter)  # Add the main splitter
        self.wrapperWidget.setLayout(self.wrapperLayout)

        # Set 
the wrapper widget as the central widget
        self.setCentralWidget(self.wrapperWidget)

    def toggle_sidebar(self)
:
        # Check if sidebar is visible
        if self.sidebarWidget.isVisible():
            self.sidebarWidget.hide()

            self.toggleSidebarButton.setText('Open')  # Change button text when hidden
            self.splitter.setSiz
es(
                [0, self.width()]
            )  # Sidebar hidden, chat takes full width
        else:
            s
elf.sidebarWidget.show()
            self.toggleSidebarButton.setText('Close')  # Reset button text when visible
       
     self.splitter.setSizes([400, self.width() - 200])  # Reset the sidebar width

    def show_clear_chat_confirmation(
self):
        # Create a confirmation popup
        confirmation = QMessageBox()
        confirmation.setWindowTitle('C
reate Chat')
        confirmation.setText('Are you sure you want to clear?')
        confirmation.setStandardButtons(QMe
ssageBox.Yes | QMessageBox.No)
        confirmation.setIcon(QMessageBox.Question)

        # Check the user's response
 
       result = confirmation.exec_()

        if result == QMessageBox.Yes:
            self.listWidget.clear()
        
    self.chatHistory = []

    def on_db_type_changed(self, db_type):
        self.sql_db_type = db_type

    def update
_chat(self):
        if not self.sql_model:
            self.add_message_user('Not connected to the database.')
        
    return

        # Get the user's input
        user_text = self.lineEdit.text()
        self.lineEdit.clear()

     
   self.add_message_user(user_text)
        self.message_used_for_sql = user_text
        self.ehnace_user_text = AIProc
essingWorkerUserExpresstion(
            user_text,
            self.text_ehancer,
            self.chatHistory,
       
     self.db_tables_names,
        )
        self.ehnace_user_text.responce_ehancer_.connect(
            self.user_expr
esstion_after_ehnance
        )

        self.ehnace_user_text.start()

    def user_expresstion_after_ehnance(self, use
r_text):
        if user_text == 'User API key is invalid':
            self.statusLabel.setText('ERROR: While Connectin
g to model')
            self.statusLabel.setStyleSheet('color: red')
            return
        self.db_schema = ''
   
     for i in user_text['useful_tables']:
            if i in self.db_tables_names:
                self.db_schema += se
lf.db_tables_details[i] + '\n'

        # Start a separate thread to process AI response
        self.ai_sql_worker = AI
ProcessingWorkerSQL(
            self.sql_model,
            user_text,
            self.chatHistory,
            self.d
b_connection,
            self.db_schema,
            self.sql_db_type,
        )
        self.ai_sql_worker.response_ge
nerated.connect(self.add_ai_response)
        self.ai_sql_worker.start()

    def add_ai_response(self, ai_response, dat
a, user_en):
        if 'User API key is invalid' in ai_response:
            self.statusLabel.setText(
                
'User API key is invalid',
            )
            self.statusLabel.setStyleSheet('color: red')
            return
   
     self.add_ai_sql_message(ai_response, data, user_en)

    def add_message_user(self, text):
        message_widget =
 QWidget()
        message_layout = QHBoxLayout()
        message_layout.setAlignment(Qt.AlignRight)
        message_wid
get.setLayout(message_layout)
        message_layout.addWidget(QLabel(text))
        list_item = QListWidgetItem()
     
   list_item.setSizeHint(message_widget.sizeHint())
        self.listWidget.addItem(list_item)
        self.listWidget.s
etItemWidget(list_item, message_widget)
        self.listWidget.scrollToBottom()

    def add_ai_sql_message(self, text,
 dataframe, user_en):
        message_widget = QWidget()
        message_layout = QHBoxLayout()
        message_layout.s
etAlignment(Qt.AlignLeft)
        message_widget.setLayout(message_layout)
        temp_var = QTextEdit()
        temp_v
ar.setMarkdown('```sql' + text + '```')
        temp_var.setReadOnly(True)
        temp_var.setMaximumSize(
            
int(self.screen_geometry.size().width() * 0.5),
            self.screen_geometry.size().height(),
        )
        mess
age_layout.addWidget(temp_var)
        print(dataframe)
        print(dataframe is not None)

        action_button = No
ne
        action_button_graph = None

        if dataframe is not None and dataframe != 'the sql query made a error':
 
           action_button = self.create_button(
                'Download', lambda: self.handle_action(dataframe)
       
     )
            message_layout.addWidget(action_button)
            action_button_graph = self.create_button(
       
         'Visualize',
                lambda: self.handle_action_graph(
                    pd.DataFrame(dataframe), use
r_en, text
                ),
            )
            message_layout.addWidget(action_button_graph)

        # Event h
andlers for button visibility
        def show_buttons(event):
            if action_button:
                action_butt
on.setVisible(True)
            if action_button_graph:
                action_button_graph.setVisible(True)

        de
f hide_buttons(event):
            if action_button:
                action_button.setVisible(False)
            if acti
on_button_graph:
                action_button_graph.setVisible(False)

        message_widget.enterEvent = show_buttons

        message_widget.leaveEvent = hide_buttons
        list_item = QListWidgetItem()
        list_item.setSizeHint(me
ssage_widget.sizeHint())
        self.listWidget.addItem(list_item)
        self.listWidget.setItemWidget(list_item, mes
sage_widget)
        self.listWidget.scrollToBottom()
        if self.message_used_for_sql:
            self.ai_explain_
worker = AIProcessingWorkerExplain(
                self.explain_model,
                user_en,
                text,
 
               dataframe,
                self.chatHistory,
                self.db_schema,
            )
            se
lf.chatHistory.append(HumanMessage(str(user_en)))
            self.chatHistory.append(AIMessage(text))
            self.
ai_explain_worker.responce_generated.connect(
                self.add_ai_explain_response
            )

            se
lf.ai_explain_worker.start()
            self.message_used_for_sql = None

    def create_button(self, text, action):
  
      button = QPushButton(text)
        button.setVisible(False)
        button.clicked.connect(action)
        return 
button

    def add_ai_explain_response(self, text):
        message_widget = QWidget()
        message_layout = (
     
       QVBoxLayout()
        )  # Use QVBoxLayout for vertical stacking of content
        message_layout.setAlignment(Q
t.AlignLeft)
        self.chatHistory.append(AIMessage(text))

        message_label = QTextEdit()
        # Display the
 HTML content in the QTextEdit
        message_label.setMarkdown(text)
        message_label.setReadOnly(True)
        m
essage_label.adjustSize()

        message_label.setMaximumSize(
            int(self.screen_geometry.size().width() * 0
.5),
            self.screen_geometry.size().height(),
        )
        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        
message_widget.setLayout(message_layout)
        message_layout.addWidget(message_label)

        list_item = QListWidge
tItem()
        list_item.setSizeHint(message_widget.sizeHint())
        self.listWidget.addItem(list_item)
        self
.listWidget.setItemWidget(list_item, message_widget)
        self.listWidget.scrollToBottom()

    def handle_action_gra
ph(self, df, user_en, sql):
        self.graph_df = pd.DataFrame(df)
        self.graph_user_en = user_en
        self.g
raph_sql = sql
        self.run_graph_generation()

    def run_graph_generation(self, complate=''):
        self.graphg
en = AIProcessingWorkerGraph(
            self.visual_model,
            self.graph_user_en,
            self.graph_df,

            self.graph_sql,
            complate,
        )
        self.graphgen.responce_generated.connect(self.create
_math_plot)
        self.graphgen.start()

    def create_math_plot(self, code):
        try:
            # Execute the 
code
            print(code)
            local_scope = {'df': self.graph_df}
            exec(code, {}, local_scope)
   
     except Exception as e:
            print(code)
            print(f'Error executing code: {e}')
            # Log th
e error
            print(f'Error executing code: {e}')
            # Retry the graph generation
            self.run_gr
aph_generation(str(e))

    def handle_action(self, df):
        print('Downloading data...')
        try:
            i
f df is not pd.DataFrame:
                df = pd.DataFrame(list(df))
            # Convert timezone-aware datetime obje
cts to timezone-unaware
            for col in df.select_dtypes(include=['datetimetz']).columns:
                df[col]
 = df[col].apply(
                    lambda x: x.strftime('%Y-%m-%d %H:%M:%S.%f %Z')
                    if pd.notnull(
x)
                    else ''
                )
            for col in df.select_dtypes(include=['timedelta']).columns:

                df[col] = df[col].apply(lambda x: str(x) if pd.notnull(x) else '')

            # Print the DataFrame t
o verify its content

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            excel_filename = f'do
wnloaded_data_{timestamp}.xlsx'
            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
          
      df.to_excel(writer, index=False)

            print(f'Data downloaded successfully as '{excel_filename}'.')
      
      self.statusLabel.setText(
                f'Data downloaded successfully as '{excel_filename}'.'
            )
   
         self.statusLabel.setStyleSheet('color: green;')

            # Open the downloaded Excel file (Windows-specific
)
            if os.name == 'nt':
                os.startfile(excel_filename)
            else:
                print(f
'Please open the file manually: {excel_filename}')

        except Exception as e:
            print(f'Error while downl
oading data: {e}')
            self.statusLabel.setText('Error downloading data.')
            self.statusLabel.setStyle
Sheet('color: red')

    def connect_to_database(self):
        self.statusLabel.setText('Connecting...')
        self.s
tatusLabel.setStyleSheet('color: orange;')
        self.connectButton.setEnabled(False)
        # Start the worker threa
d to connect to the database
        self.db_worker = DatabaseConnectionWorker(
            self.hostInput.text(),
     
       self.dbnameInput.text(),
            self.userInput.text(),
            self.passwordInput.text(),
            se
lf.portInput.text(),
            self.apiKeyInput.text(),
            self.sql_db_type,
        )
        self.db_worker
.connection_result.connect(self.on_connection_result)
        self.db_worker.start()

    def on_connection_result(
    
    self,
        success,
        db_connection,
        sql_model,
        explain_model,
        table_info,
        
visual_model,
        ehancer_model,
        table_names,
    ):
        if success:
            self.db_tables_details 
= table_info
            self.db_tables_names = table_names
            self.db_connection = db_connection
            s
elf.sql_model = sql_model
            self.explain_model = explain_model
            self.visual_model = visual_model
  
          self.text_ehancer = ehancer_model
            self.statusLabel.setText('Success')
            self.statusLabel
.setStyleSheet('color: green;')
            # Save credentials after successful connection
            self.save_credent
ials()
        else:
            self.statusLabel.setText('Not Connected')
            self.statusLabel.setStyleSheet('c
olor: red')
        self.connectButton.setEnabled(True)

    def generate_key(self):
        key = Fernet.generate_key()

        with open(KEY_FILE, 'wb') as key_file:
            key_file.write(key)

    def load_key(self):
        if not 
os.path.exists(KEY_FILE):
            self.generate_key()
        with open(KEY_FILE, 'rb') as key_file:
            ret
urn key_file.read()

    def encrypt(self, data):
        return self.cipher_suite.encrypt(data.encode()).decode()

    
def decrypt(self, data):
        return self.cipher_suite.decrypt(data.encode()).decode()

    def save_credentials(self
):
        credentials = {
            'host': self.hostInput.text(),
            'dbname': self.dbnameInput.text(),
   
         'user': self.userInput.text(),
            'password': self.passwordInput.text(),
            'port': self.port
Input.text(),
            'api_key': self.apiKeyInput.text(),
            'db_type': self.sql_db_type,
        }
       
 encrypted_credentials = {k: self.encrypt(v) for k, v in credentials.items()}
        with open(CONFIG_FILE, 'w') as f:

            json.dump(encrypted_credentials, f)

    def load_credentials(self):
        if os.path.exists(CONFIG_FILE):

            with open(CONFIG_FILE, 'r') as f:
                encrypted_credentials = json.load(f)
                cred
entials = {
                    k: self.decrypt(v) for k, v in encrypted_credentials.items()
                }
         
       self.db_type = credentials.get('db_type', 'postgresql')
                if self.db_type == 'postgresql':
        
            self.postgresRadio.setChecked(True)
                elif self.db_type == 'mysql':
                    self.m
ysqlRadio.setChecked(True)
                elif self.db_type == 'sqlite':
                    self.sqliteRadio.setChecke
d(True)
                elif self.db_type == 'oracle':
                    self.oracleRadio.setChecked(True)
           
     elif self.db_type == 'mssql':
                    self.mssqlRadio.setChecked(True)
                self.hostInput.s
etText(credentials.get('host', ''))
                self.dbnameInput.setText(credentials.get('dbname', ''))
            
    self.userInput.setText(credentials.get('user', ''))
                self.passwordInput.setText(credentials.get('pass
word', ''))
                self.portInput.setText(credentials.get('port', '5432'))
                self.apiKeyInput.set
Text(credentials.get('api_key', ''))
        else:
            self.statusLabel.setText('No saved credentials found.')
 
           self.statusLabel.setStyleSheet('color: red')

    def save_chat_history(self):
        with open('chat_histor
y.json', 'w') as f:
            json.dump([str(msg) for msg in self.chatHistory], f)

    def closeEvent(self, event):
 
       self.save_chat_history()
        event.accept()


app = QApplication(sys.argv)
apply_stylesheet(app, theme='theam
.xml')
window = MainWindow()
window.show()

sys.exit(app.exec())
```


file-2(test_sql_1.py)
```python
from langchain_co
re.prompts import PromptTemplate
from langchain_community.utilities import SQLDatabase
from langchain_google_genai impor
t ChatGoogleGenerativeAI
from langchain.chains import LLMChain


def init_database(
    user: str,
    password: str,
  
  host: str,
    port: str,
    database: str,
    db_type: str = 'postgresql',
) -> SQLDatabase:
    if db_type == 'pos
tgresql':
        db_uri = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}'
    elif db_type == 'mysq
l':
        db_uri = f'mysql+mysqldb://{user}:{password}@{host}:{port}/{database}'
    elif db_type == 'sqlite':
       
 db_uri = f'sqlite:///{database}'
    elif db_type == 'oracle':
        db_uri = f'oracle+cx_oracle://{user}:{password}@
{host}:{port}/{database}'
    elif db_type == 'mssql':
        db_uri = f'mssql+pyodbc://{user}:{password}@{host}:{port}
/{database}'
    return SQLDatabase.from_uri(db_uri)


def get_sql_chain(api_key):
    print('sql_1')
    template = Pro
mptTemplate(
        input_variables=['schema', 'chat_history', 'question', 'db_type'],
        template='')
    print('
sql_2')
    # llm = ChatMistralAI(model='codestral-2405', mistral_api_key=MINSTRAL_API_KEY)
    llm = ChatGoogleGenerati
veAI(
        model='gemini-1.5-flash', google_api_key=api_key, temperature=0
    )
    chain = LLMChain(
        llm=ll
m,
        prompt=template,
    )

    return chain


def get_response(api_key):
    print('nor_1')
    template = Promp
tTemplate(
        input_variables=['schema', 'chat_history', 'query', 'question', 'response'],
        template='')
   
 print('nor_2')
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-flash', google_api_key=api_key, temperature=
0.1
    )
    chain = LLMChain(llm=llm, prompt=template)
    return chain


def generate_plotly_code(api_key) -> str:
  
  print('plotly_1')
    template = PromptTemplate(
        input_variables=['question', 'sql', 'df_metadata', 'sample_da
ta'],
        template='')
    print('plotly_2')
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-flash', goo
gle_api_key=api_key, temperature=0
    )

    chain = LLMChain(llm=llm, prompt=template)

    return chain


def better_
question(api_key):
    template = PromptTemplate(
        input_variables=['user_input', 'chat_history', 'tables'],
    
    template=''
        
    )
    llm = ChatGoogleGenerativeAI(
        model='gemini-1.5-flash', google_api_key=api_ke
y, temperature=0.1
    )

    chain = LLMChain(llm=llm, prompt=template)

    return chain
``` 
```
---

     
 
all -  [ What are the best practices for loading and splitting Confluence data into a vectorstore for RAG? ](https://www.reddit.com/r/LangChain/comments/1g46j4w/what_are_the_best_practices_for_loading_and/) , 2024-10-17-0912
```
Hello fellow developers,

I'm working on a project that involves integrating our internal Confluence knowledge base with
 a RAG system. I'm facing some challenges and would appreciate your insights:

1. Splitting unstructured data:
   * Init
ially used a basic text splitter with overlapping (suboptimal results)
   * Tried an HTML splitter, but it separates hea
ders from text and cuts off important information - doesn't seem to be the best approach
   * What's the most effective 
approach for maintaining context and relevance?
2. Dealing with outdated content:
   * Our Confluence pages and spaces a
ren't consistently updated
   * How can we ensure our RAG system uses the most current information?<
   * Do you have an
y idea how to fix/improve the 'outdated' data problem?

Has anyone tackled similar issues? I'd love to hear about your e
xperiences and any best practices you've discovered.
```
---

     
 
all -  [ How can I add an extra column in the SQL agent response table ? ](https://www.reddit.com/r/LangChain/comments/1g45i24/how_can_i_add_an_extra_column_in_the_sql_agent/) , 2024-10-17-0912
```
Hello everyone , I'm learning about Langgraph agents and developing a project around it.   
  
It's an SQL agent and is 
derived from here \[ [https://docs.smith.langchain.com/tutorials/Developers/agents#sql-agent](https://docs.smith.langcha
in.com/tutorials/Developers/agents#sql-agent) \].  
You can see one of the QnA below .    
My SQL table 'Employee' has N
ame , Location , Experience , Skills , Graduation , Post Graduation , PhD fields only that you guys can also see in the 
AI response.

[Chat with SQL agent](https://preview.redd.it/378uewfsnwud1.png?width=491&format=png&auto=webp&s=219a2738e
e2e55de584a35de3804f2172dcb5cb6)

  
Now , I want to add a column 'Match %' when AI is returning the response to the use
r so that when a user queries for candidate using the job description, he get a column 'Match %' that tells us how much 
% does candidate match.  
  
How can I add this functionality ?
```
---

     
 
all -  [ How to stop generation during streaming? ](https://www.reddit.com/r/LangChain/comments/1g458w6/how_to_stop_generation_during_streaming/) , 2024-10-17-0912
```
I use astream\_events in python to stream model output. Now I want to terminate the model when running a specific tool. 
Just stopping the streaming does not stop the generation and will save no computation costs.

I tried stop sequences but
 they only work when the model generates them itself, not when the tooloutput contains it. So how can I achieve this?
```
---

     
 
all -  [ Need help with understanding Langgraph :) ](https://www.reddit.com/r/LangChain/comments/1g43vje/need_help_with_understanding_langgraph/) , 2024-10-17-0912
```
I have enrolled in the Langgraph course from Langchain academy and I am in the verge of completion 🏁. I could understand
 the concept of graph and states. 🙂

But I have few doubts and that creates roadblocks in my learning journey. 😭

1. Is 
anything created via Langgraph is considered as Agents ? 

2. Is Langgraph designed to work with web frameworks like Dja
ngo, FastAPI or is it just a background process ? 

3. How can I provide human input/feedback via UI to Langgraph (via h
ttp request) ? (If integration with web frameworks is possible) 

4. Is it something that needs to be deployed in Langch
ain cloud and accessed via API ? 

Please help me to understand and help of any kind would be greatly appreciated. 

Tha
nks in advance 🥲👍🏻
```
---

     
 
all -  [ What gets deployed into LangGraph Cloud? ](https://www.reddit.com/r/LangChain/comments/1g410ef/what_gets_deployed_into_langgraph_cloud/) , 2024-10-17-0912
```
I was reading over the LangGraph docs and wasn’t clear on what happens during deployment. 

When you deploy a graph into
 LangGraph Cloud, what do you get?

An API endpoint you can interact with, or it just runs the Python code and reports b
ack to LangSmith?

How would human in the loop interaction work once the graph was deployed?

Appreciate any insight!
```
---

     
 
all -  [ FloAI: A composable AI Agent Builder (looking for feedback) ](https://www.reddit.com/r/LangChain/comments/1g40v7h/floai_a_composable_ai_agent_builder_looking_for/) , 2024-10-17-0912
```
[Flo](https://github.com/rootflo/flo-ai) was born out of a need for a more streamlined and powerful solution for buildin
g AI agentic workflows. While frameworks like **CrewAI** fell short in offering the flexibility developers needed, and L
angGraph became a challenge to set up and run, Flo provides an ideal middle ground. It’s designed to be the “Keras for T
ensorFlow” of agentic workflows, offering pre-built components for rapid prototyping while allowing deep control for cus
tom, production-level systems. Whether you’re developing simple or intricate workflows, Flo makes AI composability easy 
and powerful.

With its flexible architecture, you can create teams of agents, leverage different router types, and buil
d AI workflows that scale easily. Flo empowers developers to take control of their AI systems, making it a breeze to ada
pt, prototype, and push the boundaries of agentic AI.

Do check the repository and happy to take feedback: [https://gith
ub.com/rootflo/flo-ai](https://github.com/rootflo/flo-ai).  
Give us a star if you think what we plan to build is intere
sting
```
---

     
 
all -  [  Project Alice - v0.2 => open source platform for agentic workflows  ](https://www.reddit.com/r/LangChain/comments/1g3r1ol/project_alice_v02_open_source_platform_for/) , 2024-10-17-0912
```
Hello everyone! A few months ago I launch a project I'd been working on called Project Alice. And today I'm happy to sha
re an incredible amount of progress, and excited to get people to try it out.

To that effect, I've created a few videos
 that show you how to install the platform and an overview of it:

* [Part 1](https://www.youtube.com/watch?v=ojhcb9ADJq
U) (11:38)
* [Part 2](https://www.youtube.com/watch?v=oXGk6g_gPtU) (8:38)

Repository: [Link](https://github.com/Mariano
Molina/project_alice)

# What is it though?

A free open source framework and platform for agentic workflows. It include
s a frontend, backend and a python logic module. It takes 5 minutes to install, no coding needed, and you get a frontend
 where you can create your own agents, chats, task/workflows, etc, run your tasks and/or chat with your agents. You can 
use local models, or most of the most used API providers for AI generation.

You don't need to know how to code at all, 
but if you do, you have full flexibility to improve any aspect of it since its all open source. The platform has been pu
rposefully created so that it's code is comprehensible, easy to upgrade and improve. Frontend and backend are in TS, pyt
hon module uses Pydantic almost to a pedantic level.

It has a total of 22 apis at the moment:

        OPENAI
        O
PENAI_VISION
        OPENAI_IMG_GENERATION
        OPENAI_EMBEDDINGS
        OPENAI_TTS
        OPENAI_STT
        OPENA
I_ASTT
        AZURE
        GEMINI
        GEMINI_VISION
        GEMINI_IMG_GEN => Google's sdk is broken atm
        M
ISTRAL
        MISTRAL_VISION
        MISTRAL_EMBEDDINGS
        GEMINI_STT
        GEMINI_EMBEDDINGS
        COHERE
   
     GROQ
        GROQ_VISION
        GROQ_TTS
        META
        META_VISION
        ANTHROPIC
        ANTHROPIC_VISI
ON
        LM_STUDIO
        LM_STUDIO_VISION
        GOOGLE_SEARCH
        REDDIT_SEARCH
        WIKIPEDIA_SEARCH
     
   EXA_SEARCH
        ARXIV_SEARCH
        GOOGLE_KNOWLEDGE_GRAPH

And an uncountable number of models that you can depl
oy with it.

It is going to keep getting better. If you think this is nice, wait until the next update drops. And if you
 feel like helping out, I'd be super grateful. I'm about to tackle RAG and ReACT capabilities in my agents, and I'm sure
 a lot of people here have some experience with that. Maybe the idea of trying to come up with a (maybe industry?) stand
ard sounds interesting?

Check out the videos if you want some help installing and understanding the frontend. Ask me an
y questions otherwise!
```
---

     
 
all -  [ NaturalAgents - notion-style editor to easily create AI Agents ](https://www.reddit.com/r/OpenSourceAI/comments/1g3qsir/naturalagents_notionstyle_editor_to_easily_create/) , 2024-10-17-0912
```
[NaturalAgents](https://github.com/NaturalAgents/NaturalAgents) is the easiest way to create AI Agents in a notion-style
 editor without code - using plain english and simple macros. It's fully open-source and will be actively maintained.

H
ow this is different from other agent builders -

1. No boilerplate code (imagine langchain for multiple agents)
2. No c
ode experience
3. Can easily share and build with others
4. Readable/organized agent outputs
5. Abstracts agent communic
ations without visual complexity (image large drag and drop flowcharts)

Would love to hear thoughts and feel free to re
ach out if you're interested in contributing!
```
---

     
 
all -  [ Vector Store Usage for RAG ](https://www.reddit.com/r/LangChain/comments/1g3puzu/vector_store_usage_for_rag/) , 2024-10-17-0912
```
I'm a newbie building an RAG application for a simple documentation Q&A where the user enters the URL for a documentatio
n and can ask questions on it. I understand I need a vector store for storing the embeddings. My question is would I nee
d separate collections for every documentation. And if a user enters a documentation that already exists, should it be o
verwritten with new embeddings?
```
---

     
 
all -  [ Does RAG Have a Scaling Problem? ](https://www.reddit.com/r/LlamaIndex/comments/1g3put2/does_rag_have_a_scaling_problem/) , 2024-10-17-0912
```
My team has been digging into the scalability of vector databases for RAG (Retrieval-Augmented Generation) systems, and 
we feel we might be hitting some limits that aren’t being widely discussed.

We tested Pinecone (using both LangChain an
d LlamaIndex) out to 100K pages. We found those solutions started to lose search accuracy in as few as 10K pages. At 100
K pages in the RAG, search accuracy dropped 10-12%.

To be clear, we think this is a vector issue not an orchestrator is
sue. Though we did find particular problems trying to scale LangChain ingestion because of Unstructured (more on the end
 of the piece about that). 

We also tested our approach at [EyeLevel.ai](http://eyelevel.ai/), which does not use vecto
rs at all (I know it sounds crazy), and found only a 2% drop in search accuracy at 100K pages. And showed better accurac
y by significant margins from the outset.

I'm posting our research here to start a conversation on non-vector based app
roaches to RAG. We think there's a big opportunity to do things differently that's still very compatible with orchestrat
ors like LangChain.  We'd love to build a community around it. 

**Here's our research below. I would love to know if an
yone else is exploring non-vector approaches to RAG and of course your thoughts on the research.**

**We explain the res
earch and results on YT as well.**  
[https://youtu.be/qV1Ab0qWyT8?si=dgPL21xAHeI2jXJo](https://youtu.be/qV1Ab0qWyT8?si=
dgPL21xAHeI2jXJo)

[Image: The chart shows accuracy loss at just 10,000 pages of content using a Pinecone vector databas
e with both LangChain and Llamaindex-based RAG applications.  Conversely, EyeLevel's GroundX APIs for RAG show almost no
 loss.](https://preview.redd.it/74oi00ko1sud1.png?width=1600&format=png&auto=webp&s=5f86d97d682631215832ab3f92b34c9641f8
d3e9)

# What’s Inside

In this report, we will review how the test was constructed, the detailed findings, our theories
 on why vector similarity search experienced challenges and suggested approaches to scale RAG without the performance hi
t. We also encourage you to read our [prior research](https://www.eyelevel.ai/post/most-accurate-rag) in which EyeLevel’
s GroundX APIs bested LangChain, Pinecone and Llamaindex based RAG systems by 50-120% on accuracy over 1,000 pages of co
ntent.  

The work was performed by Daniel Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a
 computer scientist and former senior engineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods 
of this test will be open sourced and available shortly. Others are invited to run the data and corroborate or challenge
 these findings. 

# Defining RAG 

**Feel free to skip this section if you’re familiar with RAG.**  

RAG stands for “R
etrieval Augmented Generation”. When you ask a RAG system a query, RAG does the following steps: 

1. Retrieval: Based o
n the query from the user, the RAG system retrieves relevant knowledge from a set of documents. 
2. Augmentation: The RA
G system combines the retrieved information with the user query to construct a prompt. 
3. Generation: The augmented pro
mpt is passed to a large language model, generating the final output. 

The implementation of these three steps can vary
 wildly between RAG approaches. However, the objective is the same: to make a language model more useful by feeding it i
nformation from real-world, relevant documents. 

RAG allows a language model to reference application specific informat
ion from human documents, allowing developers to build tailored and specific products 

# Beyond The Tech Demo 

When mo
st developers begin experimenting with RAG they might grab a few documents, stick them into a RAG document store and be 
blown away by the results. Like magic, many RAG systems can allow a language model to understand books, company document
s, emails, and more. 

However, as one continues experimenting with RAG, some difficulties begin to emerge. 

1. Many do
cuments are not purely textual. They might have images, tables, or complex formatting. While many RAG systems can parse 
complex documents, the quality of parsing varies widely between RAG approaches. We explore the realities of parsing in[ 
another article](https://www.eyelevel.ai/post/most-accurate-rag). 
2. As a RAG system is exposed to more documents, it h
as more opportunities to retrieve the wrong document, potentially causing a degradation in performance   
3. Because of 
technical complexity, the underlying non-determinism of language models, and the difficulty of profiling the performance
 of LLM applications in real world settings, it can be difficult to predict the cost and level of effort of developing R
AG applications. 

In this article we’ll focus on the second and third problems listed above; performance degradation of
 RAG at scale and difficulties of implementation 

# The Test 

To test how much larger document sets degrade the perfor
mance of RAG systems, we first defined a set of 92 questions based on real-world documents.  

[A few examples of the re
al-world documents used in this test, which contain answers to our 92 questions. ](https://preview.redd.it/ojr3ab7s1sud1
.png?width=1429&format=png&auto=webp&s=6ee3ce03d23a9ca0a35b181869c71a213d0da362)

A few examples of the real-world docum
ents used in this test, which contain answers to our 92 questions. 

We then constructed four document sets to apply RAG
 to. All four of these document sets contain the same 310 pages of documents which answer our 92 test questions. However
, each document set also contains a different number of irrelevant pages from miscellaneous documents. We started with 1
,000 pages and scaled up to 100,000 in our largest test. 

[We asked the same questions based on the same set of documen
ts \(blue\), but exposed the RAG system to varying amounts of unrelated documents \(red\). This diagram shows the number
 of relevant pages in each document set, compared to the total size of each document set.](https://preview.redd.it/r9nhj
ahv1sud1.png?width=1431&format=png&auto=webp&s=c09a05becc851cd1199fe39c92ee1753bc9f8950)

We asked the same questions ba
sed on the same set of documents (blue), but exposed the RAG system to varying amounts of unrelated documents (red). Thi
s diagram shows the number of relevant pages in each document set, compared to the total size of each document set.

An 
ideal RAG system would, in theory, behave identically across all document sets, as all document sets contain the same an
swers to the same questions. In practice, however, added information in a docstore can trick a RAG system into retrievin
g the wrong context for a given query. The more documents there are, the more likely this is to happen. Therefore, RAG p
erformance tends to degrade as the number of documents increases. 

In this test we applied each of these three popular 
RAG approaches to the four document sets mentioned above:

* LangChain: a popular python library designed to abstract ce
rtain LLM workflows. 
* LlamaIndex: a popular python library which has advanced vector embedding capability, and advance
d RAG functionality. 
* EyeLevel’s GroundX: a feature complete retrieval engine built for RAG. 

By applying each of the
se RAG approaches to the four document sets, we can study the relative performance of each RAG approach at scale. 

For 
both LangChain and LlamaIndex we employed Pinecone as our vector store and OpenAI’s text-embedding-ada-002 for embedding
. GroundX, being an all-in-one solution, was used in isolation up to the point of generation. All approaches used OpenAI
's gpt-4-1106-preview for the final generation of results. Results for each approach were evaluated as being true or fal
se via human evaluation. 

# The Effect of Scale on RAG 

We ran the test as defined in the previous section and got the
 following results. 

[The performance of different RAG approaches varies greatly, both in base performance and the rate
 of performance degradation at scale. We explore differences in base performance thoroughly in another article ](https:/
/preview.redd.it/o0x2y1dx1sud1.png?width=1600&format=png&auto=webp&s=fe6ce89f4b7657de3221b0e5b93110ecca20c546)

The perf
ormance of different RAG approaches varies greatly, both in base performance and the rate of performance degradation at 
scale. We explore differences in base performance thoroughly in another article 

As can be seen in the figure above, th
e rate at which RAG degrades in performance varies widely between RAG approaches. Based on these results one might expec
t GroundX to degrade in performance by 2% per 100,000 documents, while LCPC and LI might degrade 10-12% per 100,000 docu
ments. The reason for this difference in robustness to larger document sets, likely, has to do with the realities of usi
ng vector search as the bedrock of a RAG system.  

In theory a high dimensional vector space can hold a vast amount of 
information. 100,000 in binary is 17 values long (11000011010100000). So, if we *only* use binary vectors with unit comp
onents in a high dimensional vector space, we could store each page in our 100,000 page set with only a 17 dimensional s
pace. Text-embedding-ada-002, which is the encoder used in this experiment, outputs a 1536-dimension vector. If one calc
ulates 2\^1536 (effectively calculating how many things one could describe using only binary vectors in this space) the 
result would be a number that’s significantly greater than the number of atoms in the known universe. Of course, actual 
embeddings are not restricted to binary numbers; they can be expressed in decimal numbers of very high precision. Even r
elatively small vector spaces can hold a vast amount of information. 

The trick is, how do you get information into a v
ector space *meaningfully*? RAG needs content to be placed in a vector space such that similar things can be searched, t
hus the encoder has to practically organize information into useful regions. It’s our theory that modern encoders don’t 
have what it takes to organize large sets of documents in these vector spaces, even if the vector spaces can theoretical
ly fit a near infinite amount of information. The encoder can only put so much information into a vector space before th
e vector space gets so cluttered that distance-based search is rendered non-performant. 

[There is a big difference bet
ween a space being able to fit information, and that information being meaningfully organized. ](https://preview.redd.it
/inp40j1f2sud1.png?width=948&format=png&auto=webp&s=f7b0c838224196d7d0f24cb6db7abae3dbd8431d)

There is a big difference
 between a space being able to fit information, and that information being meaningfully organized. 

EyeLevel’s GroundX 
doesn’t use vector similarity as its core search strategy, but rather a tuned comparison based on the similarity of sema
ntic objects. There are no vectors used in this approach. This is likely why GroundX exhibits superior performance in la
rger document sets. 

In this test we employed what is commonly referred to as “naive” RAG. LlamaIndex and LangChain all
ow for many advanced RAG approaches, but they had little impact on performance and were harder to employ at larger scale
s. We cover that in another article which will be released shortly.

# The Surprising Technical Difficulty of Scale 

Wh
ile 100,000 pages seems like a lot, it’s actually a fairly small amount of information for industries like engineering, 
law, and healthcare. Initially we imagined testing on much larger document sets, but while conducting this test we were 
surprised by the practical difficulty of getting LangChain to work at scale; forcing us to reduce the scope of our test.
 

To get RAG up and running for a set of PDF documents, the first step is to parse the content of those PDFs into some 
sort of textual representation. LangChain uses libraries from [Unstructured.io](http://unstructured.io/) to perform pars
ing on complex PDFs, which works seamlessly for small document sets. 

Surprisingly, though, the speed of LangChain pars
ing is incredibly slow. Based on our analysis it appears that Unstructured uses a variety of models to detect and parse 
out key elements within a PDF. These models should employ GPU acceleration, but they don’t. That results in LangChain ta
king days to parse a modestly sized set of documents, even on very large (and expensive) compute instances. To get LangC
hain working we needed to reverse engineer portions of Unstructured and inject code to enable GPU utilization of these m
odels. 

It appears that this is a known issue in Unstructured, as seen in the notes below. As it stands, it presents si
gnificant difficulty in scaling LangChain to larger document sets, given LangChain abstracts away fine grain control of 
Unstructured. 

https://preview.redd.it/zebe6lqg2sud1.png?width=1346&format=png&auto=webp&s=393cefee81ed7de8f8b936c0087b
aa0c326fa1d0

[**Source: Github**](https://github.com/Unstructured-IO/unstructured-inference/blob/64cd41c37fe4535702b3be
9c6b58f380ca4c7edd/unstructured_inference/inference/layout.py#L175)

We only made improvements to LangChain parsing up t
o the point where this test became feasible. If you want to modify LangChain for faster parsing, here are some resources
: 

* The default directory loader of LangChain is Unstructured ([source1](https://python.langchain.com/v0.1/docs/module
s/data_connection/document_loaders/file_directory/),[ source2](https://github.com/langchain-ai/langchain/blob/410e9add44
43607618a75827afe1a676fcd7c0a7/libs/community/langchain_community/document_loaders/directory.py#L38)). 
* Unstructured u
ses “hi res” for the PDFs by default if text extraction cannot be performed on the document ([source1](https://github.co
m/langchain-ai/langchain/blob/410e9add4443607618a75827afe1a676fcd7c0a7/libs/community/langchain_community/document_loade
rs/directory.py#L38) ,[ source2](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a5
7bf078/unstructured/partition/strategies.py#L103) ). Other options are available like “fast” and “OCR only”, which have 
different processing intensities 
* “Hi Res” involves: 
   * Converting the pdf into images ([source](https://github.com
/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078/unstructured/partition/pdf_image/pdfminer_pr
ocessing.py#L36)) 
   * Running a layout detection model to understand the layout of the documents ([source](https://git
hub.com/Unstructured-IO/unstructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured_inference/mode
ls/yolox.py#L3)). This model benefits greatly from GPU utilization, but does not leverage the GPU unless ONNX is install
ed ([source](https://onnxruntime.ai/docs/get-started/with-python.html)) 
   * OCR extraction using tesseract (by default
) ([source](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078/unstructured/p
artition/utils/config.py#L103)) which is a very compute intensive process ([source](https://github.com/tesseract-ocr/tes
seract/issues/263)) 
   * Running the page through a table layout model ([source](https://github.com/Unstructured-IO/uns
tructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured_inference/models/tables.py#L140)) 

While
 our configuration efforts resulted in faster processing times, it was still too slow to be feasible for larger document
 sets. To reduce time, we did “hi res” parsing on the relevant documents and “fast” parsing on documents which were irre
levant to our questions. With this configuration, parsing 100,000 pages of documents took 8 hours. If we had applied “hi
 res” to all documents, we imagine that parsing would have taken 31 days (at around 30 seconds per page). 

At the end o
f the day, this test took two senior engineers (one who’s worked at a directorial level at several AI companies, and a m
ulti company CTO with decades of applied experience of AI at scale) several weeks to do the development necessary to wri
te this article, largely because of the difficulty of applying LangChain to a modestly sized document set.  To get LangC
hain working in a production setting, we estimate that the following efforts would be required: 

* Tesseract would need
 to be interfaced with in a way that is more compute and time efficient. This would likely require a high-performance CP
U instance, and modifications to the LangChain source code. 
* The layout and table models would need to be made to run 
on a GPU instance 
* To do both tasks in a cost-efficient manner, these tasks should probably be decoupled. However, thi
s is not possible with the current abstraction of LangChain. 

On top of using a unique technology which is highly perfo
rmant, GroundX also abstracts virtually all of these technical difficulties behind an API. You upload your documents, th
en search the results. That’s it. 

If you want RAG to be even easier, one of the things that makes Eyelevel so compelli
ng is the service aspect they provide to GroundX. You can work with Eyelevel as a partner to get GroundX working quickly
 and performantly for large scale applications. 

# Conclusion 

When choosing a platform to build RAG applications, eng
ineers must balance a variety of key metrics. The robustness of a system to maintain performance at scale is one of thos
e critical metrics. In this head-to-head test on real-world documents, EyeLevel’s GroundX exhibited a heightened level o
f performance at scale, beating LangChain and LlamaIndex. 

Another key metric is efficiency at scale. As it turns out, 
LangChain has significant implementation difficulties which can make the large-scale distribution of LangChain powered R
AG difficult and costly. 

Is this the last word? Certainly not. In future research, we will test various advanced RAG t
echniques, additional RAG frameworks such as Amazon Q and GPTs and increasingly complex and multimodal data types. So st
ay tuned. 

If you’re curious about running these results yourself, please reach out to us at [info@eyelevel.ai.Vector](
mailto:info@eyelevel.ai.Vector) databases, a key technology in building retrieval augmented generation or RAG applicatio
ns, has a scaling problem that few are talking about. 

According to new research by [EyeLevel.ai](https://www.eyelevel.
ai/), an AI tools company, the precision of vector similarity search degrades in as few as 10,000 pages, reaching a 12% 
performance hit by the 100,000-page mark.

The research also tested [EyeLevel’s enterprise-grade RAG platform](https://w
ww.eyelevel.ai/product/apis) which does not use vectors. EyeLevel lost only 2% accuracy at scale.

The findings suggest 
that while vector databases have become highly popular tools to build RAG and LLM-based applications, developers may fac
e unexpected challenges as they shift from testing to production and attempt to scale their applications.  

The work wa
s performed by Daniel Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a computer scientist a
nd former senior engineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods of this test will be 
open sourced and available shortly. Others are invited to run the data and corroborate or challenge these findings. 

My
 team has been digging into the scalability of vector databases for RAG (Retrieval-Augmented Generation) systems, and we
 feel we might be hitting some limits that aren’t being widely discussed.

We tested Pinecone (using both LangChain and 
LlamaIndex) out to 100K pages. We found those solutions started to lose search accuracy in as few as 10K pages. At 100K 
pages in the RAG, search accuracy dropped 10-12%.

We also tested our approach at [EyeLevel.ai](http://eyelevel.ai/), wh
ich does not use vectors at all (I know it sounds crazy), and found only a 2% drop in search accuracy at 100K pages. And
 showed better accuracy by significant margins from the outset.

**Here's our research below. I would love to know if an
yone else is exploring non-vector approaches to RAG and of course your thoughts on the research.**

**We explain the res
earch and results on YT as well.**  
[https://youtu.be/qV1Ab0qWyT8?si=dgPL21xAHeI2jXJo](https://youtu.be/qV1Ab0qWyT8?si=
dgPL21xAHeI2jXJo) 
```
---

     
 
all -  [ I have a python codebase that uses streamlit, want to move to react for easier integration. ](https://www.reddit.com/r/LangChain/comments/1g3milz/i_have_a_python_codebase_that_uses_streamlit_want/) , 2024-10-17-0912
```
As the title states I've been working on a predominately python codebase with a lot of langchian concepts that have been
 written in python and I want to migrate away from streamlit to some sort of react library, please do let me know if the
re are any solutions availabel thank you.
```
---

     
 
all -  [ Python or Typescript for RAG? ](https://www.reddit.com/r/Rag/comments/1g3lgzy/python_or_typescript_for_rag/) , 2024-10-17-0912
```
I am working on a project for my Bachelor's thesis and want to build a Retrieval-Augmented Generation (RAG) system. For 
Langchain and LlamaIndex, there are both Python and TypeScript versions available. I am new to this topic but would like
 to set up a web application in the medium term, which will be programmed in Next.js later on. Since the project will ru
n on a server, does the backend language really matter? Or should I stick to one language, since TypeScript can be used 
in Next.js? The problem I see is that the documentation for Langchain and LlamaIndex in TypeScript is tiny compared to P
ython. As a beginner in this field, will I still be able to manage with TypeScript? I would prefer not to get stuck, but
 I would also like to cover my future projects. Another advantage would be that I could use the code for Obsidian plugin
 development, which can also be done in TypeScript. What do you think? Thanks in advance!
```
---

     
 
all -  [ Has anyone seen AI agents working in production at scale? ](https://www.reddit.com/r/LocalLLaMA/comments/1g3jkct/has_anyone_seen_ai_agents_working_in_production/) , 2024-10-17-0912
```
Has anyone seen AI agents working in production at scale? 

It doesn't matter if you're using the Swarm, langchain, or a
ny other AI agent orchestration framework if the underlying issue is that AI agents too slow, too expensive, and too unr
eliable. I wrote about [AI agent hype vs. reality](https://www.kadoa.com/blog/ai-agents-hype-vs-reality) a while ago, an
d I don't think it has changed yet.

>By combining tightly constrained LLMs, good evaluation data, human-in-the-loop ove
rsight, and traditional engineering methods, we can achieve reliably good results for automating medium-complex tasks.


>Will AI agents automate tedious repetitive work, such as web scraping, form filling, and data entry? Yes, absolutely.


>Will AI agents autonomously book your vacation without your intervention? Unlikely, at least in the near future.

What 
are your real-world use cases and experiences?
```
---

     
 
all -  [ LangGraph 101 - Tutorial with Practical Example ](https://www.reddit.com/r/LangChain/comments/1g3i734/langgraph_101_tutorial_with_practical_example/) , 2024-10-17-0912
```
Hi folks!

It's been a while but I just finished uploading my latest tutorial. I built a super simple, but extremely pow
erful two-node LangGraph app that can retrieve data from my resume and a job description and then use the information to
 respond to any question. It could for example:

* Re-write parts or all of my resume to match the job description.
* Ge
nerate relevant interview questions and provide feedback.
* Write job-specific cover letters.
* etc.

# [>>> Watch here 
<<<](https://youtu.be/7KIrBjQTGLA)

You get the idea! I know the official docs are somewhat complicated, and sometimes b
roken, and a lot of people have a hard time starting out using LangGraph. If you're one of those people or just getting 
started and want to learn more about the library, [check out the tutorial](https://youtu.be/7KIrBjQTGLA)!

Cheers! :)
```
---

     
 
all -  [ Does RAG Have a Scaling Problem? ](https://www.reddit.com/r/Rag/comments/1g3h9w2/does_rag_have_a_scaling_problem/) , 2024-10-17-0912
```
My team has been digging into the scalability of vector databases for RAG (Retrieval-Augmented Generation) systems, and 
we feel we might be hitting some limits that aren’t being widely discussed. 

We tested Pinecone (using both LangChain a
nd LlamaIndex) out to 100K pages. We found those solutions started to lose search accuracy in as few as 10K pages. At 10
0K pages in the RAG, search accuracy dropped 10-12%.

We also tested our approach at [EyeLevel.ai](http://EyeLevel.ai), 
which does not use vectors at all (I know it sounds crazy), and found only a 2% drop in search accuracy at 100K pages. A
nd showed better accuracy by significant margins from the outset. 

**Here's our research below. I would love to know if
 anyone else is exploring non-vector approaches to RAG and of course your thoughts on the research.** 

**We explain the
 research and results on YT as well.**   
[**https://www.youtube.com/watch?v=qV1Ab0qWyT8**](https://www.youtube.com/watc
h?v=qV1Ab0qWyT8)

[Image: The chart shows accuracy loss at just 10,000 pages of content using a Pinecone vector database
 with both LangChain and Llamaindex-based RAG applications.  Conversely, EyeLevel's GroundX APIs for RAG show almost no 
loss.](https://preview.redd.it/3okwujg7fqud1.png?width=1600&format=png&auto=webp&s=dfe1b2bb38f6d4a0fc5e7798ebd60fa23df13
c80)

# What’s Inside

In this report, we will review how the test was constructed, the detailed findings, our theories 
on why vector similarity search experienced challenges and suggested approaches to scale RAG without the performance hit
. We also encourage you to read our [prior research](https://www.eyelevel.ai/post/most-accurate-rag) in which EyeLevel’s
 GroundX APIs bested LangChain, Pinecone and Llamaindex based RAG systems by 50-120% on accuracy over 1,000 pages of con
tent.  

The work was performed by Daniel Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a 
computer scientist and former senior engineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods o
f this test will beopen sourced and available shortly. Others are invited to run the data and corroborate or challenge t
hese findings. 

# Defining RAG 

**Feel free to skip this section if you’re familiar with RAG.**  

RAG stands for “Ret
rieval Augmented Generation”. When you ask a RAG system a query, RAG does the following steps: 

1. Retrieval: Based on 
the query from the user, the RAG system retrieves relevant knowledge from a set of documents. 

1. Augmentation: The RAG
 system combines the retrieved information with the user query to construct a prompt. 

1. Generation: The augmented pro
mpt is passed to a large language model, generating the final output. 

The implementation of these three steps can vary
 wildly between RAG approaches. However, the objective is the same: to make a language model more useful by feeding it i
nformation from real-world, relevant documents. 

[RAG allows a language model to reference application specific informa
tion from human documents, allowing developers to build tailored and specific products ](https://preview.redd.it/z5t4h8a
cfqud1.png?width=1158&format=png&auto=webp&s=ce9d6e5ce5acb057243db14193f90f0a2072ccf2)

# Beyond The Tech Demo 

When mo
st developers begin experimenting with RAG they might grab a few documents, stick them into a RAG document store and be 
blown away by the results. Like magic, many RAG systems can allow a language model to understand books, company document
s, emails, and more. 

However, as one continues experimenting with RAG, some difficulties begin to emerge. 

1. Many do
cuments are not purely textual. They might have images, tables, or complex formatting. While many RAG systems can parse 
complex documents, the quality of parsing varies widely between RAG approaches. We explore the realities of parsing in[ 
another article](https://www.eyelevel.ai/post/most-accurate-rag). 

1. As a RAG system is exposed to more documents, it 
has more opportunities to retrieve the wrong document, potentially causing a degradation in performance   

1. Because o
f technical complexity, the underlying non-determinism of language models, and the difficulty of profiling the performan
ce of LLM applications in real world settings, it can be difficult to predict the cost and level of effort of developing
 RAG applications. 

In this article we’ll focus on the second and third problems listed above; performance degradation 
of RAG at scale and difficulties of implementation 

# The Test 

To test how much larger document sets degrade the perf
ormance of RAG systems, we first defined a set of 92 questions based on real-world documents.  

[A few examples of the 
real-world documents used in this test, which contain answers to our 92 questions. ](https://preview.redd.it/vbm3jcxffqu
d1.png?width=1429&format=png&auto=webp&s=9eed23eb6d7e0c6d8816063e7d3a9d86ad1848e8)

We then constructed four document se
ts to apply RAG to. All four of these document sets contain the same 310 pages of documents which answer our 92 test que
stions. However, each document set also contains a different number of irrelevant pages from miscellaneous documents. We
 started with 1,000 pages and scaled up to 100,000 in our largest test.   
 

[We asked the same questions based on the 
same set of documents \(blue\), but exposed the RAG system to varying amounts of unrelated documents \(red\). This diagr
am shows the number of relevant pages in each document set, compared to the total size of each document set.](https://pr
eview.redd.it/u4d6keqpfqud1.png?width=1431&format=png&auto=webp&s=b0161c312eca9b0633b5df58713f1ed92893cb30)

An ideal RA
G system would, in theory, behave identically across all document sets, as all document sets contain the same answers to
 the same questions. In practice, however, added information in a docstore can trick a RAG system into retrieving the wr
ong context for a given query. The more documents there are, the more likely this is to happen. Therefore, RAG performan
ce tends to degrade as the number of documents increases. 

In this test we applied each of these three popular RAG appr
oaches to the four document sets mentioned above:

* LangChain: a popular python library designed to abstract certain LL
M workflows. 
* LlamaIndex: a popular python library which has advanced vector embedding capability, and advanced RAG fu
nctionality. 
* EyeLevel’s GroundX: a feature complete retrieval engine built for RAG. 

By applying each of these RAG a
pproaches to the four document sets, we can study the relative performance of each RAG approach at scale. 

For both Lan
gChain and LlamaIndex we employed Pinecone as our vector store and OpenAI’s text-embedding-ada-002 for embedding. Ground
X, being an all-in-one solution, was used in isolation up to the point of generation. All approaches used OpenAI's gpt-4
-1106-preview for the final generation of results. Results for each approach were evaluated as being true or false via h
uman evaluation. 

# The Effect of Scale on RAG 

We ran the test as defined in the previous section and got the followi
ng results. 

[The performance of different RAG approaches varies greatly, both in base performance and the rate of perf
ormance degradation at scale. We explore differences in base performance thoroughly in another article ](https://preview
.redd.it/f3uqn8vsfqud1.png?width=1600&format=png&auto=webp&s=69abb4cf7d224a523457ac0b769a6dc2ae0148c7)

As can be seen i
n the figure above, the rate at which RAG degrades in performance varies widely between RAG approaches. Based on these r
esults one might expect GroundX to degrade in performance by 2% per 100,000 documents, while LCPC and LI might degrade 1
0-12% per 100,000 documents. The reason for this difference in robustness to larger document sets, likely, has to do wit
h the realities of using vector search as the bedrock of a RAG system.  

In theory a high dimensional vector space can 
hold a vast amount of information. 100,000 in binary is 17 values long (11000011010100000). So, if we *only* use binary 
vectors with unit components in a high dimensional vector space, we could store each page in our 100,000 page set with o
nly a 17 dimensional space. Text-embedding-ada-002, which is the encoder used in this experiment, outputs a 1536-dimensi
on vector. If one calculates 2\^1536 (effectively calculating how many things one could describe using only binary vecto
rs in this space) the result would be a number that’s significantly greater than the number of atoms in the known univer
se. Of course, actual embeddings are not restricted to binary numbers; they can be expressed in decimal numbers of very 
high precision. Even relatively small vector spaces can hold a vast amount of information. 

The trick is, how do you ge
t information into a vector space *meaningfully*? RAG needs content to be placed in a vector space such that similar thi
ngs can be searched, thus the encoder has to practically organize information into useful regions. It’s our theory that 
modern encoders don’t have what it takes to organize large sets of documents in these vector spaces, even if the vector 
spaces can theoretically fit a near infinite amount of information. The encoder can only put so much information into a 
vector space before the vector space gets so cluttered that distance-based search is rendered non-performant. 

[There i
s a big difference between a space being able to fit information, and that information being meaningfully organized. ](h
ttps://preview.redd.it/uzbfvsyufqud1.png?width=948&format=png&auto=webp&s=5dc7fad4f9e976a595281bac95e34f64e472613a)

Eye
Level’s GroundX doesn’t use vector similarity as its core search strategy, but rather a tuned comparison based on the si
milarity of semantic objects. There are no vectors used in this approach. This is likely why GroundX exhibits superior p
erformance in larger document sets. 

In this test we employed what is commonly referred to as “naive” RAG. LlamaIndex a
nd LangChain allow for many advanced RAG approaches, but they had little impact on performance and were harder to employ
 at larger scales. We cover that in another article which will be released shortly.

# The Surprising Technical Difficul
ty of Scale 

While 100,000 pages seems like a lot, it’s actually a fairly small amount of information for industries li
ke engineering, law, and healthcare. Initially we imagined testing on much larger document sets, but while conducting th
is test we were surprised by the practical difficulty of getting LangChain to work at scale; forcing us to reduce the sc
ope of our test. 

To get RAG up and running for a set of PDF documents, the first step is to parse the content of those
 PDFs into some sort of textual representation. LangChain uses libraries from [Unstructured.io](http://Unstructured.io) 
to perform parsing on complex PDFs, which works seamlessly for small document sets. 

Surprisingly, though, the speed of
 LangChain parsing is incredibly slow. Based on our analysis it appears that Unstructured uses a variety of models to de
tect and parse out key elements within a PDF. These models should employ GPU acceleration, but they don’t. That results 
in LangChain taking days to parse a modestly sized set of documents, even on very large (and expensive) compute instance
s. To get LangChain working we needed to reverse engineer portions of Unstructured and inject code to enable GPU utiliza
tion of these models. 

It appears that this is a known issue in Unstructured, as seen in the notes below. As it stands,
 it presents significant difficulty in scaling LangChain to larger document sets, given LangChain abstracts away fine gr
ain control of Unstructured. 

https://preview.redd.it/dhchak3xfqud1.png?width=1346&format=png&auto=webp&s=2e1e6b84f79d8
89c901f05140771f68bb0f00150

[**Source: Github**](https://github.com/Unstructured-IO/unstructured-inference/blob/64cd41c
37fe4535702b3be9c6b58f380ca4c7edd/unstructured_inference/inference/layout.py#L175)

We only made improvements to LangCha
in parsing up to the point where this test became feasible. If you want to modify LangChain for faster parsing, here are
 some resources: 

* The default directory loader of LangChain is Unstructured ([source1](https://python.langchain.com/v
0.1/docs/modules/data_connection/document_loaders/file_directory/),[ source2](https://github.com/langchain-ai/langchain/
blob/410e9add4443607618a75827afe1a676fcd7c0a7/libs/community/langchain_community/document_loaders/directory.py#L38)). 
*
 Unstructured uses “hi res” for the PDFs by default if text extraction cannot be performed on the document ([source1](ht
tps://github.com/langchain-ai/langchain/blob/410e9add4443607618a75827afe1a676fcd7c0a7/libs/community/langchain_community
/document_loaders/directory.py#L38) ,[ source2](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5
788dcb5b3a7a7a57bf078/unstructured/partition/strategies.py#L103) ). Other options are available like “fast” and “OCR onl
y”, which have different processing intensities 
* “Hi Res” involves: 
   * Converting the pdf into images ([source](htt
ps://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078/unstructured/partition/pdf_im
age/pdfminer_processing.py#L36)) 
   * Running a layout detection model to understand the layout of the documents ([sour
ce](https://github.com/Unstructured-IO/unstructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured
_inference/models/yolox.py#L3)). This model benefits greatly from GPU utilization, but does not leverage the GPU unless 
ONNX is installed ([source](https://onnxruntime.ai/docs/get-started/with-python.html)) 
   * OCR extraction using tesser
act (by default) ([source](https://github.com/Unstructured-IO/unstructured/blob/23e570fc8ac71c5d1a5788dcb5b3a7a7a57bf078
/unstructured/partition/utils/config.py#L103)) which is a very compute intensive process ([source](https://github.com/te
sseract-ocr/tesseract/issues/263)) 
   * Running the page through a table layout model ([source](https://github.com/Unst
ructured-IO/unstructured-inference/blob/76619ca66f47d013f6656fce775f6fddde5d36ae/unstructured_inference/models/tables.py
#L140)) 

While our configuration efforts resulted in faster processing times, it was still too slow to be feasible for 
larger document sets. To reduce time, we did “hi res” parsing on the relevant documents and “fast” parsing on documents 
which were irrelevant to our questions. With this configuration, parsing 100,000 pages of documents took 8 hours. If we 
had applied “hi res” to all documents, we imagine that parsing would have taken 31 days (at around 30 seconds per page).
 

At the end of the day, this test took two senior engineers (one who’s worked at a directorial level at several AI com
panies, and a multi company CTO with decades of applied experience of AI at scale) several weeks to do the development n
ecessary to write this article, largely because of the difficulty of applying LangChain to a modestly sized document set
.  To get LangChain working in a production setting, we estimate that the following efforts would be required: 

* Tesse
ract would need to be interfaced with in a way that is more compute and time efficient. This would likely require a high
-performance CPU instance, and modifications to the LangChain source code. 
* The layout and table models would need to 
be made to run on a GPU instance 
* To do both tasks in a cost-efficient manner, these tasks should probably be decouple
d. However, this is not possible with the current abstraction of LangChain. 

On top of using a unique technology which 
is highly performant, GroundX also abstracts virtually all of these technical difficulties behind an API. You upload you
r documents, then search the results. That’s it. 

If you want RAG to be even easier, one of the things that makes Eyele
vel so compelling is the service aspect they provide to GroundX. You can work with Eyelevel as a partner to get GroundX 
working quickly and performantly for large scale applications. 

# Conclusion 

When choosing a platform to build RAG ap
plications, engineers must balance a variety of key metrics. The robustness of a system to maintain performance at scale
 is one of those critical metrics. In this head-to-head test on real-world documents, EyeLevel’s GroundX exhibited a hei
ghtened level of performance at scale, beating LangChain and LlamaIndex. 

Another key metric is efficiency at scale. As
 it turns out, LangChain has significant implementation difficulties which can make the large-scale distribution of Lang
Chain powered RAG difficult and costly. 

Is this the last word? Certainly not. In future research, we will test various
 advanced RAG techniques, additional RAG frameworks such as Amazon Q and GPTs and increasingly complex and multimodal da
ta types. So stay tuned. 

If you’re curious about running these results yourself, please reach out to us at info@eyelev
el.ai.Vector databases, a key technology in building retrieval augmented generation or RAG applications, has a scaling p
roblem that few are talking about. 

According to new research by [EyeLevel.ai](https://www.eyelevel.ai/), an AI tools c
ompany, the precision of vector similarity search degrades in as few as 10,000 pages, reaching a 12% performance hit by 
the 100,000-page mark.

The research also tested [EyeLevel’s enterprise-grade RAG platform](https://www.eyelevel.ai/prod
uct/apis) which does not use vectors. EyeLevel lost only 2% accuracy at scale.

The findings suggest that while vector d
atabases have become highly popular tools to build RAG and LLM-based applications, developers may face unexpected challe
nges as they shift from testing to production and attempt to scale their applications.  

The work was performed by Dani
el Warfield, a data scientist and RAG engineer and Dr. Benjamin Fletcher, PhD, a computer scientist and former senior en
gineer at IBM Watson. Both men work for EyeLevel.ai. The data, code and methods of this test will be open sourced and av
ailable shortly. Others are invited to run the data and corroborate or challenge these findings. 
```
---

     
 
all -  [ Help scaling LLM classifications and validations ](https://www.reddit.com/r/LangChain/comments/1g3h1hd/help_scaling_llm_classifications_and_validations/) , 2024-10-17-0912
```
I'm working on an application that will classify 10 million records according to pretty well-defined standards. Ideally,
 I'd classify them with a LLM and LangChain, then run validation on it to double check the classifications. Here's the i
ssue: I'm a little lost. I've built smaller-scale RAG systems, but I have no idea how to do this at scale. Any help woul
d be greatly appreciated.

Big apologies if I shouldn't be posting the question here.
```
---

     
 
MachineLearning -  [ [D] How are folks building conversational Retrieval Augmented Generation apps ](https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/) , 2024-10-17-0912
```
I've read through various resources such as:  
- [https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/](htt
ps://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/)  
- [https://python.langchain.com/docs/tutorials/qa\_cha
t\_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)  
- [https://langchain-ai.github.io/langgraph
/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) 
 
- [https://docs.llamaindex.ai/en/stable/module\_guides/deploying/chat\_engines/](https://docs.llamaindex.ai/en/stable/
module_guides/deploying/chat_engines/)  
- [https://huggingface.co/datasets/nvidia/ChatRAG-Bench](https://huggingface.co
/datasets/nvidia/ChatRAG-Bench) 

But these feel overly reductive, since they don't address complexities like:  
1) when
 to retrieve vs. just respond immediately to reduce latency  
2) rely on existing context previously retrieved in the co
nversation instead of retrieving again at the current turn  
3) partition LLM context between retrieved information and 
past conversation history.

I'm sure some teams already have good systems for this, would appreciate pointers!
```
---

     
 
MachineLearning -  [ Built a web agent which call fill Google forms based on the user details [P] ](https://www.reddit.com/r/MachineLearning/comments/1fozud5/built_a_web_agent_which_call_fill_google_forms/) , 2024-10-17-0912
```
GitHub repo : [https://github.com/shaRk-033/web-agent](https://github.com/shaRk-033/web-agent)

Tried to solve it using 
two approaches:

# 1: Basic Scraping and Filling

This is the straightforward approach. The agent scrapes the form’s HTM
L and uses fixed XPaths to find and fill in the required fields.

* It pulls the form’s HTML, locates the fields with se
t XPaths, and inputs the answers. It’s a direct and simple method.
* If the form changes or an element isn’t where it’s 
expected, the process can fail and may need manual adjustments.

[basic approach](https://preview.redd.it/5e8g4a1k4xqd1.
png?width=1055&format=png&auto=webp&s=d8e984e4feaee2f0453b08c8696768c40a2a5c20)

2. Using LangChain Agents and tool call
ing

* LangChain Agent**:** The agent handles everything by using the LLM’s reasoning to decide what to do next, includi
ng generating those tricky XPaths.
* Error Handling**:** If something goes wrong (like an element not found), the agent 
tries again with better XPaths until it gets the job done.

[using langchain agents](https://preview.redd.it/948i88pl4xq
d1.png?width=782&format=png&auto=webp&s=ed1e6c19efec9f4cbbbd6ab5a22558f221cf745f)

Any recommendations to improve this w
ould be welcome. Also, if anyone has ideas on building similar web agents to automate other tasks, it would be great to 
hear them. :)
```
---

     
 
MachineLearning -  [ [P] Swapping Embedding Models for an LLM ](https://www.reddit.com/r/MachineLearning/comments/1fktvbj/p_swapping_embedding_models_for_an_llm/) , 2024-10-17-0912
```
How tightly coupled is an embedding model to a language model?

Taking an example from Langchain's tutorials, they use O
llama's _nomic-embed-text_ for embedding and _Llama3.1_ for the understanding and Q/A. I don't see any documentation abo
ut Llama being built on embeddings from this embedding model. 

Intuition suggests that a different embedding model may 
produce outputs of other sizes or produce a different tensor for a character/word, which would have an impact on the res
ults of the LLM. So would changing an embedding model require retraining/fine-tuning the LLM as well?

I need to use a e
mbedding model for code snippets and text. Do I need to find a specialized embedding model for that? If yes, how will ll
ama3.1 ingest the embeddings?
```
---

     
