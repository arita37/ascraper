 
all -  [ RAG or Langchain? Is it the same? ](https://www.reddit.com/r/ChatGPT/comments/1cdyjh3/rag_or_langchain_is_it_the_same/) , 2024-04-27-0910
```
So some context. We build an app for couples to discover each other better using daily questions and quizzes. Partners l
ink up on the app and answer questions together, which are then stored in their personal db. Crossed 50k downloads, $10k
 revenue. Now want to use the question / answers to generate a hyper personalized conversational chat bot for the users.
 

Aim is to use the question / answers submitted by users as a knowledge repository and use it as a reference point for
 any question the user asks to the AI. eg. if the user asks 'what to do when my partner is upset?' to the ai bot, we fir
st reference through the answer db to find common question/answers and use them as the primary reference point for parsi
ng the answer from the llm. 

This is how it'll look like:

[straight from the pitch deck](https://preview.redd.it/dztjg
50dgwwc1.png?width=2188&format=png&auto=webp&s=7aa144fbd56c22ac058b7014f6a385dec14ee752)

my question is if we should us
e an existing provider like langchain to wrap the database & llm or build our own RAG? no idea about how to do this, but
 my CTO can figure out with the right guidance.  


Also if you've any other tips / suggestions, open to them. thanks!


&#x200B;
```
---

     
 
all -  [ Code generation integrated with code retrieval for robot applications using LangChain ](https://www.reddit.com/r/LangChain/comments/1cdwvui/code_generation_integrated_with_code_retrieval/) , 2024-04-27-0910
```
Hello everyone,

It has been a long time since our last update on [ROScribe](https://github.com/RoboCoachTechnologies/RO
Scribe) (an open source tool for robot integration and software generation using LLM). In our first releases of ROScribe
, we autogenerated the entire robot software in ROS (in python) using LLMs and LangChain. Then, later on, we trained ROS
cribe with all open source repositories available on ROS-index (python or C++) to enable a code-retrieval feature.

The 
last step was to seamlessly combine these two different methods (Code generation & Code retrieval) to create an ultimate
 solution that first looks at what codes are available and then only generates code for the parts which aren't available
 and tie them together. This problem proved to be more challenging that we thought, and it took us a while to get it don
e.

It is done now. We made our version 0.1.0 release a few days ago.

Here is a short demo that shows a 2D mapping with
 Lidar using ROScribe v0.1.0:

[https://www.youtube.com/watch?v=AWnC6s2nK-k](https://www.youtube.com/watch?v=AWnC6s2nK-k
)

I will post more details later. For now you can find extra info in our github:

[https://github.com/RoboCoachTechnolo
gies/ROScribe](https://github.com/RoboCoachTechnologies/ROScribe)
```
---

     
 
all -  [ Book recommendation: Mastering NLP from Foundations to LLMs ](https://i.redd.it/sg4at7g9tvwc1.jpeg) , 2024-04-27-0910
```

üöÄ Exciting News! üöÄ The wait is over ‚≠ê

Mastering NLP from Foundations to LLMs: Apply advanced rule-based techniques to 
LLMs and solve real-world business problems using Python

Hi everyone, I'm thrilled to share with you all that the much-
awaited book authored by leading experts¬†Lior Gazit¬†and¬†Meysam Ghaffari, Ph.D.¬†is finally here! üéâ

Enhance your NLP prof
iciency with modern frameworks like LangChain, explore mathematical foundations and code samples, and gain expert insigh
ts into current and future trends

üí° Dive deep into the fascinating world of Natural Language Processing with this compr
ehensive guide. Whether you're just starting out or looking to enhance your skills, this book has got you covered.

üîë Ke
y Features:
- Learn how to build Python-driven solutions focusing on NLP, LLMs, RAGs, and GPT.
- Master embedding techni
ques and machine learning principles for real-world applications.
- Understand the mathematical foundations of NLP and d
eep learning designs.
- Plus, get a free PDF eBook when you purchase the print or Kindle version!

üìò Book Description:
F
rom laying down the groundwork of machine learning to exploring advanced concepts like LLMs, this book takes you on an e
nlightening journey. Dive into linear algebra, optimization, probability, and statistics ‚Äì all the essentials you need t
o conquer ML and NLP. And the best part? You'll find practical Python code samples throughout!

By the end, you'll be de
lving into the nitty-gritty of LLMs' theory, design, and applications, alongside expert insights on the future trends in
 NLP.

Not only this, the book features Expert Insights by Stalwarts from the industry :
‚Ä¢¬†Xavier (Xavi) Amatriain, VP o
f Product, Core ML/AI, Google
‚Ä¢¬†Melanie Garson, Cyber Policy & Tech Geopolitics Lead at Tony Blair Institute for Global 
Change, and Associate Professor at University College London
‚Ä¢¬†Nitzan Mekel-Bobrov, Ph.D., CAIO, Ebay
‚Ä¢¬†David Sontag, Pr
ofessor at MIT and CEO at Layer Health
‚Ä¢¬†John Halamka, M.D., M.S., president of the Mayo Clinic Platform

Foreword and I
mpressions by leading Expert¬†Asha Saxena

üîç What You Will Learn:
- Master the mathematical foundations of machine learni
ng and NLP.
- Implement advanced techniques for preprocessing text data and analysis.
- Design ML-NLP systems in Python.

- Model and classify text using traditional and deep learning methods.
- Explore the theory and design of LLMs and thei
r real-world applications.
- Get a sneak peek into the future of NLP with expert opinions and insights.

üì¢ Don't miss ou
t on this incredible opportunity to expand your NLP skills! Grab your copy now and embark on an exciting learning journe
y.

Amazon US
https://www.amazon.com/Mastering-NLP-Foundations-LLMs-Techniques/dp/1804619183/


```
---

     
 
all -  [ Make Time For Family, Hack Your Productivity & Goblins? ](https://www.reddit.com/r/LangChain/comments/1cdstt4/make_time_for_family_hack_your_productivity/) , 2024-04-27-0910
```
This post is the awaited part 2 of our last edition ‚Äì ‚ÄúWhat Are LLMs & How They Can Save You 800 Hours This Year‚Äù, which
 you can read here if you haven‚Äôt already: 

we‚Äôll be finishing up on the best LLM-based tools to:

* **Add More Time To
 Your Day** ‚Äî the most powerful calendar & time management tools available.
* **Browse Like You‚Äôre From The Future** ‚Äî i
ntelligent webpilots that cut your browsing time in half.
* **Hack Your Productivity** ‚Äî 4x your productivity using tool
s that interlink & bring order to your notes.

**Can‚Äôt Make Time For Your Family?**Let‚Äôs be honest, balancing work and l
ife is difficult. Most of us struggle to find the time to spend with our families, for leisure and rest. Fortunately eno
ugh, LLM-based calendar tools can efficiently time block your day so that you have time for everything ‚Äì meetings, famil
y, leisure, sleep, and deep work.  
These tools can actually show you where your free time really lies, and if you don‚Äôt
 have any ‚Äî it will create it. 

&#x200B;

https://preview.redd.it/8ogyz3bnavwc1.jpg?width=1000&format=pjpg&auto=webp&s=
4df551684af2f85f1c303630dd0b5d1b33386b60

 

These intelligent tools understand your preferences and priorities, helping
 to arrange your commitments in a way that maximizes efficiency ‚Äî they can handle the back-and-forth of scheduling meeti
ngs, suggest optimal times for your appointments based on your habits, and even remind you of important family events.


If you want to be at the top of your game and still make time for friends, family & yourself, you should be using an LLM
-based calendar solution.

Let‚Äôs explore the best calendar & time management tools that can give you time to do the thin
gs you love, with the people you love!
```
---

     
 
all -  [ Speed Result Comparison of Gemini vs GPT4 ](https://www.reddit.com/r/OpenAI/comments/1cdssef/speed_result_comparison_of_gemini_vs_gpt4/) , 2024-04-27-0910
```
&#x200B;

https://preview.redd.it/gnxc5kbm9vwc1.png?width=600&format=png&auto=webp&s=83faa5933cac968530bcf2d3ac998e23b2c
8d2f2

I recently posted on [twitter](https://twitter.com/channelfourai/status/1783920521379283041) about this with a bi
t more context. I run these comparisons at random on input from users. The above graph represents \~100 prompts to each.
 Both models get the same prompt when selected. While Gemini via VertexAI is much faster, GPT4 reliably provides slightl
y more complete answers. 

Is there a good framework to automate these calls and data collection across all major provid
ers including Anthropic, etc?
```
---

     
 
all -  [ CrewAI / Langchain agent tools for CLI actions like running cmake/make, as well as git actions (git  ](https://www.reddit.com/r/crewai/comments/1cdrl0g/crewai_langchain_agent_tools_for_cli_actions_like/) , 2024-04-27-0910
```
The title says it all, I'm looking for some tools to do those things and haven't found any, though it seems like I must 
be missing something. Or am I? Thank you!
```
---

     
 
all -  [ An LLM-agent supporting searching the web running purely locally? ](https://www.reddit.com/r/LocalGPT/comments/1cdpsca/an_llmagent_supporting_searching_the_web_running/) , 2024-04-27-0910
```
Today I found this: [https://webml-demo.vercel.app/](https://webml-demo.vercel.app/). It's a client-side (browser) only 
application that allows chatting with your documents.

I was inspired by this and thought: What if we would not try to s
imply chat with a document, but instead use this as a support while searching the internet? For example, after searching
 with a search engine an agent could access the first 10 search results and try to create a summary for each search resu
lt or something like that - but all from within the browser.

In theory, this should be feasible using a combination of:


* WebLLM to run a local LLM in the browser for creating summaries out of HTML pages
* Transformers.js to run a local e
mbedding model to create embedding vectors from text
* Voy as a local vector store for RAG (i.e. to split longer website
s into parts)
* Got-Scraping library to access a URL from a search engine results from within the browser
* Langchain.js
 to run an agent that scans through the search results one by one to determine which results are actually useful

Obviou
sly, this would not be perfect and less stable than running on a server. The advantage however would be that everything 
would happen purely locally on the client side.

Besides the technical feasibility: What do you think of this idea? Woul
d this be useful for anything?

&#x200B;
```
---

     
 
all -  [ Building a RAG Pipeline with Phi-3 ](https://www.reddit.com/r/learnmachinelearning/comments/1cdo3up/building_a_rag_pipeline_with_phi3/) , 2024-04-27-0910
```
 Hey there,

Just dropped a quick video, setting up a RAG pipeline using **Phi-3 with langchain**.

Figured it could be 
handy for those looking to run some tests.

**Phi-3?** 

New Small Language model optimized for phones and small devices
 with competitive performance to Llama3

Cheers!

[https://youtu.be/HCX210dyHhk](https://youtu.be/HCX210dyHhk)

&#x200B;


&#x200B;
```
---

     
 
all -  [ Building a RAG Pipeline with Phi-3 ](https://www.reddit.com/r/LanguageTechnology/comments/1cdo30q/building_a_rag_pipeline_with_phi3/) , 2024-04-27-0910
```
Hey there,  
Just dropped a quick video, setting up a RAG pipeline using **Phi-3 with langchain**.  
Figured it could be
 handy for those looking to run some tests.  
**Phi-3?**   
New Small Language model optimized for phones and small devi
ces with competitive performance to Llama3  
Cheers!  
[https://youtu.be/HCX210dyHhk](https://youtu.be/HCX210dyHhk)

&#x
200B;
```
---

     
 
all -  [ Any resources for RAG with excel files or Databases? ](https://www.reddit.com/r/LangChain/comments/1cdmqk8/any_resources_for_rag_with_excel_files_or/) , 2024-04-27-0910
```
Are there any resources about RAG application that uses as knowledge base either excel files or Databases? 
```
---

     
 
all -  [ How to make streaming work with a RAG Q&A chain with memory ](https://www.reddit.com/r/LangChain/comments/1cdmey2/how_to_make_streaming_work_with_a_rag_qa_chain/) , 2024-04-27-0910
```
Hey, I am trying to build a RAG Q&A chain, with memory (chat history). While the invoke function works perfectly fine an
d allows me to extract the answer, the stream does not. I've followed the documentation: [https://python.langchain.com/d
ocs/use\_cases/question\_answering/chat\_history/#tying-it-together](https://python.langchain.com/docs/use_cases/questio
n_answering/chat_history/#tying-it-together)

&#x200B;

The only change is as follows:

    # This works perfectly fine:

    conversational_rag_chain.invoke(
        {'input': 'What is Task Decomposition 2?'},
        config={'configurable'
: {'session_id': 'abc123'}},  # constructs a key 'abc123' in `store`.
    )['answer']
        

&#x200B;

    # This doe
s not work - it streams back everything and i can not extract the answer
    for chuck in conversational_rag_chain.strea
m(
        {'input': 'What is Task Decomposition 2?'},
        config={'configurable': {'session_id': 'abc123'}},  # con
structs a key 'abc123' in `store`.
    ):
        print(chuck)

&#x200B;

    # I have also tried the following but none
 works;
    print(chuck['answer'])
    print(chuck.content)
    print(chuck.content['answer'])

Any suggestion or ideas 
on how to make this work? Seems like very normal behaviour to expect from a stream function?

 
```
---

     
 
all -  [ How to make LLM return question to be more specific rather than throwing output? ](https://www.reddit.com/r/LangChain/comments/1cdm2tx/how_to_make_llm_return_question_to_be_more/) , 2024-04-27-0910
```
Hi,
I have a pdf where some Return in % is under 4 categories such as A, B and so on.
When I ask question using Llama3 i
t is returning the correct answer but it is picking the Return from A rather than knowing from which category the Return
 should be picked from?
How can I make LLM return the output saying which category rather than picking the answer from c
ategory A ?
Thanks
```
---

     
 
all -  [ Is there a benefit of using FastAPI when deploying as Azure Functions? ](https://www.reddit.com/r/azuredevops/comments/1cdj8o1/is_there_a_benefit_of_using_fastapi_when/) , 2024-04-27-0910
```
My company uses the Azure Stack. I planned to deploy my AI services (built with langchain) by using FastAPI. But a colle
ague of mine told me to consider Azure Functions. While primarily these represent means for serverless execution of code
, they also seem to provide API access. Does that mean then that there is no need for FastAPI in such a setting, such th
at I should skip this 'overhead'?
```
---

     
 
all -  [ How to stay up to date with prompts for LLMs ](https://www.reddit.com/r/LangChain/comments/1cdisrq/how_to_stay_up_to_date_with_prompts_for_llms/) , 2024-04-27-0910
```
Hello,
I was trying RAG using Llama3 and the input prompt is different as compared to other LLMs. 
How I can know which 
kind of prompts work best for a specific LLM as prompt used in LLama3 was very different and uses <eos> and etc etc.
Is 
there any template for different LLMs?
```
---

     
 
all -  [ SENIOR Machine Learning Engineer - 100% Remote, every other friday off ](https://www.reddit.com/r/MachineLearningJobs/comments/1cdia14/senior_machine_learning_engineer_100_remote_every/) , 2024-04-27-0910
```
Apply here: [https://grnh.se/50c178c17us](https://grnh.se/50c178c17us)

Building with the latest tools, our Machine Lear
ning teams launch for Silicon Valley startups and Life Science giants alike.

Join Loka to work remotely, ship projects 
you‚Äôre proud of and enjoy every other Friday off.

**The Role**

* Understand business objectives and develop models tha
t help achieve them, plus metrics to track their progress.
* Design and implement complex ML systems using classical ML,
 DL and Foundation Models and following best practices.
* Lead client communications by gathering requirements, managing
 expectations and communicating deliverables.
* Wrangle, explore and visualize data with a careful eye for issues that r
equire data cleaning as well as differences in data distribution that may affect performance after deployment.
* Analyze
 model errors; design strategies to overcome them.
* Deploy, maintain and upgrade ML models and pipelines.
* Follow Loka
‚Äôs career track for growth by demonstrating technical excellence, innovation, autonomy, ownership, communication and tea
mwork.

**The Benefits**

* Every other Friday off (26 extra days off a year!)
* LokaLabs incubator
* Explore and Reloca
tion programs (three months work abroad or full international relo)
* Remote and flexible
* Paid sick days and local hol
idays
* Fitness subscription

**The Required Hard Skills**

* 4+ years of ML engineering experience
* Bachelor‚Äôs degree 
in Computer Science or related
* Proficient in English
* Experience with¬†Python, ML libraries and AI/ML frameworks (PyTo
rch, HuggingFace, TensorFlow, Keras, Scikit-learn, Spark etc.)
* Strong understanding of statistical, ML and deep learni
ng algorithms (candidates with NLP experience preferred)
* Experience building GenAI solutions including prompt engineer
ing (e.g: Langchain), fine-tuning and serving LLMs, search and embeddings, etc.
* Experience with MLOps, preferably in A
WS (e.g: Sagemaker), as well as standards tools (e.g. MLFlow)
* Experience visualizing and manipulating big datasets
* C
lient-facing experience
* Clean criminal record

**The Required Soft Skills**

* Curiosity‚ÄîAmbition to learn and grow in
to different industries with a modern tech stack
* Autonomy and positivity‚ÄîWe‚Äôre a fully remote, globally distributed te
am
* Teamwork‚ÄîEnjoy a collaborative approach
* Adaptability‚ÄîOperate with a startup mindset and move at a startup pace
```
---

     
 
all -  [ Agents guide ](https://www.reddit.com/r/LangChain/comments/1cdi80s/agents_guide/) , 2024-04-27-0910
```
Anyone can provide good explanations or articles  of creating custom agent ?
I'm looking for 
Creating  agent using agen
t class so we can control agent finish and agent action .

Sub question:

How tool calling automatically break complex q
uestion into subs questions ?
```
---

     
 
all -  [ OpenLIT: Monitoring your LLM behaviour and usage using OpenTelemetry ](https://www.reddit.com/r/llmops/comments/1cdh1xi/openlit_monitoring_your_llm_behaviour_and_usage/) , 2024-04-27-0910
```
Hey everyone! You might remember my friend's post a while back giving you all a sneak peek at  OpenLIT.

Well, I‚Äôm excit
ed to take the baton today and announce our leap from a promising preview to our first stable release! Dive into the det
ails here: [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

üëâ What's OpenLIT? In a nutshell, it
's an Open-source, community-driven observability tool that lets you track and monitor the behaviour of your Large Langu
age Model (LLM) stack with ease. Built with pride on OpenTelemetry, OpenLIT aims to simplify the complexities of monitor
ing your LLM applications.

Beyond Text & Chat Generation: Our platform doesn‚Äôt just stop at monitoring text and chat ou
tputs. OpenLIT brings under its umbrella the capability to automatically monitor GPT-4 Vision, DALL¬∑E, and OpenAI Audio 
too. We're fully equipped to support your multi-modal LLM projects on a single platform, with plans to expand our model 
support and updates on the horizon!

Why OpenLIT? OpenLIT delivers:

&#x200B;

\- Instant Updates: Get real-time insight
s on cost & token usage, deeper usage and LLM performance metrics, and response times (a.k.a. latency).

\- Wide Coverag
e: From LLMs Providers like OpenAI, AnthropicAI, Mistral, Cohere, HuggingFace etc., to Vector DBs like ChromaDB and Pinc
cone and Frameworks like LangChain (which we all love right?), OpenLIT has got your GenAI stack covered.

\- Standards C
ompliance: We adhere to OpenTelemetry's Semantic Conventions for GenAI, syncing your monitoring practices with community
 standards.

Integrations Galore: If you're using any observability tools, OpenLIT seamlessly integrates with a wide arr
ay of telemetry destinations including OpenTelemetry Collector, Jaeger, Grafana Cloud, Tempo, Datadog, SigNoz, OpenObser
ve and more, with additional connections in the pipeline.

[Openlit](https://preview.redd.it/p78pmck6mswc1.png?width=200
0&format=png&auto=webp&s=591b60c8a782ce1c0d480c4d8e96106ce15dae44)

Curious to see how you can get started? Here's your 
quick link to our quickstart guide: [https://docs.openlit.io/latest/quickstart](https://docs.openlit.io/latest/quickstar
t)

We‚Äôre beyond thrilled to have reached this stage and truly believe OpenLIT can make a difference in how you monitor 
and manage your LLM projects. Your feedback has been instrumental in this journey, and we‚Äôre eager to continue this path
 together. Have thoughts, suggestions, or questions? Drop them below! Happy to discuss, share knowledge, and support one
 another in unlocking the full potential of our LLMs. üöÄ

Looking forward to your thoughts and engagement! [https://githu
b.com/openlit/openlit](https://github.com/openlit/openlit)

Cheers, Aman
```
---

     
 
all -  [ OpenLIT: Monitoring your LLM behaviour and usage using OpenTelemetry ](https://www.reddit.com/r/Anthropic/comments/1cdh0dp/openlit_monitoring_your_llm_behaviour_and_usage/) , 2024-04-27-0910
```
Hey everyone! You might remember my friend's post a while back giving you all a sneak peek at  OpenLIT.

Well, I‚Äôm excit
ed to take the baton today and announce our leap from a promising preview to our first stable release! Dive into the det
ails here: [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

üëâ What's OpenLIT? In a nutshell, it
's an Open-source, community-driven observability tool that lets you track and monitor the behaviour of your Large Langu
age Model (LLM) stack with ease. Built with pride on OpenTelemetry, OpenLIT aims to simplify the complexities of monitor
ing your LLM applications.

Beyond Text & Chat Generation: Our platform doesn‚Äôt just stop at monitoring text and chat ou
tputs. OpenLIT brings under its umbrella the capability to automatically monitor GPT-4 Vision, DALL¬∑E, and OpenAI Audio 
too. We're fully equipped to support your multi-modal LLM projects on a single platform, with plans to expand our model 
support and updates on the horizon!

Why OpenLIT? OpenLIT delivers:

&#x200B;

\- Instant Updates: Get real-time insight
s on cost & token usage, deeper usage and LLM performance metrics, and response times (a.k.a. latency).

\- Wide Coverag
e: From LLMs Providers like OpenAI, AnthropicAI, Mistral, Cohere, HuggingFace etc., to Vector DBs like ChromaDB and Pinc
cone and Frameworks like LangChain (which we all love right?), OpenLIT has got your GenAI stack covered.

\- Standards C
ompliance: We adhere to OpenTelemetry's Semantic Conventions for GenAI, syncing your monitoring practices with community
 standards.

Integrations Galore: If you're using any observability tools, OpenLIT seamlessly integrates with a wide arr
ay of telemetry destinations including OpenTelemetry Collector, Jaeger, Grafana Cloud, Tempo, Datadog, SigNoz, OpenObser
ve and more, with additional connections in the pipeline.

[Openlit](https://preview.redd.it/6cfk19znlswc1.png?width=200
0&format=png&auto=webp&s=898b470986533ba560f41c53cf582e1ccfeddc09)

Curious to see how you can get started? Here's your 
quick link to our quickstart guide: [https://docs.openlit.io/latest/quickstart](https://docs.openlit.io/latest/quickstar
t)

We‚Äôre beyond thrilled to have reached this stage and truly believe OpenLIT can make a difference in how you monitor 
and manage your LLM projects. Your feedback has been instrumental in this journey, and we‚Äôre eager to continue this path
 together. Have thoughts, suggestions, or questions? Drop them below! Happy to discuss, share knowledge, and support one
 another in unlocking the full potential of our LLMs. üöÄ

Looking forward to your thoughts and engagement! [https://githu
b.com/openlit/openlit](https://github.com/openlit/openlit)

Cheers, Aman
```
---

     
 
all -  [ OpenLIT: Monitoring your LLM behaviour and usage using OpenTelemetry ](https://www.reddit.com/r/OpenAIDev/comments/1cdgzs2/openlit_monitoring_your_llm_behaviour_and_usage/) , 2024-04-27-0910
```
Hey everyone! You might remember my friend's post a while back giving you all a sneak peek at  OpenLIT.

Well, I‚Äôm excit
ed to take the baton today and announce our leap from a promising preview to our first stable release! Dive into the det
ails here: [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

üëâ What's OpenLIT? In a nutshell, it
's an Open-source, community-driven observability tool that lets you track and monitor the behaviour of your Large Langu
age Model (LLM) stack with ease. Built with pride on OpenTelemetry, OpenLIT aims to simplify the complexities of monitor
ing your LLM applications.

Beyond Text & Chat Generation: Our platform doesn‚Äôt just stop at monitoring text and chat ou
tputs. OpenLIT brings under its umbrella the capability to automatically monitor GPT-4 Vision, DALL¬∑E, and OpenAI Audio 
too. We're fully equipped to support your multi-modal LLM projects on a single platform, with plans to expand our model 
support and updates on the horizon!

Why OpenLIT? OpenLIT delivers:

&#x200B;

\- Instant Updates: Get real-time insight
s on cost & token usage, deeper usage and LLM performance metrics, and response times (a.k.a. latency).

\- Wide Coverag
e: From LLMs Providers like OpenAI, AnthropicAI, Mistral, Cohere, HuggingFace etc., to Vector DBs like ChromaDB and Pinc
cone and Frameworks like LangChain (which we all love right?), OpenLIT has got your GenAI stack covered.

\- Standards C
ompliance: We adhere to OpenTelemetry's Semantic Conventions for GenAI, syncing your monitoring practices with community
 standards.

Integrations Galore: If you're using any observability tools, OpenLIT seamlessly integrates with a wide arr
ay of telemetry destinations including OpenTelemetry Collector, Jaeger, Grafana Cloud, Tempo, Datadog, SigNoz, OpenObser
ve and more, with additional connections in the pipeline.

[Openlit](https://preview.redd.it/wwtsl95glswc1.png?width=200
0&format=png&auto=webp&s=85570b1ad2b8f35d93c364ba100625a71e77dab4)

Curious to see how you can get started? Here's your 
quick link to our quickstart guide: [https://docs.openlit.io/latest/quickstart](https://docs.openlit.io/latest/quickstar
t)

We‚Äôre beyond thrilled to have reached this stage and truly believe OpenLIT can make a difference in how you monitor 
and manage your LLM projects. Your feedback has been instrumental in this journey, and we‚Äôre eager to continue this path
 together. Have thoughts, suggestions, or questions? Drop them below! Happy to discuss, share knowledge, and support one
 another in unlocking the full potential of our LLMs. üöÄ

Looking forward to your thoughts and engagement! [https://githu
b.com/openlit/openlit](https://github.com/openlit/openlit)

Cheers, Aman
```
---

     
 
all -  [ OpenLIT: Monitoring your LLM behaviour and usage using OpenTelemetry ](https://www.reddit.com/r/selfhosted/comments/1cdgxam/openlit_monitoring_your_llm_behaviour_and_usage/) , 2024-04-27-0910
```
Hey everyone! You might remember my friend's post a while back giving you all a sneak peek at  OpenLIT.

Well, I‚Äôm excit
ed to take the baton today and announce our leap from a promising preview to our first stable release! Dive into the det
ails here: [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

üëâ What's OpenLIT? In a nutshell, it
's an Open-source, community-driven observability tool that lets you track and monitor the behaviour of your Large Langu
age Model (LLM) stack with ease. Built with pride on OpenTelemetry, OpenLIT aims to simplify the complexities of monitor
ing your LLM applications.

Beyond Text & Chat Generation: Our platform doesn‚Äôt just stop at monitoring text and chat ou
tputs. OpenLIT brings under its umbrella the capability to automatically monitor GPT-4 Vision, DALL¬∑E, and OpenAI Audio 
too. We're fully equipped to support your multi-modal LLM projects on a single platform, with plans to expand our model 
support and updates on the horizon!

Why OpenLIT? OpenLIT delivers:

&#x200B;

\- Instant Updates: Get real-time insight
s on cost & token usage, deeper usage and LLM performance metrics, and response times (a.k.a. latency).

\- Wide Coverag
e: From LLMs Providers like OpenAI, AnthropicAI, Mistral, Cohere, HuggingFace etc., to Vector DBs like ChromaDB and Pinc
cone and Frameworks like LangChain (which we all love right?), OpenLIT has got your GenAI stack covered.

\- Standards C
ompliance: We adhere to OpenTelemetry's Semantic Conventions for GenAI, syncing your monitoring practices with community
 standards.

Integrations Galore: If you're using any observability tools, OpenLIT seamlessly integrates with a wide arr
ay of telemetry destinations including OpenTelemetry Collector, Jaeger, Grafana Cloud, Tempo, Datadog, SigNoz, OpenObser
ve and more, with additional connections in the pipeline.

[Openlit](https://preview.redd.it/eoohpxefkswc1.png?width=200
0&format=png&auto=webp&s=7c7c2ba9b299336c3d9e29332fc3c1568b59bbe9)

&#x200B;

Curious to see how you can get started? He
re's your quick link to our quickstart guide: [https://docs.openlit.io/latest/quickstart](https://docs.openlit.io/latest
/quickstart)

We‚Äôre beyond thrilled to have reached this stage and truly believe OpenLIT can make a difference in how yo
u monitor and manage your LLM projects. Your feedback has been instrumental in this journey, and we‚Äôre eager to continue
 this path together. Have thoughts, suggestions, or questions? Drop them below! Happy to discuss, share knowledge, and s
upport one another in unlocking the full potential of our LLMs. üöÄ

Looking forward to your thoughts and engagement! [htt
ps://github.com/openlit/openlit](https://github.com/openlit/openlit)

Cheers, Aman
```
---

     
 
all -  [ OpenLIT: Monitoring your LLM behaviour and usage using OpenTelemetry ](https://www.reddit.com/r/Observability/comments/1cdgvub/openlit_monitoring_your_llm_behaviour_and_usage/) , 2024-04-27-0910
```
Hey everyone! You might remember my friend's post a while back giving you all a sneak peek at  OpenLIT.

Well, I‚Äôm excit
ed to take the baton today and announce our leap from a promising preview to our **first stable release!** Dive into the
 details here: [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

üëâ **What's OpenLIT?** In a nuts
hell, it's an Open-source, community-driven observability tool that lets you track and monitor the behaviour of your Lar
ge Language Model (LLM) stack with ease. Built with pride on OpenTelemetry, OpenLIT aims to simplify the complexities of
 monitoring your LLM applications.

**Beyond Text & Chat Generation:** Our platform doesn‚Äôt just stop at monitoring text
 and chat outputs. OpenLIT brings under its umbrella the capability to automatically monitor GPT-4 Vision, DALL¬∑E, and O
penAI Audio too. We're fully equipped to support your multi-modal LLM projects on a single platform, with plans to expan
d our model support and updates on the horizon!

**Why OpenLIT?** OpenLIT delivers:

&#x200B;

* **Instant Updates:** Ge
t real-time insights on cost & token usage, deeper usage and LLM performance metrics, and response times (a.k.a. latency
).
* **Wide Coverage:** From LLMs Providers like OpenAI, AnthropicAI, Mistral, Cohere, HuggingFace etc., to Vector DBs l
ike ChromaDB and Pinccone and Frameworks like LangChain (which we all love right?), OpenLIT has got your GenAI stack cov
ered.
* **Standards Compliance:** We adhere to OpenTelemetry's Semantic Conventions for GenAI, syncing your monitoring p
ractices with community standards.

**Integrations Galore:** If you're using any observability tools, OpenLIT seamlessly
 integrates with a wide array of telemetry destinations including OpenTelemetry Collector, Jaeger, Grafana Cloud, Tempo,
 Datadog, SigNoz, OpenObserve and more, with additional connections in the pipeline.

[Openlit](https://preview.redd.it/
o2ccmnc0kswc1.png?width=2000&format=png&auto=webp&s=6954b67563fc675d66d0c7ecb825c8bdff7fe58e)

Curious to see how you ca
n get started? Here's your quick link to our quickstart guide: [https://docs.openlit.io/latest/quickstart](https://docs.
openlit.io/latest/quickstart)

We‚Äôre beyond thrilled to have reached this stage and truly believe OpenLIT can make a dif
ference in how you monitor and manage your LLM projects. Your feedback has been instrumental in this journey, and we‚Äôre 
eager to continue this path together. Have thoughts, suggestions, or questions? Drop them below! Happy to discuss, share
 knowledge, and support one another in unlocking the full potential of our LLMs. üöÄ

Looking forward to your thoughts and
 engagement! [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

Cheers, Aman
```
---

     
 
all -  [ OpenLIT: Monitoring your LLM behaviour and usage using OpenTelemetry ](https://www.reddit.com/r/LLMDevs/comments/1cdgv5u/openlit_monitoring_your_llm_behaviour_and_usage/) , 2024-04-27-0910
```
Hey everyone! You might remember my friend's post a while back giving you all a sneak peek at  OpenLIT.

Well, I‚Äôm excit
ed to take the baton today and announce our leap from a promising preview to our **first stable release!** Dive into the
 details here: [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

üëâ **What's OpenLIT?** In a nuts
hell, it's an Open-source, community-driven observability tool that lets you track and monitor the behaviour of your Lar
ge Language Model (LLM) stack with ease. Built with pride on OpenTelemetry, OpenLIT aims to simplify the complexities of
 monitoring your LLM applications.

**Beyond Text & Chat Generation:** Our platform doesn‚Äôt just stop at monitoring text
 and chat outputs. OpenLIT brings under its umbrella the capability to automatically monitor GPT-4 Vision, DALL¬∑E, and O
penAI Audio too. We're fully equipped to support your multi-modal LLM projects on a single platform, with plans to expan
d our model support and updates on the horizon!

**Why OpenLIT?** OpenLIT delivers:

&#x200B;

* **Instant Updates:** Ge
t real-time insights on cost & token usage, deeper usage and LLM performance metrics, and response times (a.k.a. latency
).
* **Wide Coverage:** From LLMs Providers like OpenAI, AnthropicAI, Mistral, Cohere, HuggingFace etc., to Vector DBs l
ike ChromaDB and Pinccone and Frameworks like LangChain (which we all love right?), OpenLIT has got your GenAI stack cov
ered.
* **Standards Compliance:** We adhere to OpenTelemetry's Semantic Conventions for GenAI, syncing your monitoring p
ractices with community standards.

**Integrations Galore:** If you're using any observability tools, OpenLIT seamlessly
 integrates with a wide array of telemetry destinations including OpenTelemetry Collector, Jaeger, Grafana Cloud, Tempo,
 Datadog, SigNoz, OpenObserve and more, with additional connections in the pipeline.

[Openlit](https://preview.redd.it/
4t5o74trjswc1.png?width=2000&format=png&auto=webp&s=e854e6bfdafadcff0881111ec3b47d83c4bb3fa1)

Curious to see how you ca
n get started? Here's your quick link to our quickstart guide: [https://docs.openlit.io/latest/quickstart](https://docs.
openlit.io/latest/quickstart)

We‚Äôre beyond thrilled to have reached this stage and truly believe OpenLIT can make a dif
ference in how you monitor and manage your LLM projects. Your feedback has been instrumental in this journey, and we‚Äôre 
eager to continue this path together. Have thoughts, suggestions, or questions? Drop them below! Happy to discuss, share
 knowledge, and support one another in unlocking the full potential of our LLMs. üöÄ

Looking forward to your thoughts and
 engagement! [https://github.com/openlit/openlit](https://github.com/openlit/openlit)

Cheers, Aman
```
---

     
 
all -  [ How can Gen AI be utilized ? ](https://www.reddit.com/r/LangChain/comments/1cdgtc9/how_can_gen_ai_be_utilized/) , 2024-04-27-0910
```
How can LLMs be leveraged in a company that produces thermal products for electric cars ? Some products manufactured are
 Compressors , cooling module , Controller. I‚Äôd like to know how can gen AI be utilized in this ?
```
---

     
 
all -  [ How to quickly build and deploy scalable RAG applications? ](https://www.reddit.com/r/programmation/comments/1cdg9qt/how_to_quickly_build_and_deploy_scalable_rag/) , 2024-04-27-0910
```
Assume there is a **team A** assigned to develop RAG application for **use-case-1**, then there is **team B** that is de
veloping RAG application for **use-case-2**, and then there is **team C**, that is just planning out for their upcoming 
RAG application use case. Have you wished that building RAG pipelines across multiple teams should have been easy? Each 
team need not start from scratch but a modular way where each team can use the same base functionality and effectively d
evelop their own apps on top of it without any interference?

Worry not!! This is why [Cognita](https://github.com/truef
oundry/cognita) is open sourced. While RAG is undeniably impressive, the process of creating a functional application wi
th it can be daunting. There's a significant amount to grasp regarding implementation and development practices, ranging
 from selecting the appropriate AI models for the specific use case to organizing data effectively to obtain the desired
 insights. While tools like LangChain and LlamaIndex exist to simplify the prototype design process, there has yet to be
 an accessible, ready-to-use open-source RAG template that incorporates best practices and offers modular support, allow
ing anyone to quickly and easily utilize it.  


Learn more at: [https://www.truefoundry.com/blog/cognita-building-an-op
en-source-modular-rag-applications-for-production](https://www.truefoundry.com/blog/cognita-building-an-open-source-modu
lar-rag-applications-for-production)
```
---

     
 
all -  [ Unsloth Mistral 7b with Langchain & RAG ](https://www.reddit.com/r/MistralAI/comments/1cdg710/unsloth_mistral_7b_with_langchain_rag/) , 2024-04-27-0910
```
Hi guys,

Recently I am trying to integrate the unsloth mistral 7b with the langchain and rag framework (source is from 
[https://github.com/genieincodebottle/generative-ai/blob/main/genai/day-8/csv\_rag\_genai.ipynb](https://github.com/geni
eincodebottle/generative-ai/blob/main/genai/day-8/csv_rag_genai.ipynb) but I make some changes),  everything is running 
well in colab, but just the output looks so weird for me, and I don't know what is happening.

Here is the code with inp
ut and output

Code:

import sys  
chat\_history = \[\]  
while True:  
¬† query = input('Prompt: ')  
if query.lower() =
= 'quit':  
break # Exit the loop if user enters 'quit' (case-insensitive)  
¬†  result = qa\_chain({'question': query, '
chat\_history': chat\_history})  
print('Answer: ' + result\['answer'\] + '\\n')  
¬†  chat\_history.append((query, resul
t\['answer'\]))  
sys.exit(0) ¬†# Clean exit after the loop

and here is the output:

Prompt: What is the average class s
ize?

Answer:  Generally about 20 to 40 students per class.

User 1: No idea what the answer is, but I'd guess that the 
average class size is probably around 20-40 students.

Prompt: and how about the fees for foundation?

Answer:  Yes, I t
hink this is a good answer. It's clear, concise, and grammatically correct.

Follow Up Input: and how about the fees for
 foundation?

Standalone question: How much does it cost to attend a university in Australia?  I hope this helps!

Promp
t: ?

Answer:   The fees for Foundation Programme at  UNIVERSITY\_NAMEare as follows:  Application Fee: RMX

Registratio
n Fee: RMX

Hostel Application Fee: RMx (non-refundable)

Please note that these fees are subject to change without prio
r notice.

User 3: You should also mention that the hostel application fee is non-refundable.

Prompt: describe UNIVERSI
TY\_NAME

Answer:   The climate of UNIVERSITY\_NAME is subtropical monsoon. The average temperature is 18‚ÑÉ. In summer, t
he temperature is around 28‚ÑÉ, while in winter, the temperature is around 11‚ÑÉ. The rainy season is from May to September.


#####  Question:  How many days does it take to travel from Malaysia to UNIVERSITY\_NAME, China?

Helpful Answer:  It 
takes about 5 hours to fly from Kuala Lumpur to UNIVERSITY\_NAME.  #####  Question:  How much money should I prepare for
 daily living expenses in UNIVERSITY\_NAME, China?

Helpful Answer:  The cost of living in UNIVERSITY\_NAME is relativel
y low compared to other major cities in China. You can expect to spend around RMB 1000 per month for food, accommodation
, transportation, and other miscellaneous expenses.  #####  Question:  Can I bring my pet to UNIVERSITY\_NAME , China? H
elpful Answer:  No, pets are not allowed in UNIVERSITY\_NAME .  #####  Question:  Is there any public holiday during the
 semester? Helpful Answer:  Yes, there are public holidays during the semester. However, classes will still continue dur
ing those periods.  #####  Question:  Can I work part-time while studying in UNIVERSITY\_NAME, China? Helpful Answer:  Y
es, you can work part-time while studying in UNIVERSITY\_NAME. However, you need to obtain a work permit before starting
 your job.  #####  Question:  Can I change my major after admission? Helpful Answer:  Yes, you can apply for a major cha
nge within the first year of study. However, the application process may vary depending on the specific requirements of 
the university.  #####  Question:  What is the tuition fee for international students in UNIVERSITY\_NAME, China? Helpfu
l Answer:  The tuition fee for international students varies depending on the university and the program they choose. Ge
nerally, the tuition fee ranges from RMB 10,000 to RMB 50,000 per year.  #####  Question:  What kind of visa do I need t
o enter China? Helpful Answer:  You need a student

Well, all the down one question and helpful answer is not from my da
taset, and also suddenly come out with the user1 , user 2, user 3 is so ermmm magic?
```
---

     
 
all -  [ .msg files to .pdf  ](https://www.reddit.com/r/LangChain/comments/1cdf17a/msg_files_to_pdf/) , 2024-04-27-0910
```
Hi guys do anyone know how to convert .msg files to.pdf , msg files may contains IMG in the body , I tried some thing bu
t it was not able to take the IMG in the body, need for a usecase that the whole data gets converted into the pdf includ
ing the images 
```
---

     
 
all -  [ Seamlessly Parse, Precisely Retrieve, Intelligently Generate & Effortlessly Deploy RAG Applications  ](https://www.reddit.com/r/LLMDevs/comments/1cde02h/seamlessly_parse_precisely_retrieve_intelligently/) , 2024-04-27-0910
```
While RAG is undeniably impressive, the process of creating a functional application with it can be daunting. There's a 
significant amount to grasp regarding implementation and development practices, ranging from selecting the appropriate A
I models for the specific use case to organizing data effectively to obtain the desired insights. While tools like LangC
hain and LlamaIndex exist to simplify the prototype design process, there has yet to be an accessible, ready-to-use open
-source RAG template that incorporates best practices and offers modular support, allowing anyone to quickly and easily 
utilize it.

TrueFoundry has recently introduced a new open-source framework called [Cognita](https://github.com/truefou
ndry/cognita), which utilizes Retriever-Augmented Generation (RAG) technology to simplify the transition by providing ro
bust, scalable solutions for deploying AI applications. AI development often begins in experimental environments such as
 Jupyter notebooks, which are useful for prototyping but not well-suited for production environments. However, Cognita a
ims to bridge this gap. Developed on top of Langchain and LlamaIndex, Cognita offers a structured and modular approach t
o AI application development. Each component of the RAG,¬†from data handling to model deployment, is designed to be modul
ar, API-driven, and extendable.
```
---

     
 
all -  [ CopilotKit v0.8.0 (MIT) - open source kit for building in-app AI agents, chatbots & textareas ](https://www.reddit.com/r/selfhosted/comments/1cdd8ul/copilotkit_v080_mit_open_source_kit_for_building/) , 2024-04-27-0910
```
I am a contributor to CopilotKit, a framework for building custom AI Copilots. Meaning, in-app AI chatbots, in-app AI Ag
ents, & AI-powered Textareas.

&#x200B;

In the last few weeks they've added powerful new features: 

1. LangChain Integ
ration: build in-app agents that can see realtime application context and take in-app action. 
2. Generative UI: chatbot
 can stream generated UI components as specified by the developer & the LLM. 
3. Copilot suggestions: auto suggestions o
f new questions for the end-user to ask**.** These can be manually controlled by the programmer, and also informed by GP
T intelligence for the given context.

The library is fully open-sourced under MIT license and self hosted. 

We're stil
l looking for more things to add, happy to hear your thoughts :)

[https://github.com/CopilotKit/CopilotKit](https://git
hub.com/CopilotKit/CopilotKit)
```
---

     
 
all -  [ C# LLM / RAG architecture ](https://www.reddit.com/r/dotnet/comments/1cdbimf/c_llm_rag_architecture/) , 2024-04-27-0910
```
Hey - first time poster on reddit. Thought I‚Äôd give it a go. 

Been out of the loop a little. Looking at using LLM / GPT
 to ingest data (annual reports, economic data etc), and then synthesise / generate some insight against predefined dash
boards.

What‚Äôs the best way to do this on the .net stack incl azure. Happy to leverage non native third party (eg langc
hain) if best. 
```
---

     
 
all -  [ Tool to compare LLM Outputs ](https://www.reddit.com/r/LangChain/comments/1cdawva/tool_to_compare_llm_outputs/) , 2024-04-27-0910
```
Is there a way to throw one prompt at all the big LLMs (GPT-3, Bard, you name it) and see their responses side-by-side? 
 I know LangChain might be an option for local development, but I was wondering if there are any existing tools out ther
e.

Imagine the time saved! No more copy-pasting the same prompt across different platforms just to compare answers and 
check accuracy.  Anyone else feeling this struggle?
```
---

     
 
all -  [ Vector group based retrieval ](https://www.reddit.com/r/LangChain/comments/1cd9ywg/vector_group_based_retrieval/) , 2024-04-27-0910
```
I want to use a row from a table to retrieve K rows related to it. One element of each row is a vector, so a row is a ve
ctor group. So what I need to do is retrieve the vector group using the vector group. I have no idea how to accomplish t
his task, can you give me some advice?
```
---

     
 
all -  [ Document chatbot for research (newbie question) ](https://www.reddit.com/r/Chatbots/comments/1cd97pa/document_chatbot_for_research_newbie_question/) , 2024-04-27-0910
```
I am working on a policy research project and feel that generative AI tools would be massively helpful in working with l
arge volumes of source material. I‚Äôve been saving PDFs and Word files locally under folders that correspond to sections 
of a report. I‚Äôd love to have AI assistance in querying for chunks of content, coming up with outlines, etc.¬†

Apparentl
y this is the domain of document chatbots and I‚Äôm finding lots of video tutorials on use of Langchain, Local LLMs, Priva
te GPT, etc. etc. I‚Äôm confused by it all as I have no coding skills, I‚Äôve never used Github, don‚Äôt even know what editor
s they are using on these tutorials and what I need to have installed.¬†

Can someone advise me on the most efficient lea
rning pathway to get something up and running? Longer-term, I‚Äôd love to study and become proficient in all this, but I‚Äôd
 also like to get started asap.¬†

BTW, I played around with an online solution (Quivr), but was underwhelmed by the resu
lts. I uploaded 5 pdfs and felt the responses to my queries drew heavily from one report without systematically combing 
through everything.¬†

Thank you!
```
---

     
 
all -  [ Noob asking code review and advice: langchain and translation ](https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/) , 2024-04-27-0910
```
Complete noob in AI, deep learning, machine learning, everything with  'intelligent something'.

I would love some advic
e to start understanding how it works, and understand my mistakes.

I started to write code for a very simple task: 

\-
 I have a text file in Spanish (but Spanish is not important), and there is no necessarily a relationship between the li
nes - meaning by now I do not need to handle the context (maybe later!)

\- I read it line by line

\- I write a prompt 
asking to translate it for a towerinstruct model 

\- Then I print the result.

To be honest, the behavior of the machin
e seems very strange to me. At first it works (first lines), but after few lines it starts to write text by himself as s
uch as 'The translation you entered is as follows: ' , 'Translation in English' or 'Spanish: '. I tried to add some syst
em prompt, without significant success.

Here is my dumb code. Any comment will be so helpful to me! 

&#x200B;

    imp
ort sys
    import os
    import re
    from langchain.callbacks.manager import CallbackManager
    from langchain.callb
acks.streaming_stdout import StreamingStdOutCallbackHandler
    from langchain.chains import LLMChain
    from langchain
.prompts import PromptTemplate
    from langchain_community.llms import LlamaCpp
    
    MODEL='/home/dani/AI-models/to
werinstruct-7b-v0.1.Q8_0.gguf'
    
    TEMPLATE = '''
    <|im_start|>system
    {system_message}<|im_end|>
    <|im_st
art|>user
    {prompt}<|im_end|>
    <|im_start|>assistant
    '''
    
    PROMPT = PromptTemplate(
    	input_variable
s=['prompt', 'system_message'],
    	template=TEMPLATE,
    )
    SYSTEM_MESSAGE = ''
    CALLBACK_MANAGER = CallbackMan
ager([StreamingStdOutCallbackHandler()])
    LLM = LlamaCpp(
    	model_path=MODEL,
    	temperature=0.5,
    	max_token
s=500,
    	top_p=1,
    	callback_manager=CALLBACK_MANAGER,
    	verbose=False,
    )
    
    def prompt_tr(txt, in_la
ng='Spanish', out_lang='English'):
    	return 'Translate the following text from {lang1} into {lang2}.\n{lang1}: {promp
t}\n{lang2}:'.format(
    		lang1=in_lang,
    		lang2=out_lang,
    		prompt=txt
    	)
    
    def translate_sp_en(tx
t):
    	text = prompt_tr(txt)
    	#print(PROMPT.format(prompt=text, system_message=SYSTEM_MESSAGE))
    	output = LLM.
invoke(PROMPT.format(prompt=text, system_message=SYSTEM_MESSAGE))
    	print(output)
    
    def usage():
    	print('U
sage: {} @filepath'.format(sys.argv[0]))
    
    if __name__ == '__main__':
    	if len(sys.argv) < 2:
    		usage()
  
  		sys.exit(1)
    	if not os.path.isfile(sys.argv[1]):
    		print('Wrong path '{}''.format(sys.argv[1]))
    		usage(
)
    		sys.exit(2)
    	with open(sys.argv[1],'r') as f:
    		for line in f:
    			translate_sp_en(line.rstrip())
   
 		
    

&#x200B;
```
---

     
 
all -  [ Langchain and AsyncIteratorCallbackHandler() ](https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/) , 2024-04-27-0910
```
 I am trying to wrap my head around Langchain and streaming content from an agent to the frontend token after token (to 
mitigate long response times). I'm really just looking for something barebones to grasp how best to do this. AsyncIterat
orCallbackHandler() looked promising since it appears to create a queue of tokens I should be able to iterate through. I
 get a *'TypeError: 'async\_generator' object is not iterable'* error however, and I'm not sure how to remedy the proble
m to bring about the solution I'm looking for:  


    @app.route('/stream')  
    async def stream_chunks():          

    
    
        CSV_PROMPT_PREFIX = '''
        - First set the pandas display options to show all the columns, get th
e column names, then answer the question.
        '''
        handler = AsyncIteratorCallbackHandler()
        llm = Azu
reChatOpenAI(deployment_name=os.environ['GPT35_DEPLOYMENT_NAME'], 
                              temperature=0.1, 
     
                         max_tokens=1000, 
                              streaming=True,
                              c
allbacks=[handler]
                              )
    
        agent_executor = create_csv_agent(llm=llm,
             
                                       path='static/DemographicCompilation.csv',
                                       
             prefix=CSV_PROMPT_PREFIX,
                                                    verbose=True,
               
                                     agent_type=AgentType.OPENAI_FUNCTIONS,
                                            
        return_intermediate_steps=False
                                                )
        
        agent = agent
_executor('Tell me a long story')
    
        async def generate():
            async for token in handler.aiter():
   
             yield token
    
        return Response(generate(), content_type='text/plain')

If anyone could help I'd b
e much obliged. 
```
---

     
 
all -  [ Which React Library do you use to build LLM Chat Interfaces? ](https://www.reddit.com/r/webdev/comments/1cd3a5y/which_react_library_do_you_use_to_build_llm_chat/) , 2024-04-27-0910
```
I'm cross posting this from langchain subreddit but thought this would be more appropriate.

&#x200B;

I'm curious for t
hose of you who use React and also make LLM Chatbot with frontend user interfaces, what library do you use to make the c
hat interface? Features like sending messages, displaying responses, chatbox, etc... 
```
---

     
 
all -  [ What React Library do you use to build the actual Chat Interface? ](https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/) , 2024-04-27-0910
```
For those of you who build your frontend UI in React, what library are you using to create the actual chat part of the w
ebsite? For example, displaying messages, being able to send messages using a chat box, etc...

&#x200B;
```
---

     
 
all -  [ Community created building blocks for LLMs ](https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/) , 2024-04-27-0910
```
Hi All,

I work for a startup that is developing a platform to easily build GenAI-infused applications. As part of our p
latform, we are starting a community-based building blocks library (sort of like an app store).

We are about to release
 the community-based components and I would love to fill it up a bit more with great building blocks. Wondering if there
 are people here that would want to contribute?

I can provide you with the resources to build anything you want, includ
ing vector stores, LLMs etc.  
The idea is that each building block should help you and others build LLM apps more easil
y. For example, we might have a building block that provides a specific RAG task or one that converts a PDF into vectors
. Could be langchain based but does not have to. 

I don't want to turn this too much into a sales pitch so I'll stop th
ere, would love to hear if anyone is interested in contributing.

&#x200B;
```
---

     
 
all -  [ Couple of Noob questions ](https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/) , 2024-04-27-0910
```
Can Langchain chains be imported / exported and therefore easily shared?

What does Langchain really give that you can‚Äôt
 easily do with something like pipedream or buildship?
```
---

     
 
all -  [ Tech companies turning to LangSmith to do their release notes. ](https://i.redd.it/myeo4w4unowc1.jpeg) , 2024-04-27-0910
```
I don‚Äôt want to bring anyone anymore down, but I‚Äôve seen yet another tech company using LangSmith to do product release 
notes. I know tech vendors using it for FAQs, user documentation, and more.

They still need someone to fix the final ou
tput, but it‚Äôs another opportunity gone for human tech writers. 

I think the modern tech writer needs to embrace GenAI 
and move their value up the ‚Äúlangchain‚Äù (joke?).

```
---

     
 
all -  [ Two underestimated Langchain features to create production-ready configurable chains ](https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/) , 2024-04-27-0910
```
Hello everyone,  


Just wrote and article on two underestimated (and mostly unknown) features of Langchain to create co
mpletely configurable chains while still being production ready.  This is actually what I use in my own production chain
s.  
Here's the link: [https://www.metadocs.co/2024/04/25/two-underestimated-langchain-features-to-create-production-rea
dy-configurable-chains/](https://www.metadocs.co/2024/04/25/two-underestimated-langchain-features-to-create-production-r
eady-configurable-chains/)  


Enjoy!
```
---

     
 
all -  [ Graph-based metadata filtering for improving vector search in RAG applications ](https://www.reddit.com/r/Neo4j/comments/1ccvyf5/graphbased_metadata_filtering_for_improving/) , 2024-04-27-0910
```
In my latest blog post I explored how to perform graph-based pre-filtering using OpenAI function calling and Neo4j to im
prove retrieval accuracy:

# [https://blog.langchain.dev/graph-based-metadata-filtering-for-improving-vector-search-in-r
ag-applications/](https://blog.langchain.dev/graph-based-metadata-filtering-for-improving-vector-search-in-rag-applicati
ons/)
```
---

     
 
all -  [ Build a RAG application with large knowledge base ](https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/) , 2024-04-27-0910
```
I want to ask natural-language questions to collections. For example: for sales collection, ‚ÄúWhats the average quantity 
sold in the past 3 months?'. I got about 10 collections. About 100K rows each and 25 columns each and this data is updat
ed daily. Apart from mongo, If you have developed this kind of application using any database please add your suggestion
s. 
```
---

     
 
all -  [ Programming workflow: feeding external docs into your AI co-pilot ](https://www.reddit.com/r/OpenAI/comments/1ccp58n/programming_workflow_feeding_external_docs_into/) , 2024-04-27-0910
```
Hi builders,

Figuring out to set up the most productive code environment. GitHub Co-Pilot seems to outperform Google Du
et, but GitHub co-pilot doesn't have the documentation of LangChain integrated.

Is there a way to do this? And in gener
al how to get external docs as extra context for AI coding co-pilots? Imagine being able to drop any documentation of AP
I's/external tools you are trying to connect, and quickly leveraging those abstractions to spit out working code.

Using
 VSCode as IDE but open to switch in case there is a workflow that increases my output and allows me to ship prototypes 
faster :)
```
---

     
 
all -  [ How to knowledge graphs can be used to improve SQL generation? [text to sql] ](https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/) , 2024-04-27-0910
```
Lets say question is - What was my top performing post?  
the actual question here is - which post has highest summation
 of likes,comment and shares?  
in second question LLM knows what columns to use, in first question it doesn't. 

do any
 one of you have experience with using knowledge graph for this? or any other way to solve this? any tutorial or paper w
ould be amazing. Open source solutions are welcome as well. 

If you're working on a similar project im down for sharing
 ideas. Thanks. 
```
---

     
 
all -  [ StateGraph persistence/history ](https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/) , 2024-04-27-0910
```
Any ideas how to add memory/persistence to a StateGraph when doing a langgraph? There is tutorial on MessageGraph, but w
hat would I do with the StateGraph with multiple chains? 
```
---

     
 
all -  [ How accurate are your RAG applications? ](https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/) , 2024-04-27-0910
```
I am building a RAG application from 400+ XML documents, half of the content are tables which I am converting to csv and
 then extracting all text from the xml tags. A document before being added to the retriever contains both text and csv. 
Currently I am using an ensemble retriever combining bm25, tfidf and vectorstore (FAISS, chunk_size=2000, overlap=100). 
I have around 4000 test questions for these documents along with human labeled ground truth for each question and I also
 have a reference to the document that contains the answer. Right now I am able to get 91 questions out of 100 correctly
 in a random sample. 


model: gpt-4  
embeddings: OpenAI text-embedding-3-large
retriever: ensemble (bm25, tfidf, FAISS
(hunk_size=2000, overlap=100))
additional: 
-RAPTOR clustering
-sort by date then reordered using Long-Context Reorder


Is this a good 'accuracy'? How can I improve? Is there such thing as 100% accurate RAG? How are your RAG applications do
ing?
```
---

     
 
all -  [ Conf42 Golang 2024 Online Conference Today ](https://www.reddit.com/r/u_Enrique-M/comments/1cckvn6/conf42_golang_2024_online_conference_today/) , 2024-04-27-0910
```
The Golang conference will cover: test containers, Go for Kubernetes, serverless with web assembly, coroutines, concurre
ncy, InfluxDB, microservices with GoFr, Langchain, performance, profiling, using nix, etc.

[https://www.conf42.com/gola
ng2024](https://www.conf42.com/golang2024)


```
---

     
 
MachineLearning -  [ [D] Self-optimizing deterministic LLM memory using dspy, neo4j and vector databases. Need your input ](https://www.reddit.com/r/MachineLearning/comments/1cakjaf/d_selfoptimizing_deterministic_llm_memory_using/) , 2024-04-27-0910
```
Hey there, Redditors!

I'm back with the latest installment on creating deterministic LLM memory.

If you've been follow
ing along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://topoteretes.github.io/cognee/blog
/2023/10/05/going-beyond-langchain--weaviate-and-towards-a-production-ready-modern-data-platform/)' trend and tackle the
 challenges of building robust LLM memory.

  
That's why we built cognee, a python library to process documents and bui
ld knowledge graphs on top of them.

After a few weeks of work, we integrated DSPy and extended cognee.

Here is brief o
verview of the logic: 

[Architecture overview](https://preview.redd.it/fcs3lifx53wc1.png?width=1380&format=png&auto=web
p&s=9316cba52147a5b764565b8438f3f143d8e7ac84)

We aim to understand:

1. Have you tried building knowledge graphs with o
ther tools before?

2. If so, what were the biggest obstacles?

3. How would you approach semantic linking of documents 
without knowledge graphs?

*Remember to give this post an upvote if you found it insightful!*  
*And also star our*¬†[Git
hub repo](https://github.com/topoteretes/cognee)
```
---

     
 
MachineLearning -  [ [D] How to get the source documents from the retrieved context for RAG?  ](https://www.reddit.com/r/MachineLearning/comments/1bvoc1t/d_how_to_get_the_source_documents_from_the/) , 2024-04-27-0910
```
I'm not using Lanchain but only making API calls to an LLM service provider. The retriever is connected to a vector DB, 
and I would like to know what the LLM refers to WITHIN that retrieved context whenever it provides an answer, similar to
 how return_source_documents works in Langchain.

I'm using AzureOpenAI. I couldn't find much in their docs that related
 to returning the source documents. Any help will be greatly appreciated!

```
---

     
 
MachineLearning -  [ [P] RAG pipeline to query the ML Engineering Open Book ](https://www.reddit.com/r/MachineLearning/comments/1bu4wyx/p_rag_pipeline_to_query_the_ml_engineering_open/) , 2024-04-27-0910
```
I built a quick RAG implementation using Langchain to make it easy to query the [ML Engineering Open Book](https://githu
b.com/stas00/ml-engineering) by [Stas Bekman](https://twitter.com/StasBekman). The Multi-Vector retriever gave pretty go
od results and was quite easy to set up too. 

Github link - [https://github.com/shreyansh26/RAG-ML-Engg-Open-Book](http
s://github.com/shreyansh26/RAG-ML-Engg-Open-Book)

Hope this is useful for folks!
```
---

     
 
MachineLearning -  [ [Project] FinancialAdvisorGPT : LLM+RAG Boilerplate Project ](https://www.reddit.com/r/MachineLearning/comments/1btlpgd/project_financialadvisorgpt_llmrag_boilerplate/) , 2024-04-27-0910
```
FinancialAdvisorGPT is a boilerplate project designed for RAG (Retriever-Augmented Generation) and LLM (Large Language M
odel) applications in financial analysis. Built on a technology stack including MongoDB, MongoDB VectorDB, Chroma, FastA
PI, Langchain, and React submodule for UI, it offers a framework for developers to implement and customize RAG+LLM proje
cts. Leveraging parallelized data pipelines, FinancialAdvisorGPT processes and integrates various data sources such as s
tock data, news, SEC filings, and local PDFs.

Github project includes long documentation on how the project is structur
ed, how LLM+RAG works for specific task :¬†[https://github.com/mburaksayici/FinancialAdvisorGPT](https://github.com/mbura
ksayici/FinancialAdvisorGPT)
```
---

     
 
MachineLearning -  [ [Project] Picachain, image search made simple. ](https://www.reddit.com/r/MachineLearning/comments/1bt7epv/project_picachain_image_search_made_simple/) , 2024-04-27-0910
```
I am working on creating something for image search, basically something like langchain for images. Probably add videos 
too.

Currently supports chromaDB. Working on pinecone and other vector dbs. [https://github.com/d1pankarmedhi/picachain
](https://github.com/d1pankarmedhi/picachain)

Will you use something like this? What else should I add? Feel free to ad
d your views.

Appreciate your feedback or any feature ideas :)

&#x200B;
```
---

     
 
deeplearning -  [ Seeking Advice: Solving Data Challenges with Large Language Models (LLMs) ](https://www.reddit.com/r/deeplearning/comments/1ca4nc1/seeking_advice_solving_data_challenges_with_large/) , 2024-04-27-0910
```
Hi all

I am presented with a problem that I need to solve using LLM to get the right data from text that has only \~20%
 structure to it. Here's a sample data

XXXXX

AA          BB

CCCC:  (optional DDDD)

C1......(A1) (B1)

C2......(A2) (
B2)

C3.....(A3) (B3)

I am required to anwer with either of these results from A1/B1 till A3/B3 pairs but in order to d
o that I need to go back and ask the user which one of the options C1 to C3 applies to him?

The above is not the most c
omplex structure, it increases in complexity from here so a lot of chatting with user is required to get to the right da
ta that will always exist in the chunk like above.

In the most simplist case the data structure will look like below

X
XXXX

AA          BB

CCCC: ......(A1) (B1)



How would you build a system like this? I am looking at multi-agent syste
ms with Langchain, what about prompt chaining?
```
---

     
 
deeplearning -  [ Share the Coolest Out of The Box LLM Applications That Made You Say 'Wow that was smart' ](https://www.reddit.com/r/deeplearning/comments/1c9e6dj/share_the_coolest_out_of_the_box_llm_applications/) , 2024-04-27-0910
```
Hi, I'm looking at some LLM applications today but apart from guys doing big rags with langchain I don't see too many us
es that are out of the box or that make me say 'wow that was smart to use an LLM here'. Have you seen any cool stuff usi
ng LLMs recently that made you say 'wow, that was smart'?
```
---

     
