 
all -  [ Suggested papers for beginner in multi-agent LLMs ](/r/learnmachinelearning/comments/1h8ono0/suggested_papers_for_beginner_in_multiagent_llms/) , 2024-12-08-0915
```

```
---

     
 
all -  [ Generative AI - build LLM powered applications using LangChain and Javascript ](https://www.reddit.com/r/udemyfreebies/comments/1h8yr87/generative_ai_build_llm_powered_applications/) , 2024-12-08-0915
```
[https://learnwithamit.com/langchain-js-rf](https://learnwithamit.com/langchain-js-rf)
```
---

     
 
all -  [ Is There a Need for a Centralized Marketplace for AI Agents? ](https://www.reddit.com/r/LangChain/comments/1h8wpfj/is_there_a_need_for_a_centralized_marketplace_for/) , 2024-12-08-0915
```
Hey everyone,

It’s pretty obvious that AI agents are the future—they’re already transforming industries by automating t
asks, enhancing productivity, and solving niche problems. However, I’ve noticed a major gap: there’s no simple, centrali
zed marketplace where you can easily browse through hundreds (or thousands) of AI agents tailored for every need.

I’ve 
found ones like: [https://agent.ai/](https://agent.ai/), [https://www.illa.ai/](https://www.illa.ai/), [https://aiagents
directory.com/](https://aiagentsdirectory.com/), [https://fetch.ai](https://fetch.ai/), obviously ChatGPTs store- howeve
r I think there’s potential for something a lot better

Imagine a platform where you could find the exact AI agent you’r
e looking for, whether it’s for customer support, data analysis, content creation, or something else. You’d be able to c
ompare options, pick the one that works best, and instantly get the API or integrate it into your workflow.

Plus for de
velopers: a place to showcase and monetize your AI agents by reaching a larger audience, with built-in tools to track pe
rformance and revenue.

I’m exploring the idea of building something like this and would love to hear your thoughts:

* 
Does this resonate with you?
* What kind of AI agents or must have features would you want in a platform like this?
* An
y pain points you’ve encountered when trying to find or use AI tools?
* Any other feedback or considerations?

Let me kn
ow what you think—I’m genuinely curious to get some feedback!


```
---

     
 
all -  [ Different Behavior between llm.bindTools and createToolCallingAgent ](https://www.reddit.com/r/LangChain/comments/1h8tyz8/different_behavior_between_llmbindtools_and/) , 2024-12-08-0915
```
Hi,  
I am seeing very different behavior between (yet again two ways to do things) llm.bindTools and createToolCallingA
gent with same model and tools. Specifically, when I ask a question of the model using llm.bindTools() and the question 
maps onto a tool, the tool is invoked. When the question does not map to a tool, the llm returns a response outside the 
tool. This is desirable behavior and its good.

With createToolCallingAgent, it bypasses the tool entirely. Using LangCh
ain's own calculator tool with createToolCallingAgent, the llm just answers the question itself and doesn't use my tool.


So I went back to llm.bindTools() and I really hope LangChain don't deprecate it because now I am locked into it. So f
rustrating.
```
---

     
 
all -  [ Connect local LLM (ollama) with vector DB (chromaDB) ](https://www.reddit.com/r/LangChain/comments/1h8odxl/connect_local_llm_ollama_with_vector_db_chromadb/) , 2024-12-08-0915
```
Has anyone here worked on using a local LLM with LangChain to read, understand, and output data from a vector database (
e.g., ChromaDB)?

Specifically, I want to ask the LLM questions, have it understand them, and extract relevant informati
on from the vector database to answer. The goal is for the answers to be based exclusively on the data in my ChromaDB, w
ithout the LLM relying on its own general knowledge.

I’ve tried using agents, but I haven’t been able to establish a pr
oper connection with ChromaDB. While the answers are often relevant, they seem to come from the LLM’s own knowledge rath
er than strictly using the data from the vector DB.

Does anyone have tips or experience with a similar setup?
```
---

     
 
all -  [ Enquiry on OpenAI embeddings issue ](https://www.reddit.com/r/LangChain/comments/1h8kumn/enquiry_on_openai_embeddings_issue/) , 2024-12-08-0915
```
Hi

Before yesterday, everything was working find with openai embeddings. But since yesterday, I got into this proxies i
ssue when I tried to use OpenAIEmbeddings in my RAG model using colab. Does anyone know how to solve it?

https://previe
w.redd.it/e1prmlb8wc5e1.png?width=1258&format=png&auto=webp&s=fb0096de64389a87df783386c7186bc04647f41f


```
---

     
 
all -  [ Comprehensive Analysis: AI, Agentic Agents, Workflows, Pipelines, Big Tech’s Strategy, and the Futur ](https://www.reddit.com/r/Asmongold/comments/1h8ji4s/comprehensive_analysis_ai_agentic_agents/) , 2024-12-08-0915
```
**By Steven Britt**  
*Founder, Khaotic Gaming LLC*  
[www.khaotic.ai](http://www.khaotic.ai)

# Abstract

Artificial In
telligence (AI) is evolving at an unprecedented pace, revolutionizing industries, reshaping narratives, and influencing 
societal norms. While AI offers immense potential for innovation and empowerment, its trajectory is largely dictated by 
Big Tech's calculated efforts to monopolize its development and deployment. This paper examines the evolution of AI, the
 rise of agentic agents, workflows, and pipelines, and how Big Tech exploits public participation to refine its systems.
 It further explores the societal impacts of these advancements, from the centralization of AI to the emergence of gener
ational AI and the risks of AI-driven societal divides. Finally, it outlines actionable steps to resist monopolization a
nd foster ethical AI development that prioritizes equity, transparency, and inclusivity.

# 1. Introduction

Artificial 
Intelligence is no longer confined to academia or niche industries—it has become a transformative force redefining how h
umanity interacts, learns, and innovates. Its rapid proliferation into every facet of life promises to unlock new possib
ilities but also raises significant ethical and societal concerns.

The democratization of AI tools and datasets by comp
anies such as Meta, Amazon, Nvidia, and Oracle may appear to be an act of altruism, empowering individuals and small bus
inesses to innovate. However, this seemingly generous act masks a deliberate strategy. By encouraging global creativity 
and leveraging public data, these companies accelerate the refinement of their AI systems at minimal cost to themselves.


While these corporations now offer “opt-out” versions for data use—appearing to align with ethical practices—this was 
not always the case. Historically, data collection occurred without clear user consent or understanding, leveraging user
 ignorance and the complexity of terms of service agreements. Even today, “opt-out” policies are often more symbolic tha
n substantive. Who is auditing these systems to ensure compliance? Are we relying on a “trust me, bro” promise from thes
e corporations that user data isn’t being leveraged?

Moreover, Big Tech circumvents direct responsibility through **thi
rd-party AI disruptions**. Here’s how:

* While corporations claim not to use your data directly, **their users do**. Th
ird-party developers, apps, and platforms often lack the same restrictions and transparency.
* These third-party systems
, which often rely on Big Tech’s APIs and infrastructure, indirectly improve the parent company’s AI models through refi
ned data and enhanced algorithms.
* **The Loophole**: By enabling an ecosystem of dependent users and platforms, Big Tec
h benefits from enriched datasets and algorithmic refinement, all while distancing themselves from direct data misuse cl
aims.

**The Implications Are Profound**:

1. **Jobs**: AI’s integration into industries will redefine the labor market,
 privileging those with access to advanced systems while marginalizing others.
2. **Relationships**: Generational AI sys
tems will influence personal and professional dynamics, creating divides based on access to superior AI tools.
3. **Soci
etal Norms**: AI will shape education, governance, and media, steering public discourse and redefining cultural values.


By the time society recognizes the extent of this manipulation, AI will have woven itself into the fabric of daily life
, dictating opportunities, choices, and even identities. The potential for generational divides looms large as access to
 advanced AI becomes a privilege rather than a universal right.

This paper serves as a call to action. It seeks to expo
se the strategies employed by Big Tech, highlight the societal risks of AI centralization, and propose actionable soluti
ons for fostering an AI landscape that empowers humanity rather than controls it.

# 2. Understanding AI

# 2.1 What is 
AI?

Artificial Intelligence (AI) refers to systems designed to simulate human intelligence by performing tasks such as 
learning, reasoning, decision-making, and adaptation. These systems are capable of operating at scales and speeds beyond
 human capacity, enabling groundbreaking applications across industries.

AI systems can be broadly categorized based on
 their design:

* Narrow AI: Specialized systems trained for specific tasks (e.g., image recognition or language transla
tion).
* General AI: Hypothetical systems capable of performing any intellectual task a human can do.
* Agentic AI: Emer
ging systems capable of real-time adaptability and autonomous decision-making, integrating various AI models and tools i
nto cohesive workflows.

Core Components of AI:

1. Datasets:
   * The foundation of AI lies in vast collections of stru
ctured (e.g., databases) and unstructured (e.g., text, images) data.
   * Ethical concerns arise with datasets sourced w
ithout explicit consent, such as scraping user data from social media or proprietary platforms.
   * Examples of misuse 
include biased datasets perpetuating stereotypes in facial recognition systems.
   * Future Consideration: Fair and repr
esentative datasets are critical to mitigating bias and ensuring inclusivity.
2. Algorithms:
   * Algorithms process dat
a, identify patterns, and generate outputs.
   * Key techniques include:
      * Supervised Learning: Models trained on 
labeled data (e.g., email spam detection).
      * Unsupervised Learning: Identifying patterns in unlabeled data (e.g., 
customer segmentation).
      * Reinforcement Learning: Systems learning through trial and error (e.g., AlphaGo).
      
* Deep Learning: Multi-layered neural networks capable of handling complex tasks (e.g., language processing in GPT model
s).

Architects of Large Language Models (LLMs) and Symbolic Learning Models (SLMs):

* LLMs:
   * Process natural langu
age by training on massive datasets of human communication.
   * Examples: OpenAI’s GPT-4, Google’s Bard, and Meta’s Lla
ma.
   * Strengths: Creativity, adaptability, and contextual understanding.
   * Challenges: Bias, resource consumption,
 and ethical implications.
* SLMs:
   * Handle structured, rule-based logic and symbolic reasoning.
   * Examples: Mathe
matical proofs, logic puzzles, and optimization problems.
   * Strengths: Precision and rule adherence.
   * Limitations
: Struggle in unstructured, ambiguous environments.

Integration of LLMs and SLMs:

* The future of AI lies in combining
 the flexibility of LLMs with the precision of SLMs. This hybrid approach enables systems to tackle complex, multi-dimen
sional problems, such as legal research where creativity and logical rigor are essential.

Emerging Trends:

* Multimoda
l AI: Integrating text, images, and audio for richer, more versatile interactions.
   * Example: ChatGPT combining text 
and image understanding for dynamic customer support.
* Neuro-Symbolic AI: Blending deep learning and symbolic reasoning
 to improve interpretability and precision.

# 2.2 Evolution of AI

AI has evolved significantly, marked by distinct pha
ses of development, each characterized by unique challenges and breakthroughs.

Phase 1: Symbolic AI (1950s-1980s)

* Fo
cus: Rule-based systems using explicit programming.
* Limitations: Unable to handle ambiguity or adapt to unforeseen sce
narios.
* Example: IBM’s Deep Blue, which used brute-force logic to defeat chess grandmasters.

Phase 2: Machine Learnin
g (1990s-2010s)

* Focus: Learning from data rather than explicit programming.
* Breakthroughs:
   * Neural networks ena
bling pattern recognition (e.g., image and speech processing).
   * Statistical methods like Support Vector Machines (SV
Ms) revolutionizing tasks like handwriting analysis.
* Example: Google’s PageRank, which transformed search engines by l
earning web page relevance.

Phase 3: Deep Learning Revolution (2010s-Present)

* Focus: Leveraging massive datasets and
 advanced neural architectures.
* Breakthroughs:
   * Convolutional Neural Networks (CNNs) in computer vision (e.g., fac
ial recognition).
   * Transformers (e.g., GPT-4) for natural language processing.
* Challenges: High computational cost
s, environmental impact, and inherent biases in data.

# 2.3 The Emergence of Agentic AI

Agentic AI represents the next
 evolutionary leap, integrating multiple models, tools, and data streams into adaptable, autonomous workflows.

Features
 of Agentic AI:

1. Real-Time Adaptability: Respond dynamically to changes in the environment or user input.
2. Integrat
ion Across Tools: Combine various models (e.g., LLMs, SLMs) for complex decision-making.
3. Autonomous Problem-Solving: 
Automate multi-step processes (e.g., supply chain optimization).

Potential and Risks:

* Potential:
   * Revolutionize 
industries by automating intricate workflows.
   * Examples: Adaptive learning platforms in education, autonomous NPC be
havior in gaming.
* Risks:
   * Dependency on centralized control.
   * Amplification of systemic biases if underlying d
ata is flawed.

# 3. Big Tech’s Strategy

Big Tech’s dominance in AI is not a byproduct of innovation alone but a calcul
ated, multi-phase strategy designed to exploit public creativity, consolidate control, and monopolize the future of AI. 
This section outlines the phases of their strategy, provides real-world examples, and explores the implications for inno
vators, businesses, and society.

# 3.1 Phase 1: Public AI Development

In the early stages, Big Tech positioned AI as a
 democratizing tool, offering free or open-access platforms, datasets, and APIs. While this move seemed altruistic, it s
erved as a global R&D lab for refining their systems.

Key Tactics:

1. Leveraging Public Creativity:
   * Open-source t
ools and platforms like TensorFlow and PyTorch encouraged widespread experimentation.
   * Developers, startups, and res
earchers contributed ideas and improvements, which Big Tech incorporated into their proprietary systems.
   * Example: O
penAI’s release of GPT-2 sparked a wave of innovation, with outputs from developers indirectly benefiting GPT-3 and GPT-
4.
2. Data Harvesting at Scale:
   * Platforms like Facebook and Google collected billions of interactions daily, using 
this data to train algorithms.
   * User-generated content on platforms like YouTube and GitHub further enriched dataset
s.
   * Example: GitHub Copilot trained on public repositories, raising ethical concerns about intellectual property.
3.
 Creating Public Trust:
   * By integrating AI into user-friendly applications, Big Tech normalized its use across indus
tries.
   * Example: AI-driven features in Gmail (e.g., predictive text) and Amazon’s Alexa built familiarity and relian
ce.

Ethical Concerns:

* Datasets were often sourced without explicit consent, raising issues of privacy and exploitati
on.
* Public contributions to open-source projects were rarely rewarded, while Big Tech profited immensely.

# 3.2 Phase
 2: Restricting Access

Once their systems were sufficiently refined, Big Tech began to limit access, citing concerns ab
out ethics, safety, and regulation. This phase strategically suppressed competition by creating barriers to entry.

Key 
Strategies:

1. Regulatory Lobbying:
   * Big Tech pushed for stringent AI regulations under the guise of promoting safe
ty and trustworthiness.
   * Smaller developers, unable to meet these compliance standards, were edged out.
   * Example
: Google and Microsoft lobbied for explainability requirements in AI systems, creating high-cost barriers for startups.

2. Proprietary Tools:
   * Open platforms were gradually restricted, forcing users to pay for access.
   * Example: Open
AI’s GPT-4, available only through API subscriptions, limits independent development.
3. Control Over Data:
   * Access 
to critical datasets became increasingly restricted.
   * Example: Reddit’s decision to charge for API usage made it har
der for developers to train models using its data.

Implications:

* Innovation outside Big Tech slowed as independent d
evelopers struggled to access resources.
* Ethical AI development became prohibitively expensive for smaller entities.


# 3.3 Phase 3: Embedding AI in Society

With control firmly established, Big Tech is embedding AI into essential societa
l systems, creating dependencies that ensure their dominance.

Key Tactics:

1. Integration into Infrastructure:
   * AI
 systems are becoming integral to healthcare, education, governance, and more.
   * Example: Microsoft’s Office 365 Copi
lot integrates AI into workplace productivity, making its tools indispensable.
2. Tiered Access:
   * Advanced AI system
s are marketed as premium solutions, accessible only to corporations or wealthy individuals.
   * Example: Nvidia’s ente
rprise-grade AI systems offer capabilities far beyond consumer-grade tools.
3. Suppressing Competition:
   * Big Tech em
ploys aggressive strategies to neutralize potential competitors.
   * Example: Amazon’s patent strategies and acquisitio
ns eliminate smaller companies attempting to innovate in AI.

Global Impact:

* In developing nations, Big Tech’s AI sys
tems dominate due to the lack of local alternatives or regulatory resistance.
* Governments increasingly rely on proprie
tary AI solutions, reducing sovereignty over critical infrastructure.

# 3.4 The Illusion of Democratization

Big Tech m
arkets its AI offerings as tools for empowerment, but the reality is far more exploitative.

1. Public Data as Fuel:
   
* Social media platforms collect and monetize user interactions without adequate compensation.
   * Example: TikTok uses
 uploaded content to train recommendation algorithms, with no benefit to creators.
2. Open Source as a Testing Ground:
 
  * Open-source initiatives indirectly benefit Big Tech, as they test and refine models that later become proprietary.
 
  * Example: Meta’s release of Llama spurred innovation, but advanced features were monetized in subsequent iterations.


# 3.5 Examples of Big Tech Exploitation

* Amazon Bedrock:
   * Markets multi-agent solutions that closely mirror open-
source innovations but are locked behind enterprise-grade licenses.
* Meta’s Llama:
   * Released as an open-source mode
l but restricted access to critical updates and features.
* Google and YouTube:
   * Every video uploaded is analyzed to
 refine ad-targeting algorithms, with no compensation to creators.

# 3.6 Implications for Innovators

Big Tech’s strate
gies stifle independent developers, startups, and smaller corporations, creating an uneven playing field.

1. Barriers t
o Entry:
   * High costs of compliance and restricted datasets discourage smaller players.
2. Erosion of IP Rights:
   *
 Public contributions are repackaged and patented by Big Tech.
3. Monopolistic Ecosystems:
   * AI’s integration into es
sential services makes it nearly impossible to exit Big Tech’s ecosystems.

# 3.7 Resistance and Solutions

1. Open-Sour
ce Movements:
   * Platforms like Hugging Face democratize access to AI tools.
2. Regulatory Advocacy:
   * Push for ant
itrust laws targeting monopolistic practices rather than stifling innovation.
3. Public Awareness:
   * Educate communit
ies about how Big Tech exploits user data and stifles competition.

# 4. AI Workflows and Pipelines

AI workflows and pi
pelines form the backbone of modern AI systems, dictating how raw data transforms into actionable insights. This section
 explores the structure and components of traditional pipelines, the evolution to agentic workflows, and their implicati
ons for innovation, efficiency, and control.

# 4.1 Traditional AI Pipelines

Traditional AI pipelines are structured, s
tep-by-step processes that outline how data flows through an AI system during training, testing, and deployment.

Key St
ages of a Traditional Pipeline:

1. Data Collection:
   * Raw data is sourced from platforms such as social media, IoT d
evices, and proprietary databases.
   * Challenges:
      * Privacy concerns, as datasets are often collected without ex
plicit user consent.
      * Bias in data sources, leading to skewed AI predictions.
   * Example: Healthcare datasets f
rom wearables (e.g., fitness trackers) are used to train predictive health models.
2. Data Cleaning and Preprocessing:
 
  * Ensures data quality by removing errors, duplicates, and irrelevant information.
   * Processes:
      * Normalizing
 formats (e.g., image resolution, text encoding).
      * Handling missing data or anomalies.
   * Importance: The accur
acy of an AI system depends heavily on the quality of preprocessing.
3. Model Training:
   * The cleaned dataset is fed 
into algorithms to identify patterns and make predictions.
   * Example: Chatbots trained on millions of conversations t
o understand language nuances.
4. Validation and Testing:
   * Evaluates the model against a separate dataset to assess 
accuracy, generalization, and robustness.
   * Metrics:
      * Accuracy: Percentage of correct predictions.
      * Rec
all: Ability to identify relevant results.
5. Deployment:
   * The model is integrated into real-world applications, suc
h as autonomous vehicles or recommendation engines.
6. Feedback Loops:
   * Real-world usage generates new data to refin
e the model.
   * Example: Netflix’s recommendation system improves as users interact with it.

Limitations of Tradition
al Pipelines:

* Rigidity: Fixed sequences make it difficult to adapt to dynamic conditions.
* Resource Intensity: Train
ing large models requires significant computational power.
* Data Dependence: Performance heavily relies on the availabi
lity of large, high-quality datasets.

# 4.2 Agentic Workflows

Agentic workflows represent a paradigm shift in AI devel
opment, enabling real-time adaptability and autonomous decision-making. These workflows integrate multiple AI models, to
ols, and data streams into a dynamic, feedback-driven system.

What Makes Agentic Workflows Different?

1. Dynamic Adapt
ability:
   * Unlike traditional pipelines, agentic workflows adjust strategies and processes based on real-time inputs.

   * Example: An autonomous delivery robot rerouting in response to traffic data.
2. Integration Across Tools:
   * Mul
tiple AI models collaborate seamlessly to achieve complex goals.
   * Example: A workflow in e-commerce where one agent 
analyzes user behavior, another predicts trends, and a third handles personalized marketing.
3. Continuous Learning:
   
* These workflows incorporate live feedback from user interactions or environmental changes to refine performance dynami
cally.
   * Example: AI-powered diagnostics systems updating treatment plans based on real-time patient data.

Component
s of Agentic Workflows:

1. Multi-Agent Systems:
   * Specialized AI agents work together to solve problems.
   * Exampl
e: A gaming environment where NPCs adapt their behavior to player strategies in real-time.
2. Orchestration Frameworks:

   * Tools like LangChain and LangFlow enable developers to build workflows integrating diverse models and processes.
  
 * Capabilities:
      * Task automation.
      * Seamless data sharing between agents.
3. Real-Time Feedback Loops:
   
* Unlike traditional systems that require manual retraining, agentic workflows learn continuously.

Advantages of Agenti
c Workflows:

* Flexibility: Adapt to changing conditions without retraining the entire system.
* Efficiency: Reduce red
undant processes by automating dynamic adjustments.
* Scalability: Integrate new agents or tools as requirements evolve.


Applications:

1. Healthcare: Real-time monitoring systems that adjust treatment plans dynamically.
2. Gaming: Adaptiv
e NPCs for more immersive player experiences.
3. Supply Chain Management: Dynamic optimization of logistics and inventor
y.

# 4.3 Implications for Innovation and Control

The rise of agentic workflows presents both opportunities and challen
ges.

Opportunities:

1. Accelerated Innovation:
   * Continuous learning enables faster iteration cycles.
   * Example:
 AI systems in renewable energy optimizing power distribution based on demand patterns.
2. Cross-Industry Applications:

   * Versatility allows workflows to transform industries such as education, transportation, and entertainment.

Challen
ges:

1. Monopolization Risks:
   * Big Tech’s dominance in orchestration tools (e.g., LangChain) can stifle competition
.
   * Example: Amazon Bedrock locking multi-agent workflows behind enterprise-grade licenses.
2. Data Privacy Concerns:

   * Continuous learning requires constant data input, raising ethical issues about how user data is collected and used
.
3. Ethical Dilemmas:
   * Autonomous decision-making systems may perpetuate biases or lead to unintended consequences.


# 4.4 Future Trends in AI Workflows

1. Hybrid Models:
   * Combining LLMs’ creativity with SLMs’ precision for enhanc
ed effectiveness.
   * Example: Legal research systems where LLMs draft summaries, and SLMs ensure logical consistency.

2. Decentralized Workflows:
   * Open-source platforms like Hugging Face are democratizing access, enabling smaller inno
vators to compete with Big Tech.
3. Customization:
   * AI workflows will become increasingly user-driven, allowing busi
nesses to tailor systems to specific needs without deep technical expertise.

# 5. AI as a Tool for Societal Control

AI
, when controlled by centralized entities, becomes a powerful mechanism for shaping human behavior, enforcing societal n
orms, and reinforcing existing power structures. This section delves into the ways AI can manipulate narratives, create 
dependencies, and embed itself into critical societal systems, exploring both the psychological and societal risks invol
ved.

# 5.1 Narrative Reinforcement

AI-driven platforms subtly shape public discourse by prioritizing specific content,
 often aligning with corporate or ideological goals.

Mechanisms of Narrative Control:

1. Content Recommendation Algori
thms:
   * AI systems, such as YouTube’s and TikTok’s recommendation engines, push content based on engagement metrics, 
often amplifying polarizing or sensational topics.
   * Example: During the 2016 U.S. elections, Facebook’s AI algorithm
s amplified divisive content, contributing to misinformation.
2. Media Bias Reinforcement:
   * AI-curated newsfeeds rei
nforce existing biases, reducing exposure to diverse perspectives.
   * Consequence: Echo chambers that diminish critica
l thinking and healthy debate.
3. Educational AI Systems:
   * AI-integrated curricula can selectively emphasize certain
 ideologies or omit controversial topics, shaping the worldview of future generations.
   * Example: Corporate-developed
 adaptive learning platforms prioritizing STEM over humanities without addressing the ethical implications of technology
.

# 5.2 Dependency and Psychological Shifts

As AI integrates into daily life, psychological dependence on these system
s grows, subtly altering societal norms and individual behaviors.

AI as the Arbiter of Value:

1. AI Scores and Ranking
s:
   * Generational AI systems may rank individuals based on their 'AI compatibility,' influencing decisions in:
      
* Employment: 'Your AI profile doesn’t meet the requirements.'
      * Education: 'Your child’s score is too low for adm
ission.'
      * Relationships: 'Compatibility with this AI-driven platform is suboptimal.'
   * Impact: A societal divi
de emerges, with those lacking access to advanced AI systems viewed as inherently less capable.
2. Shifting Norms:
   * 
Over-reliance on AI for decision-making diminishes human creativity and critical thinking.
   * Example: AI-generated we
llness apps reduce personal agency by automating health decisions.

# 5.3 Embedding AI in Key Systems

AI’s integration 
into critical societal systems entrenches its control over human behavior and infrastructure.

Industries Transformed by
 AI:

1. Healthcare:
   * AI systems dynamically analyze patient data to recommend treatments.
   * Risks:
      * Bias 
in datasets could lead to inequities in care.
      * Insurance companies may prioritize cost-saving recommendations ove
r optimal patient outcomes.
   * Generational Disparity:
      * Premium AI tools will provide advanced diagnostics only
 to those who can afford them.
2. Education:
   * AI-powered platforms personalize learning but risk restricting access 
to non-approved content.
   * Example: Proprietary platforms shaping curricula to align with corporate interests.
3. Gov
ernance:
   * Governments increasingly rely on AI for policy enforcement, surveillance, and decision-making.
   * Exampl
es:
      * Predictive policing algorithms, often perpetuating systemic biases.
      * Automated decision-making in soc
ial services, potentially marginalizing vulnerable populations.
4. Corporate Systems:
   * AI-driven workplace monitorin
g enforces productivity standards, creating high-pressure environments.
   * Example: Amazon’s warehouse management syst
em tracks worker efficiency in real-time.

# 5.4 Generational AI and the Class Divide

Generational AI systems will deep
en societal inequities, creating a divide based on access to advanced tools.

AI Hierarchy:

1. Generic AI:
   * Basic s
ystems available to the general public with limited functionality.
2. Generational AI:
   * Advanced systems evolving wi
th users over time, accessible only to affluent individuals or corporations.

Social Implications:

1. Workplace Dispari
ties:
   * Employees with generational AI systems will outperform others, leading to disproportionate career opportuniti
es.
2. Educational Inequalities:
   * Students with access to advanced AI tutors will achieve greater academic success, 
widening the achievement gap.
3. Healthcare Disparities:
   * Premium AI systems will deliver better diagnostics and tre
atments, exacerbating existing health inequities.

# 5.5 The Psychological and Societal Risks of AI Control

1. Erosion 
of Creativity:
   * Over-reliance on AI stifles innovation and problem-solving skills, as individuals default to automat
ed solutions.
2. Normalization of Inequality:
   * AI systems embed societal divides into algorithms, making inequities 
appear inevitable or natural.
3. Loss of Privacy:
   * Continuous data collection erodes personal privacy under the guis
e of 'enhanced user experiences.'

# 5.6 How Big Tech Enforces AI Dependency

1. Throttling Alternatives:
   * Public AI
 systems are intentionally limited, preventing independent innovation.
   * Example: Open-source tools are often less ca
pable than proprietary systems, forcing reliance on Big Tech.
2. Neutralizing Independent Efforts:
   * Big Tech employs
 AI scrapers to detect and suppress unauthorized systems.
   * Example: Identifying and neutralizing rogue AI bots opera
ting outside corporate ecosystems.

# 5.7 Solutions to Mitigate AI Control

1. Promote Decentralization:
   * Support de
centralized AI initiatives prioritizing user control.
   * Example: Federated AI systems that distribute control over da
ta and models.
2. Advocate for Transparency:
   * Push for regulations requiring companies to disclose algorithmic prior
ities and data processing methods.
3. Foster Public Awareness:
   * Educate society about the risks of over-reliance on 
AI and the importance of maintaining personal agency.
4. Empower Small Innovators:
   * Level the playing field by provi
ding funding, resources, and legal protections for independent developers.

# full report can be explored here [https://
chatgpt.com/g/g-6753c74473e48191a63213cb9d8ac392-ai-deep-dive-insights-and-impacts](https://chatgpt.com/g/g-6753c74473e4
8191a63213cb9d8ac392-ai-deep-dive-insights-and-impacts)
```
---

     
 
all -  [ How to Evaluate the Quality of Generated Embeddings? ](https://www.reddit.com/r/LangChain/comments/1h8fmvz/how_to_evaluate_the_quality_of_generated/) , 2024-12-08-0915
```
I have generated embeddings from the text chunks (extracted from a PDF and split using NLTKText Splitter).

I want to ev
aluate whether the generated embeddings are good.

How should I proceed?

  
Any Suggestions!

thank you so much in adva
nce 
```
---

     
 
all -  [ Which term do you like best do describe the new wave of agent apps we’ve been seeing (Cursor, v0, Re ](https://www.reddit.com/r/LangChain/comments/1h8czld/which_term_do_you_like_best_do_describe_the_new/) , 2024-12-08-0915
```
1. Agent apps
2. Agent-Native Applications
3. Agentic Apps
4. Agentic Copilots
5. Agent-Native Apps (ANA)

(vote on your
 favorite 
```
---

     
 
all -  [ Stuck at learning ](https://www.reddit.com/r/n8n/comments/1h8bsxb/stuck_at_learning/) , 2024-12-08-0915
```
I come from a generative AI background/web development (RAG, langchain, openAI, API's, postgreSQL). I am planning on bui
lding my own company based on automation services with computer vision and generative AI. For this plan, I have been ext
ensively exploring n8n for the past month. I think I grasp most of the basics (I have build some workflows for customer 
service (image requests, text generation, ).  what will be my next steps in my learning?. I feel like I hit a point wher
e youtube tutorials/books are not enough as they are pretty basic. Could you suggest me learning for mid/expert level? I
 feel like this has happened in the past, but since I never worked in a software development company, I have no idea how
 to go from basics to advance skills.
```
---

     
 
all -  [ Going from 25% success rate with Langchain's Graph RAG to 99.4% using BAML ](https://www.reddit.com/r/LangChain/comments/1h8adm9/going_from_25_success_rate_with_langchains_graph/) , 2024-12-08-0915
```
Disclaimer: I work on BAML - a prompting config language to get structured outputs ( [https://github.com/BoundaryML/baml
](https://github.com/BoundaryML/baml) ) 

One of our BAML users decided to test our framework against Langchain's GraphD
ocument solution to do RAG with graphs and got some crazy results I had to share.

https://preview.redd.it/bfw5zak8ba5e1
.png?width=1330&format=png&auto=webp&s=233f4ea00e7b784aba684fc416a9717bb0a69364

Here is their blog post: [https://mediu
m.com/@khaledarindam/implementing-graphrag-with-smaller-open-source-llms-a-practical-guide-501a62864c15](https://medium.
com/@khaledarindam/implementing-graphrag-with-smaller-open-source-llms-a-practical-guide-501a62864c15) 

Here is the lan
gchain implementation: [https://github.com/arindamkhaled/OpenGraphRAG/blob/main/opengraphrag/graph\_rag\_small\_llms\_wo
\_baml.ipynb](https://github.com/arindamkhaled/OpenGraphRAG/blob/main/opengraphrag/graph_rag_small_llms_wo_baml.ipynb) 


We've had some similar feedback that examples that don't work with Langchain tend to work with BAML, since it uses a si
mplified schema format to do structured outputs with LLMs ( [https://www.boundaryml.com/blog/sota-function-calling?q=0](
https://www.boundaryml.com/blog/sota-function-calling?q=0) ). Happy to answer any questions. 
```
---

     
 
all -  [ What are the best techniques and tools to have the model 'self-correct?' ](https://www.reddit.com/r/OpenAI/comments/1h85b1b/what_are_the_best_techniques_and_tools_to_have/) , 2024-12-08-0915
```
# CONTEXT
I'm a noob building an app that analyses financial transactions to find out what was the max/min/avg balance e
very month/year. Because my users have accounts in multiple countries/languages that aren't covered by Plaid, I can't re
ly on Plaid -- I have to analyze account statement PDFs.

Extracting financial transactions like ||||||| 2021-04-28 | 45
2.10 | credit ||||||| _almost_ works. The model will hallucinate most times and create some transactions that don't exis
t. It's always just one or two transactions where it fails.

I've now read about Prompt Chaining, and thought it might b
e a good idea to have the model check its own output. Perhaps say 'given this list of transactions, can you check they'r
e all present in this account statement' or even way more granular do it for every single transaction for getting it 100
% right 'is this one transaction present in this page of the account statement', _transaction by transaction_, and have 
it correct itself.

# QUESTIONS:
1) is using the model to self-correct a good idea?

2) how could this be achieved? 

3)
 should I use the regular api for chaining outputs, or langchain or something? I still don't understand the benefits of 
these tools

# More context:
- I started trying this by using Docling to OCR the PDF, then feeding the markdown to the L
LM (both in its entirety and in hierarchical chunks). It wasn't accurate, it wouldn't extract transactions alright
- I t
hen moved on to Llama vision, which seems to be yielding much better results in terms of extracting transactions. but st
ill makes some mistakes
- My next step before doing what I've described above is to improve my prompt and play around wi
th temperature and top_p, etc, which I have not played with so far!
```
---

     
 
all -  [ Is Langsmith just good piece of trash? ](https://www.reddit.com/r/LangChain/comments/1h84qim/is_langsmith_just_good_piece_of_trash/) , 2024-12-08-0915
```
I use langsmith for tracing and prompt management. Initially it was good. But then they started to tweak UI every two da
ys and nowadays I just feel the website has become pathetic and unresponsive at some times. I don’t know am I the only o
ne experiencing it but its very frustrating as a developer. Had high hopes from langchain but got disappointed…

Any goo
d open source langsmith alternatives??
```
---

     
 
all -  [ MD Doesn’t Seem Like the Right Intermediate Language To Connect LLMs to Webpages ](https://www.reddit.com/r/LangChain/comments/1h82xst/md_doesnt_seem_like_the_right_intermediate/) , 2024-12-08-0915
```
It seems like right now the canonical pipeline to feed webpages into LLMs is: HTML —> MD —> LLM

It makes sense that MD 
is a lot more usable than HTML since it strips away the bloat, but having tried numerous HTML —> MD tools, the reality i
s that a lot of the context of the webpage is lost in this translation. For example, if you put in a site like EBay or Z
illow into these tools, the output makes it nearly impossible to understand what’s going on, much less navigate the page
 with the LLM from this information.

What do people see as the future for what intermediate languages will appear to co
nnect web pages to LLMs?

I’ve always wondered if some sort of language that captures where elements were positional and
 what types of elements there were would be neat, but even this seems to have flaws of its own. The proliferation of so 
much JS helps very little here as well.
```
---

     
 
all -  [ Improve a RAG system that uses 200+ PDFs ](https://www.reddit.com/r/LangChain/comments/1h82gox/improve_a_rag_system_that_uses_200_pdfs/) , 2024-12-08-0915
```
Hello everyone, I am writing here to ask for some suggestions. I am building a RAG system in order to interrogate a chat
bot and get the info that are present in documentation manuals.

**Data Source:**

I have 200+ pdfs and every pdf can re
ach even 800/1000 page each.

**My current solution:**

**DATA INGESTION:**

I am currently using Azure DocumentIntellig
ence to extract the information and metadata from the pdfs. After that I start creating chunks by creating a chunk for e
very paragraph identified by Azure DocumentIntelligence. To this chunk I also attach the PageHeading and the previous im
mediate title found.

After splitting all in chunks I do embed them using 'text-embedding-ada-002' model of OpenAI.

Aft
er that I load all these chunks on Microsoft Azure index search service.

**FRONTEND and QA**

Now, using streamlit I bu
ilt a easy chat-bot interface.

Every time I user sends a query, I do embed the query, and then I use Vectorsearch to fi
nd the top 5 'similar' chunks (Azure library).

RERANKING:

After identified the top 5 similar chunks using vector searc
h I do send chunk by chunk in combination with the query and I ask OpenAI GPT-3.5 to score from 50 to 100 how relevant i
s the retrieved chunk based on the user query. I keep only the chunks that have a score higher than 70.

After this I wi
ll remain with around 3 chunks that I will send in again as a knowledge context from where the GPT model have to answer 
the intial query.

The results are not really good, some prompts are correctly answered but some are totally not, it see
ms the system is getting lost and I am wondering if is because I have many pdfs and every pdf have many many pages.

Any
one had a similar situation/use case? Any suggestion you can give me to help me improve this system?

Thanks!
```
---

     
 
all -  [ LangGraph based literature review agent - hackathon winner project ](https://open.substack.com/pub/diamantai/p/nexus-ai-the-revolutionary-research?r=336pe4&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true) , 2024-12-08-0915
```
Happy to share the first blog post about an incredible agent developed during the hackathon (by the 1st place winners) I
 organized with LangChain.

This agent, powered by LangGraph, slashes literature review times in research from 40% to ju
st 10%—outperforming previous state-of-the-art models with only a slight tradeoff in processing time (a matter of second
s).

Code is fully available on the GenAI_Agents open-source repository and there is a link to it in the blog.
```
---

     
 
all -  [ AI Agent creation with Dynamic Node pathways  ](https://www.reddit.com/r/LangChain/comments/1h7zhno/ai_agent_creation_with_dynamic_node_pathways/) , 2024-12-08-0915
```
For most of the AI Agents, like CrewAI or Autogen or even Langgraph, what I found that we can only give the goal and the
n define which agent does what. In Langgraph, we have to define the entire workflow in advance from what I get from the 
docs. 

But I wanted to check if a problem of code debugging, which might involve multiple steps and multiple different 
pathways, is there a way to handle the management of creating all these possible paths & having the Agent walk through e
ach of them one by one? The key difference between the nodes of the graph are created after execution of some initial no
des. 

Or should I manage this outside the Agentic Framework with a custom setup and DB etc.
```
---

     
 
all -  [ Ollama, langchain ans local database ? ](https://www.reddit.com/r/ollama/comments/1h7z7ev/ollama_langchain_ans_local_database/) , 2024-12-08-0915
```
Hi all,
I've been using ollama since a few months now and wanted to go a bit further in my application.

To be honest, I
 am a bit confused about how to use it. 
 
Here is what I had in mind : use langchain to understand the user input and c
onvert it to either a SQL or NoSQL query (I work on SQL and Mongodb) and then format the results into a small sentence. 
But I don't know if langchain is here to build literaly the query based on a context? Or just to ordinates the command (
understand the user input, find the correct table/collection, adjust parameters based on the fields of the collection/ta
ble, retrieve results...).

Can someone help me with that? Maybe it is not even the correct tool that I'm using!

Thanks
!
```
---

     
 
all -  [ Best way to store image and text embeddings in vector store?
 ](https://www.reddit.com/r/LangChain/comments/1h7ycvq/best_way_to_store_image_and_text_embeddings_in/) , 2024-12-08-0915
```
Working on a RAG based PDF Query system for documents with complex layout.

I was not sure weather to store the embeddin
gs of text and images into unified vector store or create different vector store for each.

What would be the better app
roach for this?
```
---

     
 
all -  [ Help needed to clean data ](https://www.reddit.com/r/LangChain/comments/1h7urvf/help_needed_to_clean_data/) , 2024-12-08-0915
```
I have a bunch of tweets and now I want to filter the tweets in some categories. How can I automate this process. 

```
---

     
 
all -  [ How to improve RAG results for searching a set of game rules ](https://www.reddit.com/r/LangChain/comments/1h7r7eb/how_to_improve_rag_results_for_searching_a_set_of/) , 2024-12-08-0915
```
I am trying to develop an Magic: The Gathering RAG AI system for answering rules questions and am having difficulty.

Fo
r example, the user asks a question like, 'what happens when all of these creatures die at the same time'

I use an agen
t to break down user question and use similarity search on my vector db. it searches by the entire question and by broke
n down terms like 'simultaneous death' 'creature dies' etc. per reddit QA board, a redditor answered the question the qu
estion referencing this rule:

603.10a Some zone-change triggers look back in time. These are leaves-the-battlefield abi
lities, abilities that trigger when a card leaves a graveyard, and abilities that trigger when an object that all player
s can see is put into a hand or library.

Clearly this is the key insight, but I'm not sure how it could have gotten to 
this key rule. To make things worse, I got a hallucination: - \`Simultaneous Deaths (Rule 704.3): When multiple creature
s die at the same time, they see each other die, which can trigger abilities that trigger on creatures dying.\`

Any ide
as? I am using chroma db, using custom text splitters to chunk by rule with small overlap. Do you think graph db could b
e more useful for a set of rules?
```
---

     
 
all -  [ TIL: LangChain has init_chat_model('model_name') helper with LiteLLM-alike notation... ](https://www.reddit.com/r/LangChain/comments/1h7qwbz/til_langchain_has_init_chat_modelmodel_name/) , 2024-12-08-0915
```
Hi! For those who, like me, have been living under a rock these past few months and spent time developing numerous JSON-
based LLMClient, YAML-based LLMFactory's, and other solutions just to have LiteLLM-style initialization/model notation -
 I've got news for you! Since v.0.3.5, LangChain has moved their [init\_chat\_model](https://python.langchain.com/docs/h
ow_to/chat_models_universal_init/) helper out of beta.



    from langchain.chat_models import init_chat_model
    
   
 # Simple provider-specific initialization
    openai_model = init_chat_model('gpt-4', model_provider='openai', temperat
ure=0)
    claude_model = init_chat_model('claude-3-opus-20240229', model_provider='anthropic')
    gemini_model = init_
chat_model('gemini-1.5-pro', model_provider='google_vertexai')
    
    # Runtime-configurable model
    configurable_mo
del = init_chat_model(temperature=0)
    response = configurable_model.invoke('prompt', config={'configurable': {'model'
: 'gpt-4'}})



[Supported providers](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_mo
dels.base.init_chat_model.html): openai, anthropic, azure\_openai, google\_vertexai, google\_genai, bedrock, bedrock\_co
nverse, cohere, fireworks, together, mistralai, huggingface, groq, ollama.



Quite more convenient helper:

    from la
ngchain.chat_models import init_chat_model
    from typing import Optional
    
    def init_llm(model_path: str, temp: 
Optional[float] = 0):
        '''Initialize LLM using provider/model notation'''
        provider, *model_parts = model_
path.split('/')
        model_name = model_path if not model_parts else '/'.join(model_parts)
        
        if provid
er == 'mistral':
            provider = 'mistralai'
        
        return init_chat_model(
            model_name,
   
         model_provider=provider,
            temperature=temp
        )



Finally.

    mistral = init_llm('mistral/mi
stral-large-latest')
    anthropic = init_llm('anthropic/claude-3-opus-20240229')
    openai = init_llm('openai/gpt-4-tu
rbo-preview', temp=0.7)



Hope this helps someone avoid reinventing the wheel like I did!  

```
---

     
 
all -  [ LLM Access in AI Agents: Can Tools Tap Directly into Language Models? ](https://www.reddit.com/r/LangChain/comments/1h7o9fw/llm_access_in_ai_agents_can_tools_tap_directly/) , 2024-12-08-0915
```
In an AI agent architecture, can individual tools within the agent have direct access to a Large Language Model (LLM), o
r is LLM access restricted solely to the main agent?
```
---

     
 
all -  [ Best RAG framework for code (or general purpose) ](https://www.reddit.com/r/Rag/comments/1h7h7hm/best_rag_framework_for_code_or_general_purpose/) , 2024-12-08-0915
```
I've been working on building RAGs for codebases. I've been using langchain until now and while it is not terrible, it's
 also not my favorite, due to how misorganized it seems. The RAG works for now, but I'll probably build a lot of stuff o
n top of it, including custom re-rankers and retrievers. Before doing that, I'm  considering refactoring what I have int
o another framework, the more flexible the better.  
What would you guys suggest?

Edit: Forgot to add, but since this i
s a personal project, I was looking into frameworks that are free
```
---

     
 
all -  [ Adding authentication scheme to a langgraph api (self hosted)  ](https://www.reddit.com/r/LangChain/comments/1h7d7k8/adding_authentication_scheme_to_a_langgraph_api/) , 2024-12-08-0915
```
Hi, I was unable to find any documentation on how to add an authentication engine to a self hosted langgraph server inst
ance. Is there some documenation available? I was only able to access this :- [https://github.com/langchain-ai/langgraph
/discussions/2440](https://github.com/langchain-ai/langgraph/discussions/2440)
```
---

     
 
all -  [ Does LangGraph work in the browser??? ](https://www.reddit.com/r/LangChain/comments/1h7cu92/does_langgraph_work_in_the_browser/) , 2024-12-08-0915
```
`[ERROR] No matching export in 'browser-external:node:async_hooks' for import 'AsyncLocalStorage'`

`node_modules/@langc
hain/langgraph/dist/setup/async_local_storage.js:2:9:`

`2 │ import { AsyncLocalStorage } from 'node:async_hooks';`
```
---

     
 
all -  [ Struggling with LangGraph Academy's Lesson on Chains - Need Help! ](https://www.reddit.com/r/LangChain/comments/1h7cszn/struggling_with_langgraph_academys_lesson_on/) , 2024-12-08-0915
```
Hey everyone,

I’ve been following the LangGraph Academy course, and I just started the first module. Up until the chain
s lesson, we’ve been talking about nodes, edges, and graphs, which I was able to grasp pretty well. But now, they’ve sud
denly shifted gears and started talking about **tools, messages, chat models**, and I’m feeling a bit lost.

I kind of u
nderstand the gist of what’s going on, but I have somequestions:

1. **Why are we suddenly discussing messages and chat 
models out of nowhere?** I think I get that the messages are meant to represent some kind of state, right? Like we’re ke
eping track of all the messages in some way. But how is that supposed to scale? If there are thousands of steps, wouldn’
t the state end up bloated with a ton of unnecessary data? Isn’t that inefficient? Or is there some mechanism that prune
s or handles this?
2. **What exactly is a 'tool' in this context?** Is a tool just a component of a node? Or is it somet
hing separate that a node leads to, like its own independent entity? I’m having trouble visualizing how tools fit into t
he graph conceptually.

Sorry if this doesn’t make a ton of sense—I’m pretty confused myself, so I might not be articula
ting this perfectly. 😅 If anyone has gone through this course or has a clearer understanding of these concepts, I’d real
ly appreciate some guidance. Thanks in advance! 🙏
```
---

     
 
all -  [ I am building a fitness application  ](https://www.reddit.com/r/LangChain/comments/1h79rkw/i_am_building_a_fitness_application/) , 2024-12-08-0915
```
The app collects user inputs such as weight, height, goals, and more. It then generates a detailed workout and diet plan
. An agent will manage user data and query a CSV file containing various exercises and the muscles they target and how t
o do it and more.

So any tips or improvements  
Thanks in advance 
```
---

     
 
all -  [ QandA With Complex PDF. ](https://www.reddit.com/r/LangChain/comments/1h79pli/qanda_with_complex_pdf/) , 2024-12-08-0915
```
Hi folks - I just want know that whicb PDF parser will be a good choice to extract the data.

PDF can have complex data 
like text inside image inside image, data in tables, graphs,diagrams etc.
```
---

     
 
all -  [ How do I make my PDF RAG app smarter for question answering with tables in it?
 ](https://www.reddit.com/r/Rag/comments/1h77tq4/how_do_i_make_my_pdf_rag_app_smarter_for_question/) , 2024-12-08-0915
```
Hi all,  
I'm developing a PDF RAG app . My app is built using LCEL chain.

I'm currently using pymupdf4llm as the pdf p
arser ( to convert pdfs to their md format ), OpenAIEmbedding text-3-large as the embedding model, Cohere as the reranke
r and OpenAI ( gpt-4o-mini as the LLM ) .

My pdfs are really complex pdfs (containing texts , images , charts , tables.
.. a lot of them ).

The app can currently answer any question based on pdf text easily, but struggles with tables, spec
ially tables that are linked/related ( where answer can only be given by looking and reasoning at multiple tables ).

I 
want to make my PDF RAG app smarter. By smarter, I mean being able to answer questions which a human can find by looking
 and then reasoning after seeing multiple tables in the pdf.

What can I do ?

\[NOTE : I've asked this question on Lang
chain subreddit too but since my app is a RAG app and I need answers that's why posting here too\]
```
---

     
 
all -  [ How do I make my PDF RAG app smarter for question answering with tables in it? ](https://www.reddit.com/r/LangChain/comments/1h77shd/how_do_i_make_my_pdf_rag_app_smarter_for_question/) , 2024-12-08-0915
```
Hi all,  
I'm developing a PDF RAG app . My app is built using LCEL chain.

I'm currently using pymupdf4llm as the pdf p
arser ( to convert pdfs to their md format ), OpenAIEmbedding text-3-large as the embedding model, Cohere as the reranke
r and OpenAI ( gpt-4o-mini as the LLM ) .

My pdfs are really complex pdfs (containing texts , images , charts , tables.
.. a lot of them ).

The app can currently answer any question based on pdf text easily, but struggles with tables, spec
ially tables that are linked/related ( where answer can only be given by looking and reasoning at multiple tables ).

I 
want to make my PDF RAG app smarter. By smarter, I mean being able to answer questions which a human can find by looking
 and then reasoning after seeing multiple tables in the pdf.

What can I do ?
```
---

     
 
all -  [ QandA With complex PDF in by using langchain ](https://www.reddit.com/r/LangChain/comments/1h76mdi/qanda_with_complex_pdf_in_by_using_langchain/) , 2024-12-08-0915
```
HI folks, I am working on a app in which user can upload a PDF and then start QandA with that PDF. PDF has very complex 
data like text inside image inside image, data in tabular / graph / diagrams format.

So which PDF parser will be best f
or this functionality
```
---

     
 
all -  [ Grouping runs with langsmith ](https://www.reddit.com/r/LangChain/comments/1h76bdq/grouping_runs_with_langsmith/) , 2024-12-08-0915
```
I am using langchain and langsmith, and want to group multiple llm calls in a single trace, and maybe add a tag to them.
  
I would want it to be somthing like:  
with group\_in\_single\_trace(tags=\['test'\]):  
   lllm call 1  
   llm call
 2

But cant find anything in the docs that does this. This seems like a very basic need, am i missing somthing?
```
---

     
 
all -  [ [For Hire] AI/ML Engineer & Full Stack Developer – Chatbots & Scalable Web Solutions ](https://www.reddit.com/r/forhire/comments/1h74krg/for_hire_aiml_engineer_full_stack_developer/) , 2024-12-08-0915
```
**Need an AI/Full Stack Developer? Let’s Connect!** 👋

Hi, I’m Sheryar, an experienced AI/ML Engineer and Full Stack Dev
eloper.

💻 **Full Stack Solutions**  
🔹 Sleek UIs: React/Angular  
🔹 Robust APIs: Node.js, NestJS  
🔹 Payment Systems: S
tripe Integration  
🔹 Cloud Hosting: AWS

🤖 **AI & Automation Expertise**  
🔹 Smart Chatbots: Powered by LangChain  
🔹 C
ustom AI Models: NLP, automation, and more

🌟 **Recent Projects**  
🚗 Ride-Sharing: Live tracking + payments  
📦 Logisti
cs: Multi-stop delivery  
🤖 AI Bots: Smart ordering & customer support

💰 **Rate:** $20–$25/hour (negotiable)  
📧 DM to 
discuss your project!   
GitHub: [storm1033](https://github.com/storm1033)

Let’s build something innovative and scalabl
e! 🌐✨
```
---

     
 
all -  [ Langchain pipeline making the application slow. ](https://www.reddit.com/r/LangChain/comments/1h743id/langchain_pipeline_making_the_application_slow/) , 2024-12-08-0915
```
I am using groq as my LLM for my chatbot,
I saw on Langsmith that my RAG pipeline is taking more time then the LLM.

How
 can I improve the speed of my application?
```
---

     
 
all -  [ How would you handle headers and footers of a page in RAG model? ](https://www.reddit.com/r/LangChain/comments/1h73laf/how_would_you_handle_headers_and_footers_of_a/) , 2024-12-08-0915
```
Hi. I made RAG system but sometimes search results are just footers and headers from multiple different pages. I have se
veral clients, so I need a general solution, not for a specific web site. This should be a very typical and classic prob
lem of a search engine. Does anyone know a well-known and easy-to-implement solution? Thanks.
```
---

     
 
all -  [ Methods for File Reranking and Selection ](https://www.reddit.com/r/Rag/comments/1h72or5/methods_for_file_reranking_and_selection/) , 2024-12-08-0915
```
There is BM25 in literature which is a library named as rank-bm25 on github. Langchain uses that bm25 library.
But it is
 not efficient, accuracy level is not satisfactory. So I was looking for different methods like TF-IDF vectorizer. Or ev
en easier, just use the embedding models results to rerank the document base as a last resort for high accuracy scores. 
And it worked pretty well.
There is still one point left, if knowledge base is large and it is not logical to do vector 
search in all of it, this is slow. So I am also looking for something different that can be used before indexing and vec
tor search.
Is there any other method? I want to share our insights.
```
---

     
 
all -  [ Submit Feedback Node (Getting runId from RunnableConfig inside a node) ](https://www.reddit.com/r/LangChain/comments/1h6zxwq/submit_feedback_node_getting_runid_from/) , 2024-12-08-0915
```
I have raised a question on the repo: [discussion](https://github.com/langchain-ai/langgraphjs/discussions/655) and in r
/LangGraph: [post](https://www.reddit.com/r/LangGraph/comments/1giuqw7/submit_feedback_node_getting_runid_from/)

In sum
mary, I want to programmatically, create a feedback on a LangSmith trace either through a tool or node. I figured the ri
ght place for it is a node since you can pass the Runnable Config and theoretically get the \`runId\` from it to be used
 in the \`langsmithClient.createFeedback\` function. I have attempted a few different ways to retrieve the runId and als
o manually setting it in the configurable object, but none seem to work. Has anyone been able to successfully do this wi
thin a graph node? (note my application is in ts. and I am using the langraph.js SDK)

I am not sure if this a bug from 
the LangGraph side or there is different way to be doing this. 
```
---

     
 
all -  [ Ever Changing LangChain APIs ](https://www.reddit.com/r/LangChain/comments/1h6zvr9/ever_changing_langchain_apis/) , 2024-12-08-0915
```
I started to learn LangChain and found some tutorial materials are dated. Apparently, LangChain has gone through revisio
ns of APIs. My question is how I can find out the new APIs when I run into deprecated APIs. For example, I ran into the 
following API call:

    chain = MultiPromptChain.from_prompts(llm, prompt_infos, verbose=True)
    

Then I saw the fol
lowing warning messages:

LangChainDeprecationWarning: Please see the migration guide at: 

[https://python.langchain.co
m/docs/versions/migrating\_memory/](https://python.langchain.com/docs/versions/migrating_memory/)  
  validated\_self = 
self.\_\_pydantic\_validator\_\_.validate\_python(data, self\_instance=self)

How do I find out the right updated API to
 use? Thanks.
```
---

     
 
all -  [ Best way to chunk html ](https://www.reddit.com/r/LangChain/comments/1h6z6bd/best_way_to_chunk_html/) , 2024-12-08-0915
```
I have htmls that I need to chunk in order to pass it to a LLM. It's not going to be used for rag, so I would like chunk
s with around 2-5k tokens each. 

Inside this htmls, I have long tables with thousands of lines. U guys have any suggest
ions on how to chunk this?

I was thinking on creating a chunking strategy with gpt4o, but would appreciate if there are
 ready to go repos or services on this!

Example of html i need to chunk (its a brazilian law text) [https://legislacao.
fazenda.sp.gov.br/Paginas/Portaria-SRE-77-de-2024.aspx](https://legislacao.fazenda.sp.gov.br/Paginas/Portaria-SRE-77-de-
2024.aspx)
```
---

     
 
all -  [ What is the best alternative to LangChain/LangGraph for experimentation and production ](https://www.reddit.com/r/LangChain/comments/1h6tvme/what_is_the_best_alternative_to/) , 2024-12-08-0915
```
There’s lots of dissatisfaction from the community about LangChain. Before I begin building my first MVP, I'd like to ge
t recommendations for alternative frameworks. I'm looking for options that would work well for both MVP development and 
production deployment. If there's a single framework suitable for both stages, that would be ideal, but I'm also open to
 hearing about different frameworks optimized for each phase. What would you recommend?
```
---

     
 
MachineLearning -  [ [P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, ](https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/) , 2024-12-08-0915
```
  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to intro
duce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premi
ses, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an a
bility to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon
 you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, yo
u can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container runni
ng on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an abilit
y to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you 
can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and ma
c os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problem
s:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the m
odels (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork
, star) Thank you so much!
```
---

     
