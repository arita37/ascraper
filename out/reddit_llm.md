 
all -  [ Opinions on my new front-end? ](https://www.reddit.com/gallery/1h4gpo5) , 2024-12-02-0914
```

```
---

     
 
all -  [ What is the process of extracting keywords from multiple pdfs. ](https://www.reddit.com/r/Langchaindev/comments/1h4dyoi/what_is_the_process_of_extracting_keywords_from/) , 2024-12-02-0914
```
I am trying to implement a feature that can extract all the topics and its subtopics from pdfs or docs uploaded by the u
ser. The issue is i can't figure out how do I do a vector search on the pdfs vector storage. I want this kind of structu
re attached in the image. I get it i can structurer the data using LLM, but how do I get all the topics from the pdfs up
loaded. Either I can extract keywords from each chunk by giving it to llm but that will use soo manny tokens. I am new t
o langchain as well. Also show up a screenshot or something how do you guys setup your agents in js.

https://preview.re
dd.it/pn7apmyp0b4e1.png?width=1316&format=png&auto=webp&s=51cf4a5e0c82603f21f81258ae8284b0db1c5363


```
---

     
 
all -  [ [For Hire] Frontend and Backend Developer with the top and latest development technologies for a gre ](https://www.reddit.com/r/freelance_forhire/comments/1h4c8dm/for_hire_frontend_and_backend_developer_with_the/) , 2024-12-02-0914
```
Hi Redditors,

I'm Emad a Full-stack Web developer/engineer with 8 years of experience, and I'm looking for new projects
 to start working on, please take a look at my portfolio here [https://www.sx-portfolio.com](https://www.sx-portfolio.co
m) (let me know if you need more examples or direct links to live websites in the PM)

**Here is my skill set:**

**For 
Frontend:**

* HTML / CSS
* JS / ESNext
* Webpack / Gulp / Grunt / Gatsby
* React (With Redux, Zustand, or MobX)
* Next.
js
* Vuejs (With VueX)
* Angular
* TypeScript

**For Backend:**

* Node.js
* Python
* Express.js
* MongoDB
* Mongoose
* 
Nest.js
* GraphQL
* Meteor (with Apollo and React)
* TypeScript

**For AI-based projects:**

* Python
* Langchain
* Fast
API
* Uvicorn
* Pydantic

**For Desktop Apps:**

* Electron

**For Mobile Apps (iOS and Android):**

* React Native
* Ex
po

**For Design:**

* Photoshop
* Illustrator

**Here is my Resume:**  [My resume link!](http://www.sx-portfolio.com/we
bsite-resources/My%20resume.pdf)

**And here are some testimonials from my clients :)**

* [Slips](https://www.reddit.co
m/r/testimonials/comments/agfmwf/pos_swordx10_is_a_brilliant_developer_and_a/)
* [VidSync](https://www.reddit.com/r/test
imonials/comments/5gvz3d/pos_uswordx10_excellent_frontend_dev/)
* [Las Cruces Directory](https://www.reddit.com/r/testim
onials/comments/5hyks4/uswordx10_is_awesome/)
* [SigurenZalog](https://www.reddit.com/r/testimonials/comments/5jsfqf/pos
_quality_web_design_from_uswordx10/)
* [Bitlounge](https://www.reddit.com/r/testimonials/comments/5lh2pz/pos_uswordx10_t
op_fontend_devdesigner/)
* [Foxul](https://www.reddit.com/r/testimonials/comments/5l3p8j/pos_quality_web_coding_from_usw
ordx10/)
* [LootTicket](https://www.reddit.com/r/testimonials/comments/5nfh1j/pos_uswordx10_is_an_amazing_front_end_dev/
)

**My Pricing:**

I work hourly for $35/hr and my fixed prices depend on your project's complexity.

Don't worry about
 the price, just PM me with your project and I'm sure we can figure out something that goes with your budget. :)

If you
 have any questions don't hesitate to PM me and I will be more than happy to answer you :) and here is my portfolio agai
n if you need my contact details [www.sx-portfolio.com](https://www.sx-portfolio.com) (Click the red handle in the top-r
ight or pm me for it)
```
---

     
 
all -  [ Best approach for automating WhatsApp communication between field teams and management. ](https://www.reddit.com/r/LangChain/comments/1h4atay/best_approach_for_automating_whatsapp/) , 2024-12-02-0914
```

Looking for advice on automating our WhatsApp communication:

Current setup:
- Field team reports hourly data in Group 
A
- Staff reviews data
- Staff forwards to Group B (management)

Need to:
- Automate this while maintaining data review 
capability
- Store structured data from WhatsApp responses for reporting
- Generate automated reports from collected dat
a

Considering WhatsApp Business API with chatbot or third-party solutions.

Anyone implemented similar automation? Look
ing for platform recommendations and rough cost estimates.

Thanks!
```
---

     
 
all -  [ [For Hire] React, NEXT, Nest, Express, Langchain and Full Stack Developer. ](https://www.reddit.com/r/forhire/comments/1h4ai5f/for_hire_react_next_nest_express_langchain_and/) , 2024-12-02-0914
```
Hi Reddit! 👋  
I'm Sheryar, a Full Stack Developer skilled in **React, Next.js, NestJS, Node.js, AWS**, and **LangChain*
*. I specialize in:

✅ **Frontend**: Responsive, pixel-perfect UIs with React/Angular  
✅ **Backend**: Scalable APIs & m
icroservices (Node.js, NestJS)  
✅ **Databases**: Advanced PostgreSQL with JSON handling  
✅ **Payments**: Stripe integr
ation for secure transactions  
✅ **Cloud**: AWS deployments  
✅ **Chatbots & Voicebots**: Development with LangChain fo
r intelligent automation

**Recent Work**:  
🚗 Ride-sharing app with Stripe payments & live tracking  
📦 Urban logistics
 platform with multi-stop deliveries  
📊 Custom CRM with Twilio API integration  
🤖 Chatbot & Voicebot solutions for aut
omation and customer support

💰 **Rat**e: $15–$20/hour (negotiable)  
📧 DM me to discuss your project or view my portfol
io!  
**GitHub**: [storm1033](https://github.com/storm1033)

Let’s build something amazing together! 🚀
```
---

     
 
all -  [ Create your own basic RAG ](https://www.reddit.com/r/ArtificialInteligence/comments/1h48jdg/create_your_own_basic_rag/) , 2024-12-02-0914
```
[https://danielkliewer.com/2024/12/01/basic-rag](https://danielkliewer.com/2024/12/01/basic-rag)

In this guide I go thr
ough how to set up basic Retrieval Augmented Generation.

You can adapt it to your own projects.

I am really excited ab
out learning this. I knew what it was in concept but now I can use it for my own projects.

To save you a click the main
 juxt of the guide is the following code:

    import os
    import sys
    import glob
    from dotenv import load_dote
nv
    
    # Load environment variables from .env file
    load_dotenv()
    
    # Updated imports
    from langchain_
openai.embeddings import OpenAIEmbeddings
    from langchain_chroma.vectorstores import Chroma
    from langchain_openai
.llms import OpenAI
    from langchain.chains import RetrievalQA
    
    # Updated document loaders
    from langchain_
community.document_loaders import TextLoader, PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterText
Splitter
    
    def main():
       # Load OpenAI API key
       openai_api_key = os.getenv('OPENAI_API_KEY')
       if
 not openai_api_key:
           print('Please set your OPENAI_API_KEY in the .env file.')
           sys.exit(1)
      

       # Define the folder path (change 'data' to your folder name)
       folder_path = './data'
       if not os.path.
exists(folder_path):
           print(f'Folder '{folder_path}' does not exist.')
           sys.exit(1)
      
       # 
Read all files in the folder
       documents = []
       for filepath in glob.glob(os.path.join(folder_path, '**/*.*'),
 recursive=True):
           if os.path.isfile(filepath):
               ext = os.path.splitext(filepath)[1].lower()
   
            try:
                   if ext == '.txt':
                       loader = TextLoader(filepath, encoding='utf
-8')
                       documents.extend(loader.load_and_split())
                   elif ext == '.pdf':
           
            loader = PyPDFLoader(filepath)
                       documents.extend(loader.load_and_split())
            
       else:
                       print(f'Unsupported file format: {filepath}')
               except Exception as e:

                   print(f'Error reading '{filepath}': {e}')
      
       if not documents:
           print('No docume
nts found in the folder.')
           sys.exit(1)
      
       # Split documents into chunks
       text_splitter = Rec
ursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
       texts = text_splitter.split_documents(documents)

      
       # Initialize embeddings and vector store
       embeddings = OpenAIEmbeddings()
       vector_store = Chro
ma(embedding_function=embeddings, persist_directory='./chroma_store')
      
       # Add texts to vector store in batch
es
       batch_size = 500  # Adjust this number as needed
       for i in range(0, len(texts), batch_size):
           
batch_texts = texts[i:i+batch_size]
           vector_store.add_documents(batch_texts)
      
       # Set up retriever

       retriever = vector_store.as_retriever(search_kwargs={'k': 3})
      
       # Set up the language model
       ll
m = OpenAI(temperature=0.7)
      
       # Create the RetrievalQA chain
       qa_chain = RetrievalQA.from_chain_type(

           llm=llm,
           chain_type='stuff',  # Options: 'stuff', 'map_reduce', 'refine', 'map_rerank'
           
retriever=retriever
       )
      
       # Interactive prompt for user queries
       print('The system is ready. You 
can now ask questions about the content.')
       while True:
           query = input('Enter your question (or type 'ex
it' to quit): ')
           if query.lower() in ('exit', 'quit'):
               break
           try:
               re
sponse = qa_chain.run(query)
               print(f'\nAnswer: {response}\n')
           except Exception as e:
         
      print(f'An error occurred: {e}\n')
              
    if __name__ == '__main__':
       main()

I hope you find th
is helpful. I am excited to explore the possibilities so if you have already used a RAG for your use case I would love t
o here what you use it for.

I think I will use it with the new AI agents set up I created which I described in a previo
us post.

I keep my blog free from monetization of any kind. I am just sharing this because I am excited about learning.
 Hope you enjoy.
```
---

     
 
all -  [ How I Built a Multi-Modal Search Pipeline with Voyager-3 ](https://www.reddit.com/r/LangChain/comments/1h47x6j/how_i_built_a_multimodal_search_pipeline_with/) , 2024-12-02-0914
```
Hey all,

I recently dove deep into multi-modal embeddings and built a pipeline that combines text and image data into a
 unified vector space. It’s a pretty cool way to connect and retrieve content across multiple modalities, so I thought I
’d share my experience and steps in case anyone’s interested in exploring something similar.



Here’s a breakdown of wh
at I did:

**Why Multi-Modal Embeddings?**

The main idea is to embed text and images into the same vector space, allowi
ng for seamless searches across modalities. For example, if you search for “cat,” the pipeline can retrieve related imag
es of cats *and* the text describing them—even if the text doesn’t explicitly mention the word “cat.”



**The Tools I U
sed**

1. **Voyager-3**: A state-of-the-art multi-modal embedding model.

2. **Weaviate**: A vector database for storing
 and querying embeddings.

3. **Unstructured**: A Python library for extracting content (text and images) from PDFs and 
other documents.

4. **LangGraph**: For building an end-to-end retrieval pipeline.



**How It Works**

1. **Extracting 
Text and Images**:

Using Unstructured, I pulled text and images from a sample PDF, chunked the content by title, and gr
ouped it into meaningful sections.

2. **Creating Multi-Modal Embeddings**:

I used Voyager-3 to embed both text and ima
ges into a shared vector space. This ensures the embeddings are contextually linked, even if the connection isn’t explic
itly clear in the data.

3. **Storing in Weaviate**:

The embeddings, along with metadata, were stored in Weaviate, whic
h makes querying incredibly efficient.

4. **Querying the Data**:

To test it out, I queried something like, “What does 
this magazine say about waterfalls?” The pipeline retrieved both text and images relevant to waterfalls—even if the text
 didn’t mention “waterfall” directly but was associated with a photo of one.

5. **End-to-End Pipeline**:

Finally, I bu
ilt a retrieval pipeline using LangGraph, where users can ask questions, and the pipeline retrieves and combines relevan
t text and images to answer.



**Why This Is Exciting**

This kind of multi-modal search pipeline has so many practical
 applications:

• Retrieving information from documents, books, or magazines that mix text and images.

• Making sense o
f visually rich content like brochures or presentations.

• Cross-modal retrieval—searching for text with images and vic
e versa.

I detailed the entire process in a [blog post here](https://medium.com/vectrix-ai/how-to-build-a-powerful-mult
i-modal-search-pipeline-with-voyager-3-6024ff98d9ca), where I also shared some code snippets and examples.

If you’re in
terested in trying this out, I’ve also uploaded the code to [GitHub](https://github.com/vectrix-ai/vectrix-graphs/blob/m
ain/examples/multi-model-embeddings.ipynb). Would love to hear your thoughts, ideas, or similar projects you’ve worked o
n!

Happy to answer any questions or go into more detail if you’re curious. 😊
```
---

     
 
all -  [ Virtual try on API  ](https://www.reddit.com/r/LangChain/comments/1h4119c/virtual_try_on_api/) , 2024-12-02-0914
```
Hey  im trying to build a platform for virtual try on, does anyone know a free api which I can use for building this 
```
---

     
 
all -  [ Need Opinions on a Unique PII and CCI Redaction Use Case with LLMs ](https://www.reddit.com/r/LangChain/comments/1h40ifi/need_opinions_on_a_unique_pii_and_cci_redaction/) , 2024-12-02-0914
```
I’m working on a **unique** Personally identifiable information **(PII) redaction use case**, and I’d love to hear your 
thoughts on it. Here’s the situation:

Imagine you have PDF documents of HR letters, official emails, and documents of t
hese sorts. Unlike typical PII redaction tasks, **we don’t want to redact information identifying the data subject.** Fo
r context, a 'data subject' refers to the individual whose data is being processed (e.g., the main requestor, or the per
son who the document is addressing). Instead, we aim to redact **information identifying other specific individuals (not
 the data subject)** in documents.

Additionally, we don’t want to redact **organization-related information**—just the 
personal details of individuals other than the data subject. Later on, we’ll expand the redaction scope to include **Com
mercially Confidential Information (CCI)**, which adds another layer of complexity.

**Example:** in an HR Letter, the d
ata subject might be 'John Smith,' whose employment details are being confirmed. Information about John (e.g., name, pos
ition, start date) would not be redacted. However, details about 'Sarah Johnson,' the HR manager, who is mentioned in th
e letter, should be redacted if they identify her personally (e.g., her name, her email address). Meanwhile, the company
's email (e.g., [hr@xyzCorporation.com](mailto:hr@xyzCorporation.com)) would be kept since it's organizational, not pers
onal.

# Why an LLM Seems Useful?

I think an LLM could play a key role in:

1. **Identifying the Data Subject**: The LL
M could help analyze the document context and pinpoint who the data subject is. This would allow us to create a clear li
st of **what to redact and what to exclude**.
2. **Detecting CCI**: Since CCI often requires understanding nuanced busin
ess context, an LLM would likely outperform traditional keyword-based or rule-based methods.

# The Proposed Solution:


* Start by using an LLM to **identify the data subject** and generate a list of entities to redact or exclude.
* Then, u
se **Presidio** (or a similar tool) for the actual redaction, ensuring scalability and control over the redaction proces
s.

# My Questions:

1. **Do you think this approach makes sense?**
2. **Would you suggest a different way to tackle thi
s problem?**
3. How well do you think an LLM will handle CCI redaction, given its need for contextual understanding?

I’
m trying to balance accuracy with efficiency and avoid overcomplicating things unnecessarily. Any advice, alternative to
ols, or insights would be greatly appreciated!

Thanks in advance!
```
---

     
 
all -  [ Please suggest resources for learning GenAI, Langchain, for other AI skills for web development ](https://www.reddit.com/r/developersIndia/comments/1h3z077/please_suggest_resources_for_learning_genai/) , 2024-12-02-0914
```
I am a MERN Stack Developer having good amount of experience of developing full stack application. I want to learn GenAI
 skills which I can integrate along with my applications. Can you please suggest me some good resources for learning Gen
AI, Langchain, RAG, etc ?
```
---

     
 
all -  [ Just Built an Agentic RAG Chatbot From Scratch—No Libraries, Just Code! ](https://www.reddit.com/r/LangChain/comments/1h3y86k/just_built_an_agentic_rag_chatbot_from_scratchno/) , 2024-12-02-0914
```
Hey everyone!

I’ve been working on building an Agentic RAG chatbot completely from scratch—no libraries, no frameworks,
 just clean, simple code. It’s pure HTML, CSS, and JavaScript on the frontend with FastAPI on the backend. Handles embed
dings, cosine similarity, and reasoning all directly in the codebase.

I wanted to share it in case anyone’s curious or 
thinking about implementing something similar. It’s lightweight, transparent, and a great way to learn the inner working
s of RAG systems.

If you find it helpful, giving it a ⭐ on GitHub would mean a lot to me: \[Agentic RAG Chat\](https://
github.com/AndrewNgo-ini/agentic\_rag). Thanks, and I’d love to hear your feedback! 😊
```
---

     
 
all -  [ Issues when prompting for credentials using Chainlit UI + Langgraph ](https://www.reddit.com/r/LangChain/comments/1h3u7ei/issues_when_prompting_for_credentials_using/) , 2024-12-02-0914
```
Hello,

I’m building a basic ReAct (Reasoning and Acting) AI agent using Langgraph and Chainlit. The LLM has access to a
 tool that requires the user to provide a username and a password before it can execute any actions.

The workflow is as
 follows:

1. User input  
2. Tool call  
3. Request for credentials  
4. Tool execution  
5. End

I am experiencing iss
ues with properly asking the user for their credentials. I am using Chainlit version 1.3.0.

Do you have any examples I 
can refer to?
```
---

     
 
all -  [ Is there any free embeddings model API? ](https://www.reddit.com/r/LangChain/comments/1h3irug/is_there_any_free_embeddings_model_api/) , 2024-12-02-0914
```
I am searching for an free embeddings model with API, not self hosted ones. I am building a personal project on Android 
application that does RAG. Now the catch is, Android studio doesn't support pytorch version >1.4. Though there are free 
versions that have very limited tokens, that isn't enough for me.
```
---

     
 
all -  [ Frontend for solopreneur project ](https://www.reddit.com/r/LangChain/comments/1h3i3u6/frontend_for_solopreneur_project/) , 2024-12-02-0914
```
Hi there :)   
I'm running a quick Agents-RAG prototype with n8n (on top of langchain) and Streamlit on GC for the front
 end.

Now I'm taking a look at some Streamlit alternatives. I was taking a look at openWebUI but I have no time to lear
n that stack. So I'm wondering if I should consider G Mesop or even Django.

I'm out of cognitive energy to learn much m
ore and would love to keep it simple. SO my questions are:  
\- Do you think it makes sense to move from Streamlit to Me
sop?   
\- What about the learning curve for Django?  
\- For simple GUI customizations (navigation, popups, etc), Does 
it make any sens to work on openwebui?

Don't even know if any of my questions makes any sense.... just need some input-
feedback-guidance
```
---

     
 
all -  [ Why is using a small model considered ineffective? I want to build a system that answers users' ques ](https://www.reddit.com/r/LangChain/comments/1h3esmm/why_is_using_a_small_model_considered_ineffective/) , 2024-12-02-0914
```
Why didn’t I train a small model on this data (questions and answers) and then using RAG to improve the accuracy of answ
ering the questions?

The advantages of a small model are that I can guarantee the confidentiality of the information, w
ithout sending it to an American company. It's fast and doesn’t require high infrastructure.

Why does a model with 67 m
illion parameters end up taking more than 20 MB when uploaded to Hugging Face?

However, most people criticize small mod
els. Some studies and trends from large companies are focused on creating small models specialized in specific tasks (ag
ent models), and some research papers suggest that this is the future!
```
---

     
 
all -  [ Output format adjustments  ](https://www.reddit.com/gallery/1h3c23c) , 2024-12-02-0914
```
I’m currently working on an app that helps visualise problem breakdowns in mind maps. As you can see I have problem gett
ing the text from the agents back in a way that’s nice to visualise, anyone got tricks ?
```
---

     
 
all -  [ Choosing an AI model for My knowledge management app  ](https://www.reddit.com/r/LangChain/comments/1h3b0kt/choosing_an_ai_model_for_my_knowledge_management/) , 2024-12-02-0914
```
Hi , I'm working on My internship project  that's a knowledge Management  system using fastapi  and I have to make a cha
tbot that generate answers based on the documents inserted in the database I used langchian and an open source model to 
generate the embeddings using  the pgvector extension in postgreSQL, the problem still in generating the answers from th
eses embeddeds I want a performing  free AI model and in the same time I can't install it locally . what you suggest ???
 
```
---

     
 
all -  [ (Resume review) Roast my Resume, 2024 grad. Career advice needed.  ](https://i.redd.it/67pjrd0jwz3e1.jpeg) , 2024-12-02-0914
```
Current role: Junior DevOps Engineer (started recently) 
Target role: Backend Developer

Basically I feel underpaid and 
undervalued in current job and want to switch. 

```
---

     
 
all -  [ How To Pre Process Documents? ](https://www.reddit.com/r/ArtificialInteligence/comments/1h34byp/how_to_pre_process_documents/) , 2024-12-02-0914
```
Still fumbling my way through the 'how to' for creating an enterprise AI app. I'm finding that multiple apps are needed 
for best case scenarios. Right now I have:

0. ??? - Pre process documents
1. Pinecone
2. Cleanlab (optional)
3. An LLM 
(nvm which one)

While I understand you can upload PDFs and chunk them, with varying results, it clearly makes more sens
e to pre process/chunk the documents in some form if you can then upload those. If so who is doing that?

Is this LangCh
ain or a competitor?

Edit: I should mention referring to text data like legal filings. So a 1000 pg list of different l
aws for instance, not numerical data.
```
---

     
 
all -  [ Delete checkpoint from redis in Langgraph? ](https://www.reddit.com/r/LangChain/comments/1h32oqn/delete_checkpoint_from_redis_in_langgraph/) , 2024-12-02-0914
```
Hiii!
Does anyone know how to delete a checkpoint (the whole conversation of a thread id) in redis?

Thanks in advance:)

```
---

     
 
all -  [ GPT-4o-Realtime-Preview Azure Support? ](https://www.reddit.com/r/LangChain/comments/1h2vy5y/gpt4orealtimepreview_azure_support/) , 2024-12-02-0914
```
Is there a way to support the only audio model in Azure like the one on OpenAI?

```typescript
import { AzureChatOpenAI 
} from '@langchain/openai';
const llm: any = new AzureChatOpenAI({
    modelName: 'gpt-4o-realtime-preview',
    deploym
entName: config.azureOpenAIApiDeploymentName,
    openAIApiVersion: '2024-10-01',
    azureOpenAIApiInstanceName: config
.azureOpenAIInstanceName,
    maxTokens: config.maxToken,
    temperature: config.temperature,
    audio: {'voice': 'all
oy', 'format': 'wav'},
    modalities: ['text', 'audio'],
});
```

Similar configuration as seen on the direct OpenAI au
dio model: https://www.datacamp.com/tutorial/gpt-4o-audio-preview

But when using gpt-4o-realtime-preview that is the on
ly audio model on Azure. This error rises:
```
Result: Failure Exception: 404 Resource not found Troubleshooting URL: ht
tps://js.langchain.com/docs/troubleshooting/errors/MODEL_NOT_FOUND
```
```
---

     
 
all -  [ Interface for my chatbot ](https://www.reddit.com/r/LangChain/comments/1h2ut1g/interface_for_my_chatbot/) , 2024-12-02-0914
```
Hi all,

I'm a mechanical engineer and I'm developing a chatbot to pitch it to my current company. I've build it using L
angGraph. I know it may be not optimised 100% but I'm happy with the answers that it is giving me. To call the graph all
 I use is :   
  
\# Specify a thread  
config = {'configurable': {'thread\_id': '1'}}  
  
\# Run  
messages = graph.in
voke({'user\_question': 'Question here...'},config)  
messages\['messages'\]\[-1\].pretty\_print()

This will generate a
n AIMessage answer.

Is there a quick way to create an interface for presentation purposes ? Instead of compiling a Jupi
ter Notebook I want to be able to ask questions from an interface.  


Appreciate any help !
```
---

     
 
all -  [ Create Own DataSet form PDF's ](https://www.reddit.com/r/LangChain/comments/1h2sh7h/create_own_dataset_form_pdfs/) , 2024-12-02-0914
```
What is the best way to create the largest number of questions and answers from PDF?  
Another way other than extracting
 questions manually using ChatGPT
```
---

     
 
all -  [ Claude 3.5 Sonnet V2 + LangChain on AWS ](https://www.reddit.com/r/Anthropic/comments/1h2pysr/claude_35_sonnet_v2_langchain_on_aws/) , 2024-12-02-0914
```
Hi,

Has anyone experience with using Claude 3.5 Sonnet V2 (2024-10-22) together with LangChain (Agent with tool functio
ns) on AWS?

We have a system prompt defined as an XML document where we explain that it should use the defined tools wh
en necessary, and we pass in the tools like <tools\_definition>{…JSON schema of tools…}</tools\_definition>.

On us-west
-2 (where the model can be accessed directly through the foundation-model id “anthropic.claude-3-5-sonnet-20241022-v2”),
 the model works correctly. It calls the tools when necessary, and responds in a human language.

On us-east-1 (where th
e model can be accessed only through the inference-profile id “us.anthropic.claude-3-5-sonnet-20241022-v2”), the model’s
 response includes an XML-formatted version of the used tool’s schema, but it doesn’t actually call the tool. I also tri
ed it on us-west-2 with the inference-profile id, and it acts the same.

Is there anything I’m unaware of? Is maybe the 
V2 model accessed through the foundation-model id somewhat different than the one accessed through the inference-model i
d?
```
---

     
 
all -  [ LangChain tools ](https://www.reddit.com/r/modelcontextprotocol/comments/1h2nwyx/langchain_tools/) , 2024-12-02-0914
```
I added support for MCP tools to LangChain toolkit https://github.com/rectalogic/langchain-mcp
```
---

     
 
all -  [ Langchain’s state of AI agents report is mostly bullshit. ](https://www.reddit.com/r/DebunkingAI/comments/1h2mx32/langchains_state_of_ai_agents_report_is_mostly/) , 2024-12-02-0914
```
I took time to read Langchain’s state of AI agents report, so you don’t have to. Don’t worry, you are not missing anythi
ng. It’s mostly a marketing pamphlet masqueraded as a rigorous study.

I knew it was bullshit from the get-go as soon as
 I read their definition of AI agents.  
[https://medium.com/thoughts-on-machine-learning/langchains-state-of-ai-agents-
report-is-mostly-bullshit-c689d0021a19](https://medium.com/thoughts-on-machine-learning/langchains-state-of-ai-agents-re
port-is-mostly-bullshit-c689d0021a19)
```
---

     
 
all -  [ [For Hire] React, NEXT, Nest, Express, Langchain and Full Stack Developer. ](https://www.reddit.com/r/forhire/comments/1h2ijf3/for_hire_react_next_nest_express_langchain_and/) , 2024-12-02-0914
```
Hi Reddit! 👋  
I'm Sheryar, a Full Stack Developer skilled in **React, Next.js, NestJS, Node.js, AWS**, and **LangChain*
*. I specialize in:

✅ **Frontend**: Responsive, pixel-perfect UIs with React/Angular  
✅ **Backend**: Scalable APIs & m
icroservices (Node.js, NestJS)  
✅ **Databases**: Advanced PostgreSQL with JSON handling  
✅ **Payments**: Stripe integr
ation for secure transactions  
✅ **Cloud**: AWS deployments  
✅ **Chatbots & Voicebots**: Development with LangChain fo
r intelligent automation

**Recent Work**:  
🚗 Ride-sharing app with Stripe payments & live tracking  
📦 Urban logistics
 platform with multi-stop deliveries  
📊 Custom CRM with Twilio API integration  
🤖 Chatbot & Voicebot solutions for aut
omation and customer support

💰 **Rate**: $15–$20/hour (negotiable)  
📧 DM me to discuss your project or view my portfol
io!  
**GitHub**: [storm1033](https://github.com/storm1033)

Let’s build something amazing together! 🚀
```
---

     
 
all -  [ Pause a Langraph at an Intermediate Node and Retrieve the Current State Result ](https://www.reddit.com/r/LangChain/comments/1h2g2n9/pause_a_langraph_at_an_intermediate_node_and/) , 2024-12-02-0914
```
I have implemented a customer engagement system with a complex workflow, including tasks like scheduling calls, placing 
orders, and replying to emails. On the client side, we have three APIs: `/domain/place_order/`, `/domain/schedule_call/`
, etc.

The requirement is to execute a workflow corresponding to a specific use case, such as a subgraph for placing an
 order. During execution, the intermediate state should be stored, and the current state result should be returned.

If 
the API is triggered again with knowledge of the previous state, the system should resume from where it left off and com
plete the corresponding subgraph workflow (e.g., the call scheduling subgraph).

How can this be achieved?
```
---

     
 
all -  [ What's the pros and cons compared langchain tools vs MCP (Model Context Protocol) ](https://www.reddit.com/r/LangChain/comments/1h2csxn/whats_the_pros_and_cons_compared_langchain_tools/) , 2024-12-02-0914
```
I just had a chance to use MCP, made by claude. Seems like it's very similar to langchain tools, but don't know the main
 difference. What's it about, and how can it be different from langchain tools?
```
---

     
 
all -  [ Relevance of Message Queues in AI Agents ](https://www.reddit.com/r/LangChain/comments/1h25wdn/relevance_of_message_queues_in_ai_agents/) , 2024-12-02-0914
```
Hi everyone,

I’ve been working with message queue (MQ) software and middleware tools. I’ve been wondering how an AI mig
ht intersect with or enhance the realm of message queuing systems.



Are there applications I’m missing or any existing
 work connecting AI and message queuing systems?

How might the intersection of these two fields shape the future of mid
dleware and distributed systems?

Looking forward to hearing your insights and discussing this further!
```
---

     
 
all -  [ Anyone else interested in storing outputs as well as prompts? And if so ... what solutions are out t ](https://www.reddit.com/r/PromptEngineering/comments/1h22jdi/anyone_else_interested_in_storing_outputs_as_well/) , 2024-12-02-0914
```
Hi everyone,

I think it's my first time posting on this sub which is weird as I've been working on prompting stuff for 
quite some time now. So nice to discover that this exists!

I became very interested in LLMs and prompt engineering earl
ier this year. As the self-hosting kind of type, as much as I was instantly impressed by the advances in the GPT models 
since I last checked them out, my thoughts were also drawn to *'great ... but if I can get something useful out of this 
(the LLM) where does that data \*go\*?'* I've played around with building my own prototypes for running and then storing
 prompts. But ultimately, I'd much rather used better more polished tech that somebody else has made. I'm just having a 
hard time finding it!

To be a bit more specific:

I've worked on a prompt for discovering and parsing some corporate su
stainability data. After quite a number of iterations, it works nicely. I like the idea of using something like a prompt
 engineering IDE to iterate on the prompt further, but I also want to collect the outputs as they're being generated! I 
can do this (say) by creating a script that uses LangChain and routing the outputs to a folder within a Github repo. But
 I'd like something that's a bit easier to replicate, hands-off and (ideally also) cloud-based.

My ideal tool: great en
vironment for prompt engineering *and* really solid functionality for managing where the outputs get stored. Ideally: ch
oose your backend (say a MongoDB server) and the tool will route the outputs there (with or without the prompts). Or as 
a second best, here are some good features for sifting through them and pulling them out. Either way: give me some optio
ns for what to do with the stuff that gets generated beyond just batching it up into one huge JSON that's not really all
 that scalable.

Beyond just collecting the information your prompt was designed to generate, other useful things you ca
n do with previous outputs include passing them as context for other LLMs (ie, chained prompting across models).

As muc
h as some of the prompt eng tools are delightful, I feel like there's something of a gaping blind spot in terms of what 
to do with the information generated by our diligent work in crafting prompts. Which seems a little self-defeating and s
trange.

Does something exist that does what I'm after? And how are people approaching managing outputs in general?
```
---

     
 
all -  [ Discussion: 'Why Does the Recursion Limit Exist in LangGraph?' ](https://www.reddit.com/r/LangChain/comments/1h226yc/discussion_why_does_the_recursion_limit_exist_in/) , 2024-12-02-0914
```
Currently, in my team, we are developing agents using LangGraph. Some of these are complex agents that we dynamically co
mpile, with some cases involving N branches.

My question is: Why does the recursion limit exist? Is it primarily a perf
ormance-based limitation, or is it more about preventing issues like infinite loops in agent execution, such as in the c
ase of a ReAct agent
```
---

     
 
all -  [ Faster LLM response ](https://www.reddit.com/r/LangChain/comments/1h222g7/faster_llm_response/) , 2024-12-02-0914
```
Hello everyone

In my RAG agent, I'm making 3 requests to the LLM, the first is for determining whether to call the tool
 or not, the second is to check set a boolean in the response (JSON), the third is to provide a final answer.

In each i
nvocation to the agent, 2 network requests are made. The prompts are a little bit long, tried to make them shorter but g
ot the same response time about 13 seconds.

using gpt-40-mini, tried gpt 3.5 turbo as well.

all prompts return the fol
lowing JSON:

    {
       'message': '<Your natural language response to the user - exclude technical IDs>',
       'co
ntact_id': '<contact_id of the contractor or null>',  # Always use the actual contractor ID from metadata
       'id': <
id from metadata>,
       'should_navigate': <false>
    }
```
---

     
 
all -  [ MCP Server Tools Langgraph Integration example ](https://www.reddit.com/r/LangChain/comments/1h20lxe/mcp_server_tools_langgraph_integration_example/) , 2024-12-02-0914
```
Example of how to auto discover tools on an MCP Server and make them available to call in your Langgraph graph.

[https:
//github.com/paulrobello/mcp\_langgraph\_tools](https://github.com/paulrobello/mcp_langgraph_tools)
```
---

     
 
all -  [ WARNING:langsmith.client:Failed to multipart ingest runs ](https://www.reddit.com/r/LangChain/comments/1h1yu32/warninglangsmithclientfailed_to_multipart_ingest/) , 2024-12-02-0914
```
Hi guys, 

  
just testing LangChain, once I want to set up tracking of the project in LangSmith I got the following err
or: 

    WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication f
ailed for WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication f
ailed for . HTTPError('401 Client Error: Unauthorized for url: ', '{'detail':'Invalid token'}')trace=b91f591b-3a81-4d7d-
b45b-aa712a577433,id=0b099474-e808-412d-8ed6-e778a05597e0; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=9adae83d-b1e1-4
628-9e8d-6ceccef2ed40; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=ac3358b4-ea21-4a87-9757-88669e094a09; trace=b91f591
b-3a81-4d7d-b45b-aa712a577433,id=b91f591b-3a81-4d7d-b45b-aa712a577433; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=926
e7252-0018-415a-b1d5-f39830f202fd; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=32733c7b-cc61-4dce-b6bf-f91c7025e98d
  
  . HTTPError('401 Client Error: Unauthorized for url: ', '{'detail':'Invalid token'}')trace=b91f591b-3a81-4d7d-b45b-aa7
12a577433,id=0b099474-e808-412d-8ed6-e778a05597e0; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=9adae83d-b1e1-4628-9e8d
-6ceccef2ed40; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=ac3358b4-ea21-4a87-9757-88669e094a09; trace=b91f591b-3a81-4
d7d-b45b-aa712a577433,id=b91f591b-3a81-4d7d-b45b-aa712a577433; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=926e7252-00
18-415a-b1d5-f39830f202fd; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=32733c7b-cc61-4dce-b6bf-f91c7025e98d
    https:
//api.smith.langchain.com/runs/multiparthttps://api.smith.langchain.com/runs/multiparthttps://api.smith.langchain.com/ru
ns/multiparthttps://api.smith.langchain.com/runs/multipart

Any idea how to get it working? 

Thanks for any help

Here 
is the script: 

    # Adding Document Loader
    from langchain.chains.combine_documents import create_stuff_documents_
chain
    from langchain_community.document_loaders import WebBaseLoader
    from langchain_text_splitters import Recurs
iveCharacterTextSplitter
    from langchain_openai import AzureOpenAIEmbeddings
    from langchain_community.vectorstore
s.faiss import FAISS
    from langchain.chains import create_retrieval_chain
    from langchain.callbacks import tracing
_v2_enabled
    
    with tracing_v2_enabled() as session:
        assert session
        
        
        def get_docu
ment_from_web(url):
          loader = WebBaseLoader(url)
          docs = loader.load()
          splitter = RecursiveC
haracterTextSplitter(
              chunk_size=200,
              chunk_overlap=20
          )
          splitDocs = spl
itter.split_documents(docs)
          print(len(splitDocs))
          return splitDocs
    
        def create_db(docs):
 
          embedding = AzureOpenAIEmbeddings(
            model='text-embedding-3-small',
            azure_endpoint='x
xxx',
            api_key = 'xxx',
            openai_api_version = '2024-10-01-preview'
        )
          vector_stor
e = FAISS.from_documents(docs, embedding=embedding)
          return vector_store
    
    
        def create_chain(vec
tore_store):
    
          prompt = ChatPromptTemplate.from_template('''
    
          Answer the user question:
     
     Context: {context}
          Question: {input}
          ''')
    
          #chain = prompt | model_2
    
       
   chain = create_stuff_documents_chain(llm= model_2,
                                              prompt = prompt)
   
       
          retrieve = vectore_store.as_retriever(search_kwargs = {'k':12})
          retrieve_chain = create_retr
ieval_chain(
              retrieve,
              chain
            )
    
    
    
          return retrieve_chain
  
  
        docs = get_document_from_web('https://www.abz.com/en/articles/top-10')
        vector_store = create_db(docs)

        chain = create_chain(vector_store)
        response = chain.invoke({
            'input' : 'What....',
        
        })
    
        print(response['answer'])
    
    
    
```
---

     
 
all -  [ Using an In-Memory Graph Database for GraphRAG in GenAI Apps ](https://www.reddit.com/r/learnmachinelearning/comments/1h1yt2j/using_an_inmemory_graph_database_for_graphrag_in/) , 2024-12-02-0914
```
Hey everyone! I’ve noticed many posts here about handling niche datasets for building intelligent systems, like GenAI ap
ps. Whether it’s legal docs, medical datasets, or proprietary codebases, the challenge is always the same: how do you en
able meaningful knowledge discovery without overloading your LLM or spending a fortune on fine-tuning?

I work at Memgra
ph (full disclosure), and we’ve been digging into Retrieval-Augmented Generation (RAG) systems for months. RAG pairs LLM
s with a knowledge graph to retrieve relevant context dynamically, so the model processes only what matters. It’s faster
, scalable, and adapts to real-time data changes.

For example:

* **Cedars-Sinai** uses Memgraph for risk prediction in
 healthcare.
* **Precina Health** leverages GraphRAG to revolutionize diabetes care.

Memgraph integrates with tools lik
e LangChain and LlamaIndex and even offers features like vector search, deep-path traversals, and streaming data ingesti
on. It’s in-memory, so it’s incredibly fast.

Curious to hear how others are integrating their data with GenAI apps. Wha
t’s your approach to combining LLMs with structured and unstructured data? More details on Memgraph’s GraphRAG ecosystem
 [here](https://memgraph.com/docs/ai-ecosystem/graph-rag).
```
---

     
 
all -  [ A FREE goldmine of tutorials about GenAI Agents! ](https://github.com/NirDiamant/GenAI_Agents) , 2024-12-02-0914
```
After the hackathon I ran in conjunction with LangChain, people have expanded the GenAI_Agents GitHub repository that I 
maintain to now contain 43 (!) Agents-related code tutorials.

It covers ideas across the entire spectrum, containing we
ll-documented code written step by step.
Most of the tutorials include a short 3-minute video explanation!

The content 
is organized into the following categories:
1. Beginner-Friendly Agents
2. Educational and Research Agents
3. Business a
nd Professional Agents
4. Creative and Content Generation Agents
5. Analysis and Information Processing Agents
6. News a
nd Information Agents
7. Shopping and Product Analysis Agents
8. Task Management and Productivity Agents
9. Quality Assu
rance and Testing Agents
10. Special Advanced Techniques

📰 And that's not all! Starting next week, I'm going to write f
ull blog posts covering them in my newsletter.

The subscription and all contents are FREE

→ Subscribe here: https://di
amantai.substack.com/
```
---

     
 
all -  [ Googlegenerativeai is causing problem with async python flask workers like gevent ](https://www.reddit.com/r/LangChain/comments/1h1wop9/googlegenerativeai_is_causing_problem_with_async/) , 2024-12-02-0914
```
The web app crashes whenever I use gevent class workers with gunicorn when running my docker image which is an API for m
y web app developed usinf flask and utilizes googlegenerativeai from langchain
```
---

     
 
all -  [ Improving embedding speed.  ](https://www.reddit.com/r/LangChain/comments/1h1u1bz/improving_embedding_speed/) , 2024-12-02-0914
```
How long does it take you often to embed a text file. ? i am using.

    text-embedding-3-large plus langchain openai an
d pinecone. using semantic chunking  with gradiant method
    
    and it is taking me long time.
    
    since i am us
ing next.js serverless for deployment it is taking me more than thn 60 sec so i don't know what to do. 
```
---

     
 
all -  [ Advice: Am I doing something wrong? ](https://www.reddit.com/r/leetcode/comments/1h1u1bb/advice_am_i_doing_something_wrong/) , 2024-12-02-0914
```
Or is the market expected to improve? Applying to DE, DS, MLE, DA roles with this; no hits after 150 total. Internationa
l so sponsorship required.

https://preview.redd.it/w0qbmiu4pm3e1.png?width=1322&format=png&auto=webp&s=7c2817479728d0a3
3cb34262443467df1adda6e2


```
---

     
 
all -  [ Effective solution to host RAG app ](https://www.reddit.com/r/Rag/comments/1h1sn3m/effective_solution_to_host_rag_app/) , 2024-12-02-0914
```
I have created a simple rag chat for my company. I used llama 3.1 8b model. There are less than 70 users. I am not sure 
on how to deploy it in cloud.

Tech stack : olllama , langchain,fastapi, faiss and a simple react webpage to chat .

Whi
ch is the cost effective solution?

Getting any GPU server or using bedrock ?

If GPU machine, what should be the memory
 size should I get ?
```
---

     
 
all -  [ Conversational RAG on local files (on-premises usage) ](https://www.reddit.com/r/Python/comments/1h1qzds/conversational_rag_on_local_files_onpremises_usage/) , 2024-12-02-0914
```
Hey everyone,

**What My Project Does:**  
That is a local conversational rag on your files. Be honest, you can use this
 as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, 
soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b mod
el hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer e
mbedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdra
nt container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will 
also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React

* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official 
chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Coupl
e of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No s
election for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

**Target Audienc
e:**  
This project is designed for developers, as you’ll need to set up Docker to get it running. Unfortunately, there’
s no consumer-friendly app yet.

**Comparison**:  
The closest competitor, though already far ahead (so I doubt I can tr
uly compete with them), is **LLM Studio**.

For anyone interested in making local RAG or on-premises RAG as accessible a
s possible, you’re warmly invited to contribute!

Here is a link: [https://github.com/dmayboroda/minima](https://github.
com/dmayboroda/minima)

Thank you so much!
```
---

     
 
MachineLearning -  [ [P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, ](https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/) , 2024-12-02-0914
```
  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to intro
duce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premi
ses, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an a
bility to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon
 you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, yo
u can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container runni
ng on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an abilit
y to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you 
can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and ma
c os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problem
s:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the m
odels (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork
, star) Thank you so much!
```
---

     
 
MachineLearning -  [ [P] Open-source declarative framework to build LLM applications - looking for contributors ](https://www.reddit.com/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/) , 2024-12-02-0914
```
I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewA
I, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going
 behind the curtains even for very simple stuff.

[So I just published this open-source framework GenSphere.](https://gi
thub.com/octopus2023-inc/gensphere) The idea is have something like **Docker for LLMs**. You build applications with YAM
L files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs 
themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you 
don't lose control.

You basically code in YAML, stating what are the tasks that need to be done and how they connect. O
ther than that, you only write individual python functions to be called during the execution. No new classes and abstrac
tions to learn.

Its all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditi
onal nodes - which would allow full-fledged agentic system building! Pls reach out  if you want to contribute, there are
 tons of things to do!

PS: [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/) And go o
ver this quick [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutori
al.ipynb)
```
---

     
