 
all -  [ Looking for any literature on Multi Agent architecture/design patterns w/ langgraph ](https://www.reddit.com/r/LangChain/comments/1h26cxx/looking_for_any_literature_on_multi_agent/) , 2024-11-29-0913
```

```
---

     
 
all -  [ Relevance of Message Queues in AI Agents ](https://www.reddit.com/r/LangChain/comments/1h25wdn/relevance_of_message_queues_in_ai_agents/) , 2024-11-29-0913
```
Hi everyone,

I’ve been working with message queue (MQ) software and middleware tools. I’ve been wondering how an AI mig
ht intersect with or enhance the realm of message queuing systems.



Are there applications I’m missing or any existing
 work connecting AI and message queuing systems?

How might the intersection of these two fields shape the future of mid
dleware and distributed systems?

Looking forward to hearing your insights and discussing this further!
```
---

     
 
all -  [ Anyone else interested in storing outputs as well as prompts? And if so ... what solutions are out t ](https://www.reddit.com/r/PromptEngineering/comments/1h22jdi/anyone_else_interested_in_storing_outputs_as_well/) , 2024-11-29-0913
```
Hi everyone,

I think it's my first time posting on this sub which is weird as I've been working on prompting stuff for 
quite some time now. So nice to discover that this exists!

I became very interested in LLMs and prompt engineering earl
ier this year. As the self-hosting kind of type, as much as I was instantly impressed by the advances in the GPT models 
since I last checked them out, my thoughts were also drawn to *'great ... but if I can get something useful out of this 
(the LLM) where does that data \*go\*?'* I've played around with building my own prototypes for running and then storing
 prompts. But ultimately, I'd much rather used better more polished tech that somebody else has made. I'm just having a 
hard time finding it!

To be a bit more specific:

I've worked on a prompt for discovering and parsing some corporate su
stainability data. After quite a number of iterations, it works nicely. I like the idea of using something like a prompt
 engineering IDE to iterate on the prompt further, but I also want to collect the outputs as they're being generated! I 
can do this (say) by creating a script that uses LangChain and routing the outputs to a folder within a Github repo. But
 I'd like something that's a bit easier to replicate, hands-off and (ideally also) cloud-based.

My ideal tool: great en
vironment for prompt engineering *and* really solid functionality for managing where the outputs get stored. Ideally: ch
oose your backend (say a MongoDB server) and the tool will route the outputs there (with or without the prompts). Or as 
a second best, here are some good features for sifting through them and pulling them out. Either way: give me some optio
ns for what to do with the stuff that gets generated beyond just batching it up into one huge JSON that's not really all
 that scalable.

Beyond just collecting the information your prompt was designed to generate, other useful things you ca
n do with previous outputs include passing them as context for other LLMs (ie, chained prompting across models).

As muc
h as some of the prompt eng tools are delightful, I feel like there's something of a gaping blind spot in terms of what 
to do with the information generated by our diligent work in crafting prompts. Which seems a little self-defeating and s
trange.

Does something exist that does what I'm after? And how are people approaching managing outputs in general?
```
---

     
 
all -  [ Discussion: 'Why Does the Recursion Limit Exist in LangGraph?' ](https://www.reddit.com/r/LangChain/comments/1h226yc/discussion_why_does_the_recursion_limit_exist_in/) , 2024-11-29-0913
```
Currently, in my team, we are developing agents using LangGraph. Some of these are complex agents that we dynamically co
mpile, with some cases involving N branches.

My question is: Why does the recursion limit exist? Is it primarily a perf
ormance-based limitation, or is it more about preventing issues like infinite loops in agent execution, such as in the c
ase of a ReAct agent
```
---

     
 
all -  [ Faster LLM response ](https://www.reddit.com/r/LangChain/comments/1h222g7/faster_llm_response/) , 2024-11-29-0913
```
Hello everyone

In my RAG agent, I'm making 3 requests to the LLM, the first is for determining whether to call the tool
 or not, the second is to check set a boolean in the response (JSON), the third is to provide a final answer.

In each i
nvocation to the agent, 2 network requests are made. The prompts are a little bit long, tried to make them shorter but g
ot the same response time about 13 seconds.

using gpt-40-mini, tried gpt 3.5 turbo as well.

all prompts return the fol
lowing JSON:

    {
       'message': '<Your natural language response to the user - exclude technical IDs>',
       'co
ntact_id': '<contact_id of the contractor or null>',  # Always use the actual contractor ID from metadata
       'id': <
id from metadata>,
       'should_navigate': <false>
    }
```
---

     
 
all -  [ MCP Server Tools Langgraph Integration example ](https://www.reddit.com/r/LangChain/comments/1h20lxe/mcp_server_tools_langgraph_integration_example/) , 2024-11-29-0913
```
Example of how to auto discover tools on an MCP Server and make them available to call in your Langgraph graph.

[https:
//github.com/paulrobello/mcp\_langgraph\_tools](https://github.com/paulrobello/mcp_langgraph_tools)
```
---

     
 
all -  [ WARNING:langsmith.client:Failed to multipart ingest runs ](https://www.reddit.com/r/LangChain/comments/1h1yu32/warninglangsmithclientfailed_to_multipart_ingest/) , 2024-11-29-0913
```
Hi guys, 

  
just testing LangChain, once I want to set up tracking of the project in LangSmith I got the following err
or: 

    WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication f
ailed for WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication f
ailed for . HTTPError('401 Client Error: Unauthorized for url: ', '{'detail':'Invalid token'}')trace=b91f591b-3a81-4d7d-
b45b-aa712a577433,id=0b099474-e808-412d-8ed6-e778a05597e0; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=9adae83d-b1e1-4
628-9e8d-6ceccef2ed40; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=ac3358b4-ea21-4a87-9757-88669e094a09; trace=b91f591
b-3a81-4d7d-b45b-aa712a577433,id=b91f591b-3a81-4d7d-b45b-aa712a577433; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=926
e7252-0018-415a-b1d5-f39830f202fd; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=32733c7b-cc61-4dce-b6bf-f91c7025e98d
  
  . HTTPError('401 Client Error: Unauthorized for url: ', '{'detail':'Invalid token'}')trace=b91f591b-3a81-4d7d-b45b-aa7
12a577433,id=0b099474-e808-412d-8ed6-e778a05597e0; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=9adae83d-b1e1-4628-9e8d
-6ceccef2ed40; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=ac3358b4-ea21-4a87-9757-88669e094a09; trace=b91f591b-3a81-4
d7d-b45b-aa712a577433,id=b91f591b-3a81-4d7d-b45b-aa712a577433; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=926e7252-00
18-415a-b1d5-f39830f202fd; trace=b91f591b-3a81-4d7d-b45b-aa712a577433,id=32733c7b-cc61-4dce-b6bf-f91c7025e98d
    https:
//api.smith.langchain.com/runs/multiparthttps://api.smith.langchain.com/runs/multiparthttps://api.smith.langchain.com/ru
ns/multiparthttps://api.smith.langchain.com/runs/multipart

Any idea how to get it working? 

Thanks for any help

Here 
is the script: 

    # Adding Document Loader
    from langchain.chains.combine_documents import create_stuff_documents_
chain
    from langchain_community.document_loaders import WebBaseLoader
    from langchain_text_splitters import Recurs
iveCharacterTextSplitter
    from langchain_openai import AzureOpenAIEmbeddings
    from langchain_community.vectorstore
s.faiss import FAISS
    from langchain.chains import create_retrieval_chain
    from langchain.callbacks import tracing
_v2_enabled
    
    with tracing_v2_enabled() as session:
        assert session
        
        
        def get_docu
ment_from_web(url):
          loader = WebBaseLoader(url)
          docs = loader.load()
          splitter = RecursiveC
haracterTextSplitter(
              chunk_size=200,
              chunk_overlap=20
          )
          splitDocs = spl
itter.split_documents(docs)
          print(len(splitDocs))
          return splitDocs
    
        def create_db(docs):
 
          embedding = AzureOpenAIEmbeddings(
            model='text-embedding-3-small',
            azure_endpoint='x
xxx',
            api_key = 'xxx',
            openai_api_version = '2024-10-01-preview'
        )
          vector_stor
e = FAISS.from_documents(docs, embedding=embedding)
          return vector_store
    
    
        def create_chain(vec
tore_store):
    
          prompt = ChatPromptTemplate.from_template('''
    
          Answer the user question:
     
     Context: {context}
          Question: {input}
          ''')
    
          #chain = prompt | model_2
    
       
   chain = create_stuff_documents_chain(llm= model_2,
                                              prompt = prompt)
   
       
          retrieve = vectore_store.as_retriever(search_kwargs = {'k':12})
          retrieve_chain = create_retr
ieval_chain(
              retrieve,
              chain
            )
    
    
    
          return retrieve_chain
  
  
        docs = get_document_from_web('https://www.abz.com/en/articles/top-10')
        vector_store = create_db(docs)

        chain = create_chain(vector_store)
        response = chain.invoke({
            'input' : 'What....',
        
        })
    
        print(response['answer'])
    
    
    
```
---

     
 
all -  [ Using an In-Memory Graph Database for GraphRAG in GenAI Apps ](https://www.reddit.com/r/learnmachinelearning/comments/1h1yt2j/using_an_inmemory_graph_database_for_graphrag_in/) , 2024-11-29-0913
```
Hey everyone! I’ve noticed many posts here about handling niche datasets for building intelligent systems, like GenAI ap
ps. Whether it’s legal docs, medical datasets, or proprietary codebases, the challenge is always the same: how do you en
able meaningful knowledge discovery without overloading your LLM or spending a fortune on fine-tuning?

I work at Memgra
ph (full disclosure), and we’ve been digging into Retrieval-Augmented Generation (RAG) systems for months. RAG pairs LLM
s with a knowledge graph to retrieve relevant context dynamically, so the model processes only what matters. It’s faster
, scalable, and adapts to real-time data changes.

For example:

* **Cedars-Sinai** uses Memgraph for risk prediction in
 healthcare.
* **Precina Health** leverages GraphRAG to revolutionize diabetes care.

Memgraph integrates with tools lik
e LangChain and LlamaIndex and even offers features like vector search, deep-path traversals, and streaming data ingesti
on. It’s in-memory, so it’s incredibly fast.

Curious to hear how others are integrating their data with GenAI apps. Wha
t’s your approach to combining LLMs with structured and unstructured data? More details on Memgraph’s GraphRAG ecosystem
 [here](https://memgraph.com/docs/ai-ecosystem/graph-rag).
```
---

     
 
all -  [ A FREE goldmine of tutorials about GenAI Agents! ](https://github.com/NirDiamant/GenAI_Agents) , 2024-11-29-0913
```
After the hackathon I ran in conjunction with LangChain, people have expanded the GenAI_Agents GitHub repository that I 
maintain to now contain 43 (!) Agents-related code tutorials.

It covers ideas across the entire spectrum, containing we
ll-documented code written step by step.
Most of the tutorials include a short 3-minute video explanation!

The content 
is organized into the following categories:
1. Beginner-Friendly Agents
2. Educational and Research Agents
3. Business a
nd Professional Agents
4. Creative and Content Generation Agents
5. Analysis and Information Processing Agents
6. News a
nd Information Agents
7. Shopping and Product Analysis Agents
8. Task Management and Productivity Agents
9. Quality Assu
rance and Testing Agents
10. Special Advanced Techniques

📰 And that's not all! Starting next week, I'm going to write f
ull blog posts covering them in my newsletter.

The subscription and all contents are FREE

→ Subscribe here: https://di
amantai.substack.com/
```
---

     
 
all -  [ Googlegenerativeai is causing problem with async python flask workers like gevent ](https://www.reddit.com/r/LangChain/comments/1h1wop9/googlegenerativeai_is_causing_problem_with_async/) , 2024-11-29-0913
```
The web app crashes whenever I use gevent class workers with gunicorn when running my docker image which is an API for m
y web app developed usinf flask and utilizes googlegenerativeai from langchain
```
---

     
 
all -  [ Improving embedding speed.  ](https://www.reddit.com/r/LangChain/comments/1h1u1bz/improving_embedding_speed/) , 2024-11-29-0913
```
How long does it take you often to embed a text file. ? i am using.

    text-embedding-3-large plus langchain openai an
d pinecone. using semantic chunking  with gradiant method
    
    and it is taking me long time.
    
    since i am us
ing next.js serverless for deployment it is taking me more than thn 60 sec so i don't know what to do. 
```
---

     
 
all -  [ Advice: Am I doing something wrong? ](https://www.reddit.com/r/leetcode/comments/1h1u1bb/advice_am_i_doing_something_wrong/) , 2024-11-29-0913
```
Or is the market expected to improve? Applying to DE, DS, MLE, DA roles with this; no hits after 150 total. Internationa
l so sponsorship required.

https://preview.redd.it/w0qbmiu4pm3e1.png?width=1322&format=png&auto=webp&s=7c2817479728d0a3
3cb34262443467df1adda6e2


```
---

     
 
all -  [ (Resume review) Roast my Resume, 2024 grad. Need some career advice ](https://i.redd.it/p8n3npe2nm3e1.jpeg) , 2024-11-29-0913
```
Feel free to roast this resume, template and everything. 

So I'm currently a Junior DevOps Engineer at an SBC (Jenkins,
 CI/CD, K8s).
I want to switch to a traditional Software engineer role (backend preferably). 
Need some advice, tips on 
what to focus on. I've been practicing Golang and Python for a while now, should I stick to that or shift my focus elsew
here? 
Is there any scope for freshers in Go? 
```
---

     
 
all -  [ Effective solution to host RAG app ](https://www.reddit.com/r/Rag/comments/1h1sn3m/effective_solution_to_host_rag_app/) , 2024-11-29-0913
```
I have created a simple rag chat for my company. I used llama 3.1 8b model. There are less than 70 users. I am not sure 
on how to deploy it in cloud.

Tech stack : olllama , langchain,fastapi, faiss and a simple react webpage to chat .

Whi
ch is the cost effective solution?

Getting any GPU server or using bedrock ?

If GPU machine, what should be the memory
 size should I get ?
```
---

     
 
all -  [ Conversational RAG on local files (on-premises usage) ](https://www.reddit.com/r/Python/comments/1h1qzds/conversational_rag_on_local_files_onpremises_usage/) , 2024-11-29-0913
```
Hey everyone,

**What My Project Does:**  
That is a local conversational rag on your files. Be honest, you can use this
 as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, 
soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b mod
el hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer e
mbedding model, you can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdra
nt container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will 
also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React

* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official 
chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Coupl
e of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No s
election for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)

**Target Audienc
e:**  
This project is designed for developers, as you’ll need to set up Docker to get it running. Unfortunately, there’
s no consumer-friendly app yet.

**Comparison**:  
The closest competitor, though already far ahead (so I doubt I can tr
uly compete with them), is **LLM Studio**.

For anyone interested in making local RAG or on-premises RAG as accessible a
s possible, you’re warmly invited to contribute!

Here is a link: [https://github.com/dmayboroda/minima](https://github.
com/dmayboroda/minima)

Thank you so much!
```
---

     
 
all -  [ An example of local conversational RAG using Langchain ](https://www.reddit.com/r/LangChain/comments/1h1q0cg/an_example_of_local_conversational_rag_using/) , 2024-11-29-0913
```
Hey everyone, I would like to introduce you my latest repo, that is a local conversational rag on your files, Be honest,
 you can use this as a rag on-premises, cause it is build with docker, langchain, ollama, fastapi, hf All models downloa
d automatically, soon I'll add an ability to choose a model For now solution contains:

* Locally running Ollama (curren
tly qwen-0.5b model hardcoded, soon you'll be able to choose a model from ollama registry)
* Local indexing (using sente
nce-transformer embedding model, you can switch to other model, but only sentence-transformers applied, also will be cha
nged soon)
* Qdrant container running on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardc
oded, but i will also add an ability to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI w
ritten with React
* As a plus, you can use local rag with ChatGPT as a custom GPT, so you able to query your local data 
through official chatgpt web and mac os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CP
U machines

Couple of ideas/problems:

* Model Context Protocol support
* Right now there is no incremental indexing or 
reindexing
* No selection for the models (will be added soon)
* Different environment support (cuda, mps, custom npu's)


Here is a link: [https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)

Welcome to contribute (wa
tch, fork, star)  
Thank you so much!
```
---

     
 
all -  [ [P] Minima: local conversational retrieval augmented generation project (Ollama, Langchain, FastAPI, ](https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/) , 2024-11-29-0913
```
  
[https://github.com/dmayboroda/minima](https://github.com/dmayboroda/minima)  
  
Hey everyone, I would like to intro
duce you my latest repo, that is a local conversational rag on your files, Be honest, you can use this as a rag on-premi
ses, cause it is build with docker, langchain, ollama, fastapi, hf All models download automatically, soon I'll add an a
bility to choose a model For now solution contains:

* Locally running Ollama (currently qwen-0.5b model hardcoded, soon
 you'll be able to choose a model from ollama registry)
* Local indexing (using sentence-transformer embedding model, yo
u can switch to other model, but only sentence-transformers applied, also will be changed soon)
* Qdrant container runni
ng on your machine
* Reranker running locally (BAAI/bge-reranker-base currently hardcoded, but i will also add an abilit
y to choose a reranker)
* Websocket based chat with saving history
* Simple chat UI written with React
* As a plus, you 
can use local rag with ChatGPT as a custom GPT, so you able to query your local data through official chatgpt web and ma
c os/ios app.
* You can deploy it as a RAG on-premises, all containers can work on CPU machines

Couple of ideas/problem
s:

* Model Context Protocol support
* Right now there is no incremental indexing or reindexing
* No selection for the m
odels (will be added soon)
* Different environment support (cuda, mps, custom npu's)

Welcome to contribute (watch, fork
, star) Thank you so much!
```
---

     
 
all -  [ Query decomposition workflow in langgraph  ](https://www.reddit.com/r/LangChain/comments/1h1prti/query_decomposition_workflow_in_langgraph/) , 2024-11-29-0913
```
I'm trying to create a langgraph workflow where in the first step I want to decompose my complex query into multiple sub
 queries and go through the next workflow of retrieving relevant chunks and extracting the answer bit I want to run for 
all my sub queries in parallel without creating same workflow multiple times


Help for any architecture suggestion or a
ny langgraph features to implement for ease 
```
---

     
 
all -  [ Complete newbie here - Can anyone give me an overview for what I'm doing? ](https://www.reddit.com/r/LangChain/comments/1h1pik1/complete_newbie_here_can_anyone_give_me_an/) , 2024-11-29-0913
```
I have been working on prompt design and trying to work ChatGPT into having reasoning like Tree-of-Thought , Socratic Qu
estioning , etc... I have a bunch of research pdfs on prompting also.   For the latter, I was trying to convert like 50 
pdfs to txts and batch upload , though Astra DB had like 10 pdfs per database - not sure how I'd get it done here , and 
how to get to run like metaprompt frameworks for each reasoning process as a enhancement layer to ChatGPT

I'm barely ru
nning my first script. Trying to figure out how this thing works , I think the goal is a Generative Feedback Loop with R
AG with like a cognitive architecture for enhanced reasoning
```
---

     
 
all -  [ LangGraph Without API Calls ](https://www.reddit.com/r/learnmachinelearning/comments/1h1ld8j/langgraph_without_api_calls/) , 2024-11-29-0913
```
Good evening,

  
I am trying to learn to create Multi-Agent projects using LangGraph based off the [LangGraph Quickstar
t](https://langchain-ai.github.io/langgraph/tutorials/introduction/). I am wondering how could I go about using an API-f
ree system with LangGraph. I tried using Hugging Face models, and was able to successfully use the invoke command. Howev
er, when I get to calling the model as part of the chatbot (after setting the start, chatbot, and end nodes), I get the 
generic AttributeError: 'str' object has no attribute 'content'.

  
I am wondering if this is due to the model I am cho
osing. I can provide specific code if necessary. Also I am very open to doing it another way if necessary. Much apprecia
ted!


```
---

     
 
all -  [ LangGraph without API calls ](https://www.reddit.com/r/LangChain/comments/1h1laq9/langgraph_without_api_calls/) , 2024-11-29-0913
```
Good evening,

  
I am trying to learn to create Multi-Agent projects using LangGraph based off the [LangGraph Quickstar
t](https://langchain-ai.github.io/langgraph/tutorials/introduction/). I am wondering how could I go about using an API-f
ree system with LangGraph. I tried using Hugging Face models, and was able to successfully use the invoke command. Howev
er, when I get to calling the model as part of the chatbot (after setting the start, chatbot, and end nodes), I get the 
generic 

AttributeError: 'str' object has no attribute 'content'

  
I am wondering if this is due to the model I am ch
oosing. I can provide specific code if necessary. Also I am very open to doing it another way if necessary. Much appreci
ated!


```
---

     
 
all -  [ agent-to-agent resiliency, observability, etc - what would you like to see? ](https://www.reddit.com/r/LangChain/comments/1h1hk7i/agenttoagent_resiliency_observability_etc_what/) , 2024-11-29-0913
```
Full disclosure, actively contributing to [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw) \- an
 intelligent proxy for agents built on Envoy and redesigned for agents. Actively seeking feedback on what the community 
would like to see when it comes to agent-to-agent communication, resiliency, observability, etc. Given that a lot of peo
ple are building  task-specific agents and that  agents must communicate with each other reliably, we were seeking advic
e on what features would you like from an agent-mesh that could solve a lot of the crufty resiliency, observability chal
lenges between agents. Note: the project invests in small LLMs to handle/process certain critical tasks related to promp
ts (routing, safety, etc) so if the answer is machine learning related that's totally okay.

You can add your thoughts b
elow, or here: [https://github.com/katanemo/archgw/discussions/317](https://github.com/katanemo/archgw/discussions/317).
 I’ll merge duplicates so feel free to comment away
```
---

     
 
all -  [ How to Make Parallel Requests for the Same Text Using Different Variables ](https://www.reddit.com/r/LangChain/comments/1h1h2vn/how_to_make_parallel_requests_for_the_same_text/) , 2024-11-29-0913
```
I’m a beginner with LangChain, and I’m working on a project that requires making parallel requests to process the same i
nput text across multiple categories. Here’s the challenge: each category needs a different set of examples to guide the
 output generation.

Here’s what I’m trying to achieve:

1. I have a single input text that needs to be processed.
2. Fo
r each category (e.g., 'medications,' 'family history,' 'exams'), I have a unique prompt and a specific set of examples.

3. I want to execute these requests in parallel to improve efficiency.
4. I’m using RunnableParallel, but I’m strugglin
g to figure out the best way to handle the examples dynamically for each category.

What I’ve tried so far:

* Passing e
xamples dynamically during the request.
   * Formatting prompts by embedding examples beforehand. However, I either enco
unter issues with missing inputs variables.

I’m new to LangChain, so any help or suggestions (even simple ones!) would 
be highly appreciated.

    chains = {}
    inputs = {}
    for category, content in prompts.get('medical_queries', {}).
items():
        try:
            prompt_content = content['prompt']
            prompt_examples = content['examples']
 
   
            prompt_template = ChatPromptTemplate.from_template(prompt_content)
    
            chains[category] = L
LMChain(prompt=prompt_template, llm=llm)
    
            inputs[category] = {'input': text, 'example': content['example
s']}
        except Exception as e:
            print(f'{category}: {e}')
    
    pipeline = RunnableParallel(chains)
 
   
    responses = pipeline.invoke(inputs)
    
```
---

     
 
all -  [ Multi-agent supervisor langgrpah giving error ](https://www.reddit.com/r/GoogleColab/comments/1h1ba4c/multiagent_supervisor_langgrpah_giving_error/) , 2024-11-29-0913
```
I was making a supervised agent using langgraph with two agents (rag and sql) using the template from langchain below

O
fficial Doc one : [https://colab.research.google.com/drive/1KEe9YSTGDQopMuss3CSMHJ3VjDzzrGSh?usp=sharing](https://colab.
research.google.com/drive/1KEe9YSTGDQopMuss3CSMHJ3VjDzzrGSh?usp=sharing)

My code: [https://colab.research.google.com/dr
ive/1QszbxpiFJkhWWYhBpSmDCX\_3MMMeHVdd?usp=sharing](https://colab.research.google.com/drive/1QszbxpiFJkhWWYhBpSmDCX_3MMM
eHVdd?usp=sharing)

however when i run my code above at the end i get the error below which seems routeresponse should g
enerate in json and it doesnt. Any idea how i can fix this will be very much appreciated:

>OutputParserException: Funct
ion RouteResponse arguments:

>{  
next: 'Rag\_agent'  
}

>are not valid JSON. Received JSONDecodeError Expecting prope
rty name enclosed in double quotes: line 2 column 5 (char 6)
```
---

     
 
all -  [ Noob on chunks/message threads/chains - best way forward when analyzing bank account statement trans ](https://www.reddit.com/r/LangChain/comments/1h1aiy7/noob_on_chunksmessage_threadschains_best_way/) , 2024-11-29-0913
```
## CONTEXT:
I'm a noob building an app that takes in bank account statement PDFs and extracts the peak balance from each
 of them. I'm receiving these statements in multiple formats, different countries, languages. My app won't know their fo
rmats beforehand.

## HOW I AM TRYING TO BUILD IT:
Currently, I'm trying to build it by extracting markdown from the PDF
 with Docling and sending the markdown to OpenAI api, and asking for it to find the peak balance and for the list of tra
nsactions (so that my app has a way to verify whether it got peak balance right.)

Feeding all of the markdown and reque
sting the api to send bank a list of all transactions isn't working. The model is 'lazy' and won't return all of the tra
nsactions, no matter my prompt (for reference this is a 20 page PDF with 200+ transactions).

So I am thinking that the 
next best way to do this would be with chunks. Docling offers hierarchy-aware chunking [0] which I think it's useful so 
as not to mess with transaction data. But then what should I, a noob, learn about to better proceed on building this app
 based on chunks?

## WAYS FORWARD?
(1) So how should I work with chunks? It seems that looping over chunks and sending 
them through the API and asking for transactions back to append to an array could do the job. But I've got two more thin
gs in mind.

(2) I've hard of chains (like in langchain) which could keep the context from the previous messages and it 
might also be easier to work with?

(3) I have noticed that openai works with a messages *array*. Perhaps that's what I 
should be interacting with via my API calls (to send a thread of messages) instead of doing what I proposed in (1)? Or p
erhaps what I'm describing here is exactly what chaining (2) does?


[0] https://ds4sd.github.io/docling/usage/#convert-
from-binary-pdf-streams at the bottom
```
---

     
 
all -  [ Open Canvas provides chatgpt canvas style ui to use claude and llama3, stores style rules and user i ](https://www.reddit.com/r/LocalLLaMA/comments/1h1a1b9/open_canvas_provides_chatgpt_canvas_style_ui_to/) , 2024-11-29-0913
```
https://preview.redd.it/xg5bqv9odh3e1.png?width=3328&format=png&auto=webp&s=77d6b1e1926a06340e8a21194c73e6f29ac48331

[h
ttps://github.com/langchain-ai/open-canvas](https://github.com/langchain-ai/open-canvas)  

```
---

     
 
all -  [ Tips for improving the processing time of Langgraph Agents ](https://www.reddit.com/r/LangChain/comments/1h18b0d/tips_for_improving_the_processing_time_of/) , 2024-11-29-0913
```
Hello!! I was tasked to improve the performance and speed of our multi agent llm using langgraph and langchain

Any tips
 on how to improve the processing time?
```
---

     
 
all -  [ Text summarization using LangChain's Map-Reduce method. ](https://www.reddit.com/r/LangChain/comments/1h177es/text_summarization_using_langchains_mapreduce/) , 2024-11-29-0913
```
Hello, 

I've been experimenting with LangChain's Map-Reduce approach to summarize texts of approximately 2000 words eac
h. I am satisfied with the results, but the summarization process takes around 15-20 mins. I was looking for any ideas o
r methods to try and reduce the execution time. I'm using ollama's llama3.1:8b model.

Thanks in advance!
```
---

     
 
all -  [ My business model for a small OSS. From OSS project to SaaS (funding)  ](https://www.reddit.com/r/SaaS/comments/1h14361/my_business_model_for_a_small_oss_from_oss/) , 2024-11-29-0913
```
This is a response/follow-up (??) to a great post done yesterday about [monetization of OSS  ](https://www.reddit.com/r/
SaaS/comments/1h0ha1s/i_am_making_700_monthly_with_my_opensource/)

Now i'm going describe how i'm scaling, and do full-
time work in my project, until it becomes a startup/B2B SaaS.

**Project:** [ExtractThinker](https://github.com/enoch371
2/ExtractThinker)

Its **Document Intelligence for LLMs**, or **Langchain for document intelligence**. So more of a nich
e use case, more oriented to extraction of data (No RAG). The project will be used as ***'proof-of-skill'*** for the res
t of the business, for a complex *agentic* B2B model to eventually be also a SaaS. 

**Question:** People are asking me 
'how do you scale this???'

Flow: [Image flow ](https://imgur.com/a/EYWGoP5)

**Github project:** Well presented with do
cumentation. As example driven as possible. 

**Medium:** Over 1.5k followers, contains articles that mention the projec
t or not (depending on the publisher). They attract clients and people interested in the problem solved. **This gives co
ntractor work that feeds the project with use cases.**

**Documentation:** Medium feeds the documentation, so i 'kill tw
o birds with one stone'**.** From diagrams to in detail explanation.

  
**Plan:** Go over 500+ stars and get 2 other to
p founders (2 tech guys and 1 non-tech sales, still missing), that will eventually use this to get funding, next year, v
ia incubator/accelerator or VC seed right away.

  
I hope this helps! A few 1/2 months back i thought this project woul
d not scale (personally), but doing something on top of this, **could definitely work** (Clients eventually put some sen
se into me). Im just a technical guy, sometimes is difficult to see a business model in something like this. 

  
Thank 
you for your time. 
```
---

     
 
all -  [ Langgraph, user_input node with File Upload ](https://www.reddit.com/r/LangChain/comments/1h13hd3/langgraph_user_input_node_with_file_upload/) , 2024-11-29-0913
```
Hello!

I am trying to figure out how to use Langgraph nodes when there is a non-textual input. I am guiding a user in u
ploading files. Let's say, I have the following structure:

`builder.add_node('ask_email', email_node)`  
`builder.add_n
ode('upload_file', upload_file_node)`

I have two questions:

1. How do I manage the file upload - the UI (openWebUI) ca
n show a drop-in component that will trigger external API and this API will respond 200 OK -> is the upload a tool call?
 
2. How do we pass the information about the upload through the state?

  
I'd appreciate a direction, just can't figur
e out how to go about it.
```
---

     
 
all -  [ KeyError: 'input' in create_retrieval_chain()? ](https://www.reddit.com/r/LangChain/comments/1h0ytlt/keyerror_input_in_create_retrieval_chain/) , 2024-11-29-0913
```
I new to generative ai and langchain, bellow i am sharing code and error. I am trying to create a small application. I a
m using python==3.10.0



***CODE:***

    prompt = ChatPromptTemplate([
        ('system', 'You are an expert generativ
e ai developer, answer question from given context {context}'),
        ('user', '{question}')
    ])

    document_chai
n=create_stuff_documents_chain(llm=llm,prompt=prompt1,output_parser=output_parser)
    document_chain
    

    retrieve
r=new_db.as_retriever()
    from langchain.chains import create_retrieval_chain
    retrieval_chain=create_retrieval_cha
in(retriever,combine_docs_chain=document_chain)
    

    ## Get the response form the LLM
    response=retrieval_chain.
invoke({'question':'tell me what is generative ai, and what are Examples of Generative AI tools?'})
    response['answer
']

  
*ERROR I AM GETTING:*

    ---------------------------------------------------------------------------
    KeyErr
or                                  Traceback (most recent call last)
    Cell In[60], line 2
          1 ## Get the res
ponse form the LLM
    ----> 2 response=retrieval_chain.invoke({'question':'tell me what is generative ai, and what are 
Examples of Generative AI tols?'})
          3 # response = retrieval_chain.invoke({'input': {'question': 'tell me what 
is generative AI, and what are examples of Generative AI tools?'}})
          4 response['answer']
    
    File u:\GENE
RATIVE_AI\venv\lib\site-packages\langchain_core\runnables\base.py:5354, in RunnableBindingBase.invoke(self, input, confi
g, **kwargs)
       5348 def invoke(
       5349     self,
       5350     input: Input,
       5351     config: Optiona
l[RunnableConfig] = None,
       5352     **kwargs: Optional[Any],
       5353 ) -> Output:
    -> 5354     return self.
bound.invoke(
       5355         input,
       5356         self._merge_configs(config),
       5357         **{**self.
kwargs, **kwargs},
       5358     )
    
    File u:\GENERATIVE_AI\venv\lib\site-packages\langchain_core\runnables\base
.py:3022, in RunnableSequence.invoke(self, input, config, **kwargs)
       3020 context.run(_set_config_context, config)

       3021 if i == 0:
    -> 3022     input = context.run(step.invoke, input, config, **kwargs)
       3023 else:
    
   3024     input = context.run(step.invoke, input, config)
    
    File u:\GENERATIVE_AI\venv\lib\site-packages\langch
ain_core\runnables\passthrough.py:494, in RunnableAssign.invoke(self, input, config, **kwargs)
        488 def invoke(
 
       489     self,
        490     input: dict[str, Any],
        491     config: Optional[RunnableConfig] = None,
   
     492     **kwargs: Any,
        493 ) -> dict[str, Any]:
    --> 494     return self._call_with_config(self._invoke,
 input, config, **kwargs)
    
    File u:\GENERATIVE_AI\venv\lib\site-packages\langchain_core\runnables\base.py:1927, i
n Runnable._call_with_config(self, func, input, config, run_type, serialized, **kwargs)
       1923     context = copy_c
ontext()
       1924     context.run(_set_config_context, child_config)
       1925     output = cast(
       1926      
   Output,
    -> 1927         context.run(
       1928             call_func_with_variable_args,  # type: ignore[arg-ty
pe]
       1929             func,  # type: ignore[arg-type]
    . . .
         66     ).assign(answer=combine_docs_chain
)
         67 ).with_config(run_name='retrieval_chain')
         69 return retrieval_chain
    
    KeyError: 'input'Out
put is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...



Thanks in adv
ance, I hope to hear from you soon.  

```
---

     
 
all -  [ Is there a way to chainge chains setup without changing codes in python ](https://www.reddit.com/r/LangChain/comments/1h0v562/is_there_a_way_to_chainge_chains_setup_without/) , 2024-11-29-0913
```
Hello group,
I am working on an interesting problem to setup chains by providing a yaml file as input ( it could be a di
ctionary,  a string or list of touple).

Ask:
Using lang chains to create a function which could be able to create serie
s and parallel chains as per our yaml without changing the python code for each case scenario.

Already done: i am able 
to write a function which will create chains for each attribute individually. 

The issue: i am facing challenge in comb
ining these chains. The depth and length could be changed using the yaml file.

The code should be able to set up chains
 (series or parallel as per the yaml) without making any changes to it the python code.

Idea: i am not 100 percent sure
 but it will involve recursion 
```
---

     
 
all -  [ Prompt engineering for LLM applications ? ](https://www.reddit.com/r/LangChain/comments/1h0taz4/prompt_engineering_for_llm_applications/) , 2024-11-29-0913
```
how does prompt engineering help develop better LLM powered apps like I understand that if you are able to prompt the mo
del a certain you will get a better response but the avg user is not going to be aware of those techniques and in which 
is prompt engineering just for the more advanced user and not like an aid towards better LLM development?
```
---

     
 
all -  [ Optimize your LangChain program with Cognify! ](https://www.reddit.com/r/LangChain/comments/1h0jjbh/optimize_your_langchain_program_with_cognify/) , 2024-11-29-0913
```
Hi everyone! I'm Reyna, a PhD student working on systems for machine learning.

I want to share an exciting open-source 
project my team has built: [Cognify](https://github.com/GenseeAI/cognify). Cognify is a multi-faceted optimization tool 
that automatically enhances generation quality and reduces execution costs for generative AI workflows written in LangCh
ain, DSPy, and Python. Cognify helps you evaluate and refine your workflows at any stage of development. Use it to test 
and enhance workflows you’ve finished building or to analyze your current workflow’s potential.

Key highlights:

* Work
flow generation quality improvement by up to 48%
* Workflow execution cost reduction by up to 9x
* Multiple optimized wo
rkflow versions with quality-cost combinations for you to choose
* Automatic model selection, prompt enhancing, and work
flow structure optimization

Get Cognify at [https://github.com/GenseeAI/cognify](https://github.com/GenseeAI/cognify) a
nd read more at https://mlsys.wuklab.io/posts/cognify/. Would love to hear your feedback and get your contributions!
```
---

     
 
all -  [ RAG - how to ensure a date fields in metadata is used to get latest data? ](https://www.reddit.com/r/LangChain/comments/1h0ih0x/rag_how_to_ensure_a_date_fields_in_metadata_is/) , 2024-11-29-0913
```
I've created a RAG app that aims to be a personal assistant for everything related to my kids' school. I get a ton of em
ails from school with updates, notices, invites, and a lot more. Some info is recurring, such as the weekly wraps that e
xplain what was taught during that week; some of it is a one-off like an invite to an event.

Ultimately, I plan to have
 this either connected to Whatsapp or actually creating reminders/calendar invites for me, but for now, I'm validating i
ts results. But I've hit a wall when it comes to the freshness of data.

**Question**

When building this RAG app, I wou
ld like it to be mindful of dates as it retrieves relevant docs. For example, I get a Weekly Wrap email every week, on F
ridays. Besides wanting the retriever to find the right context (i.e. weekly wrap emails), I would also like for it to s
ort by date and focus on the latest one. What's the best way to achieve this?

**Further Context**

Here is what the app
 does:

* bulk-reads all past emails using the Gmail API, and keeps the date, sender, subject, body and attachments
* pr
ocesses each email to properly convert the body and relevant attachments to text (converts PDFs, DOCXs, and html body in
to text)
* stores the documents in MongoDB (using `MongoDBStore` from `langchain_community.storage)` , using a Document 
format (the entire body + attachment text under `page_content`, and everything else in `metadata`
* stores a summary of 
each document, also in MongoDB, in order to use Multi-Representation indexing
* maps IDs across both stores

I'm using G
PT-4o as the LLM behind the scenes.

I then build a retriever and a chain like this, and invoke the query. Since the res
ponse doesn't take the dates into account, I don't get the latest info properly...

    mongo_conn_str = 'mongodb://loca
lhost:27017/'
    mongodb_client = MongoClient(
        mongo_conn_str,
        uuidRepresentation='standard'
    )
    

    database = mongodb_client['parent-assistant']
    summary_collection = database['summaries']
    
    docs_store = 
MongoDBStore(
        mongo_conn_str,
        db_name='parent-assistant',
        collection_name='emails'
    )
    emb
eddings = OpenAIEmbeddings()
    
    summary_vector_store = Chroma(
        collection_name='summaries',
        embedd
ing_function=OpenAIEmbeddings()
    )
    
    summaries = list(summary_collection.find())
    
    summary_docs = [
   
     Document(
            page_content=s['summary'],
            metadata={'_id': s['_id']}
        )
        for i, s 
in enumerate(summaries)
    ]
    
    retriever = MultiVectorRetriever(
        docstore=docs_store,
        vectorstor
e=summary_vector_store,
        id_key='_id',
        search_kwargs={'k': 1}
    )
    retriever.vectorstore.add_documen
ts(summary_docs)
    
    template = '''Answer the following question based on this context:
    
    {context}
    
   
 Question: {question}
    '''
    
    llm = ChatOpenAI(model='gpt-4o', temperature=0.1)
    
    def format_docs(docs):

        return '\n\n'.join(doc.page_content for doc in docs)
    
    rag_chain = (
        {'context': retriever | for
mat_docs, 'question': RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    r
ag_chain.invoke('What did John Doe learn last week?')
```
---

     
 
all -  [ Junior Engineer here. Looking forwared to getting my Resume review / roasted.  ](https://www.reddit.com/r/developersIndia/comments/1h0hs5x/junior_engineer_here_looking_forwared_to_getting/) , 2024-11-29-0913
```
I am working full time for close to an year and I love working with cloud/ devops  but somware i love writing backend co
de. I am looking forwated to getting my resume  reviewed or roasted.  It is 2 pages but would it be better if i make it 
1 ? also I feel like im not good enogh at writing project or experience descriptions. The resume is mostly centerd aroun
d Software Dev type roles but apart from this i work as Technical Content Writer in some companies part time and been ju
dge to few hackathons.  should i put that here ?   
please suggest me to improve, Thank you   


https://preview.redd.it
/lkfy00999a3e1.png?width=2204&format=png&auto=webp&s=a2f654e7af7a466458fe45c157f06d992d8a9476


```
---

     
 
all -  [ Is it possible to add a tool call response to the state  ](https://www.reddit.com/r/LangGraph/comments/1h0gcxb/is_it_possible_to_add_a_tool_call_response_to_the/) , 2024-11-29-0913
```
    from
     datetime 
    import
     datetime
    from
     typing 
    import
     Literal
    
    from
     langch
ain_core.language_models.chat_models 
    import
     BaseChatModel
    from
     langchain_core.messages 
    import
  
   AIMessage, SystemMessage
    from
     langchain_core.runnables 
    import
     (
        RunnableConfig,
        Ru
nnableLambda,
        RunnableSerializable,
    )
    from
     langgraph.checkpoint.memory 
    import
     MemorySaver

    from
     langgraph.graph 
    import
     END, MessagesState, StateGraph
    from
     langgraph.managed 
    impo
rt
     IsLastStep
    from
     langgraph.prebuilt 
    import
     ToolNode
    
    from
     agents.llama_guard 
   
 import
     LlamaGuard, LlamaGuardOutput, SafetyAssessment
    from
     agents.tools.user_data_validator 
    import
 
    (
        user_data_parser_instructions,
        user_data_validator_tool,
    )
    from
     core 
    import
    
 get_model, settings
    
    
    class AgentState(MessagesState, 
    total
    =False):
        
    '''`total=False`
 is PEP589 specs.
    
        documentation: https://typing.readthedocs.io/en/latest/spec/typeddict.html#totality
     
   '''
    
        safety: LlamaGuardOutput
        is_last_step: IsLastStep
        is_data_collection_complete: bool

    
    
    tools = [user_data_validator_tool]
    
    
    current_date = datetime.now().strftime('%B %d, %Y')
    i
nstructions = f'''
        You are a professional onboarding assistant collecting user information.
        Today's date
 is {current_date}.
     
        Collect the following information:
        {user_data_parser_instructions}
     
     
   Guidelines:
        1. Collect one field at a time in order: name, occupation, location
        2. Format the respons
e according to the specified schema
        3. Ensure the data from user is proper before calling the validator
        
4. Use the {user_data_validator_tool.name} tool to validate the JSON data
        5. Keep collecting information until a
ll fields have valid values
     
        Remember: Always pass complete JSON with all fields, using null for pending in
formation
     
        Current field to collect: {{current_field}}
        '''
    
    
    def wrap_model(
    model

    : BaseChatModel) -> RunnableSerializable[AgentState, AIMessage]:
        
    model
     = 
    model
    .bind_tool
s(tools)
        preprocessor = RunnableLambda(
            lambda 
    state
    : [SystemMessage(
    content
    =ins
tructions)] + 
    state
    ['messages'],
            
    name
    ='StateModifier',
        )
        
    return
   
  preprocessor | 
    model
    
    
    def format_safety_message(
    safety
    : LlamaGuardOutput) -> AIMessage:
  
      content = f'This conversation was flagged for unsafe content: {', '.join(
    safety
    .unsafe_categories)}'
   
     
    return
     AIMessage(
    content
    =content)
    
    
    async def acall_model(
    state
    : AgentSta
te, 
    config
    : RunnableConfig) -> AgentState:
        m = get_model(
    config
    ['configurable'].get('model',
 settings.DEFAULT_MODEL))
        model_runnable = wrap_model(m)
        response = 
    await
     model_runnable.ainvo
ke(
    state
    , 
    config
    )
    
        
    # Run llama guard check here to avoid returning the message if i
t's unsafe
        llama_guard = LlamaGuard()
        safety_output = 
    await
     llama_guard.ainvoke('Agent', 
    
state
    ['messages'] + [response])
        
    if
     safety_output.safety_assessment == SafetyAssessment.UNSAFE:
  
          
    return
     {
                'messages': [format_safety_message(safety_output)],
                'safety
': safety_output,
            }
    
        
    if
     
    state
    ['is_last_step'] and response.tool_calls:
     
       
    return
     {
                'messages': [
                    AIMessage(
                        
    id
 
   =response.id,
                        
    content
    ='Sorry, need more steps to process this request.',
          
          )
                ]
            }
    
        
    # We return a list, because this will get added to the exi
sting list
        
    return
     {'messages': [response]}
    
    
    async def llama_guard_input(
    state
    : 
AgentState, 
    config
    : RunnableConfig) -> AgentState:
        llama_guard = LlamaGuard()
        safety_output = 

    await
     llama_guard.ainvoke('User', 
    state
    ['messages'])
        
    return
     {'safety': safety_outp
ut}
    
    
    async def block_unsafe_content(
    state
    : AgentState, 
    config
    : RunnableConfig) -> Agent
State:
        safety: LlamaGuardOutput = 
    state
    ['safety']
        
    return
     {'messages': [format_safety
_message(safety)]}
    
    
    # Define the graph
    agent = StateGraph(AgentState)
    agent.add_node('model', acall
_model)
    agent.add_node('tools', ToolNode(tools))
    agent.add_node('guard_input', llama_guard_input)
    agent.add_
node('block_unsafe_content', block_unsafe_content)
    agent.set_entry_point('guard_input')
    
    
    # Check for un
safe input and block further processing if found
    def check_safety(
    state
    : AgentState) -> Literal['unsafe', 
'safe']:
        safety: LlamaGuardOutput = 
    state
    ['safety']
        
    match
     safety.safety_assessment:

            
    case
     SafetyAssessment.UNSAFE:
                
    return
     'unsafe'
            
    case
    
 _:
                
    return
     'safe'
    
    
    agent.add_conditional_edges(
        'guard_input', check_safe
ty, {'unsafe': 'block_unsafe_content', 'safe': 'model'}
    )
    
    # Always END after blocking unsafe content
    ag
ent.add_edge('block_unsafe_content', END)
    
    # Always run 'model' after 'tools'
    agent.add_edge('tools', 'model
')
    
    
    # After 'model', if there are tool calls, run 'tools'. Otherwise END.
    def pending_tool_calls(
    s
tate
    : AgentState) -> Literal['tools', 'done']:
        last_message = 
    state
    ['messages'][-1]
        
    
if
     not isinstance(last_message, AIMessage):
            
    raise
     TypeError(f'Expected AIMessage, got {type(l
ast_message)}')
        
    if
     last_message.tool_calls:
            
    return
     'tools'
        
    return
 
    'done'
    
    
    agent.add_conditional_edges(
        'model', pending_tool_calls, {'tools': 'tools', 'done': EN
D}
    )
    
    onboarding_assistant = agent.compile(
    checkpointer
    =MemorySaver())
    
    
    
```
---

     
 
all -  [ Seeking Advice on Parsing and Cleaning Legacy Documents ](https://www.reddit.com/r/LangChain/comments/1h0esud/seeking_advice_on_parsing_and_cleaning_legacy/) , 2024-11-29-0913
```
Hey everyone

At work, I've been tasked with handling a collection of guides, documents, and tutorials spanning the past
 20 years. Many of these are in pretty bad shape. While most of the files are in .pdf format, I also come across .pptx f
iles. The most frustrating part is that these documents often contain numerous industry-specific abbreviations, tables, 
and screenshots taken from other software GUI's.

For now, I’m parsing these documents as they are and feeding them into
 an LLM after performing semantic similarity searches. However, when I review the retrieved content, I often find a lot 
of nonsensical chunks. I suspect this is due to the “garbage in, garbage out.”

Today, I was considering using multimoda
l LLMs to parse these PDF screenshots and extract the content more accurately, potentially leveraging an advanced model 
like 4o (or similar).

Do you think this is a good approach? Are there better ways to handle this? I’d appreciate hearin
g about your experiences or recommendations!

Thanks in advance!
```
---

     
 
all -  [ Has anyone used Milvus or Qdrant in cloud? Whats been your experience? ](https://www.reddit.com/r/LangChain/comments/1h0cxtk/has_anyone_used_milvus_or_qdrant_in_cloud_whats/) , 2024-11-29-0913
```
I am planning to build something in prod, any feedback would be great. thanks
```
---

     
 
all -  [ Observability Tools AWS ](https://www.reddit.com/r/LangChain/comments/1h0bjl3/observability_tools_aws/) , 2024-11-29-0913
```
What observability tools are you using right now ? 

I am using aws stack with bedrock and amazon knowledge base and I a
m trying to find a good observability tools like langsmith,  but in AWS environment.  
```
---

     
 
all -  [ Suggestions to host a huggingface model on VLLM + triton server ](https://www.reddit.com/r/LangChain/comments/1h0bgsl/suggestions_to_host_a_huggingface_model_on_vllm/) , 2024-11-29-0913
```
I'm trying to host Qwen 2.5 model on vLLM and triton server. Can anyone suggest me best resource that can help to do it 
correctly. I'm new to this. Any suggestions are also welcome. 

thanks in advance!
```
---

     
 
all -  [ Writer helper tool I've started working on - worth making into a thing or pretty useless? ](https://www.reddit.com/r/LangChain/comments/1h0b10x/writer_helper_tool_ive_started_working_on_worth/) , 2024-11-29-0913
```
https://reddit.com/link/1h0b10x/video/pxvzc467s83e1/player


```
---

     
 
all -  [ Calling Scrapy multiple times (getting ReactorNotRestartable ) ](https://www.reddit.com/r/scrapy/comments/1h0avo7/calling_scrapy_multiple_times_getting/) , 2024-11-29-0913
```
Hi,I know, many already asked and you provided some workarounds, but my problem remained unresolved.  
  
Here are the d
etails:  
Flow/Use Case: I am building a bot. The user can ask the bot to crawl a web page and ask questions about it. T
his process can happen every now and then, I don't know what are the web pages in advance and it all happens while the b
ot app is running,  
time  
Problem: After one successful run, I am getting the famous: twisted.internet.error.ReactorNo
tRestartable error message.I tried running Scrapy in a different process, however, since the data is very big, I need to
 create a shared memory to transfer. This is still problematic because:  
1. Opening a process takes time  
2. I do not 
know the memory size in advance, and I create a certain dictionary with some metadata. so passing the memory like this i
s complex (actually, I haven't manage to make it work yet)  
  
Do you have another solution? or an example of passing t
he massive amount of data between the processes?   
  
Here is a code snippet:  
(I call web\_crawler from another class
, every time with a different requested web address):

    import scrapy
    from scrapy.crawler import CrawlerProcess
 
   from urllib.parse import urlparse
    from llama_index.readers.web import SimpleWebPageReader  # Updated import
    #
from langchain_community.document_loaders import BSHTMLLoader
    from bs4 import BeautifulSoup  # For parsing HTML cont
ent into plain text
    
    g_start_url = ''
    g_url_data = []
    g_with_sub_links = False
    g_max_pages = 1500
  
  g_process = None
    
    
    class ExtractUrls(scrapy.Spider): 
        
        name = 'extract'
    
        # req
uest function 
        def start_requests(self):
            global g_start_url
    
            urls = [ g_start_url, ]
 
            self.allowed_domain = urlparse(urls[0]).netloc #recieve only one atm
                    
            for 
url in urls: 
                yield scrapy.Request(url = url, callback = self.parse) 
    
        # Parse function 
   
     def parse(self, response): 
            global g_with_sub_links
            global g_max_pages
            global g
_url_data
            # Get anchor tags 
            links = response.css('a::attr(href)').extract()  
            
    
        for idx, link in enumerate(links):
                if len(g_url_data) > g_max_pages:
                    print('
Genie web crawler: Max pages reached')
                    break
                full_link = response.urljoin(link)
    
            if not urlparse(full_link).netloc == self.allowed_domain:
                    continue
                if id
x == 0:
                    article_content = response.body.decode('utf-8')
                    soup = BeautifulSoup(art
icle_content, 'html.parser')
                    data = {}
                    data['title'] = response.css('title::text
').extract_first()
                    data['page'] = link
                    data['domain'] = urlparse(full_link).netl
oc
                    data['full_url'] = full_link
                    data['text'] = soup.get_text(separator='\n').str
ip() # Get plain text from HTML
                    g_url_data.append(data)
                    continue
               
 if g_with_sub_links == True:
                    yield scrapy.Request(url = full_link, callback = self.parse)
        

    # Run spider and retrieve URLs
    def run_spider():
        global g_process
        # Schedule the spider for craw
ling
        g_process.crawl(ExtractUrls)
        g_process.start()  # Blocks here until the crawl is finished
        g
_process.stop()
    
    
    def web_crawler(start_url, with_sub_links=False, max_pages=1500):
        '''Web page text
 reader.
            This function gets a url and returns an array of the the wed page information and text, without the
 html tags.
    
        Args:
            start_url (str): The URL page to retrive the information.
            with_su
b_links (bool): Default is False. If set to true- the crawler will downlowd all links in the web page recursively. 
    
        max_pages (int): Default is 1500. If  with_sub_links is set to True, recursive download may continue forever... 
this limits the number of pages to download
    
        Returns:
            all url data, which is a list of dictionar
y: 'title, page, domain, full_url, text.
        '''
        global g_start_url
        global g_with_sub_links
        
global g_max_pages
        global g_url_data
        global g_process
    
        g_start_url=start_url
        g_max_p
ages = max_pages
        g_with_sub_links = with_sub_links
        g_url_data.clear
        g_process = CrawlerProcess(s
ettings={
            'FEEDS': {'articles.json': {'format': 'json'}},
        })
        run_spider()
        return g_u
rl_data
        
        
    

  

```
---

     
 
MachineLearning -  [ [P] Open-source declarative framework to build LLM applications - looking for contributors ](https://www.reddit.com/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/) , 2024-11-29-0913
```
I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewA
I, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going
 behind the curtains even for very simple stuff.

[So I just published this open-source framework GenSphere.](https://gi
thub.com/octopus2023-inc/gensphere) The idea is have something like **Docker for LLMs**. You build applications with YAM
L files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs 
themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you 
don't lose control.

You basically code in YAML, stating what are the tasks that need to be done and how they connect. O
ther than that, you only write individual python functions to be called during the execution. No new classes and abstrac
tions to learn.

Its all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditi
onal nodes - which would allow full-fledged agentic system building! Pls reach out  if you want to contribute, there are
 tons of things to do!

PS: [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/) And go o
ver this quick [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutori
al.ipynb)
```
---

     
