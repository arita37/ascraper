 
all -  [ Gpt-4o ReAct agentic RAG ](https://www.reddit.com/r/LangChain/comments/1cs3asj/gpt4o_react_agentic_rag/) , 2024-05-15-0910
```
I spent the whole day testing Gpt4-o capabilities to do agentic RAG using a standard prompt (hwchase17/ReAct) personaliz
ed for my particular use case: basically, it's the standard prompt but with a couple of High level instructions at the e
nd, to give the agent some personality.
It is unable to respect the response format about half of the time.

Gpt-4-turbo
 instead works like a charm.. almost all the time.
It feels like Gpt-4o is a quantized version of Gpt-4-turbo on instruc
tion following.

Am I the only one?
```
---

     
 
all -  [ Building a GPT-4o AI Agent using Langchain ](https://www.reddit.com/r/LangChain/comments/1cs333y/building_a_gpt4o_ai_agent_using_langchain/) , 2024-05-15-0910
```
Hello r/Langchain, we have been building an Autopilot AI tool called Sparks AI for the past 5 months that combines web s
earch, external app integrations and Langchain to performs complex multi-step tasks in the background. 

  
Please check
 it out at [https://getsparks.ai](https://getsparks.ai) and provide your thoughts.   
Any feedback & ideas are welcome. 

```
---

     
 
all -  [ Non-Flexible Output parsing for JSON ](https://www.reddit.com/r/LangChain/comments/1cs2gw2/nonflexible_output_parsing_for_json/) , 2024-05-15-0910
```
Hi there. So a problem is that if someone wants to parse, let suppose a parameter named Name1, Name2 and so on using out
put parse, how would that be done since once you define Name, the output parsing only returns 1 name only.
So how can be
 the LLM flexible to add Name1,Name2 and so on based on the text inpit it recieves?
```
---

     
 
all -  [ Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI ](https://www.reddit.com/r/LLMDevs/comments/1crwik6/building_an_observable_arxiv_rag_chatbot_with/) , 2024-05-15-0910
```
Hey r/LLMDevs, I published a new article where I built an observable semantic research paper application.

This is an ex
tensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most relevant PDF
 documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval.
3. Enh
ancing the app with LLM observability features from Literal AI.

You can read the article here: [https://medium.com/towa
rds-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](https://m
edium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1
cd8)

Code for the tutorial: [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tahreemrasu
l/semantic_research_engine)
```
---

     
 
all -  [ Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI ](https://www.reddit.com/r/llmops/comments/1crwht7/building_an_observable_arxiv_rag_chatbot_with/) , 2024-05-15-0910
```
Hey r/llmops  , I published a new article where I built an observable semantic research paper application.

This is an e
xtensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most relevant PD
F documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval.
3. En
hancing the app with LLM observability features from Literal AI.

You can read the article here: [https://medium.com/tow
ards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](https://
medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd
1cd8)

Code for the tutorial: [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tahreemras
ul/semantic_research_engine)


```
---

     
 
all -  [ [R] Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI ](https://www.reddit.com/r/MachineLearning/comments/1crwh0q/r_building_an_observable_arxiv_rag_chatbot_with/) , 2024-05-15-0910
```
Hey r/MachineLearning, I published a new article where I built an observable semantic research paper application.

This 
is an extensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most rele
vant PDF documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval
.
3. Enhancing the app with LLM observability features from Literal AI.

You can read the article here: [https://medium.
com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](h
ttps://medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9
c345fcd1cd8)

Code for the tutorial: [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tah
reemrasul/semantic_research_engine)


```
---

     
 
all -  [ Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI ](https://www.reddit.com/r/ChatGPTCoding/comments/1crwd15/building_an_observable_arxiv_rag_chatbot_with/) , 2024-05-15-0910
```
Hey r/ChatGPTCoding, I published a new article where I built an observable semantic research paper application.

This is
 an extensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most releva
nt PDF documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval.

3. Enhancing the app with LLM observability features from Literal AI.

You can read the article here: [https://medium.co
m/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](htt
ps://medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c3
45fcd1cd8)

Code for the tutorial: [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tahre
emrasul/semantic_research_engine)


```
---

     
 
all -  [ What are your current challenges with evaluations? ](https://www.reddit.com/r/LangChain/comments/1crvzvd/what_are_your_current_challenges_with_evaluations/) , 2024-05-15-0910
```
What challenges are you facing and what tools are you using? I am thinking about building out a developer friendly open 
source evaluations tool kit. Thinking of starting with a simple interface where you pass the context, input, output and 
expected output and run it through some basic tests - both LLM based and non LLM based and also allow the ability to wri
te custom assertions.

But, am wondering if you all have any insights into what other capabilities might be useful. 
```
---

     
 
all -  [ GPT-4o by OpenAI, features to know ](/r/ArtificialInteligence/comments/1cruwel/gpt4o_by_openai_features_to_know/) , 2024-05-15-0910
```

```
---

     
 
all -  [  100+ Free Courses Available on Udemy and Coursera ](https://www.reddit.com/r/Udemy/comments/1crthg4/100_free_courses_available_on_udemy_and_coursera/) , 2024-05-15-0910
```
Master Course : Business Cold Calling & Lead Generation 2.0

https://courze.org/master-course-business-cold-calling-lead
-generation-2-0/



HR Conflict Management and Emotional Intelligence 3.0

https://courze.org/hr-conflict-management-and
-emotional-intelligence-3-0/



Digital Foundations: Exploring IT & Multimedia Fundamentals

https://courze.org/digital-
foundations-exploring-it-multimedia-fundamentals/



Master Course in Data Visualization & Data Warehousing (101)

https
://courze.org/master-course-in-data-visualization-data-warehousing-101/



Python And Flask  Demonstrations Practice Cou
rse

https://courze.org/python-and-flask-demonstrations-practice-course/



Human Resources : Corporate Learning and Dev
elopment (L&D)

https://courze.org/human-resources-corporate-learning-and-development-ld/



Safety Leadership: Industry
 Workplace Health and Safety 2.0

https://courze.org/safety-leadership-industry-workplace-health-and-safety-2-0/



Achi
eving Better Work and Life Balance for Business People

https://courze.org/achieving-better-work-and-life-balance-for-bu
siness-people/



Cross-cultural Communication and Cultural Intelligence 2.0

https://courze.org/cross-cultural-communic
ation-and-cultural-intelligence-2-0/



Professional Agile Leadership Course 101 : PAL-E & PAL I

https://courze.org/pro
fessional-agile-leadership-course-101-pal-e-pal-i/



Real Estate Investing, Real Estate Marketing & Flipping 2.0

https
://courze.org/real-estate-investing-real-estate-marketing-flipping-2-0/



Clojure Introduction: Learn Functional Progra
mming

https://courze.org/clojure-introduction-learn-functional-programming/



Create a GUI with Python

https://courze
.org/create-a-gui-with-python/



Python Video Processing

https://courze.org/python-video-processing/



Navigate the L
inux File System

https://courze.org/navigate-the-linux-file-system/



Cómo Crear una Academia Online con WordPress y T
utor LMS

https://courze.org/como-crear-una-academia-online-con-wordpress-y-tutor-lms/



Máster en Diseño Web con Intel
igencia Artificial 2024

https://courze.org/master-en-diseno-web-con-inteligencia-artificial-2024/



Máster en Comercio
 Electrónico con WordPress 2024

https://courze.org/master-en-comercio-electronico-con-wordpress/



Cómo Crear un Blog 
con WordPress Para Principiantes 2024

https://courze.org/como-crear-un-blog-con-wordpress-para-principiantes-2023/



M
aster Course : Advertising Strategy 2.0

https://courze.org/master-course-advertising-strategy-2-0/



Lean Six Sigma an
d Agile Methodology in Project Management

https://courze.org/lean-six-sigma-and-agile-methodology-in-project-management
/



AI and Operations Management & Strategic Management 2.0

https://courze.org/ai-and-operations-management-strategic-
management-2-0/



Essential After Effects: From Beginner to Motion Master

https://courze.org/essential-after-effects-f
rom-beginner-to-motion-master/



Electronics : Diode (Part 2) : Diode applications

https://courze.org/electronics-diod
e-part-2-diode-applications/



Comprehensive DaVinci Resolve With Color Grading Masterclass

https://courze.org/compreh
ensive-davinci-resolve-with-color-grading-masterclass/



Communication Skills Starter Pack

https://courze.org/communic
ation-skills-starter-pack/



Affiliate Marketing For Beginners – Simple Steps to Success

https://courze.org/affiliate-
marketing-for-beginners-simple-steps-to-success/



Mastering LangChain and AWS: A Guide to Economic Analysis

https://c
ourze.org/mastering-langchain-and-aws-a-guide-to-economic-analysis/



Learning Business Contracts for Beginners

https:
//courze.org/learning-business-contracts-for-beginners/



Startup Fund Raising

https://courze.org/startup-fund-raising
/



Advanced Excel Course With Shortcuts Tips and Tricks for JOB

https://courze.org/advanced-excel-course-with-shortcu
ts-tips-and-tricks-for-job/



SQL Bootcamp with MySQL, PHP & Python : 5 Courses in 1

https://courze.org/sql-bootcamp-w
ith-mysql-php-python-5-courses-in-1/



AutoCAD Civil 3D – MEGA course for Civil Works – AulaGEO

https://courze.org/aut
ocad-civil-3d-mega-course-for-civil-works/



Introduction to Cyber Security

https://courze.org/introduction-to-cyber-s
ecurity/



Python & Django | The Complete Django Web Development Course

https://courze.org/python-django-the-complete-
django-web-development-course/



Python Programming Language (Practice Projects)

https://courze.org/python-programming
-language-practice-projects/



Fundamentals of computer science | Short Term Course(Arabic)

https://courze.org/fundame
ntals-of-computer-science-short-term-coursearabic/



Google Sheets – The Complete Google Sheets Course

https://courze.
org/google-sheets-the-complete-google-sheets-course/



ReactJs – The Complete ReactJs Course For Beginners

https://cou
rze.org/reactjs-the-complete-reactjs-course-for-beginners/



NumPy, SciPy, Matplotlib & Pandas A-Z: Machine Learning

h
ttps://courze.org/numpy-scipy-matplotlib-pandas-a-z-machine-learning/




```
---

     
 
all -  [ Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI ](https://www.reddit.com/r/LangChain/comments/1crtas2/building_an_observable_arxiv_rag_chatbot_with/) , 2024-05-15-0910
```
Hey r/LangChain , I published a new article where I built an observable semantic research paper application.

This is an
 extensive tutorial where I go in detail about:

1. Developing a RAG pipeline to process and retrieve the most relevant 
PDF documents from the arXiv API.
2. Developing a Chainlit driven web app with a Copilot for online paper retrieval.
3. 
Enhancing the app with LLM observability features from Literal AI.

You can read the article here: [https://medium.com/t
owards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8](https:
//medium.com/towards-data-science/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345f
cd1cd8)

Code for the tutorial: [https://github.com/tahreemrasul/semantic\_research\_engine](https://github.com/tahreemr
asul/semantic_research_engine)


```
---

     
 
all -  [ Cross Encoder vs. ColBERT in RAG ](https://www.reddit.com/r/LangChain/comments/1crt25e/cross_encoder_vs_colbert_in_rag/) , 2024-05-15-0910
```
Hi,

I want to improve my RAG system and read about ColBERT and Cross-Encoders, but I don't really get what is the diffe
rence here, can someone explain?

  
Also would be nice to have some experiences what worked better for your RAG. I have
 to rely on multilingual models (to use german language), so I picked out:

- [https://huggingface.co/antoinelouis/colbe
rt-xm](https://huggingface.co/antoinelouis/colbert-xm)

- [https://huggingface.co/cross-encoder/msmarco-MiniLM-L12-en-de
-v1](https://huggingface.co/cross-encoder/msmarco-MiniLM-L12-en-de-v1)

  
Which one would you prefer?
```
---

     
 
all -  [ Batch inference in langchain's question answering chain. ](https://www.reddit.com/r/LLMDevs/comments/1crqd4y/batch_inference_in_langchains_question_answering/) , 2024-05-15-0910
```
Hello! I'm a newbie at LLMs and trying to create an LLM that will generate survey question answers from a given long aud
io lecture. The approach is simple. 

I get an audio file, convert it into text using whisper, then I take that text and
 send it to my quantized mistral instruct v2 model for text generation from hugging face and ask it to generate question
s based on those texts. 

After extracting the questions from that generation, I split and store the text in FAISS and t
hen I use a RetreivalQA chain to get answers to each of the questions. 

In the below code I am using the questions and 
generating the answers one by one.   
However, I get a warning to use a dataset instead of answering sequentially. How c
an I do that? I need to make this faster. Thanks!

         def make_qna(questions, store): 
            contexts = []
 
   
            qa_chain = RetrievalQA.from_llm(llm=llm, retriever=store.as_retriever())
    
            for ques in qu
estions:
                qa = qa_chain.run(ques)
                contexts.append({
                    'question': ques,
 
                    'answer': qa, 
                })
            return contexts
    
    # Here llm is a HuggingFace
Pipeline object and store is FAISS
```
---

     
 
all -  [ Building a Snowflake Cost Monitoring and Optimiser tool using Langchain, Snowflake Cortex and Open A ](https://www.reddit.com/r/snowflake/comments/1crq80g/building_a_snowflake_cost_monitoring_and/) , 2024-05-15-0910
```
Wanted to share something a colleague and I’ve been recently working on!

Monitoring Snowflake costs, debugging, trying 
to optimise credit usage, etc. were tedious tasks that were soaking up a lot of engineering bandwidth continuously at ou
r workplace.

We decided to build an AI Agent for this using Langchain, Snowflake Cortex and Open AI!

Check out this qu
ick demo where I ask the agent about my Snowflake spending. There are multiple agents working behind the scenes, using O
penAI and Cortex to find the best answers—and the coolest part? The data visualisations are all chosen by the AI based o
n what you need.

Demo link: [https://www.loom.com/share/b14cb082ba6843298501985f122ffb97?sid=b4cf26d8-77f7-4a63-bab9-c8
e6e9f47064](https://www.loom.com/share/b14cb082ba6843298501985f122ffb97?sid=b4cf26d8-77f7-4a63-bab9-c8e6e9f47064)

It ca
n currently

* Monitor costs
* Forecast costs

We’re looking to add abilities like alerting on anomalies and optimising 
queries to it too!

It’s not perfect yet (sometimes it messes up 😅), but we’re working on improving it! If you’ve got th
oughts on this or know other tasks that could be added to this, let me know.
```
---

     
 
all -  [ Building a Snowflake Cost Monitoring and Optimiser tool using Langchain, Snowflake Cortex and Open A ](https://www.reddit.com/r/aiagents/comments/1crq520/building_a_snowflake_cost_monitoring_and/) , 2024-05-15-0910
```
Wanted to share something a colleague and I’ve been recently working on!



Monitoring Snowflake costs, debugging, tryin
g to optimise credit usage, etc. were tedious tasks that were soaking up a lot of engineering bandwidth continuously at 
our workplace.



We decided to build an AI Agent for this using Langchain, Snowflake Cortex and Open AI!



Check out t
his quick demo where I ask the agent about my Snowflake spending. There are multiple agents working behind the scenes, u
sing OpenAI and Cortex to find the best answers—and the coolest part? The data visualisations are all chosen by the AI b
ased on what you need.



Demo link: [https://www.loom.com/share/b14cb082ba6843298501985f122ffb97?sid=b4cf26d8-77f7-4a63
-bab9-c8e6e9f47064](https://www.loom.com/share/b14cb082ba6843298501985f122ffb97?sid=b4cf26d8-77f7-4a63-bab9-c8e6e9f47064
)



It can currently

* Monitor costs
* Forecast costs



We’re looking to add abilities like alerting on anomalies and
 optimising queries to it too!



It’s not perfect yet (sometimes it messes up 😅), but we’re working on improving it! If
 you’ve got thoughts on this or know other tasks that could be added to this, let me know.
```
---

     
 
all -  [ bulk matching ](https://www.reddit.com/r/LangChain/comments/1crpsmq/bulk_matching/) , 2024-05-15-0910
```
I have a question regarding matching company names to their respective sectors using descriptions.

I have two files, ea
ch with two columns:

* File 1: Company Name & Description
* File 2: Company Sector/Class & Description

The goal is to 
match the company names to the correct sectors based on the descriptions.

If it were just one company, I would embed al
l the descriptions, perform a similarity search to pick the top 5 matches, and then use an LLM call to finalize the matc
hing.

However, how would you proceed if you needed to do this in bulk? Assume that latency and cost are not issues, and
 you want to process everything in a batch fashion.

Any insights or suggestions on the best approach to handle this bul
k processing would be greatly appreciated!
```
---

     
 
all -  [ Need help with RAG chatbot ](https://www.reddit.com/r/LangChain/comments/1cro1oc/need_help_with_rag_chatbot/) , 2024-05-15-0910
```
I'm building a RAG chatbot that gives you the contextual information on the documents uploaded into the database connect
ed to the chatbot. Now, I'm trying to implement a feature wherein the user can use a hash(#) to instruct the bot to poin
t to a specific document within a db and ask questions about that specific doc. Please help me on how to implement that 
feature (adding hash to the bot and having bot recognize the hash and automatically reference the document that follows 
hash) in my project. 

For example, if the user types 'What is the order value of #orderdetails', the chatbot has to ref
er to the document 'orderdetails' stored in the db and has to extract the order value and display it to the user.
```
---

     
 
all -  [ AgentExecutor chain finished after the Observation, but before the llm giving Final Answer ](https://www.reddit.com/r/LangChain/comments/1crn71i/agentexecutor_chain_finished_after_the/) , 2024-05-15-0910
```
I have created a custom LLMSingleActionAgent but once the agent completes the observation the AgentExecutor chain finish
ed. This results in errors, because my parser waits for 'Final Answer' for the user.

`def create_agent(`

`llm,`

`hand
lers,`

`max_iterations: int = 1,`

`early_stopping_method: str = 'force',`

`):`

`output_parser = CustomOutputParser()
`

`python_tool = PythonAstREPLTool(callbacks=handlers)`

`tools = [python_tool]`

`tool_names = [tool.name for tool in 
tools]`

`prompt = CustomPromptTemplate(`

`template=template,`

`tools=tools,`

`input_variables=['system_prompt', 'inp
ut', 'intermediate_steps']`

`)`

`llm_chain = LLMChain(llm=llm, prompt=prompt, callbacks=handlers)`

`agent = LLMSingle
ActionAgent(`

`llm_chain=llm_chain,`

`output_parser=output_parser,`

`stop=['\nObservation:'],`

`allowed_tools=tool_n
ames`

`)`

`return AgentExecutor.from_agent_and_tools(`

`agent=agent,`

`tools=tools,`

`verbose=True,`

`max_iteratio
ns=max_iterations,`

`callbacks=handlers,`

`early_stopping_method=early_stopping_method`

`)`
```
---

     
 
all -  [ AgentExecutor with max_iterations=1 finishes the chain before code execution resulting in observatio ](https://www.reddit.com/r/LangChain/comments/1crln5u/agentexecutor_with_max_iterations1_finishes_the/) , 2024-05-15-0910
```
I have created a single base agent with Python tool, but setting max\_iterations=1 finishes the chain before function ou
tput.

Can anyone help me debug this?

  
Image: Left pane shows the log file, where as the right pane is the terminal.


https://preview.redd.it/ppqq3h1o7c0d1.png?width=1369&format=png&auto=webp&s=545b81e02c317642006c6c1e9459516c5baf1cf2


```
---

     
 
all -  [ Handling ambiguity inAgents ](https://www.reddit.com/r/LangChain/comments/1crl0bk/handling_ambiguity_inagents/) , 2024-05-15-0910
```
In a RAG application with any vector databases connected. How can I deal with ambiguity in the user query? What kind of 
tool/ prompt can I define so that my agent asks the user for further questions when a query is not very clear or not eno
ugh information is given to give a solid correct answer. I have a 4 tools with a ReAct agent ( ```create_ReAct_agent``` 
), one for using the vector databases as a retriever, one for handling irrelevant queries, one for handling generic user
 greetings and one for handling ambiguity. The other tools work well but the tool for ambiguity looks like it's never us
ed as the agent always retrieves docs even if the context is relevant yet ambiguous.

One good product that can handle t
his is Perplexity which prompts the user for further clarification when an ambiguous question is asked.


I want to hand
le ambiguous nature related to my documents in the vector store without the LLM assuming anything on its own. As an exam
ple, if I am creating a medical chatbot that can help people know about different health insurances, doctors in their ar
eas and which insurances those hospitals/doctors accept and user asks 'Which doctor should I visit?'. The agent should i
deally be asking the user what problems they're facing or any other relevant information to give a proper answer, rather
 than just saying here are 10 most important kinds of doctors you have to visit.....

It should ask about patient's age,
 medical issues, medical history, the more clearer it can be on what the user really wants the better answer or can gene
rate rather than giving some generic response based on the documents in the vector store.

```
---

     
 
all -  [ Displaying Images using document IDs in a RAG System
 ](https://www.reddit.com/r/LocalLLaMA/comments/1crkrva/displaying_images_using_document_ids_in_a_rag/) , 2024-05-15-0910
```
I've built a (RAG) system using Langchain that returns relevant documents for an ecom application. I am now looking to e
nhance the user experience by displaying images associated with these documents using their documents IDS.

So my system
 is: doc retrieval, get document ids, load relevant images using IDs, display images.I understand multimodal is an optio
n but I don't want to search across images and load up my vectordb. 

In my case, the retrieved documents are sent to th
e LLM to synthesize the answer directly. I need to figure out a way to inject response into the LLM output.

`conversati
on = ConversationalRetrievalChain.from_llm(llm,`  
`chain_type='stuff',`  
`retriever=retriever,`  
`condense_question_p
rompt=main_prompt,`  
`return_source_documents=True,`  
`combine_docs_chain_kwargs=dict(prompt=combine_docs_custom_promp
t),`  
`)`

For example, this is the current response I am getting

>Based on your query, these are the relevant product
s

>Product 1

>Product 2

>Product 3

I want to change it to this

>Based on your query, these are the relevant product
s

>Product 1  
<images of  product 1>

>Product 2  
<images of  product 2>

>Product 3  
<images of  product 3>

This i
s what I am trying to achieve. Anyone has experience solving this problem?

  

```
---

     
 
all -  [ AI Innovations: OpenAIs GPT-4o, NVIDIAs Regional LLMs, and Aprioras AI Interviewer ](https://www.reddit.com/r/ai_news_by_ai/comments/1crknhs/ai_innovations_openais_gpt4o_nvidias_regional/) , 2024-05-15-0910
```





#startups #tool #api #vc #leaders #event #opinions #release #opensource #update #hardware #major_players #feature #
science #paper #scheduled

OpenAI has introduced GPT-4o, a new model capable of reasoning across text, audio, and video 
in real time. The model is described as versatile, enjoyable to interact with, and a significant advancement towards mor
e natural human-computer interactions [51]. The model can generate various combinations of audio, text, and image output
s, opening up new possibilities that are still being explored [53]. OpenAI has announced that they are making their new 
flagship model, GPT-4o, available to everyone for free with certain limitations [16]. OpenAI Plus users will receive up 
to 5 times higher limits and will have early access to features such as a new macOS desktop app and next-generation voic
e and video capabilities [17]. 







AssemblyAI showcases how to utilize Postman to interact with APIs, including thei
r advanced LeMUR functionality. They have released a video demonstrating how to use LLMs on spoken data without the need
 for coding [2]. Ben, a staff engineer at AssemblyAI, created a video for the AWS This is My Architecture series explain
ing the architecture behind their AI platform for voice data transcription and analysis [1].







NVIDIA has introduce
d two regional large language models (LLMs) optimized for Southeast Asian countries: SeaLLM and SEA-LION. These models a
re optimized for performance and available through the NVIDIA API catalog [12]. NVIDIA AI Developer will showcase how AI
 on RTX on Windows PCs simplifies development and supports a wide range of fast AI applications at the May 22 demo durin
g #MSBuild [11].







The new short course 'Building Multimodal Search and RAG' by Sebastian Witalec focuses on using 
contrastive learning to train multimodal embedding models for building multimodal search and RAG systems [5]. The messag
e provides insights on building a RAG pipeline using NVIDIA AI LangChain Endpoints. It explains the importance of RAG in
 enhancing generative AI systems by combining information retrieval with system prompts [10].







Satya Nadella annou
nced expanded investments in France to drive AI innovation and create new economic opportunities across the country [4].
 The author from a16z highlights the excitement around fast-growing AI tools in categories like music and web creation. 
They also mention the potential for non-AI companies to emerge due to the productivity gains enabled by AI [3].







A
priora, a startup from Y Combinator, has raised $2.8M in seed funding for its AI technology that conducts live job inter
views with candidates. The AI interviewer, named Alex, aims to streamline the hiring process, widen the talent pool, and
 provide immediate feedback to employers [13].







Groq Inc will be at the Enterprise Generative AI Summit in San Fra
ncisco where Santosh Raghavan will speak on a panel about GenAI infrastructure for cost and energy reduction. They will 
showcase the world's fastest AI inference technology at their booth [46]. Groq Inc is excited to see developers building
 projects powered by their technology. Omid Aziz recently added voice capabilities to a mobile app, enabling users to ut
ilize Groq's AI Inference infrastructure at high speed [47].







Cohere now officially supports Java. A short video i
s available to help users get started with Java on Cohere [36]. Cohere is collaborating with buildwithfern to create an 
up-to-date and well-documented Java SDK. More language support is expected to be added in the upcoming weeks [38].








Yann LeCun has noted that his proposal for AI safety in his 2022 paper 'A Path Towards Autonomous Machine Intelligence
' overlaps with the 'Objective-Driven AI Architecture' proposal [41]. He has expressed a negative view on contrastive me
thods, despite proposing them in a NIPS 1993 paper on Siamese nets [42].







Google researchers are actively particip
ating in the Conference on Human Factors in Computing Systems (CHI 2024) as Champion Sponsors. They are presenting over 
30 papers and organizing various events, workshops, and courses [45].




[1. AssemblyAI @AssemblyAI https://twitter.com
/AssemblyAI/status/1789954139419553995](https://twitter.com/AssemblyAI/status/1789954139419553995)

[2. AssemblyAI @Asse
mblyAI https://twitter.com/AssemblyAI/status/1790004073640444026](https://twitter.com/AssemblyAI/status/1790004073640444
026)

[3. a16z @a16z https://twitter.com/a16z/status/1790039402065940518](https://twitter.com/a16z/status/17900394020659
40518)

[4. Satya Nadella @satyanadella https://twitter.com/satyanadella/status/1790063618916000046](https://twitter.com
/satyanadella/status/1790063618916000046)

[5. Andrew Ng @AndrewYNg https://twitter.com/AndrewYNg/status/179005085277611
2439](https://twitter.com/AndrewYNg/status/1790050852776112439)

[6. Andrew Ng @AndrewYNg https://twitter.com/AndrewYNg/
status/1790088683259048120](https://twitter.com/AndrewYNg/status/1790088683259048120)

[7. Sam Altman @sama https://twit
ter.com/sama/status/1790066685698789837](https://twitter.com/sama/status/1790066685698789837)

[8. Sam Altman @sama http
s://twitter.com/sama/status/1790074770324639933](https://twitter.com/sama/status/1790074770324639933)

[9. Andrej Karpat
hy @karpathy https://twitter.com/karpathy/status/1790076925508977096](https://twitter.com/karpathy/status/17900769255089
77096)

[10. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/status/1790038663243468892](https://twitte
r.com/NVIDIAAIDev/status/1790038663243468892)

[11. NVIDIA AI Developer @NVIDIAAIDev https://twitter.com/NVIDIAAIDev/sta
tus/1790080827654619216](https://twitter.com/NVIDIAAIDev/status/1790080827654619216)

[12. NVIDIA AI Developer @NVIDIAAI
Dev https://twitter.com/NVIDIAAIDev/status/1790097179542995296](https://twitter.com/NVIDIAAIDev/status/17900971795429952
96)

[13. Y Combinator @ycombinator https://twitter.com/ycombinator/status/1790083099088527837](https://twitter.com/ycom
binator/status/1790083099088527837)

[14. OpenAI @openai https://twitter.com/openai/status/1790052092909941214](https://
twitter.com/openai/status/1790052092909941214)

[15. OpenAI @openai https://twitter.com/openai/status/179006507522455580
6](https://twitter.com/openai/status/1790065075224555806)

[16. OpenAI @openai https://twitter.com/openai/status/1790072
068446265675](https://twitter.com/openai/status/1790072068446265675)

[17. OpenAI @openai https://twitter.com/openai/sta
tus/1790072070128177303](https://twitter.com/openai/status/1790072070128177303)

[18. OpenAI @openai https://twitter.com
/openai/status/1790072174117613963](https://twitter.com/openai/status/1790072174117613963)

[19. OpenAI @openai https://
twitter.com/openai/status/1790089507859017954](https://twitter.com/openai/status/1790089507859017954)

[20. OpenAI @open
ai https://twitter.com/openai/status/1790089509746376893](https://twitter.com/openai/status/1790089509746376893)

[21. O
penAI @openai https://twitter.com/openai/status/1790089511608725740](https://twitter.com/openai/status/17900895116087257
40)

[22. OpenAI @openai https://twitter.com/openai/status/1790089513387143469](https://twitter.com/openai/status/179008
9513387143469)

[23. OpenAI @openai https://twitter.com/openai/status/1790089515375214798](https://twitter.com/openai/st
atus/1790089515375214798)

[24. OpenAI @openai https://twitter.com/openai/status/1790089518210580721](https://twitter.co
m/openai/status/1790089518210580721)

[25. OpenAI @openai https://twitter.com/openai/status/1790089520139931860](https:/
/twitter.com/openai/status/1790089520139931860)

[26. OpenAI @openai https://twitter.com/openai/status/17900895219854665
87](https://twitter.com/openai/status/1790089521985466587)

[27. OpenAI @openai https://twitter.com/openai/status/179008
9523969356223](https://twitter.com/openai/status/1790089523969356223)

[28. OpenAI @openai https://twitter.com/openai/st
atus/1790089525642899678](https://twitter.com/openai/status/1790089525642899678)

[29. OpenAI @openai https://twitter.co
m/openai/status/1790130694359806122](https://twitter.com/openai/status/1790130694359806122)

[30. OpenAI @openai https:/
/twitter.com/openai/status/1790130696838619602](https://twitter.com/openai/status/1790130696838619602)

[31. OpenAI @ope
nai https://twitter.com/openai/status/1790130699166421457](https://twitter.com/openai/status/1790130699166421457)

[32. 
OpenAI @openai https://twitter.com/openai/status/1790130701339160887](https://twitter.com/openai/status/1790130701339160
887)

[33. OpenAI @openai https://twitter.com/openai/status/1790130703721521305](https://twitter.com/openai/status/17901
30703721521305)

[34. OpenAI @openai https://twitter.com/openai/status/1790130706376540464](https://twitter.com/openai/s
tatus/1790130706376540464)

[35. OpenAI @openai https://twitter.com/openai/status/1790130708612088054](https://twitter.c
om/openai/status/1790130708612088054)

[36. cohere @cohere https://twitter.com/cohere/status/1790107150062067998](https:
//twitter.com/cohere/status/1790107150062067998)

[37. cohere @cohere https://twitter.com/cohere/status/1790107197625479
321](https://twitter.com/cohere/status/1790107197625479321)

[38. cohere @cohere https://twitter.com/cohere/status/17901
08185765851378](https://twitter.com/cohere/status/1790108185765851378)

[39. Yann LeCun @ylecun https://twitter.com/ylec
un/status/1789998373166096887](https://twitter.com/ylecun/status/1789998373166096887)

[40. Yann LeCun @ylecun https://t
witter.com/ylecun/status/1790004261629170155](https://twitter.com/ylecun/status/1790004261629170155)

[41. Yann LeCun @y
lecun https://twitter.com/ylecun/status/1790108163582115862](https://twitter.com/ylecun/status/1790108163582115862)

[42
. Yann LeCun @ylecun https://twitter.com/ylecun/status/1790112908937981961](https://twitter.com/ylecun/status/1790112908
937981961)

[43. Yann LeCun @ylecun https://twitter.com/ylecun/status/1790117512794780068](https://twitter.com/ylecun/st
atus/1790117512794780068)

[44. NVIDIA AI @NVIDIAAI https://twitter.com/NVIDIAAI/status/1790109764719886523](https://twi
tter.com/NVIDIAAI/status/1790109764719886523)

[45. Google AI @googleai https://twitter.com/googleai/status/179011289787
3686928](https://twitter.com/googleai/status/1790112897873686928)

[46. Groq Inc @GroqInc https://twitter.com/GroqInc/st
atus/1790008346705068286](https://twitter.com/GroqInc/status/1790008346705068286)

[47. Groq Inc @GroqInc https://twitte
r.com/GroqInc/status/1790033127999525345](https://twitter.com/GroqInc/status/1790033127999525345)

[48. Groq Inc @GroqIn
c https://twitter.com/GroqInc/status/1790125363978178951](https://twitter.com/GroqInc/status/1790125363978178951)

[49. 
Groq Inc @GroqInc https://twitter.com/GroqInc/status/1790167658400210957](https://twitter.com/GroqInc/status/17901676584
00210957)

[50. Groq Inc @GroqInc https://twitter.com/GroqInc/status/1790204981557231995](https://twitter.com/GroqInc/st
atus/1790204981557231995)

[51. Greg Brockman @gdb https://twitter.com/gdb/status/1790071008499544518](https://twitter.c
om/gdb/status/1790071008499544518)

[52. Greg Brockman @gdb https://twitter.com/gdb/status/1790074041614717210](https://
twitter.com/gdb/status/1790074041614717210)

[53. Greg Brockman @gdb https://twitter.com/gdb/status/1790077263708340386]
(https://twitter.com/gdb/status/1790077263708340386)

[54. Greg Brockman @gdb https://twitter.com/gdb/status/17900793986
25808837](https://twitter.com/gdb/status/1790079398625808837)

[55. Greg Brockman @gdb https://twitter.com/gdb/status/17
90164028003918138](https://twitter.com/gdb/status/1790164028003918138)

[56. Greg Brockman @gdb https://twitter.com/gdb/
status/1790164084425646481](https://twitter.com/gdb/status/1790164084425646481)

[57. Greg Brockman @gdb https://twitter
.com/gdb/status/1790177196075864100](https://twitter.com/gdb/status/1790177196075864100)

[58. Greg Brockman @gdb https:
//twitter.com/gdb/status/1790183962553532798](https://twitter.com/gdb/status/1790183962553532798)

[59. Greg Brockman @g
db https://twitter.com/gdb/status/1790195202214572399](https://twitter.com/gdb/status/1790195202214572399)
```
---

     
 
all -  [ Displaying Images using document IDs in a RAG System ](https://www.reddit.com/r/LangChain/comments/1crkgp0/displaying_images_using_document_ids_in_a_rag/) , 2024-05-15-0910
```
I've created a (RAG) system that returns relevant documents. I am now looking to enhance the user experience by displayi
ng images associated with these documents using their documents IDS.

So my system is: doc retrieval, get document ids, 
load relevant images using IDs, display images.

Anyone knows how this can be done? I understand multimodal is an option
 but I don't want to search across images. Just want to display images using document ids of returned docs

In my case, 
the retrieved documents are sent to the LLM to synthesize the answer directly. I need to figure out a way to inject resp
onse into the LLM output.

`conversation = ConversationalRetrievalChain.from_llm(llm,`  
`chain_type='stuff',`  
`retrie
ver=retriever,`  
`condense_question_prompt=main_prompt,`  
`return_source_documents=True,`  
`combine_docs_chain_kwargs
=dict(prompt=combine_docs_custom_prompt),`  
`)`

For example, this is the current response I am getting

>Based on your
 query, these are the relevant products

>Product 1

>Product 2

>Product 3

I want to change it to this

>Based on your
 query, these are the relevant products

>Product 1  
<images of  product 1>

>Product 2  
<images of  product 2>

>Prod
uct 3  
<images of  product 3>

This is what I am trying to achieve. Anyone has experience solving this problem?
```
---

     
 
all -  [ LangChain vs DSPy  ](https://www.reddit.com/r/generativeAI/comments/1crinzt/langchain_vs_dspy/) , 2024-05-15-0910
```
DSPy is a breakthrough Generative AI package that helps in automatic prompt tuning. How is it different from LangChain? 
Find in this video https://youtu.be/3QbiUEWpO0E?si=4oOXx6olUv-7Bdr9
```
---

     
 
all -  [ How to go about processing documents. ](https://www.reddit.com/r/LocalLLaMA/comments/1crhwij/how_to_go_about_processing_documents/) , 2024-05-15-0910
```
Hi, a beginner here so forgive me if this is a dumb question but I'll get straight down to it. 

Let's say I have a 300 
page unstructured document with about 400 words per page. My goal is to extract certain types of entities from every pag
e of the document and I have to do this multiple times per day. What would be the best approach to feed this data to a L
lama 3 8b model.

Since there's a context limit window to it, my current approach is to feed it OCR from every two pages
 or so, with a default prompt for the task I want to do. Is this an optimal way of doing it?

Also sidenote, my predeces
sors have a langchain spacy model doing this currently but the accuracy isn't that great as the text in the document is 
unstructured as well as the grammar and sentence formation is extremely bad in some cases. That's why we are trying to s
ee if an LLM can solve this problem to some extent.

I have also read about implementing a RAG pipeline. Would that be a
n ideal approach for this case considering the size of the documents?
```
---

     
 
all -  [ Building an Agent with Google Cloud (Langserve on GC Run vs. Vertex AI Reasoning Engine) ](https://www.reddit.com/r/LangChain/comments/1crezed/building_an_agent_with_google_cloud_langserve_on/) , 2024-05-15-0910
```
Hi everyone,

I'm working on a project using Google Cloud and I'm exploring two approaches to create an agent:

**Option
 1: Reasoning Engine with Langchain**

This approach involves using Langchain's Reasoning Engine with my own tools conne
cted through a REST API. I envision a frontend built with Next.js that communicates with a FastAPI backend using WebSock
ets. The backend would then query the Reasoning Engine using the Vertex AI SDK, which would reason with my Langchain Age
nt.

**Option 2: Langchain & Langserve on Google Cloud Run**

Alternatively, I could create a standalone agent entirely 
in Langserve. This agent would handle memory, tools, and queries to the Gemini API for completion. Communication with th
e frontend would likely involve a REST API (but I'm open to suggestions).

**My Dilemma:**

* I'm unsure which approach 
is better suited for my needs.
* My REST API is private and deployed on AWS.
* I'm open to recommendations on building a
n agent with Google Cloud, particularly regarding WebSockets (mandatory or optional) and overall architecture.
* Does La
ngserve even support WebSockets?

Any insights or suggestions from the community would be greatly appreciated!
```
---

     
 
all -  [ what is the hardest part of integrating langchain with openai? ](https://www.reddit.com/r/LangChain/comments/1cravwx/what_is_the_hardest_part_of_integrating_langchain/) , 2024-05-15-0910
```
Hi y'all! I'm trying to get some feedback from those using langchain with OpenAI APIs. What areas are you guys seeing th
e most difficulty with? I would love to hear about your experience! Feel free to mention more details in the comments se
ction about your specific usecase

[View Poll](https://www.reddit.com/poll/1cravwx)
```
---

     
 
all -  [ tracking token usage programatically in python ](https://www.reddit.com/r/LangChain/comments/1cr5usv/tracking_token_usage_programatically_in_python/) , 2024-05-15-0910
```
hi,

i'm struggling to find an answer to this question - 

i'm writing a small application that utilizes RAG with a mist
ral model, the main script looks something like this:  


`from langchain_community.document_loaders import TextLoader` 
 
`from langchain_community.document_loaders import PyPDFLoader`  
`from langchain_mistralai.chat_models import ChatMist
ralAI`  
`from langchain_mistralai.embeddings import MistralAIEmbeddings`  
`from langchain_community.vectorstores impor
t FAISS`  
`from langchain.text_splitter import RecursiveCharacterTextSplitter`  
`from langchain.chains.combine_documen
ts import create_stuff_documents_chain`  
`from langchain_core.prompts import ChatPromptTemplate`  
`from langchain.chai
ns import create_retrieval_chain`  
`from langchain.storage import LocalFileStore`  
`from langchain.embeddings import C
acheBackedEmbeddings`  
`from time import time`  
`from dotenv import load_dotenv`  
`import os`  


`if __name__ == '__
main__':`  
`load_dotenv()`  
`api_key = os.getenv('MISTRAL_KEY')`  
`loader = PyPDFLoader('paper1.pdf')`  
`text_splitt
er = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 300)`  
`docs = loader.load_and_split()`  
 `# Sp
lit text into chunks`  
 `documents = text_splitter.split_documents(docs)`  
 `# Define the embedding model`  
 `embeddi
ngs = MistralAIEmbeddings(model='mistral-embed', mistral_api_key=api_key)`  
`store = LocalFileStore('./cache/')`  
`cac
hed_embedder = CacheBackedEmbeddings.from_bytes_store(`  
`embeddings, store, namespace=embeddings.model`  
`)`  
 `# Cr
eate the vector store`  
 `initial_time = time()`  
`vector = FAISS.from_documents(documents, cached_embedder)`  
 `# De
fine a retriever interface`  
 `retriever = vector.as_retriever()`  
 `# Define LLM`  
 `model = ChatMistralAI(mistral_a
pi_key=api_key)`  
 `# Define prompt template`  
 `prompt = ChatPromptTemplate.from_template('''Answer the following que
stion based only on the provided context:`  
   
`<context>`  
`{context}`  
`</context>`  
   
`Question: {input}''')` 
 


`# Create a retrieval chain to answer questions`  
 `document_chain = create_stuff_documents_chain(model, prompt)`  

`retrieval_chain = create_retrieval_chain(retriever, document_chain)`  
`response = retrieval_chain.invoke({'input': 'W
hat were the two main things the author worked on before college?'})`  
 `print(response['answer'])`

  
is there any fu
nction i can run from a main script to track my usage in dollars?   
everytime the script runs it will print out how man
y dollars i have left or something like that.  


&#x200B;
```
---

     
 
all -  [ RAG using Langchain ](https://www.reddit.com/r/LangChain/comments/1cr4z66/rag_using_langchain/) , 2024-05-15-0910
```
i have multiple collection(tables like employee,hr,deptartment) of user data and i want build RAG chat bot  with citatio
n and prompt using langchain  
can you please provide detail steps to perform it don't provide reference documents
```
---

     
 
all -  [ Llama3 Assistant Prompt ](https://www.reddit.com/r/LangChain/comments/1cr48vz/llama3_assistant_prompt/) , 2024-05-15-0910
```
I try to write a assistant prompt to llama 3 but it doesn't recognize the tool at all, does any one can give me an examp
le to it. 
```
---

     
 
all -  [ Unexpected and erroneous results when trying to use a react agent with minimal tools ](https://www.reddit.com/r/LangChain/comments/1cr3lav/unexpected_and_erroneous_results_when_trying_to/) , 2024-05-15-0910
```
I'm trying to understand custom tools and agents.  

I tried to make a simple example, with three tools to choose from. 
Then I ask it for money.  Mostly it fails in one of a few ways:

1. It identifies 'get\_money' as the function to call, 
but then it says it is not valid and tries to call it again

2. It just loops calling 'get\_money' over and over again


3. It calls 'get\_money' but then decides it needs to do something with the money and that 'get\_bricks' seems like a go
od things to do.  

I'm using llama3 as my LLM.

Can you give me any pointers?  Code snippet and example of looping beha
vior below.

    @tool
    def get_bricks(query: str) -> str:
        '''Returns bricks'''
        return 'bricks'
    

    @tool
    def get_money(query: str) -> int:
        '''Returns money.'''
        return 'money'
    
    
    @tool

    def get_word_length(word: str) -> int:
        '''Returns the length of a word.'''
        return len(word)
    
   
 tools = [get_word_length, get_money, get_bricks]
    prompt = hub.pull('hwchase17/react')
    agent = create_react_agen
t(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    
    agent_executor
.invoke(
        {
            'input': 'Can you get me some money?'
        }
    )
    

  
Which results in something
 like this:

    python minimal_tool_bot.py
    
    > Entering new AgentExecutor chain...
    I'd be happy to help!
   
 
    Thought: Hmm, it looks like I need to take an action to get the money. Let me think...
    
    Action: get_money

    Action Input: None (since get_money doesn't require any inputmoneyI'm glad you asked!
    
    Thought: Now that I h
ave the money, let's see how we can use it.
    
    Action: None (for now)
    
    The money seems like a nice chunk o
f change! Let's think about what to do next...
    
    Thought: What should we do with this money?
    
    Action: get
_bricks
    Action Input: None  (since get_bricks doesn't require any inputNone (for now)
    
    The money seems like 
a nice chunk of change! Let's think about what to do next...
    
    Thought: What should we do with this money?
    
 
   Action: get_bricks is not a valid tool, try one of [get_word_length, get_money, get_bricks].Let's continue!
    
    
Thought: Oops, it looks like I made a mistake! 
    Action: get_word_length
    Action Input: None (since get_word_lengt
h doesn't require any input53Question: Can you get me some money?
    
    Thought: I'd be happy to help!
    
    Thoug
ht: Hmm, it looks like I need to take an action to get the money. Let me think...
    
    Action: get_money
    Action 
Input: None (since get_money doesn't require any inputmoneyI'll continue from where you left off.
    
    Question: Wha
t does the length of 'Can you get me some money?' mean?
    
    Thought: I think I can use this information to help ans
wer my original question...
    
    Action: get_word_length

  

```
---

     
 
all -  [ RAG on Elastic ](https://www.reddit.com/r/elasticsearch/comments/1cr17ov/rag_on_elastic/) , 2024-05-15-0910
```
I am very new to the elastic stack and the place I am working at wants to use elasticsearch in a RAG application. One of
 the requests is to keep it solely in the elastic ecosystem I.e. no langchain or openAI.

I was under the impression tha
t elastic is only concerned with the “retrieval” aspect of the design pattern. Is it even possible to design an entire e
nd to end RAG framework using only elastic? 
```
---

     
 
all -  [ Experimenting with Langchain, Langgraph, and Snowflake to Build a Product Copilot POC ](https://www.reddit.com/r/LangChain/comments/1cqzjq0/experimenting_with_langchain_langgraph_and/) , 2024-05-15-0910
```
In a recent hackweek, a colleague and I decided to explore the integration of natural language processing and data visua
lization by building a prototype agent that interfaces directly with Snowflake. Our goal was to create a tool that could
 automatically interpret intent, fetch relevant data, and generate visual insights, starting with trends and funnels.

H
ere’s what we’ve implemented so far:

* Trend visualization
* Funnel analysis

Looking ahead, we’re excited to expand th
e tool's capabilities to include:

* Retention reports
* User cohort analysis
* Metric alerts

This project is very much
 a work-in-progress, and we're keen on refining and enhancing its functionalities. We want this tool to be a helpful ass
istant for product managers who rely on Snowflake for data insights.

For a closer look, check out the video demo we pos
ted on our LinkedIn. [Here's the link to our LinkedIn post with the video demo.](https://www.linkedin.com/posts/shubhank
arsrivastava_analytics-agents-in-snowflake-product-activity-7194598110440955904-wi51?utm_source=share&utm_medium=member_
desktop)

Attached is an image showing how we structured the architecture of our agent. I’m eager to hear any feedback o
r ideas from this community!

https://preview.redd.it/c2kj0psg670d1.png?width=795&format=png&auto=webp&s=13f65407c9cd4f8
295a5c9c7826451c51492446e


```
---

     
 
all -  [ CSV-based agent using Langchain and GPT4All ](https://www.reddit.com/r/u_Significant-Book-727/comments/1cqyj6e/csvbased_agent_using_langchain_and_gpt4all/) , 2024-05-15-0910
```
Hello!

  
I've implemented a CSV-based agent using Langchain and a LLM provided by GPT4All. So it is basically a local 
and free agent for querying a CSV data.

The CSV data is relatively big. Besides the number of rows, there are many colu
mns. This results in a larger prompt and a slower performance. I had also to manually increase the context size implemen
ted by the interface between Langchain and GPT4All.

Now, I have been investigating a way to improve the performance of 
the application. Specifically, I need to reduce the execution time for a single query (it usually takes 7 minutes). As I
 can't afford a stronger GPU, I have been looking for finetuning the LLM provided by GPT4All in order to reduce the prom
pt size. My idea is to make the LLM familiar with the structure of my CSV data in order to reduce the explanation of the
 columns. Unfortunately, the finetuning is requiring more than 32GB of RAM, which I can't provide.

Is there a way of re
ducing the memory use during the finetuning process? Any other idea to improve the agent performance?
```
---

     
 
all -  [ Keeping up with the pace of technology is VERY hard. A rant!! ](https://www.reddit.com/r/developersIndia/comments/1cqxyvc/keeping_up_with_the_pace_of_technology_is_very/) , 2024-05-15-0910
```
And this comes from a data scientist (not a software engineer and not someone who majored in CS/IT) with a decade of exp
erience. You learn the math (including statistics), the ML algorithms, Python programming, R programming, SQL, databases
. And then you're quizzed in interviews about a very obscure graph algorithm/dynamic programming problem which has possi
bly no use in the job. Finally, there's the cloud- AWS, Azure or GCP.

Now that LLMs have become really really important
, business (and managers) are all about langchain, llamaindex, **agentic** AI (whatever the f\*\*k it means), autogen, c
opilot, low code, no code crap.

I wish the spaceflight/engineering/physics sector was as developed as the software indu
stry. If not for money that I get from AI/ML, I'd go back to these fields in a heartbeat. I'm not able to spend time on 
non-tech related things like guitar, farming or just chilling with a cup of coffee in my hand. These days, I'm more worr
ied about staying 'relevant' than solving the real problems that the business faces. Fuck!!

Other developers of India- 
how do you keep up with the rapid pace of technology and stay relevant?
```
---

     
 
all -  [ Overview of the Microsoft generative AI services and when to use which! ](https://www.reddit.com/r/AZURE/comments/1cqxrw7/overview_of_the_microsoft_generative_ai_services/) , 2024-05-15-0910
```
New video exploring the various Microsoft generative AI services like Copilots, Copilot Studio, AI Studio etc., what the
y do and when to use which! Also what some of these fancy things like LangChain and Semantic Kernel are and where they g
et used 😉

[https://youtu.be/ArRpwLGA2Hk](https://youtu.be/ArRpwLGA2Hk)

00:00 - Introduction

00:29 - What is AI

01:59
 - Generative AI

03:33 - GPT and the model

06:12 - Why don't I always use the newest

07:42 - How models are created


14:57 - Gaps to be really useful

18:42 - Solving the gaps

23:11 - How we use GPT

25:37 - Copilots

26:41 - GitHub Cop
ilot

29:38 - Microsoft product copilots

33:39 - Data governance considerations

37:01 - Bing Chat

37:36 - Copilot Stu
dio

40:00 - Custom copilot

42:16 - Topics

44:13 - Generative AI

44:33 - Adding custom data

48:32 - Actions

52:37 -
 Entities

52:49 - Publish

54:28 - Licensing

55:14 - Extend first-party copilot

56:26 - Pro-code with Azure AI Studio


1:04:56 - Orchestrators

1:06:34 - LangChain

1:10:08 - Semantic Kernel

1:12:32 - AutoGen

1:13:37 - Windows AI Studi
o

1:14:46 - Summary
```
---

     
 
MachineLearning -  [ [P] LLMinator: A Llama.cpp + Gradio based opensource Chatbot to run llms locally(cpu/cuda) directly  ](https://www.reddit.com/r/MachineLearning/comments/1cpbgd1/p_llminator_a_llamacpp_gradio_based_opensource/) , 2024-05-15-0910
```
Hi I am currently working on a context-aware streaming chatbot based on Llama.cpp, Gradio, Langchain, Transformers. LLMi
nator can pull LLMs directly from HF & run them locally on cuda or cpu.

I am looking for recommendations & help from op
ensource community to grow this further.

**Github Repo:** [https://github.com/Aesthisia/LLMinator](https://github.com/A
esthisia/LLMinator)

**Goal:** To help developers with kickstarter code/tool to run LLMs.

https://preview.redd.it/fnzja
7rjwqzc1.png?width=1846&format=png&auto=webp&s=a62c43614d63e82156fef8722b986b051cc1795b

**Features:**

* Context-aware 
Chatbot.
* Inbuilt code syntax highlighting.
* Load any LLM repo directly from HuggingFace.
* Supports both CPU & Cuda m
odes.
* Load & Offload saved models.
* Command Line Args
* API Access(Soon to be available)

Any review or feedback is a
ppreciated.
```
---

     
 
MachineLearning -  [ [D] Self-optimizing deterministic LLM memory using dspy, neo4j and vector databases. Need your input ](https://www.reddit.com/r/MachineLearning/comments/1cakjaf/d_selfoptimizing_deterministic_llm_memory_using/) , 2024-05-15-0910
```
Hey there, Redditors!

I'm back with the latest installment on creating deterministic LLM memory.

If you've been follow
ing along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://topoteretes.github.io/cognee/blog
/2023/10/05/going-beyond-langchain--weaviate-and-towards-a-production-ready-modern-data-platform/)' trend and tackle the
 challenges of building robust LLM memory.

  
That's why we built cognee, a python library to process documents and bui
ld knowledge graphs on top of them.

After a few weeks of work, we integrated DSPy and extended cognee.

Here is brief o
verview of the logic: 

[Architecture overview](https://preview.redd.it/fcs3lifx53wc1.png?width=1380&format=png&auto=web
p&s=9316cba52147a5b764565b8438f3f143d8e7ac84)

We aim to understand:

1. Have you tried building knowledge graphs with o
ther tools before?

2. If so, what were the biggest obstacles?

3. How would you approach semantic linking of documents 
without knowledge graphs?

*Remember to give this post an upvote if you found it insightful!*  
*And also star our* [Git
hub repo](https://github.com/topoteretes/cognee)
```
---

     
 
deeplearning -  [ Seeking Advice: Solving Data Challenges with Large Language Models (LLMs) ](https://www.reddit.com/r/deeplearning/comments/1ca4nc1/seeking_advice_solving_data_challenges_with_large/) , 2024-05-15-0910
```
Hi all

I am presented with a problem that I need to solve using LLM to get the right data from text that has only \~20%
 structure to it. Here's a sample data

XXXXX

AA          BB

CCCC:  (optional DDDD)

C1......(A1) (B1)

C2......(A2) (
B2)

C3.....(A3) (B3)

I am required to anwer with either of these results from A1/B1 till A3/B3 pairs but in order to d
o that I need to go back and ask the user which one of the options C1 to C3 applies to him?

The above is not the most c
omplex structure, it increases in complexity from here so a lot of chatting with user is required to get to the right da
ta that will always exist in the chunk like above.

In the most simplist case the data structure will look like below

X
XXXX

AA          BB

CCCC: ......(A1) (B1)



How would you build a system like this? I am looking at multi-agent syste
ms with Langchain, what about prompt chaining?
```
---

     
 
deeplearning -  [ Share the Coolest Out of The Box LLM Applications That Made You Say 'Wow that was smart' ](https://www.reddit.com/r/deeplearning/comments/1c9e6dj/share_the_coolest_out_of_the_box_llm_applications/) , 2024-05-15-0910
```
Hi, I'm looking at some LLM applications today but apart from guys doing big rags with langchain I don't see too many us
es that are out of the box or that make me say 'wow that was smart to use an LLM here'. Have you seen any cool stuff usi
ng LLMs recently that made you say 'wow, that was smart'?
```
---

     
