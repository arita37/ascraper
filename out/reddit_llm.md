 
all -  [ Langchain powered search ](https://www.reddit.com/r/LangChain/comments/1aojxqa/langchain_powered_search/) , 2024-02-12-0910
```
I'm looking into LangChain to power search on some inventory. I would like to get parameters for this search from the de
scription to serve the customer. However, the customer should be able to ask his search query in any possible way, but o
bviously the API in the backend can support only a limited set of inputs.   


Exactly how would this work? And what too
ls should I consider? The idea is not to provide a response in the chat, but to provide it like an inventory search.  
```
---

     
 
all -  [ Building AI Chatbot from scratch with Llama2, Langchain and Vector database using RAG workflow ](https://www.reddit.com/r/artificial/comments/1aohn26/building_ai_chatbot_from_scratch_with_llama2/) , 2024-02-12-0910
```
Pretty detailed one in case any one wants to build one 
https://youtu.be/V8329yuXHKM
```
---

     
 
all -  [ Libs and Best Practices for Rendering Chatbot Messages ](/r/reactjs/comments/1ao9ntn/libs_and_best_practices_for_rendering_chatbot/) , 2024-02-12-0910
```

```
---

     
 
all -  [ LangChain and .NET ](https://www.reddit.com/r/dotnet/comments/1aoea8o/langchain_and_net/) , 2024-02-12-0910
```
I'm actively working on developing a new set of AI features available in VS Code, as part of SnippetHub, and next up is 
improving the context for AI features.

Given that the infrastructure is mostly on .NET technologies, I'm trying to figu
re out the best way to integrate LangChain with .NET.   
Is there a NuGet package available?

If you've had the chance t
o integrate LangChain into your .NET API, I'd appreciate it if you could share what the challenges were and if you used 
a NuGet package
```
---

     
 
all -  [ Better context with LangChain and VS Code AI coding assistant ](https://www.reddit.com/r/artificial/comments/1aoe4q8/better_context_with_langchain_and_vs_code_ai/) , 2024-02-12-0910
```
I'm actively working on developing new features for the AI coding assistant, which already has a VS Code extension where
 all the features are available.

To improve context in coding assistance (like AI Chat, AI Lens, and similar), I'm cons
idering what the best option would be. I've read a lot about LangChain and see that it offers some cool options for bett
er context when AI features come into play.

Has anyone integrated LangChain and what are your experiences? 

It would b
e great if someone could share specific use-cases from production and feedback.
```
---

     
 
all -  [ AI Assisted Assistant ](https://www.reddit.com/r/LangChain/comments/1ao7wte/ai_assisted_assistant/) , 2024-02-12-0910
```
Hey everyone, I‚Äôm about to build a Assistant which takes input from user like name, email and some other attributes and 
then check in the database to verify if the info already there or not, but after that it‚Äôll ask some more follow up ques
tions to get input from user and verify more information in different format like pdf or jpeg.

Now I know it‚Äôs doable u
sing tools but exactly how? Because moving from one question to another question using API is getting boilerplate for me
, Is there any relevant or close enough resources available online to start? It‚Äôs just 1yr I‚Äôm into development so any h
elp will be appreciated.
```
---

     
 
all -  [ Finetuning Advice? ](https://www.reddit.com/r/LangChain/comments/1ao6rgk/finetuning_advice/) , 2024-02-12-0910
```
I am trying to finetune Llama2 7b on a single T4 GPU using Qlora, I found a lot of resources to help developing the pipe
line and everything is good, however I noticed something weird, everyone trying to finetune Llama2 7b using this method 
is setting max tokens(input+output) to **2048** although the maximum number of allowed tokens for Llama2 is **4092**, is
 the reason that long (inputs, outputs) pairs may need a bigger GPU? or I am missing something here.

I am asking this b
ecause my data contains long prompts and completions and I need to know if this effects how much GPU I'll need for finet
uning.
```
---

     
 
all -  [ Triform - Early Beta - Platform for Hosting and Orchestration of Python Focusing on LangChain ](https://www.reddit.com/r/LangChain/comments/1ao5p68/triform_early_beta_platform_for_hosting_and/) , 2024-02-12-0910
```
So we just opened up testing of a new platform called Triform. We have 400 registered users already, but still want more
 people to come in and check it out to be able to refine our product before we launch in production.

Anyone who signs u
p and creates at least one module and one flow in our testing system will get to keep a permanent free account on our pl
atform even in production.

Signup with your GitHub: [https://triform.ai](https://triform.ai)

Check out the readme at: 
[https://triform-docs.readthedocs.io/](https://triform-docs.readthedocs.io/)

Any and all feedback is appreciated!
```
---

     
 
all -  [ Multi-model architecture ](https://www.reddit.com/r/LangChain/comments/1ao2qn5/multimodel_architecture/) , 2024-02-12-0910
```
Does it make sense to ask several different llm and then consolidate the answers? If llm are trained on different data, 
it may help to get more diverse answers and improve results.
```
---

     
 
all -  [ Langchain RAG + CrewAI Agent? ](https://www.reddit.com/r/LangChain/comments/1anw9vt/langchain_rag_crewai_agent/) , 2024-02-12-0910
```
I seem to not be able to wrap my mind around it, but how do you feed a Langchain RAG implementation into an agent framew
ork like CrewAI? I can use both separately, but I'm not seeing how to combine the two... 
```
---

     
 
all -  [ Metadata retrieval + search ](https://www.reddit.com/r/LangChain/comments/1anuw6i/metadata_retrieval_search/) , 2024-02-12-0910
```
I am. Trying to implement a rag to discuss my data , I use chromadb and conversational retrievalchain. Let's say I uploa
d 20 documents,. 10 pdfs and 10 texts.  Everytime I aks a question to the ai I want in the answer to add the citation,  
also if I ask howamy pdf files are stored in the dB I want to know the number, I want also to. Ask find all the docs who
. Include this phrase and the ai should return me the filenames. To conclude, I want the ai to search not only the docs 
but the Metadata as well. Has anyone implemented something similar? Or has anything o suggest?

Thank you for your help 
üòä
```
---

     
 
all -  [ Just got a 4090 24GB, what should I run? ](https://www.reddit.com/r/LocalLLaMA/comments/1anu3m5/just_got_a_4090_24gb_what_should_i_run/) , 2024-02-12-0910
```
I‚Äôm upgrading from a 3080 where I messed around with small models.   Anything bigger and better I should try out?

Espec
ially things that would help manage projects or document tasks or assistant like duties, I didn‚Äôt get into langchain, et
c. before but maybe worthwhile now, or with more horsepower?

Or maybe something that makes meme pictures of dogs?
```
---

     
 
all -  [ So is this sub gonna change its name to langchain-community? ](https://www.reddit.com/r/LangChain/comments/1anlp6l/so_is_this_sub_gonna_change_its_name_to/) , 2024-02-12-0910
```
I suspect this means they are gearing up for a paid subscription release. I updated and see that ‚Äúlangchain‚Äù is deprecia
ted‚Ä¶ does this mean they are gearing up for a paid subscription version? *Microsoft Teams Classic has entered the chat *

```
---

     
 
all -  [ I don't understand....? ](https://www.reddit.com/r/LangChain/comments/1ankqzw/i_dont_understand/) , 2024-02-12-0910
```
Okay so when I'm looking at the documentation for langchain openapi toolkit, it is not making sense to me. 

I recreated
 this with my own API and yaml. I keep hitting the token limit because it feeds the entire yaml for reference. My tokens
 are 71k, just about the size of the one in the documentation. 

I would think the agent would use the documention as a 
resource, but not in sending the entire request. Maybe I'm not understanding this correctly, can someone please explain 
this to me? 

I get everything to run with all the authentication, but I keep hitting the token limit. What should I do?
 

https://python.langchain.com/docs/integrations/toolkits/openapi
```
---

     
 
all -  [ pinecone API ](https://www.reddit.com/r/StreamlitOfficial/comments/1anfqit/pinecone_api/) , 2024-02-12-0910
```
hello, I'm building a rag using langchain and Gemini, I'm stuck tho on the API part, how can I put my API online, I unde
rstand that you need to create a .env file for local deployment, but how exactly can I put my pinecone API online?
```
---

     
 
all -  [ Gemini + Google Search Agent Curious what you can build yourself in the @Google Gemini era? Spend a  ](https://www.reddit.com/r/chatgpt_newtech/comments/1anebts/gemini_google_search_agent_curious_what_you_can/) , 2024-02-12-0910
```
Gemini + Google Search Agent   Curious what you can build yourself in the  u/Google  Gemini era? Spend a few minutes wit
h LangChain  founding engineer  u/eyfriis  and quickly get up and running with a Gemini-powered agent that has access to
 Google Search  [http://aitutor21.com/bbs/board.php?bo\_table=news&wr\_id=2157](http://aitutor21.com/bbs/board.php?bo_ta
ble=news&wr_id=2157)
```
---

     
 
all -  [ Pipeline Vs Modular coding while creating LLM app ](https://www.reddit.com/r/TheLLMStack/comments/1ancqez/pipeline_vs_modular_coding_while_creating_llm_app/) , 2024-02-12-0910
```
Why every LLM framework is focusing more on creating pipeline based code structure to develop app? I can see langchain, 
llamaindex and haystack mostly focusing on developing app using pipelines rather then using individual modules. Is their
 particular advantage to this approach?
```
---

     
 
all -  [ How to fetch ID's in Pgvector ](https://www.reddit.com/r/LangChain/comments/1an9aw2/how_to_fetch_ids_in_pgvector/) , 2024-02-12-0910
```
Is there any way to  fetch ID's of a particular document stored in Postgres pgvector db to delete the particular documen
t embeddings.
```
---

     
 
all -  [ Langchain for QA ](https://www.reddit.com/r/LangChain/comments/1an90yi/langchain_for_qa/) , 2024-02-12-0910
```
Hello, everyone I created a python flask api for document chatbot.

Embedding with open ai and for Q&A Langchain Faiss i
s used.

Its take 2 sec to get the answer from chain I test this by adding time on evey step.

is there any way to make 
it fast? or other options.?

Its there any way i can make it fast?
```
---

     
 
all -  [ Data Insights suggestion using langchain ](https://www.reddit.com/r/LangChain/comments/1an371k/data_insights_suggestion_using_langchain/) , 2024-02-12-0910
```
 I'm facing a problem with Google Sheets while trying to analyze a dataset with over 100 records. It seems there's a tok
en issue preventing me from generating plots and insights effectively.

I'm using lanchains google drive library get the
 files and feeding them as docs and asking fixed question in  **response = chain.run(input\_documents=doc\_list, questio
n=prompt)**

Any quick fixes or alternative tools you recommend for handling large datasets?  


  
below is my code,  



 

from langchain import OpenAI  
import pandas as pd  
from langchain.sql\_database import SQLDatabase  
from langcha
in.chat\_models import ChatOpenAI  
from langchain.agents.agent\_toolkits import SQLDatabaseToolkit  
from langchain.age
nts import create\_sql\_agent,Tool,AgentType,initialize\_agent,create\_pandas\_dataframe\_agent  
from langchain.memory 
import ConversationBufferMemory  
import psycopg2  
import pymysql  
\# Setting up the api key  
import environ  
from l
angchain.document\_loaders import GoogleDriveLoader  
import os  
from langchain.llms import OpenAI  
from langchain.cha
ins.summarize import load\_summarize\_chain  
from langchain.chat\_models import ChatOpenAI  
import environ  
from lang
chain.chains.question\_answering import load\_qa\_chain  
env = environ.Env()  
environ.Env.read\_env()  
import json  

API\_KEY = env('apikey')  
   
def query\_agent(query):  
 '''  
Query an agent and return the response as a string.  
A
rgs:  
agent: The agent to query.  
query: The query to ask the agent.  
Returns:  
The response from the agent as a str
ing.  
'''  
 loader = GoogleDriveLoader(folder\_id='1YXa\_GCtVPu1MzK2tiW8J21CsNITHqKwl',  
 file\_types=\['sheet'\],  

 credentials\_path='credentials.json')  
 docs = loader.load()  
 llm = ChatOpenAI(temperature=0.6, openai\_api\_key=API
\_KEY,model='gpt-3.5-turbo-16k')  
 chain = load\_qa\_chain(llm, chain\_type='stuff')  
   
   
 titles = \[\] ¬†# Initia
lize an empty list to store titles  
 for doc in docs:  
 if doc.metadata is not None and 'title' in doc.metadata:  
 ti
tle = doc.metadata\['title'\]  
 if title not in titles:  
 titles.append(title) ¬†# Append the title to the list  
\#pri
nt(titles)  
 whole\_response = ''  
 google\_docs = \[\]  
 for title in titles:  
 print(f'---------------------------
-------------TITLE: {title}----------------------------------------')  
 doc\_list = \[\]  
 for doc in docs:  
 if doc.
metadata is not None and doc.metadata\['title'\] == title:  
 doc\_list.append(doc)  
 prompt = (  
 '''  
You are an AI
 assistant acting as a data analyst, you are presenting results to a dashboard.  
Please provide details about the data 
that is available in the following format:  
   
I Want the response only in below format for the given dataset:  
a pyt
hon dictionary with the following keys:  
{  
dataset\_name: will provide the Name of the dataset,  
data\_description: 
will provide a Description of the data,  
possible\_insights: \[will provide a python list of 3 possible meaningful bar 
or line plots that can be achieved from the data fields present in this data\], make sure you only suggest plots based u
pon fields available in this dataset.  
}  
'''   
)  
 response = chain.run(input\_documents=doc\_list, question=prompt
)  
   
   
 \# print('#############'+title)  
 \# print(response.\_\_str\_\_())  
 \# print('##############')  
 whole\
_response += '\\n' + response  
   
 \#filter  
 data\_dict = json.loads(response)  
 \#print(data\_dict)  
 data\_name 
= data\_dict\['dataset\_name'\]  
 data\_description = data\_dict\['data\_description'\]  
 possible\_insights = data\_d
ict\['possible\_insights'\]  
   
 \#prompt  
 for i in range(3):  
 plot\_prompt = (  
 '''  
You are an AI assistant a
cting as a data analyst, you are presenting results to a dashboard.  
you have the following data available - {name}, a 
little more about the data - {description}.  
based upon the data available, return information from data required to pl
ot the following - {plot}  
If the query requires creating a bar chart, reply as following string:  
 {{'bar': {{'column
s': \['A', 'B', 'C'\], 'data': \[\[valueA, valueB, valueC\], \[valueA, ValueB, valueC\]\]}}}}  
   
If the query require
s creating a line chart, reply as following string:  
 {{'line': {{'columns': \['A', 'B', 'C'\], 'data': \[\[valueA, val
ueB, valueC\], \[valueA, ValueB, valueC\]\]}}}}  
make sure to replace the columns and data with the actual data (from t
he provided data only) required to plot the chart, do all the required data processing like grouping data,aggregating ba
sed on condition for bar plot, and return response in the mentioned way only.  
don't forget to put all the strings in d
ouble quotes.  
'''.format(name=data\_name, description=data\_description, plot=possible\_insights\[i\])  
)  
 \#print(
f'##########################################PLOT PROMPT: {plot\_prompt}##########################################')  
 p
lot\_response = chain.run(input\_documents=doc\_list, question=plot\_prompt)  
 print(plot\_response.\_\_str\_\_() + '##
########################################')  


\# print('#############')  
 \# print(whole\_response)  
   
 \# # Conver
t the response to a string.  
 \# return response.\_\_str\_\_()
```
---

     
 
all -  [ Creating knowledge graph using local large language models ](https://www.reddit.com/r/LangChain/comments/1an0we9/creating_knowledge_graph_using_local_large/) , 2024-02-12-0910
```
I am currently trying to implement knowledge graph store using llm in my company. I followed this post using open AI [ht
tps://bratanic-tomaz.medium.com/constructing-knowledge-graphs-from-text-using-openai-functions-096a6d010c17](https://bra
tanic-tomaz.medium.com/constructing-knowledge-graphs-from-text-using-openai-functions-096a6d010c17). which really worked
 using open AI api. But when i am trying to use any other model from my local like Mistral 7B quantized or LLama2 quanti
zed version. I am getting below error **'OutputParserException: This output parser can only be used with a chat generati
on.'** I am passing the same schema as mentioned in the above code. I tried multiple way, tried using output parser like
 create\_extraction\_chain\_pydantic or create\_extraction\_chain. but still doesnt work. Can someone please provide any
 ideas or solution. If someone faced the same issue. this is the code and the bold line is  where i am facing the error.


 def extract\_and\_store\_graph(  
document: Document,  
nodes:Optional\[List\[str\]\] = None,  
rels:Optional\[List\[
str\]\]=None) -> None:  
 \# Extract graph data using OpenAI functions  
extract\_chain = get\_extraction\_chain(nodes, 
rels)  
**data = extract\_chain.run(document.page\_content)**
```
---

     
 
all -  [ Data Access for LLMs is broken. Thoughts? ](https://www.reddit.com/r/LangChain/comments/1amy5yu/data_access_for_llms_is_broken_thoughts/) , 2024-02-12-0910
```
Hey  all. To build truly useful GenAI apps, LLMs need to be able to access  propriety data from structured and unstructu
red data sources. LLMs  struggle to understand the availability, location, and methods to  retrieve relevant data.

Key 
problems:

1. LLMs can‚Äôt identify if the necessary data is available to answer a user query in any of the data sources.

2. If the data is available, LLMs can‚Äôt locate which data source to retrieve from.
3. If  the source is known, writing r
etrieval pipelines  (text-to-SQL, text-to-Cypher, text-to-JSON, etc.) for the numerous retrieval protocols is complicate
d, repetitive, and non-deterministic.

Some suggest fine-tuning or RAG as potential solutions but:

1. Fine-tuning  mode
ls with specific data is expensive, becomes outdated with the  latest data, and has in-built access control issues. Cont
inuous  retraining is costly and doesn‚Äôt keep pace with real-time data changes.
2. RAG  doesn‚Äôt work with structured dat
a. Most useful data is structured and  can be spread across numerous data sources, each with its own querying  mechanism
.
3. Writing individual retrieval pipelines are limited and complicated and don‚Äôt ensure determinism, security, and acce
ss control.

Have  you faced these issues in your projects? What workarounds or solutions  have you found effective? Any
 new tools or practices that simplify  integrating LLMs with diverse data sources?
```
---

     
 
all -  [ Self query question ](https://www.reddit.com/r/LangChain/comments/1amxvpd/self_query_question/) , 2024-02-12-0910
```
I‚Äôve been reading through everything i can find regarding self query (and also tinkering with it myself), and from what 
i can tell, i am required to use vectorStore. 

Am i misunderstanding the purpose or capabilities of the self query? 

I
d rather not update all my documents every time i query using the metadata as a filter. That strikes me as a waste of em
bedding and speed.

I‚Äôm hoping someone here can point to a misunderstanding i‚Äôm having, because i‚Äôm stumped at the momen
t. The only way i‚Äôve been successful in using this is following it exactly as the documentation shows.

For reference, i
m using pinecone/js, here‚Äôs the documentation: 

https://js.langchain.com/docs/modules/data_connection/retrievers/self_q
uery/pinecone-self-query
```
---

     
 
all -  [ How do I store the message output in a .txt in langchain? (not as Json) ](https://www.reddit.com/r/LangChain/comments/1amvuc2/how_do_i_store_the_message_output_in_a_txt_in/) , 2024-02-12-0910
```
Does Langchain have any specific module? I am using Flowise gui with langchain and try to store message output in a txt,
 not the whole conversation, only the replies from the web api.
```
---

     
 
all -  [ Getting access to all the previous AgentActions in a tool. ](https://www.reddit.com/r/LangChain/comments/1amuf4y/getting_access_to_all_the_previous_agentactions/) , 2024-02-12-0910
```
Let's say I have a summarization tool that is supposed to summarize the output from different previous tools (like Wikip
edia, Google Search whatever) and I want to get the results from all the previous tools in this tool. How do I get this?

```
---

     
 
all -  [ SENIOR ML Engineer - 100% Remote (+ every other friday off!) ](https://www.reddit.com/r/MachineLearningJobs/comments/1amn92z/senior_ml_engineer_100_remote_every_other_friday/) , 2024-02-12-0910
```
APPLY HERE: [https://grnh.se/50c178c17us](https://grnh.se/50c178c17us)

Our Machine Learning teams build with the latest
 tools and launch for Silicon Valley startups and Life Science giants alike.

Join our team to work remotely, ship proje
cts you‚Äôre proud be a part of, and enjoy every other Friday off (yes, we really take it üòé)

**The Role:**

* Understand 
business objectives and develop models that help achieve them, along with metrics to track their progress
* Design and i
mplement complex ML systems using classical ML, DL and Foundation Models by following best practices
* Lead client commu
nications gathering requirements, managing expectations, and communicating deliverables
* Wrangle, explore and visualize
 data with a careful eye for issues that require data cleaning as well as differences in data distribution that may affe
ct performance when deploying the model in the real world
* Analyze the errors of the model and design strategies to ove
rcome them
* Deploy, maintain, and upgrade ML models and pipelines.

**The Benefits:**

* Every other Friday off (26 ext
ra days off a year)
* LokaLabs‚Ñ¢ incubator
* Relocation & Explore program (3 months or full relo)
* Remote and flexible
*
 Paid sick days and local holidays
* Fitness subscriptions and more

**Main Requirements**:

* Bachelor‚Äôs Degree in Comp
uter Science or related
* 4+ years of experience with Python, ML libraries and AI/ML frameworks (PyTorch, HuggingFace, T
ensorFlow, Keras, Scikit-learn, Spark etc.)
* Strong understanding of statistical, ML and deep learning algorithms (cand
idates with NLP experience preferred)
* Experience building GenAI solutions, namely prompt engineering (e.g: Langchain),
 fine-tuning and serving LLMs, search and embeddings, etc.
* Experience with MLOps, favorably in AWS (e.g: Sagemaker) as
 well as standards tools (e.g: MLFlow)
* Experience visualizing and manipulating big datasets
* Ambition to learn and gr
ow into different industries with a modern tech stack
* Autonomy, adaptability and positivity (fully remote and globally
 distributed team)
* Proficient in English
```
---

     
 
all -  [ Text-to-SQL with extremely complex schema ](https://www.reddit.com/r/LangChain/comments/1amlftk/texttosql_with_extremely_complex_schema/) , 2024-02-12-0910
```
I am developing a text-to-sql project with llms  and sql server. where user will ask question in natural language and ll
ms will wrtie sql query, run it on my database and then give me result in natural language. The problem is schema of dat
abase is huge and tables names,column names are not self explanatory. Most of the times two tables need to joined on mor
e than one column and in where condition I consistanly want to have some conditions and daterange condition is extremely
 important as well because without date condition, the user might get data that he's not expected to have access to. is 
there any way to solve this problem? I have tried using views but that is computationally expensive and takes a lot of t
ime to execute as well. is there any other way?  
```
---

     
 
all -  [ How to tackle token limit issue with RAG ](https://www.reddit.com/r/LangChain/comments/1amktb2/how_to_tackle_token_limit_issue_with_rag/) , 2024-02-12-0910
```
I am running a RAG with GPT 3.5 Turbo Instruct. But since the context is large it is having issues with the token limit.
 How to deal with this?
```
---

     
 
all -  [ RAG does not stop Hallucinations ](https://www.reddit.com/r/LangChain/comments/1amjc9g/rag_does_not_stop_hallucinations/) , 2024-02-12-0910
```
Hi,

I have a RAG app but noticed that the RAG Method does not really stop hallucinations. So I wanted to discuss which 
prompts or techniques you are using so that your LLM is not hallucinating?

&#x200B;

Here is one Prompt of mine:mixtral
\_prompt = '''

<s> \[INST\] You are RagBot, a helpful assistant. Answer only in German. Use the following context infor
mation to answer the user's question briefly and concisely at the end. The contextual information comes from a vector se
arch, which means that not everything in the context is necessarily relevant. If the user gives you an instruction, foll
ow it (e.g. write the points in a continuous text). If you don't know the answer, just say you don't know. Don't make up
 an answer! It is important that you also do not add points to your answer that are not included in the context. If the 
user asks general questions, make small talk with them. Also make sure that you do not repeat your answer at the end!

&
#x200B;

\###Context to the question###:

{context}

&#x200B;

&#x200B;

\###Question of the user###:

{question}

&#x20
0B;

&#x200B;

Answer: \[/INST\]

'''

&#x200B;

Here are my model settings:

    llm = LlamaCpp(
    max_tokens = 1200,

    n_threads = 8
    model_path=model_path,
    temperature=0.01,
    f16_kv=True,
    n_ctx=28000,
    n_gpu_layers=1
,
    n_batch=512,
    callback_manager=callback_manager,
    verbose=True, # Verbose is required to pass to the callbac
k manager
    top_p= 0.95,
    top_k=40,
    repeat_penalty = 1.2,
    streaming=True,
    model_kwargs={
    #'repetiti
on_penalty': 1.1,
    #'mirostat': 2,
    },
    )
```
---

     
 
all -  [ Langchain LlamaCpp max_tokens or max_new_tokens? ](https://www.reddit.com/r/LangChain/comments/1amiy53/langchain_llamacpp_max_tokens_or_max_new_tokens/) , 2024-02-12-0910
```
Hi,

I am using LlamaCpp and saw different names for the maximum tokens parameter.

To set the maximum tokens, do I have
 to set a value to max\_tokens or to max\_new\_tokens? What is the difference here?

&#x200B;

Also if someone has bette
r experience with some parameters (specifically for RAG) feel free to suggest them. I am using these:

`llm = LlamaCpp(`
  
 `max_tokens = 1200,`  
 `n_threads = 8`  
 `model_path=model_path,`  
 `temperature=0.01,`  
 `f16_kv=True,`  
 `n_c
tx=28000,`  
 `n_gpu_layers=1,`  
 `n_batch=512,`  
 `callback_manager=callback_manager,`   
 `verbose=True, # Verbose i
s required to pass to the callback manager`  
 `top_p= 0.95,`  
 `top_k=40,`  
 `repeat_penalty = 1.2,`  
 `streaming=Tr
ue,`  
 `model_kwargs={`  
 `#'repetition_penalty': 1.1,`  
 `#'mirostat': 2,`  
`},`  
`)`
```
---

     
 
all -  [ Two LLM models ](https://www.reddit.com/r/LangChain/comments/1amitx2/two_llm_models/) , 2024-02-12-0910
```
Hello!  
I am building an application which main point is to help users improve the code of loaded projects.  I've build
 some pandas-based tools as well as vdb retriever tool.   
I am wondering if I could use one model (gpt-3.5-\*) to execu
te the tools and the other (gpt-4) to answer the questions? Like, the goal is for gpt-3.5 to gather information related 
to usr input, and gpt-4 to provide answer based on results found by the former.   
Any help would be appreciated, thanks
! 
```
---

     
 
all -  [ Need help working with SQL And LangChain. ](https://www.reddit.com/r/LangChain/comments/1amdj3k/need_help_working_with_sql_and_langchain/) , 2024-02-12-0910
```
Hi,  
I want to use the database as a knowledge base but don't want LangChain to execute queries on the database because
 it seems unsafe. Is there any way to convert the SQL data and store it in vectorDB for efficient RAG?
```
---

     
 
all -  [ Consigli su CV ](https://www.reddit.com/r/ItaliaCareerAdvice/comments/1am87uk/consigli_su_cv/) , 2024-02-12-0910
```
Ciao ragazzi, al momento sto cercando lavoro come data scientist/AI specialist, e questo √® il mio CV. Su 10 apply nell'u
ltimo mese, ho ricevuto solo due risposte, una positiva (fingers crossed per il colloquio mercoled√¨) e una negativa. Com
incio a pensare che ci sia qualcosa di sbagliato, ad esempio, forse mi sembra un po' troppo verboso, e un po' dispersivo
 forse? Avete consigli per migliorarlo? In pi√π, il fatto che siano due pagine effettivamente forse non va bene. Conviene
 forse che io rimuova esperienze relativamente poco rilevanti?

Ve lo lascio anonimizzato qui. Immaginate che come titol
o della prima pagina ci sia Nome Cognome. Grazie!

https://preview.redd.it/g1wqyhwwtfhc1.png?width=805&format=png&auto=w
ebp&s=f0fd4d9fa1d2d82f79af93a54dbf90de5c22b72e

https://preview.redd.it/21qcciwwtfhc1.png?width=805&format=png&auto=webp
&s=31459881cc03968353f1aaf698082f4856486427
```
---

     
 
all -  [ Summarizing past messages in an RAG conversation - is it always recommended? ](https://www.reddit.com/r/LangChain/comments/1am5nsd/summarizing_past_messages_in_an_rag_conversation/) , 2024-02-12-0910
```
Is there a consensus in terms of the quality of the AI response, between keeping the chat history in the memory as is, o
r summarizing it using ConversationSummaryMemory? 

I understand that summarizing past messages will lead to fewer token
s being used, but does it also lead to a drop in the quality of the AI answer in an RAG model, considering that the summ
ary may not necessarily include all the facts of the past messages? 

Common sense would say that yes, that may lead to 
worse answers, but wondering how the community feels about this topic.
```
---

     
 
all -  [ Data Science and Machine Learning Books Recommendation Chatbot ](https://www.reddit.com/r/learndatascience/comments/1am2nxa/data_science_and_machine_learning_books/) , 2024-02-12-0910
```
  

Hi Redditors,

I would like to share with you all my latest project: Step by step tutorial on how to create a chatbo
t that recommends Data Science and Machine Learning Books using LLM (Large Language Models), langchain and Streamlit.

T
he chatbot is trained on sample conversations and a dataset of books on Data Science and Machine Learning. The chatbot i
s able to understand the user‚Äôs intent and extract relevant entities from the user‚Äôs message.

It then uses this informa
tion to search for the best matching book in the dataset and recommends it to the user. The chatbot is also able to hand
le out-of-scope queries gracefully.

* You can find the step by step guide [here](https://medium.com/data-and-beyond/dat
a-science-and-machine-learning-books-recommendation-chatbot-83757cbb92f7)
* Link to the demo on Hugging Face Spaces is [
here](https://huggingface.co/spaces/Marce82/ML-DataScience-Books-Recomender-chatbot)
* Github repo [here](https://github
.com/marcello-calabrese/Machine-Learning-and-Data-Science-Books-recommender-Chatbot)

Happy to hear your comments, feedb
ack.

Cheers

# 
```
---

     
 
all -  [ Data Science and Machine Learning Books Recommendation Chatbot ](https://www.reddit.com/r/learnmachinelearning/comments/1am2mi4/data_science_and_machine_learning_books/) , 2024-02-12-0910
```
Hi Redditors,

I would like to share with you all my latest project: Step by step tutorial on how to create a chatbot th
at recommends Data Science and Machine Learning Books using LLM (Large Language Models), langchain and Streamlit.

The c
hatbot is trained on sample conversations and a dataset of books on Data Science and Machine Learning. The chatbot is ab
le to understand the user‚Äôs intent and extract relevant entities from the user‚Äôs message.

It then uses this information
 to search for the best matching book in the dataset and recommends it to the user. The chatbot is also able to handle o
ut-of-scope queries gracefully.

* You can find the step by step guide [here](https://medium.com/data-and-beyond/data-sc
ience-and-machine-learning-books-recommendation-chatbot-83757cbb92f7)
* Link to the demo on Hugging Face Spaces is [here
](https://huggingface.co/spaces/Marce82/ML-DataScience-Books-Recomender-chatbot)
* Github repo [here](https://github.com
/marcello-calabrese/Machine-Learning-and-Data-Science-Books-recommender-Chatbot)

Happy to hear your comments, feedback.


Cheers
```
---

     
 
all -  [ Data Science and Machine Learning Books Recommendation Chatbot Project ](https://www.reddit.com/r/datascienceproject/comments/1am2l84/data_science_and_machine_learning_books/) , 2024-02-12-0910
```
Hi Redditors, 

&#x200B;

I would like to share with you all my latest project:  Step by step tutorial on how to create 
a chatbot that recommends Data Science and Machine Learning Books using LLM (Large Language Models), langchain and Strea
mlit. 

The chatbot is trained on sample conversations and a dataset of books on Data Science and Machine Learning. The 
chatbot is able to understand the user‚Äôs intent and extract relevant entities from the user‚Äôs message.

It then uses thi
s information to search for the best matching book in the dataset and recommends it to the user. The chatbot is also abl
e to handle out-of-scope queries gracefully.

&#x200B;

* You can find the step by step guide [here](https://medium.com/
data-and-beyond/data-science-and-machine-learning-books-recommendation-chatbot-83757cbb92f7)
*  Link to the demo on Hugg
ing Face Spaces is [here](https://huggingface.co/spaces/Marce82/ML-DataScience-Books-Recomender-chatbot)
* Github repo [
here](https://github.com/marcello-calabrese/Machine-Learning-and-Data-Science-Books-recommender-Chatbot)

Happy to hear 
your comments, feedback.

&#x200B;

Cheers
```
---

     
 
all -  [ review of 10 ways to run LLMs locally ](https://www.reddit.com/r/LocalLLaMA/comments/1am0p48/review_of_10_ways_to_run_llms_locally/) , 2024-02-12-0910
```
Hey LocalLLaMA,

\[EDIT\] - thanks for all the awesome additions and feedback everyone! I'm going to go through them, tr
y them out, and add them to my blog post. It's gonna take some time though since I need to try them first, and maybe cha
nge my conclusions a bit.

I reviewed [ten different ways to run LLMs locally](https://matilabs.ai/2024/02/07/10-ways-to
-run-llms-locally-and-which-one-works-best-for-you/), and compared the different tools. Many of the tools had been share
d right here on this sub. Here are the tools I tried:

&#x200B;

1. Ollama
2. ü§ó Transformers
3. Langchain
4. llama.cpp
5
. GPT4All
6. LM Studio
7. jan.ai
8. llm ([https://llm.datasette.io/en/stable/](https://llm.datasette.io/en/stable/) \- l
ink if hard to google)
9. h2oGPT
10. localllm

My quick conclusions:

* If you are looking to develop an AI application,
 and you have a Mac or Linux machine, **Ollama** is great because it's very easy to set up, easy to work with, and fast.

* If you are looking to chat locally with documents, **GPT4All** is the best out of the box solution that is also easy 
to set up
* If you are looking for advanced control and insight into neural networks and machine learning, as well as th
e widest range of model support, you should try **transformers**
* In terms of speed, I think **Ollama** or **llama.cpp*
* are both very fast
* If you are looking to work with a CLI tool, **llm** is clean and easy to set up
* If you want to 
use Google Cloud, you should look into **localllm**

I found that different tools are intended for different purposes, s
o I summarized how they differ into a table:

&#x200B;

[Comparison of Local LLM Tools](https://preview.redd.it/9mqaz6me
9ehc1.png?width=2048&format=png&auto=webp&s=77ac88be51e473862c70625d2d62cfd13eeb6eb0)

I'd love to hear what the communi
ty thinks. How many of these have you tried, and which ones do you like? Are there more I should add?

Thanks!
```
---

     
 
all -  [ I Made an Open-Source Pinecone DB AWS Construct üèóÔ∏è ](https://www.reddit.com/r/LangChain/comments/1aly0xv/i_made_an_opensource_pinecone_db_aws_construct/) , 2024-02-12-0910
```
Managing Pinecone deployments is a thing of the past!!! üíÉ

ü•áSome noteworthy features ü•á

1. Handles CRUDs for both Pod an
d Serverless Spec indexes
2. Deploy multiple indexes at the same time with isolated state management
3. Adheres to AWS-d
efined removal policies (DESTROY, SNAPSHOT, etc.)
4. Creates stack-scoped index names, to avoid name collisions üôå

**It'
s still in beta, so feedback is more than welcome! ü´∂**

[Github](https://github.com/petterle-endeavors/pinecone-db-const
ruct)  
[PyPi](https://pypi.org/project/pinecone-db-construct/)  
[NPM](https://www.npmjs.com/package/pinecone-db-constr
uct)
```
---

     
 
all -  [ RAG Accuracy ](https://www.reddit.com/r/LangChain/comments/1alus6s/rag_accuracy/) , 2024-02-12-0910
```
I am creating a RAG program where I used 20 pdfs which contains lease agreement of different tenants.

I am using langch
ain framework to work with FAISS and openAI Embeddings.

I am using  `text-embedding-ada-002` model because I think lang
chain currently does not support v3 embedding models. I get this error while using v3 models: `model not found. Using cl
100k_base encoding.`

Here's my process:

\- I read the directory containing pdfs (Some pdf are image based, some are te
xtual).

\- I use `from unstructured.partition.pdf import partition_pdf` to extract data based on title. Below id code b
lock:

    raw_pdf_elements = partition_pdf(filename=pdf_path,
                # Unstructured first finds embedded image
 blocks
                extract_images_in_pdf=True,
                #Use layout model (YOLOX) to get bounding boxes (for
 tables) and find titles
                # Titles are any sub-section of the document
                infer_table_struct
ure=True,
                
                # Post processing to aggregate text once we have the title
                ch
unking_strategy='by_title',
                
                # Chunking params to aggregate text blocks
                
# Attempt to create a new chunk 3800 chars
                # Attempt to keep chunks > 2000 chars
                max_cha
racters=4000,
                new_after_n_chars=3800,
                combine_text_under_n_chars=2000,
                i
mage_output_dir_path=path
                )

\- Then I create documents from the extracted text along with the tenant na
me in metadata (To get information from specific tenant's lease agreement when I mention it's name in the question). 1 p
df at a time.

\- After I get document chunks from all pdfs, I create list of all the chunks.

\- I create FAISS index f
or that data.

\- Then I perform QnA.

&#x200B;

The problem is the accuracy I get from this is very low. For most of th
e question, semantic search cannot even find relevant context.

I want to know if the accuracy of RAG is generally lower
 than the expectations or am I doing something wrong?

What process do you follow for better results?
```
---

     
 
MachineLearning -  [ [D] What's the best current RAG setup that would work with a local LLM? ](https://www.reddit.com/r/MachineLearning/comments/1ag6bo7/d_whats_the_best_current_rag_setup_that_would/) , 2024-02-12-0910
```
I've tried things like langchain in the past (6-8 months ago) but they were cumbersome and didn't work as expected.

I  
need RAG to get data from various pdfs (long one, 150+ pages) - and i  need a setup that will allow me to add more and m
ore data sources.

I wanna run this locally, can get a 24gb video card (or 2x16gb ones) - so i can run using 33b or smal
ler models.

I  know things in the industry change every 2 weeks, so i'm hoping there's  an easy and efficient way of do
ing RAG (compared to 6 months ago)
```
---

     
 
MachineLearning -  [ [P]: Anukool: My job hunting assistant ](https://www.reddit.com/r/MachineLearning/comments/1adu3tw/p_anukool_my_job_hunting_assistant/) , 2024-02-12-0910
```
Hey Reddit, I've been applying for jobs and found that writing a cover letter for each position was tedious. I also delv
ed into LLM and Langchain, hoping to leverage them for a project to aid in my job hunting. So, I developed Anukool under
 the GPL license. While it's far from perfect, it has proven very useful to me, and I hope it benefits you as well. All 
I have to do is provide it with a pdf containing information about me such as my experience, skills, projects, etc and i
t will use this information along with job description to generate cover letter for me. Since I'm new to ML and LLM, any
 advice or feedback is greatly appreciated, and contributions are also welcome. I plan to utilize Llama-2 soon to furthe
r open-source the project.

Check out the GitHub link, and please star it if you find the project interesting: https://g
ithub.com/dakshesh14/anukool
```
---

     
 
MachineLearning -  [ New Data API for Astra [N] ](https://www.reddit.com/r/MachineLearning/comments/199uobn/new_data_api_for_astra_n/) , 2024-02-12-0910
```
I saw that DataStax/Astra DB [just released a new Data API to help with building production GenAI and RAG applications](
https://www.datastax.com/blog/general-availability-data-api-for-enhanced-developer-experience). This API makes the prove
n petabyte-scale of Apache Cassandra easy to use and available to any JavaScript, Python, or full-stack application deve
loper.

There will also be a joint webinar with LangChain available for registration here: [https://www.datastax.com/eve
nts/wikichat-build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel](https://www.datastax.com/events/wikichat-
build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel)
```
---

     
 
MachineLearning -  [ [D] While using function calling or tools on openai or langchain, does openai have access to the dat ](https://www.reddit.com/r/MachineLearning/comments/199t8be/d_while_using_function_calling_or_tools_on_openai/) , 2024-02-12-0910
```
I am working on a client project and I am using langchain's tools and agents. I want to know if the data is getting pass
ed to openai or is it just like that - Output of one function is being directly passed to the second function with the k
nowledge of openai.
```
---

     
 
MachineLearning -  [ [D] Code vs JSON output for LLM agents? Frameworks like LangChain rely on LLMs responding with JSON  ](https://www.reddit.com/r/MachineLearning/comments/197f416/d_code_vs_json_output_for_llm_agents_frameworks/) , 2024-02-12-0910
```
[CaP](https://arxiv.org/pdf/2209.07753.pdf), [Voyager](https://arxiv.org/pdf/2305.16291.pdf), [Octopus](https://arxiv.or
g/abs/2310.08588)

I work primarily with JSON based agents but code-as-policy agents seem to be extremely powerful. Here
 are some of the benefits and weaknesses I've seen

Pros of code

1. Less tool creation needed - The prebuilt math/file/
string/list manipulation abilities that come with code are enormous. In a JSON based agent, you would have to formally d
eclare each of these as a tool which you expose to the LLM and explain in your prompting, which is a lot of work and eat
s up a ton of the context window. 
2. Reduced number of transactions - The LLM can write scripts that invoke multiple to
ols and manipulate their results in ways that are difficult to do in a single transaction via JSON. For example, in one 
script, the model could search a DB 3 times, perform regex on the query results, convert them to integers, and add them 
up. Doing this in one step via JSON tool invocations is basically impossible. 
3. Less syntax errors - this might be tot
ally just vibe-based reasoning, but it really seems like LLMs have an easier time writing valid python than valid JSON, 
especially when you have lots of nested arguments in your methods.

Cons

1. Crazy risky - This is the obvious one. You 
have a machine executing random code. There are ways to mitigate this but still. I mean seriously we all learned not to 
use eval, so it is crazy to basically see research tending towards just running eval on the outputs of these models. 
2.
 Scripts with errors - Sometimes the model tries to get too fancy and writes complex programs that have bugs, resulting 
in many needed retries. 

Do any of you have thoughts or experience with these approaches in the wild? 

Is anybody awar
e of any experiments that compare these two approaches against each other? 

&#x200B;
```
---

     
 
deeplearning -  [ [D] WebVoyager: Navigating Digital Cosmos with LangGraph & Multimodal Models ](https://www.reddit.com/r/deeplearning/comments/1altlca/d_webvoyager_navigating_digital_cosmos_with/) , 2024-02-12-0910
```
Embark on a journey through the digital cosmos with WebVoyager, a groundbreaking Large Multimodal Model (LMM) web agent 
designed to navigate the vastness of the online universe. In collaboration with Langchain, WebVoyager represents a parad
igm shift in autonomous web agents, seamlessly integrating visual and textual information to complete user instructions 
end-to-end by interacting with real-world websites.

Link: [https://medium.com/@andysingal/webvoyager-navigating-digital
-cosmos-with-langgraph-multimodal-models-dace64196c2f](https://medium.com/@andysingal/webvoyager-navigating-digital-cosm
os-with-langgraph-multimodal-models-dace64196c2f)
```
---

     
 
deeplearning -  [ [D] Langchain Elevates with Step-Back Prompting using RAGatouille ](https://www.reddit.com/r/deeplearning/comments/1agtyeh/d_langchain_elevates_with_stepback_prompting/) , 2024-02-12-0910
```
In the dynamic realm of natural language processing, a revolutionary synergy has emerged between Langchain and Step-Back
 Prompting. This article delves into the transformative collaboration, exploring how Langchain‚Äôs cutting-edge platform i
ncorporates Step-Back Prompting to redefine language processing capabilities. Join us on a journey of innovation and dis
covery as we unravel the intricacies of this powerful integration. As we explore the uncharted territories of language m
odels, Step-Back Prompting stands as a beacon of progress, promising a journey of nuanced understanding and elevated per
formance in the world of Large Language Models. Welcome to the future of language processing, where inspiration and inno
vation converge in a symphony of words and ideas.

Link: https://medium.com/ai-advances/langchain-elevates-with-step-bac
k-prompting-using-ragatouille-b433e6f200ea
```
---

     
 
deeplearning -  [ Become an AI Developer (Free 9 Part Series) ](https://www.reddit.com/r/deeplearning/comments/1afgp2r/become_an_ai_developer_free_9_part_series/) , 2024-02-12-0910
```
Just sharing a free series I stumbled across on Linkedin - DataCamp's 9-part AI code-along series.

This specific sessio
n linked below is 'Building Chatbots with OpenAI API and Pinecone' but there are 8 others to have a look at and code alo
ng to.

*Start from basics to build on skills with GPT, Pinecone and LangChain to create a chatbot that answers question
s about research papers. Make use of retrieval augmented generation, and learn how to combine this with conversational m
emory to hold a conversation with the chatbot. Code Along on DataCamp Workspace:* [*https://www.datacamp.com/code-along/
building-chatbots-openai-api-pinecone*](https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone)

Find
 all of the sessions at: [https://www.datacamp.com/ai-code-alongs](https://www.datacamp.com/ai-code-alongs)
```
---

     
 
deeplearning -  [ DSPy Explained! ](https://www.reddit.com/r/deeplearning/comments/1adypks/dspy_explained/) , 2024-02-12-0910
```
DSPy is the next big advancement for AI and building applications with LLMs!

Pioneered by frameworks such as LangChain 
and LlamaIndex, we can build much more powerful systems by chaining together LLM calls! This means that the output of on
e call to an LLM is the input to the next, and so on. We can think of chains as programs, with each LLM call analogous t
o a function that takes text as input and produces text as output.

DSPy offers a new programming model, inspired by PyT
orch, that gives you a massive amount of control over these LLM programs. Further the Signature abstraction wraps prompt
s and structured input / outputs to clean up LLM program codebases.

DSPy then pairs the syntax with a super novel compi
ler that jointly optimizes the instructions for each component of an LLM program, as well as sourcing examples of the ta
sk.

Here is my review of the ideas in DSPy, covering the core concepts and walking through the introduction notebooks s
howing how to compile a simple retrieve-then-read RAG program, as well as a more advanced Multi-Hop RAG program where yo
u have 2 LLM components to be optimized with the DSPy compiler! I hope you find it useful!

https://www.youtube.com/watc
h?v=41EfOY0Ldkc
```
---

     
