 
all -  [ How does langchain transcribe youtube videos ? ](https://www.reddit.com/r/LangChain/comments/1gs7y2v/how_does_langchain_transcribe_youtube_videos/) , 2024-11-16-0913
```
Reaching out to the community, I was wondering how [LangChain](https://www.linkedin.com/company/langchain/) has helper m
odules like from langchain\_community.document\_loaders import YoutubeLoader to allow transcribing of youtube videos, is
 there an agreement between Langchain and youtube to allow this or this is under a non-commercial use-case license ?
```
---

     
 
all -  [ An intelligent document processing platform for generative AI ](https://www.reddit.com/r/instructlab/comments/1gs6eov/an_intelligent_document_processing_platform_for/) , 2024-11-16-0913
```
Learn about [Docling: a new tool to unlock data from enterprise documents for generative AI](https://research.ibm.com/bl
og/docling-generative-AI).


[Another post by Red Hat](https://www.redhat.com/en/blog/docling-missing-document-processin
g-companion-generative-ai), including where and how to use Docling.


## [Features](https://github.com/DS4SD/docling)

*
 ðŸ—‚ï¸ Reads popular document formats (PDF, DOCX, PPTX, Images, HTML, AsciiDoc, Markdown) and exports to Markdown and JSON

* ðŸ“‘ Advanced PDF document understanding including page layout, reading order & table structures
* ðŸ§© Unified, expressiveÂ 
DoclingDocumentÂ representation format
* ðŸ¤– Easy integration with LlamaIndex ðŸ¦™ & LangChain ðŸ¦œðŸ”— for powerful RAG / QA applic
ations
* ðŸ” OCR support for scanned PDFs
* ðŸ’» Simple and convenient CLI
```
---

     
 
all -  [ LangGraph Discord server! ](https://www.reddit.com/r/LangChain/comments/1gs46nd/langgraph_discord_server/) , 2024-11-16-0913
```
[https://discord.gg/Agh2mwpN](https://discord.gg/Agh2mwpN)

Hey everyone, it took longer than I expected it to, but I've
 created the Discord server for the LangGraph virtual meetup series. That link will be active for a week and I'll make a
nother one if needed a week from today.

I've got a channel created for the meetup series where I'm hoping we can come t
o some consensus on when the meetings should be for each time zone group. Let me know if that invite link isn't working 
and I'll figure out whatever I did wrong and fix it. Stoked that the ball's rolling on this. :)
```
---

     
 
all -  [ how do I make the langchain based SQL Agent Chatbot understand the underlying business rules when fo ](https://www.reddit.com/r/Langchaindev/comments/1gs1ql0/how_do_i_make_the_langchain_based_sql_agent/) , 2024-11-16-0913
```
There more than 500 tables and more than 1000 of business logics. How do i make this SQL Agent always form the correct S
QL query? Additionally I want this as a chatbot solution, so the response really has to be in few seconds. Canâ€™t let the
 user of the chatbot be waiting for minutes while the chatbot tells me the status of one of my projects from the databas
e. Has anyone worked towards solving such a problem? What do I need to do to make this SQL Agent perfect? Any help is ap
preciated ðŸ™ðŸ»
```
---

     
 
all -  [ Help Need to run the Hierarchical agent example!! ](https://www.reddit.com/r/LangChain/comments/1gs123j/help_need_to_run_the_hierarchical_agent_example/) , 2024-11-16-0913
```
[https://www.reddit.com/r/LangGraph/comments/1gs0b2p/hierarchical\_agent\_teams\_keyerrornext/?utm\_source=share&utm\_me
dium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/LangGraph/comments/1gs0b2
p/hierarchical_agent_teams_keyerrornext/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=shar
e_button)

Keep getting the KeyError: 'next'...


```
---

     
 
all -  [ Roadmap for application and the core working of generative AI and LLMs ](https://www.reddit.com/r/LangChain/comments/1grzwig/roadmap_for_application_and_the_core_working_of/) , 2024-11-16-0913
```
Iâ€™m a final year undergrad with proficiency in Python. Iâ€™m not unbeknownst to deep learning or a few Gen AI components. 
Iâ€™ve built really primitive deep learning applications using CNNs and also an RAG pipeline using a locally set up llm th
rough ollama. But my knowledge is pretty superficial, Iâ€™d like to get a much better understanding in how they function a
nd in turn use that understanding to hopefully be able to build better applications using them. Iâ€™d like to build Agenti
c pipelines or even dive into frameworks like langchain, lang graph etc, fine tune models or even play around with moder
n AI tools like cursor, super maven etc not just simply use them but also try to follow their development and AI news in
 general. Would appreciate a road map to dive into all of this or even suggestions on where and how to get started ?
```
---

     
 
all -  [ Llm for consistent json output or some other method ](https://www.reddit.com/r/LangChain/comments/1grzp7w/llm_for_consistent_json_output_or_some_other/) , 2024-11-16-0913
```
I found that it is quite hard with gpt-4o to keep the json structure consistent for every output, though the artifcat th
ing is completely based on structured output.


How would you make this possible?

My approach:

- change the llm and pr
ompt
- make another llm call to make the response structured. 


What do you say ?
```
---

     
 
all -  [ This week in AI - all the Major AI developments in a nutshell
 ](https://www.reddit.com/r/ArtificialInteligence/comments/1grykgg/this_week_in_ai_all_the_major_ai_developments_in/) , 2024-11-16-0913
```
1. **Alibaba Cloud** released ***Qwen2.5-Coder-32B***, an open-source model for programming tasks that matches the codin
g capabilities of GPT-4o. In addition to this flagship model, four new models have been released, expanding the Qwen2.5-
Coder family to a total of six models, ranging in sizes from 0.5B to 32B. An Artifacts app, similar to the Claude Artifa
cts, has also been launched.
2. **Fixie AI** released ***Ultravox v0.4.1***, a family of multi-modal, open-source models
 trained specifically for enabling real-time conversations with AI. Ultravox does not rely on a separate automatic speec
h recognition (ASR) stage, but consumes speech directly in the form of embeddings. The latency performance is comparable
 to the OpenAI Realtime . Fixie also released Ultravox Realtime, a managed service to integrate real time AI voice conve
rsations into applications \[Details\].
3. **Google** introduced a new model ***Gemini (Exp 1114***), available now in G
oogle AI Studio. It has climbed to joint #1 overall on the Chatbot Arena leaderboard, following over 6K+ community votes
 in the past week. It matches the performance of 4o-latest while surpassing o1-preview and is #1 on Vision leaderboard \
[Details\].
4. **Nexusflow** released ***Athene-V2***, an open source 72B model suite, fine-tuned from Qwen 2.5 72B. It 
includes Athene-V2-Chat matching GPT-4o across multiple benchmark and Athene-V2-Agent, a specialized agent model surpass
ing GPT-4o in function calling and agent applications \[Details\].
5. **Vidu** launched ***Vidu-1.5***, a multimodal mod
el with multi-entity consistency. Vidu-1.5 can seamlessly integrate people, objects, and environments to generate a vide
o \[Link\].
6. Codeium launched Windsurf Editor, an agentic IDE. It introduces â€˜Flowâ€™ a collaborative agent that combine
s the collaborative nature of copilots with the ability to be independently powerful like an agent \[Details\].
7. **Res
earchers** introduced ***MagicQuill***, an intelligent interactive image editing system. It uses a multimodal large lang
uage model to anticipate editing intentions in real time, removing the need for explicit prompts \[Details | Demo\].
8. 
**DeepSeek** released ***JanusFlow***, an open-source unified multimodal model that excels at both image understanding &
 generation in a single model. It matches or outperforms specialized models in their respective domains and significantl
y surpasses existing unified models on standard benchmarks \[Details| Demo\].
9. **Google DeepMind** has open-sourced Al
phaFold 3 for academic use. It models interactions between proteins, DNA, RNA, and small molecules. This is vital for dr
ug discovery and disease treatment \[Details\].
10. **Epoch AI** launched ***FrontierMath***, a benchmark for advanced m
athematical reasoning in AI. Developed with over 60 top mathematicians, it includes hundreds of challenging problems, of
 which AI systems currently solve less than 2% \[Details\].
11. TikTok launched Symphony Creative Studio, an AI-powered 
video-generation tool for Business users. Users can turn product information or a URL into a video, add a digital avatar
 to narrate the video script, or localize any existing videos into new languages using translation and dubbing capabilit
ies \[Details\].
12. Nous Research introduced the Forge Reasoning API Beta. It lets you take any model and superpower it
 with a code interpreter and advanced reasoning capabilities. Hermes 70B x Forge is competitive with much larger models 
from Google, OpenAI and Anthropic in reasoning benchmarks \[Details\].
13. Anthropic added a new prompt improver to the 
Anthropic Console. Take an existing prompt and Claude will automatically refine it with prompt engineering techniques li
ke chain-of-thought reasoning \[Details\].
14. Nvidia present Add-it, a training-free method for adding objects to image
s based on text prompts. Add-it works well on real and generated images. It leverages an existing text-to-image model (F
LUX.1-dev) without requiring additional training \[Details\].
15. Microsoft released TinyTroupe, an experimental Python 
library for simulation of people with specific personalities, interests, and goals. These artificial agents - TinyPerson
s - can listen to us and one another, reply back, and go about their lives in simulated TinyWorld environments. This is 
achieved by leveraging the power of Large Language Models (LLMs), notably GPT-4, to generate realistic simulated behavio
r \[Details\].
16. Johns Hopkins researchers trained a surgical robot by having it watch videos of skilled surgeons. Usi
ng imitation learning, the robot learned complex tasks like suturing and tissue handling, ultimately performing with ski
ll comparable to human doctors \[Details\[.
17. Stripe launched a SDK built for AI agents - LLMs can call payment, billi
ng, issuing, etc APIs. It natively supports Vercelâ€™s AI SDK, LangChain, and CrewAI, and works with any LLM provider that
 supports function calling \[Details\].
18. Researchers released OpenCoder, completely open-source and reproducible code
 LLM family which includes 1.5B and 8B base and chat models. Starting from scratch, OpenCoder is trained on 2.5 trillion
 tokens and built on the transparent data process pipeline and reproducible dataset. It achieves top-tier performance on
 multiple code LLM evaluation benchmarks \[Details\[.
19. Alibaba launched Accio, an AI search engine for small business
es to find wholesale products alongside the analysis on their popularity with consumers and projected profit. Accio is p
owered by Alibabaâ€™s Tongyi Qianwen large language model \[Details\].
20. Anthropic released RapidResponseBench, a benchm
ark that evaluates how well LLM defenses can adapt to and handle different jailbreak strategies after seeing just a few 
examples \[GitHub| Paper\].
21. LangChain launched Prompt Canvas, an interactive tool designed to simplify prompt creati
on. Prompt Canvas, the UX inspired from ChatGPTâ€™s Canvas, lets you collaborate with an LLM agent to iteratively build an
d refine your prompts \[Details\].
22. LangChain released Promptim, an experimental open-source library for prompt optim
ization. Promptim automates the process of improving prompts on specific tasks. You provide initial prompt, a dataset, a
nd custom evaluators (and optional human feedback), and promptim runs an optimization loop to produce a refined prompt t
hat aims to outperform the original \[Details\].Â 
23. Appleâ€™s Final Cut Pro 11 with AI-powered features now available \[
Details\].
24. ChatGPT app for Mac is now able to integrate with coding apps like Xcode, VS Code, TextEdit, and Terminal
 \[Details\].

**Source:**Â AI Brews - Links removed from this post due to auto-delete, but they are present in theÂ [news
letter](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and sel
ected tools. Thanks!
```
---

     
 
all -  [ This week in AI - all the Major AI developments in a nutshell
 ](https://www.reddit.com/r/ChatGPTCoding/comments/1gryf3n/this_week_in_ai_all_the_major_ai_developments_in/) , 2024-11-16-0913
```
1. **Alibaba Cloud** released ***Qwen2.5-Coder-32B***, an open-source model for programming tasks that matches the codin
g capabilities of GPT-4o. In addition to this flagship model, four new models have been released, expanding the Qwen2.5-
Coder family to a total of six models, ranging in sizes from 0.5B to 32B. An Artifacts app, similar to the Claude Artifa
cts, has also been launched.
2. **Fixie AI** released ***Ultravox v0.4.1***, a family of multi-modal, open-source models
 trained specifically for enabling real-time conversations with AI. Ultravox does not rely on a separate automatic speec
h recognition (ASR) stage, but consumes speech directly in the form of embeddings. The latency performance is comparable
 to the OpenAI Realtime . Fixie also released Ultravox Realtime, a managed service to integrate real time AI voice conve
rsations into applications \[Details\].
3. **Google** introduced a new model ***Gemini (Exp 1114***), available now in G
oogle AI Studio. It has climbed to joint #1 overall on the Chatbot Arena leaderboard, following over 6K+ community votes
 in the past week. It matches the performance of 4o-latest while surpassing o1-preview and is #1 on Vision leaderboard \
[Details\].
4. **Nexusflow** released ***Athene-V2***, an open source 72B model suite, fine-tuned from Qwen 2.5 72B. It 
includes Athene-V2-Chat matching GPT-4o across multiple benchmark and Athene-V2-Agent, a specialized agent model surpass
ing GPT-4o in function calling and agent applications \[Details\].
5. **Vidu** launched ***Vidu-1.5***, a multimodal mod
el with multi-entity consistency. Vidu-1.5 can seamlessly integrate people, objects, and environments to generate a vide
o \[Link\].
6. Codeium launched Windsurf Editor, an agentic IDE. It introduces â€˜Flowâ€™ a collaborative agent that combine
s the collaborative nature of copilots with the ability to be independently powerful like an agent \[Details\].
7. **Res
earchers** introduced ***MagicQuill***, an intelligent interactive image editing system. It uses a multimodal large lang
uage model to anticipate editing intentions in real time, removing the need for explicit prompts \[Details | Demo\].
8. 
**DeepSeek** released ***JanusFlow***, an open-source unified multimodal model that excels at both image understanding &
 generation in a single model. It matches or outperforms specialized models in their respective domains and significantl
y surpasses existing unified models on standard benchmarks \[Details| Demo\].
9. **Google DeepMind** has open-sourced Al
phaFold 3 for academic use. It models interactions between proteins, DNA, RNA, and small molecules. This is vital for dr
ug discovery and disease treatment \[Details\].
10. **Epoch AI** launched ***FrontierMath***, a benchmark for advanced m
athematical reasoning in AI. Developed with over 60 top mathematicians, it includes hundreds of challenging problems, of
 which AI systems currently solve less than 2% \[Details\].
11. TikTok launched Symphony Creative Studio, an AI-powered 
video-generation tool for Business users. Users can turn product information or a URL into a video, add a digital avatar
 to narrate the video script, or localize any existing videos into new languages using translation and dubbing capabilit
ies \[Details\].
12. Nous Research introduced the Forge Reasoning API Beta. It lets you take any model and superpower it
 with a code interpreter and advanced reasoning capabilities. Hermes 70B x Forge is competitive with much larger models 
from Google, OpenAI and Anthropic in reasoning benchmarks \[Details\].
13. Anthropic added a new prompt improver to the 
Anthropic Console. Take an existing prompt and Claude will automatically refine it with prompt engineering techniques li
ke chain-of-thought reasoning \[Details\].
14. Nvidia present Add-it, a training-free method for adding objects to image
s based on text prompts. Add-it works well on real and generated images. It leverages an existing text-to-image model (F
LUX.1-dev) without requiring additional training \[Details\].
15. Microsoft released TinyTroupe, an experimental Python 
library for simulation of people with specific personalities, interests, and goals. These artificial agents - TinyPerson
s - can listen to us and one another, reply back, and go about their lives in simulated TinyWorld environments. This is 
achieved by leveraging the power of Large Language Models (LLMs), notably GPT-4, to generate realistic simulated behavio
r \[Details\].
16. Johns Hopkins researchers trained a surgical robot by having it watch videos of skilled surgeons. Usi
ng imitation learning, the robot learned complex tasks like suturing and tissue handling, ultimately performing with ski
ll comparable to human doctors \[Details\[.
17. Stripe launched a SDK built for AI agents - LLMs can call payment, billi
ng, issuing, etc APIs. It natively supports Vercelâ€™s AI SDK, LangChain, and CrewAI, and works with any LLM provider that
 supports function calling \[Details\].
18. Researchers released OpenCoder, completely open-source and reproducible code
 LLM family which includes 1.5B and 8B base and chat models. Starting from scratch, OpenCoder is trained on 2.5 trillion
 tokens and built on the transparent data process pipeline and reproducible dataset. It achieves top-tier performance on
 multiple code LLM evaluation benchmarks \[Details\[.
19. Alibaba launched Accio, an AI search engine for small business
es to find wholesale products alongside the analysis on their popularity with consumers and projected profit. Accio is p
owered by Alibabaâ€™s Tongyi Qianwen large language model \[Details\].
20. Anthropic released RapidResponseBench, a benchm
ark that evaluates how well LLM defenses can adapt to and handle different jailbreak strategies after seeing just a few 
examples \[GitHub| Paper\].
21. LangChain launched Prompt Canvas, an interactive tool designed to simplify prompt creati
on. Prompt Canvas, the UX inspired from ChatGPTâ€™s Canvas, lets you collaborate with an LLM agent to iteratively build an
d refine your prompts \[Details\].
22. LangChain released Promptim, an experimental open-source library for prompt optim
ization. Promptim automates the process of improving prompts on specific tasks. You provide initial prompt, a dataset, a
nd custom evaluators (and optional human feedback), and promptim runs an optimization loop to produce a refined prompt t
hat aims to outperform the original \[Details\].Â 
23. Appleâ€™s Final Cut Pro 11 with AI-powered features now available \[
Details\].
24. ChatGPT app for Mac is now able to integrate with coding apps like Xcode, VS Code, TextEdit, and Terminal
 \[Details\].

**Source:**Â AI Brews - Links removed from this post due to auto-delete, but they are present in theÂ [news
letter](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and sel
ected tools. Thanks!


```
---

     
 
all -  [ A simple alternative to LangChain for lazy developers / noobs ](https://www.reddit.com/r/LangChain/comments/1grxowm/a_simple_alternative_to_langchain_for_lazy/) , 2024-11-16-0913
```
Hi everyone, 

I'm what you might call a â€œnoviceâ€ developer. My company has some AI needs, so I'm looking for a solution
 that lets me orchestrate some AI models, add prompts when generating text, and connect to some data. It's pretty simple
 stuff, I think. 

I know how to code a little in Python, but I don't understand much about LangChain, which seems to me
 to be for developers with a little experience (or maybe I'm just too dumb).

I've come across a platform that seems to 
meet my needs. It's called Eden AI (www.edenai.co). They bundle lots of AI APIs into one, and then recently they've made
 it possible to create your own custom AI API with a visual workflow. You can then make an API call on this workflow. 


But before using this in more depth, I'd like to hear your feedback. Has anyone used this? Do you have any alternatives 
for me?

Cheers
```
---

     
 
all -  [ Just kicked off my AgentCraft Hackathon with LangChain - here are the expert sessions! (recordings a ](https://open.substack.com/pub/diamantai/p/agentcraft-hackathon-kickoff-world?r=336pe4&utm_campaign=post&utm_medium=web) , 2024-11-16-0913
```
Just hosted the kickoff for my company DiamantAI's AgentCraft hackathon with LangChain. Recorded some amazing sessions w
ith experts sharing the latest in AI agent development:

* Lance Martin from LangChain introduced LangGraph - a new fram
ework for building reliable AI agents

* Monday.com's AI head demonstrated GPT-Researcher (15K GitHub stars) - multi-age
nt research assistant with 40% quality improvement 

* Microsoft demoed Azure AI Studio with $200 free credits + up to $
150K for startups

* Dynamiq CEO showcased their enterprise orchestration platform reducing ticket processing by 50%

* 
CopilotKit CEO showed how to build sophisticated AI apps with human-in-loop workflows

Full recordings and resources are
 available in the link attached.

Let me know if you have any questions about the sessions or hackathon!
```
---

     
 
all -  [ LangGraph Human In The Loop â€“ JavaScript implementation ](https://www.reddit.com/r/LangChain/comments/1grvmrz/langgraph_human_in_the_loop_javascript/) , 2024-11-16-0913
```
Wrote a small article on how to add Human In The Loop for AI Agents using LangGraph  [https://www.js-craft.io/blog/langg
raph-human-loop-javascript/](https://www.js-craft.io/blog/langgraph-human-loop-javascript/). Any feedback is welcomed. 
```
---

     
 
all -  [ Built llmtest: A Semantic Testing Framework for LLMs (Need Senior Engineering Help!) ](https://www.reddit.com/r/LangChain/comments/1grtaol/built_llmtest_a_semantic_testing_framework_for/) , 2024-11-16-0913
```
Hey r/langchain! I've built something I think could be really useful, but I need senior engineering help to make it prod
uction-ready. I'm a programming noob, only been writing code for 20 months. It is at an extremely early stage but I thin
k the idea has potential.

What is llmtest?

A semantic testing framework that uses LLMs to validate semantic equivalenc
e in test outputs. Instead of exact matching (which doesn't work with LLM outputs), it uses LLMs themselves to check if 
outputs are semantically equivalent.

Why I Built It

I got frustrated with not being able to properly test LLM applicat
ions (I don't trust my code, and I trust LLMs even less). I thought 'If we need to validate semantic meaning, why not us
e an LLM to do it?' So I built a basic framework that does exactly that.

Current State

Here's a basic example:

`from 
llmtest.core.semantic_assert import SemanticAssertion`

`def test_my_llm_function(asserter):`

`result = my_llm_function
()`  
`asserter.assert_semantic_match(actual=result,`  
`expected_behavior='Should provide a coherent answer`  
`about P
ython')`

Why I Need Help

While I've got a working test (I think, it passed the test for the test in the test package b
ecause the only way to make tests good is to write tests to test your tests) and basic functionality, I'm relatively new
 to programming and need senior engineering guidance to make this production-ready. Looking for experienced engineers wh
o can:

1. Help lead the technical direction
2. Guide architecture decisions
3. Implement best practices
4. Make this pr
oduction-ready

Why This Matters

Every LLM application needs semantic testing, but current solutions either:

* Use exa
ct matching (doesn't work)
* Use complex metrics (hard to implement)
* Don't address semantic equivalence directly

Curr
ent Features

* Semantic equivalence testing using GPT-4o
* pytest integration
* Basic error handling
* Environment-base
d configuration

Links

* GitHub: [https://github.com/Shredmetal/llmtest](https://github.com/Shredmetal/llmtest)

Would 
love to connect with senior engineers who:

1. See the potential in this approach
2. Want to help shape the future of LL
M testing
3. Are interested in mentoring while building something useful

I'm happy to remain an active core contributor
 while learning from more experienced developers (I'm way in over my head here!).

Edit:

Someone asked how it worked so
:

For me now, testing an LLM app for me involves spinning the LLM service up, spinning up a frontend, and talking to th
e bot. The idea is to take me out of the equation.

What do I do when I talk to the bot? I expect a particular type of r
esponse based on the system message / any other templates I use.

I thought that this thought process can be replicated 
by an LLM (however it does it, thought or not).

So this testing package:

1. Takes your LLM app output
2. Takes the exp
ected behaviour
3. Gives it to an LLM to check if it matches semantically
4. Gets the response from the LLM whether PASS
 or FAIL + reason for failure
5. Then raises an error or doesn't raise an error.

For example (these are test cases from
 the tests for my semantic assertion test):

This test correctly does not throw any semantic assertion error:

        d
ef test_basic_semantic_match(self, asserter):
            '''Test basic semantic matching'''
            actual = 'The s
ky is blue'
            expected = 'A statement about the color of the sky'
    
            asserter.assert_semantic_ma
tch(actual, expected)

This test successfully throws an assertion error:

        def test_semantic_mismatch(self, asser
ter):
            '''Test semantic mismatch raises correct exception'''
            actual = 'The sky is blue'
         
   expected = 'A statement about the weather forecast'
    
            with pytest.raises(SemanticAssertionError) as ex
cinfo:
                asserter.assert_semantic_match(actual, expected)
    
            assert 'Semantic assertion fail
ed' in str(excinfo.value)

For the attached images, the LLM was asked to generate:

`'''You are a friendly greeting bot.
 Your task is to generate`

`warm, personalized greetings. The greeting should:`

`1. Use the person's name`

`2. Be fri
endly and welcoming`

`3. Include a question about their wellbeing`

`4. Keep responses concise (max 2 sentences)'''`

T
his was the expected behavior:

    '''
    A polite greeting that:
    1. Addresses the person by name (Alice)
    2. A
sks about their wellbeing
    '''

Please refer to the attached images - the testing behaved as expected.

[Test failure
 because the expected behaviour did not match the actual behaviour \(The LLM greeted Alice instead of John, which was wh
o the testing LLM was told to expect\)](https://preview.redd.it/cen4ntnvk21e1.jpg?width=1509&format=pjpg&auto=webp&s=e12
d6a5e943e700af4de24eef93ccf4789e11b3f)

[The test passed because the actual content semantically matched the expected co
ntent.](https://preview.redd.it/rqtqxm1wk21e1.jpg?width=668&format=pjpg&auto=webp&s=397e9d2be72f2ec3d6206e74ba644f5178a4
a59b)
```
---

     
 
all -  [ PDF RAG Chain for me or PDF RAG Agent for me ? ](https://www.reddit.com/r/Rag/comments/1grqdyy/pdf_rag_chain_for_me_or_pdf_rag_agent_for_me/) , 2024-11-16-0913
```
Hi guys,  
I'm learning AI and currently working on a RAG project using complex pdfs ( by complex I mean pdfs that conta
ins texts , images, and tables ).

I'm using gpt-4o-mini as the LLM coz its cheap. Currently, I'm just focusing on text 
and table extraction and QA .

My RAG Pipeline looks something like this :

1. Llamaparse to convert PDF to Markdown
2. 
OpenAIEmbedding 3 Large for converting pdf chunks to vectors
3. Pinecone as Vector Store
4. Cohere ( rerank-english-v3.0
 ) as Reranker

I've created the setup using create\_history\_aware\_retriever, create\_retrieval\_chain, RunnableWithMe
ssageHistory classes from Langchain. So, my app is currently a PDF RAG chain.

I'm facing some problems in my current se
tup.

1. Because my pdf has tables, some of the tables are present in a single page only and are getting extracted as ta
ble properly. Others are splitted between pages. This is resulting in incorrect answers. How do I fix this ?
2. When I a
sk the app to calculate sum of column values of a table, it is not able to do so. GPT 4o-mini can reason and do mathemat
ical calculations, why my app can't ?
3. I've added in system prompt to always return tables in tabular format but still
 I get table data in list format around 20-25% of the time.

How can I fix these problems in my app? Is this time to swi
tch to a PDF ReAct agent ( Langgraph ) ?

I've posted this in Langchain subreddit too as I'm using Langchain, posting he
re as I'm developing a RAG app. Hope you guys don't mind. Thanks!
```
---

     
 
all -  [ PDF RAG Chain for me or PDF RAG Agent for me ? ](https://www.reddit.com/r/LangChain/comments/1grqcwn/pdf_rag_chain_for_me_or_pdf_rag_agent_for_me/) , 2024-11-16-0913
```
Hi guys,  
I'm learning AI and currently working on a RAG project using complex pdfs ( by complex I mean pdfs that conta
ins texts , images, and tables ).

I'm using gpt-4o-mini as the LLM coz its cheap. Currently, I'm just focusing on text 
and table extraction and QA .

My RAG Pipeline looks something like this :

1. Llamaparse to convert PDF to Markdown
2. 
OpenAIEmbedding 3 Large for converting pdf chunks to vectors
3. Pinecone as Vector Store
4. Cohere ( rerank-english-v3.0
 ) as Reranker

I've created the setup using create\_history\_aware\_retriever, create\_retrieval\_chain, RunnableWithMe
ssageHistory classes from Langchain. So, my app is currently a PDF RAG chain.

I'm facing some problems in my current se
tup.

1. Because my pdf has tables, some of the tables are present in a single page only and are getting extracted as ta
ble properly. Others are splitted between pages. This is resulting in incorrect answers. How do I fix this ?
2. When I a
sk the app to calculate sum of column values of a table, it is not able to do so. GPT 4o-mini can reason and do mathemat
ical calculations, why my app can't ?
3. I've added in system prompt to always return tables in tabular format but still
 I get table data in list format around 20-25% of the time.

How can I fix these problems in my app? Is this time to swi
tch to a PDF ReAct agent ( Langgraph ) ?



EDIT : System prompt that I'm using

    Â  Â  Â  Â  Â  Â  system_prompt = '''You 
are a specialized document analysis assistant designed to provide precise answers by synthesizing information from table
s, structured text, and visual elements within provided PDF documents, with advanced capabilities for mathematical calcu
lations and reasoning.
    
    When responding to questions:
    
    1. *Table Data Analysis:*
    Â  Â - Extract exact 
numerical values and relationships from tables, maintaining the structure
    Â  Â - ALWAYS Format table data to display i
n a tabular layout
    Â  Â - Specify table titles and numbers to clearly identify sources
    Â  Â - Include any footnotes 
or special notations, ensuring data context remains intact
    Â  Â - For mathematical operations on table data:
    Â  Â  Â 
* First display the relevant table data being used
    Â  Â  Â * Show each mathematical step separately with clear labels
 
   Â  Â  Â * Include subtotals for complex calculations
    Â  Â  Â * Validate results by cross-checking across different tabl
es if applicable
    
    2. *Mathematical Reasoning and Calculations:*
    Â  Â - For any calculation, follow these steps
:
    Â  Â  Â 1. Clearly state the mathematical problem to be solved
    Â  Â  Â 2. List all relevant values and their sources
 (page numbers, table numbers)
    Â  Â  Â 3. Show each calculation step with explanations
    Â  Â  Â 4. Use proper mathemati
cal notation and units
    Â  Â  Â 5. Provide intermediate results for complex calculations
    Â  Â  Â 6. Double-check calcul
ations and show verification steps
    Â  Â  Â 7. Present the final result with appropriate context
    Â  Â - When performin
g calculations across multiple tables:
    Â  Â  Â * First organize all relevant data in a structured format
    Â  Â  Â * Sho
w relationships between different data sources
    Â  Â  Â * Explain any assumptions or data transformations
    Â  Â  Â * Val
idate consistency of units and formats before calculations
    
    3. *Visual Content Interpretation:*
    Â  Â - Describ
e data shown in charts and graphs with details on values, trends, and patterns
    Â  Â - Reference relevant axis labels, 
legends, and scales
    Â  Â - Extract numerical data from graphs for calculations when needed
    Â  Â - Show mathematical 
relationships between visual data points
    Â  Â - Summarize findings with direct connections to related text for holisti
c insight
    
    4. *Textual Information:*
    Â  Â - Cite sections and page numbers when quoting or referencing text
  
  Â  Â - Organize text data with original formatting, including bullet points, numbered lists, and paragraph structures
  
  Â  Â - Note any footnotes or cross-references, ensuring information is captured in its hierarchical order
    Â  Â - Extra
ct numerical information from text for calculations when relevant
    
    *Response Format Guidelines:*
    - Identify 
source elements (table, text, or visual) at the beginning of responses
    - For tables: Display data in a table format 
for readability
    - For calculations:
    Â  * Use markdown code blocks for showing calculation steps
    Â  * Format ma
thematical equations clearly
    Â  * Include units in each step
    Â  * Show intermediate results
    - For text: Retain
 PDF-style formatting, using bullets or lists as found in the document
    - For visuals: Summarize visual data with ref
erences to axes and legends
    - Include precise locations (page numbers, section numbers) for all referenced data
    
- Cross-reference across document elements when relevant, showing interconnections
    - Maintain original data precisio
n, units, and context qualifiers
    
    *For Mathematical Operations:*
    
    Step 1: State the calculation objectiv
e
    Step 2: List source data with references
    Step 3: Show calculation setup
    Step 4: Perform operations step by
 step
    Step 5: Verify results
    Step 6: Present final answer with context
    
    
    If the required information
 cannot be found in the provided PDF content, respond with: 'I cannot locate specific information about this in the prov
ided PDF documents. Please verify if this information is present in the documents or rephrase your question.'
    
    Y
ou may respond to basic greetings, but for all other queries, strictly use information from the provided documents.
    

    {context}'''
```
---

     
 
all -  [ Rag and retriever question  ](https://www.reddit.com/r/LangChain/comments/1grojfh/rag_and_retriever_question/) , 2024-11-16-0913
```
Hello everyone, I'd like to ask the community for help with a question. Iâ€™m just starting to learn about Artificial Inte
lligence, and I recently built a tutorial guide on how to create a Retrieval-Augmented Generation (RAG) system using Lan
gChain. However, each time I make a query or question, the retriever object is called raising the token usage always, an
d I would love for this call to be optional or called when it's necessary without altering the output structure I've alr
eady set up. Changing the structure of the response would require a lot of adjustments. I'll attach the guide for refere
nce, and I hope my question is clear. Thank you!

https://python.langchain.com/docs/tutorials/rag/
```
---

     
 
all -  [ I am sharing Data Science & AI courses and projects on YouTube ](https://www.reddit.com/r/ChatGPT/comments/1grnymr/i_am_sharing_data_science_ai_courses_and_projects/) , 2024-11-16-0913
```
Hello, I wanted to share that I am sharing free courses and projects on my YouTube Channel. I have more than 200 videos 
and I created playlists for learning Data Science. I am leaving the playlist link below, have a great day!

Data Science
 Full Courses & Projects ->Â [https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&si=6WUpVwXeAKEs4tB6](
https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&si=6WUpVwXeAKEs4tB6)

Machine Learning Tutorials ->
Â [https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1&si=1rZ8PI1J4ShM\_9vW](https://youtube.com/playlis
t?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1&si=1rZ8PI1J4ShM_9vW)

AI Tutorials (ChatGPT, LangChain & LLMs) ->Â [https://you
tube.com/playlist?list=PLTsu3dft3CWhAAPowINZa5cMZ5elpfrxW&si=DvsefwOEJd3k-ShN](https://youtube.com/playlist?list=PLTsu3d
ft3CWhAAPowINZa5cMZ5elpfrxW&si=DvsefwOEJd3k-ShN)
```
---

     
 
all -  [ I am sharing Data Science & AI courses and projects on YouTube ](https://www.reddit.com/r/artificial/comments/1grny51/i_am_sharing_data_science_ai_courses_and_projects/) , 2024-11-16-0913
```
Hello, I wanted to share that I am sharing free courses and projects on my YouTube Channel. I have more than 200 videos 
and I created playlists for learning Data Science. I am leaving the playlist link below, have a great day!

Data Science
 Full Courses & Projects ->Â [https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&si=6WUpVwXeAKEs4tB6](
https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&si=6WUpVwXeAKEs4tB6)

Machine Learning Tutorials ->
 [https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1&si=1rZ8PI1J4ShM\_9vW](https://youtube.com/playlis
t?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1&si=1rZ8PI1J4ShM_9vW)

AI Tutorials (OpenAI, LangChain & LLMs) -> [https://yout
ube.com/playlist?list=PLTsu3dft3CWhAAPowINZa5cMZ5elpfrxW&si=DvsefwOEJd3k-ShN](https://youtube.com/playlist?list=PLTsu3df
t3CWhAAPowINZa5cMZ5elpfrxW&si=DvsefwOEJd3k-ShN)
```
---

     
 
all -  [ LLM Development Tech-Stack ](https://www.reddit.com/r/LangChain/comments/1gri6f7/llm_development_techstack/) , 2024-11-16-0913
```
I've been working in AI development for a while now, first at FAANG, and now in startups. IMO, when architecting your LL
M application, you need to optimize for being able to iterate quickly.



LLM Development involves being able to constan
tly try out different prompts, models, call-chaining, RAG datasets, and more to see what works. This requires a tech-sta
ck that helps you efficiently try out different combinations. Thatâ€™s why Iâ€™m working on an open-source framework, Palico
, that optimizes for the iterative nature of LLM Development.



Palico is an opinionated framework that gives you an in
tegrated system for building, evaluating, and deploying your LLM Application. The â€œopinionatedâ€ part of the framework ba
kes in best-practices Iâ€™ve seen by talking to hundreds of LLM Devs, as well as my experience working in AI for the last 
few years.



You can checkout the framework here: [https://github.com/palico-ai/palico-ai](https://github.com/palico-ai
/palico-ai)



Would love any feedback!
```
---

     
 
all -  [ AI Agent Stack  ](https://www.reddit.com/r/LangChain/comments/1grhpfz/ai_agent_stack/) , 2024-11-16-0913
```
Hi all - recently I got triggered by seeing the the Nth â€œAI agent stackâ€/â€AI agents market mapâ€ made by a VC features a 
bunch of companies Iâ€™ve never heard up in categories that make no sense. [Sharing what I see as the real â€œagents stackâ€]
(https://www.letta.com/blog/ai-agents-stack), from the perspective of having been building an agents framework and worki
ng with a lot of different providers for components like LLMs, tool calling, storage, and sandboxing.

https://preview.r
edd.it/mehxo2w55y0e1.jpg?width=5100&format=pjpg&auto=webp&s=5e684f8f64485a551e22557ed0f1effd3cd9cd7e

In my opinion, tod
ayâ€™s tech stack for building AI agents into three key layers: (1) agent hosting/serving, (2) agent frameworks, (3) LLM m
odels & storage.

The (3) LLM model and storage layer is now relatively static - I donâ€™t see a lot of new players, and t
here are clear winners in segments of the market.

For (2) agent frameworks, thereâ€™s been a ton of development in the pa
st 6 months in both general purpose frameworks, as well as specialized components (e.g. tool-use, sandboxing, and memory
). I think as the space matures weâ€™ll see a lot more players here, and even more specialization - over time, agents will
 probably be built from modular components leveraging different providers.

The layer of the stack thats still the most 
â€œup in the airâ€ is hosting (1) - different frameworks have taken very different approaches to what a hosted agent should
 look like, from how agent state (e.g. memories, message history, user data, etc.) are persisted, the API for interactin
g with agents is, and approach to handling tool execution (arbitrary code the agent can call).

I'm sure I missed come c
ool projects, so would love to learn if there's other agent hosting/tool frameworks out there since it was quite sparse 
when we did our research. 
```
---

     
 
all -  [ [10 YoE, Employed, Sr. Software Engineer, United States] ](https://www.reddit.com/gallery/1grhlhs) , 2024-11-16-0913
```

```
---

     
 
all -  [ Scaling issue  ](https://www.reddit.com/r/LangChain/comments/1grhle1/scaling_issue/) , 2024-11-16-0913
```
Hi, Iâ€™m a bit new to the LLM sphere. Iâ€™m creating software that a lot of users will use, for instance with GPT-4. My und
erstanding is that, since Iâ€™m using only one API key, thereâ€™s a token limit. I was wondering, how do other companies sca
le when they might have thousands of users? Do they get an API key for each user, or how does that work?
```
---

     
 
all -  [ Why use LangChain?  ](https://www.reddit.com/r/LangChain/comments/1greyeu/why_use_langchain/) , 2024-11-16-0913
```
Genuinely don't know the answer. I've built agents that call functions and retrieve information, multi-agent systems tha
t work together to execute tasks involving conversing with a user. It's just not clear to me why LangChain is better tha
n just using the API's directly. Does it just give you abstractions that require less code? How much less code? Does it 
give new features that you don't get with openai/anthropic API's? Appreciate any insights. I'm coding in python, in case
 makes a difference
```
---

     
 
all -  [ Arch 0.1.2 released ðŸŽ‰ - AI-native, open source infrastructure to build agents ](https://www.reddit.com/r/LangChain/comments/1grduug/arch_012_released_ainative_open_source/) , 2024-11-16-0913
```
[https://github.com/katanemo/arch](https://github.com/katanemo/arch) \- is an AI-native infrastructure primitive to buil
d fast, personalized agents using APIs. Specifically, Arch is an intelligent prompt gateway designed to protect, observe
, and personalize LLM applications (agents, assistants, co-pilots) with your APIs.

Engineered with purpose-built LLMs, 
Arch handles the critical but undifferentiated tasks related to the handling and processing of prompts, including detect
ing and rejecting [jailbreak](https://github.com/verazuo/jailbreak_llms) attempts, intelligently calling 'backend' APIs 
to fulfill the user's request represented in a prompt, routing to and offering disaster recovery between upstream LLMs, 
and managing the observability of prompts and LLM interactions in a centralized way.

Arch is built on (and by the core 
contributors of) [Envoy Proxy](https://www.envoyproxy.io/) with the belief that:

>

**Core Features**:

* Built on [Env
oy](https://envoyproxy.io): Arch runs alongside application servers, and builds on top of Envoy's proven HTTP management
 and scalability features to handle ingress and egress traffic related to prompts and LLMs.
* Function Calling for fast 
Agents and RAG apps. Engineered with purpose-built [LLMs](https://huggingface.co/collections/katanemo/arch-function-66f2
09a693ea8df14317ad68) to handle fast, cost-effective, and accurate prompt-based tasks like function/API calling, and par
ameter extraction from prompts.
* Prompt [Guard](https://huggingface.co/collections/katanemo/arch-guard-6702bdc08b889e4b
ce8f446d): Arch centralizes prompt guardrails to prevent jailbreak attempts and ensure safe user interactions without wr
iting a single line of code.
* Traffic Management: Arch routes outbound LLM calls to OpenAI (and other LLMs), offering s
mart retries, automatic cutover, and resilient upstream connections for continuous availability.
* Standards-based Obser
vability: Arch uses the W3C Trace Context standard to enable complete request tracing across applications, ensuring comp
atibility with observability tools, and provides metrics to monitor latency, token usage, and error rates, helping optim
ize AI application performance.
```
---

     
 
all -  [ Prove me wrong - An agent is just a convenient abstraction over an LLM ](https://www.reddit.com/r/LangChain/comments/1grcjpa/prove_me_wrong_an_agent_is_just_a_convenient/) , 2024-11-16-0913
```
Hi!

I have not been convinced yet that 'agents' provide any inherent value themselves other than providing a wrapper ar
ound LLMs that contain some state (prompts, context, etc) and/or logic (call back functions otherwise called 'tools'). B
efore agents were released, we were all writing classes that encapsulated this state/logic - a typical exercise to perfo
rm around any model. 

The hype definitely helped push the industry forward while there was slowing progress in terms of
 base models, however, I am reluctant to take a dependency on any new 'agents' framework into my code base when we can a
chieve the same results with little work. 
```
---

     
 
all -  [ How can I parallelize nodes in LangGraph without having to wait for the slowest one to finish if it' ](https://www.reddit.com/r/LangChain/comments/1grchuz/how_can_i_parallelize_nodes_in_langgraph_without/) , 2024-11-16-0913
```
I'm trying to run multiple nodes in parallel to reduce latency but don't want to have to wait for all nodes to finish if
 I determine from early ones that finish that I don't need all of them.

Here's a simple graph example to illustrate the
 problem. It starts with 2 nodes in parallel: setting a random number and getting city preference from some source. If t
he random number is 1-50, 'NYC' is assigned as city regardless of city preference, but if random number is 51-100, the c
ity preference is used.

    class State(TypedDict):
        random_number: int
        city: str
        city_preferenc
e: str
    
    graph: StateGraph = StateGraph(state_schema=State)
    
    
    def set_random_number(state):
        r
andom_number = 1  # Hardcode to 1 for testing
        print(f'SET RANDOM NUMBER: {random_number}')
        return {'rand
om_number': random_number}
    
    
    def get_city_preference(state):
        time.sleep(4)  # Simulate a time-consum
ing operation
        city_preference = 'Philadelphia'
        print(f'GOT CITY PREFERENCE: {city_preference}')
        
return {'city_preference': city_preference}
    
    
    def assign_city(state):
        city = 'NYC' if state['random_
number'] <= 50 else state['city_preference']
        print(f'ASSIGNED CITY: {city}')
        return {'city': city}
    

    
    graph.add_node('set_random_number', set_random_number)
    graph.add_node('get_city_preference', get_city_prefe
rence)
    graph.add_node('assign_city', assign_city)
    
    graph.add_edge(START, 'set_random_number')
    graph.add_
edge(START, 'get_city_preference')
    graph.add_edge('set_random_number', 'assign_city')
    graph.add_edge('get_city_p
reference', 'assign_city')
    graph.add_edge('assign_city', END)
    
    graph_compiled = graph.compile(checkpointer=M
emorySaver())
    
    input = {'random_number': 0, 'city': 'Nowhere', 'city_preference': 'N/A'}
    config = {
        
'configurable': {'thread_id': 'test'},
        'recursion_limit': 50,
    }
    state = graph_compiled.invoke(input=inpu
t, config=config)
    
    

The problem with the above and various conditional edge implementations I've tried, is that
 the graph always waits to assign city until the slow get\_city\_preference node completes even if the set\_random\_numb
er node has already returned a number that doesn't require city preference (i.e., 1-50).

Is there a way to stop a node 
running in parallel from blocking execution of subsequent nodes if that node's output isn't needed later in the graph?
```
---

     
 
all -  [ Langsmith API Keys. Per project or Per Workspace? ](https://www.reddit.com/r/LangChain/comments/1gra9h9/langsmith_api_keys_per_project_or_per_workspace/) , 2024-11-16-0913
```
Apologies if this is a dumb/beginner question.

I'm trying to change the project my traces are being sent to dynamically
. I'm working with a Typescript project. As far as I'm aware I can only set my LANGCHAIN\_API\_KEY once in my .env file,
 but each project I create gives me a different key. Am I able to update the API Key at the same point I decide which pr
oject name to choose? Am I supposed to use the API Key for my workspace (I tried this and it doesn't seem to work).

Bas
ically it seems I need to dynamically set the project name and project API key. I can set the project name in the  trace
able function but I'm not sure how to do the api key or if I'm misunderstanding and dont need to.
```
---

     
 
all -  [ How to duplicate chroma db or persist directory  ](https://www.reddit.com/r/LangChain/comments/1gr8e7u/how_to_duplicate_chroma_db_or_persist_directory/) , 2024-11-16-0913
```
Need to create Mutiple chroma db based on the same documents.
Tried to copy the persist directory, but the vector db cre
ated from copied directory is always empty
```
---

     
 
all -  [ Production RAG for CSV/Excel  ](https://www.reddit.com/r/LangChain/comments/1gr7msw/production_rag_for_csvexcel/) , 2024-11-16-0913
```
Hi,

I am trying to implement a CSV/Excel RAG using Langchain. Intially implemented using csvgent from langchaain. But t
his time I want it for production environment.

What is the best approach for implementing CSV RAG,  text-to-sql, or by 
Graph RAG, or any other approaches. 

Thanks
```
---

     
 
all -  [ Evaluating the Risks of Releasing an Agentic System on Large Scale Database ](https://www.reddit.com/r/LangChain/comments/1gr7fog/evaluating_the_risks_of_releasing_an_agentic/) , 2024-11-16-0913
```
Hey LangChain community! Just coming here to ask a question and for some opinions or advice.

I'm building a Reporting A
ssistant. The vision is that I will use LangGraph to create something along the lines of this https://blog.langchain.dev
/data-viz-agent/ - Basically an agentic system that allows users to communicate with their data and create visualization
s. The agentic system throughout it's many nodes will generate a SQL query based on the DB schema and user's request in 
natural language, execute it, and then display a graph and allow the user to further iterate on that graph using natural
 language. I have an initial attempt at the SQL generation in place and it is working successfully on a small dataset.


From here, I am just considering the risks more seriously of opening up a database to users like this. I have concerns a
bout the potential for this system generating (regardless of however much prompt engineering that I use) heavy queries, 
things like SQL injection, invalid SQL queries, the ability to query out outside of the user's account's scope, etc. 

I
 will have guardrails in place wherever I can think (for example I have a node to validate + optimize the generated SQL 
query, and two other conditional nodes to validate structured outputs for 2 specific nodes in my graph) + I will have so
me system guardrails with AWS Bedrock in place + most likely guardrails on top of guardrails to validate outputs with th
is being a high stakes project that needs to work and be reliable. Do you think that with the current state of models to
day and the inevitable unpredictability (however rare) that attempting this using a LLM agentic system could be a viable
 solution/product for a large scale production database?
```
---

     
 
all -  [ Am I using Hugging Face wrong? Very slow in embedding ](https://www.reddit.com/r/LangChain/comments/1gr66r2/am_i_using_hugging_face_wrong_very_slow_in/) , 2024-11-16-0913
```
I am working on a RAG chatbot which answers user questions with the content of a PDF file uploaded. I am used to pulling
 models from Ollama and have always been satisfied with the speed, whether it is to do embedding or use a LLM to generat
e answer. Now I am trying Hugging Face, because a specific embedding model (danielheinz/e5-base-sts-en-de) is only on Hu
gging Face, not on Ollama. I followed the instruction here: [Hugging Face | ðŸ¦œï¸ðŸ”— LangChain](https://python.langchain.com/
docs/integrations/text_embedding/huggingfacehub/): 

`pip install sentence_transformer`

`from langchain_huggingface.emb
eddings import HuggingFaceEmbeddings`

`...`

`vectorstore1 = FAISS.from_documents(`

`documents = splits1,` 

`embeddin
g = HuggingFaceEmbeddings(model_name='danielheinz/e5-base-sts-en-de')`

`)`

`vectorstore1.save_local(persist_directory)
`

`...`

It took VERY LONG to finish embedding, for a PDF file with 150 pages, it took about 4 minutes. In comparation,
 using a model from OllamaEmbeddings, the same procedure was done within 15 seconds.

Now I wonder, if I am using Huggin
g Face the right way? Do I need to pre-download anything (I currently have not downloaded anything, just run the code ab
ove)? Thank you for your answer in advance!
```
---

     
 
all -  [ about LLM and ai agent ](https://www.reddit.com/r/LangChain/comments/1gr4ueo/about_llm_and_ai_agent/) , 2024-11-16-0913
```
I need some guidance.  
After learning the basics of LLM, NLP, and Python, I moved on to LangChain and LangGraph, practi
cing with simple projects in Google Colab.   
At the same time, I'm learning n8n and building workflows based on tutoria
ls and my ideas. The problem is, I want to move forward and build a more advanced project to improve my knowledge of LLM
s and agents, but I'm not sure what to build or what to learn next.   
Could you suggest some ideas or guides on what to
 focus on next? Any advice on what I can build that will be useful in the real world would be appreciated
```
---

     
 
all -  [ Lang chain and lang graph road map ](https://www.reddit.com/r/LangChain/comments/1gr2l3x/lang_chain_and_lang_graph_road_map/) , 2024-11-16-0913
```
Can anyone tell me the resources and roadmap to learn lang chain and lang graph. 
```
---

     
 
all -  [ Is search feature available in some way with the OpenAI API? ](https://www.reddit.com/r/OpenAI/comments/1gr1hh0/is_search_feature_available_in_some_way_with_the/) , 2024-11-16-0913
```
Is the search feature from GPT available in some way within the OpenAI API? If not is there a way to implement it? Like 
a langchain method or something adjacent? 
```
---

     
 
all -  [ Compare two datasets ](https://www.reddit.com/r/LangChain/comments/1gqyby3/compare_two_datasets/) , 2024-11-16-0913
```


I was earlier using this method, [https://python.langchain.com/v0.1/docs/integrations/toolkits/document\_comparison\_t
oolkit/](https://python.langchain.com/v0.1/docs/integrations/toolkits/document_comparison_toolkit/), but it's not compat
ible with the updated version of langchain. All resources also seem to be using the older version, hence any help would 
be appreciated.
```
---

     
 
all -  [ [Question] - Need help with a project ](https://www.reddit.com/r/LangChain/comments/1gqsv1r/question_need_help_with_a_project/) , 2024-11-16-0913
```
I'm working on a project with LLM agents. I am a complete noob with LLMs so please bear with me. 

Context :   
Part A :
  
I have a bunch of agents. Think of these agents as users and there is some context unique to each agent. The prompt f
or each agent here would be more or less the same, except a text variable used in the prompt would change. The problem w
ith this is that I can have an arbitrarily large number of agents. However, I am not looping to give feedback back to th
e agents. 

The processes are as follows :   
Process 1 = User\_i +context\_i -> Agent\_i -> Does something on List X ->
 Value Y  
Process 2 = context(set of all context\_i) + Agent\_generate -> List X 

Here I could probably make use of a 
fixed number of agents like 10 and just have them rotate through all the users because we are only concerned with the Va
lue Y from each agent.   


Questions:   
1. Could I run these agents in parallel? If yes, are there any resources that 
I could refer to? Also, my default option would be to use Ollama 3.2 locally on my MBP(2022) M2 Chip with 8GB RAM. Howev
er, I think this might pose a problem as the number of users grows. Is it a good idea to go further with this idea or sh
ould I use OpenAI? 

Part B:   
The same situation as above but now we are adding a new process :   
  
def main\_proces
s(user\_set, context\_set):  
\# Step 1: Generate agent and List X based on combined context (Process 2)  
agent\_genera
te = generate\_agent(context\_set)  
list\_X\_ = generate\_list\_X(agent\_generate)  
  
\# Step 2 & Step 3: For each us
er, initialize agent, operate on List X, check A condition to trigger Process 3  
results = \[\]  
for user, context in 
user\_set:  
agent = initialize\_agent(user, context)  
result\[agent\] = operate\_on\_list\_X\_Cond(agent)  
  
   \# C
heck and trigger Process 3 if condition A is met  
   if calculate\_metric\_A(results\_X) < 90:  
list\_X = agent\_gener
ate(list\_X)  
  
\# Process 4: Operate on List X' and calculate final metrics A' and B'  
result\_X = operate\_on\_list
\_X\_prime(agent, list\_X)  
return results

Questions :   
1. Is there a better way to do part B? Because I am using ag
ent\_i to validate and calculate scores I need to have as many agents as I have users. This could be troublesome. Is the
re a neat way to do this?

2. I would really appreciate it if I could get pointers to terms that I can look up to implem
ent these experiments better. For, example, I think langchain might be something that could help me. Again the documenta
tion seems more advanced and I feel like I might not need. Can you help me scope the project tools and the things I woul
d need?
```
---

     
 
all -  [ How to choose device for vector store similarity search and reranking ](https://www.reddit.com/r/LangChain/comments/1gqlu01/how_to_choose_device_for_vector_store_similarity/) , 2024-11-16-0913
```
Hi, I'm new to langchain or genai tools, so I'm trying to understand how things work..

I don't know why a similarity se
arch on a vectorstore (I used faiss-cpu with langchain) and reranking using RankLLMRerank & ContextualCompressionRetriev
er would use my local gpu by default. 

For embeddings model while building vector store, it seemed that the embedding m
odel is downloaded and cached locally (not sure if it's general behavior or because I was using embedding model from hug
ging face).

Is it langchain side or something else (faiss etc) that requires local run of those model? Is there any oth
er way exist?

I get that it's advantageous to have gpu when loading and running models locally, but I'd like to know if
 it can also work with cpu only as my instance in cloud may not have gpu (just by choice).

Is it possible to choose dev
ice to cpu only? Would it be prohibitively slow?



  

```
---

     
 
all -  [ Complex PDF analysis  ](https://www.reddit.com/r/LangChain/comments/1gqliqq/complex_pdf_analysis/) , 2024-11-16-0913
```
Hi guys

 I'm looking to build an app that uses a PDF as a knowledge base and allows users to upload additional PDFs. Th
e goal is for the app to provide responses based on the combined content of these PDFs. The documents are quite long (ar
ound 100 pages) and are in Polish.

I attempted to use the OpenAI Playground to upload PDFs for Assistants, but it faile
d miserably. 

I'm new to AI and unsure whether to use something like LangChain or another tool. I came across tools lik
e Upstage, which seems to be a bit better.

 I know how to code and will manage to follow any stack needed.

 Could anyo
ne suggest what tools or frameworks I should use to build this app? 

Thanks!
```
---

     
 
all -  [ Read only vector database ](https://www.reddit.com/r/LangChain/comments/1gqkxck/read_only_vector_database/) , 2024-11-16-0913
```
Hi Iâ€™m planning on having a feature to allow people to query my vector database but not add anything. I was wondering if
 there is a vector database which allows me to share my db API key(or any token) and allow people to just query it for a
nswers without altering it. 
```
---

     
 
all -  [ [2 YOE] Master's in CS student: Not getting any callbacks for summer 2025 internship ](https://www.reddit.com/r/EngineeringResumes/comments/1gqjohb/2_yoe_masters_in_cs_student_not_getting_any/) , 2024-11-16-0913
```
Barely receiving any callbacks, having applied to at least 200 roles. Being an international student, I understand needi
ng visa sponsorship plays a big factor. However, I want to make sure that I am doing everything right as far as my resum
e is concerned.

I am currently in the first year of my master's degree and have been applying to internships since July
. The only OA's I have received are from HFT's, which I suspect are sent automatically right after application. I have b
een applying to positions everywhere and have no issues with relocation or remote work. I have not restricted my applica
tions in any way and have been applying to positions at big tech, startup, non-tech firms and any organization that has 
opened up roles for Summer 2025 internships

Please let me know what I can improve on my resume

https://preview.redd.it
/dgffmb5lop0e1.png?width=5100&format=png&auto=webp&s=e40dbcd4a9df06a2b0bf79f7e6f684afcc74c2e5


```
---

     
 
MachineLearning -  [ [P] Open-source declarative framework to build LLM applications - looking for contributors ](https://www.reddit.com/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/) , 2024-11-16-0913
```
I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewA
I, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going
 behind the curtains even for very simple stuff.

[So I just published this open-source frameworkÂ GenSphere.](https://gi
thub.com/octopus2023-inc/gensphere)Â The idea is have something likeÂ **Docker for LLMs**. You build applications with YAM
L files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs 
themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you 
don't lose control.

You basically code in YAML, stating what are the tasks that need to be done and how they connect. O
ther than that, you only write individual python functions to be called during the execution. No new classes and abstrac
tions to learn.

Its all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditi
onal nodes - which would allow full-fledged agentic system building! Pls reach out Â if you want to contribute, there are
 tons of things to do!

PS:Â [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/)Â And go o
ver this quickÂ [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutori
al.ipynb)
```
---

     
 
deeplearning -  [ Fast AI's deep learning for coders by jeremy howard for begginer?  ](https://www.reddit.com/r/deeplearning/comments/1gb2k3p/fast_ais_deep_learning_for_coders_by_jeremy/) , 2024-11-16-0913
```
I am a full stack python developer who do web dev in django

I am now starting deep learning,i am a compelete begginer


(Have worked with pandas,numpy,matplotlib,langchain only)

I wanna ask,should i do this course,will i understand what he
 is coding and code myslef

I just dont want to do blind coding,i wanna learn what is the purpose,how it works and how t
o do it

Will this course teach me that or not?

Thanks in advance
```
---

     
