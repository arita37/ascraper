 
all -  [ üë®‚Äçüéì Video Guide: Cold Outreach Personalization with AI ü§ñ ](https://www.reddit.com/r/Latenode/comments/181l143/video_guide_cold_outreach_personalization_with_ai/) , 2023-11-23-0910
```
https://preview.redd.it/23wyukzj1z1c1.jpg?width=1280&format=pjpg&auto=webp&s=50a59f2e12cfab21c4d0f80332d0748b75dcbd9a

*
Improving marketing strategies is key to growing your business, and cold outreach is a particularly effective tactic for
 finding new partners*

Today, a Latenode expert will demonstrate a **simple method for creating thousands of personaliz
ed emails** in just a few clicks! You'll discover techniques for automatic data enrichment, email personalization, and a
s a bonus, you'll receive a **free scenario template for automation**. All of this in our latest video.

üîó Watch now: [Y
ouTube](https://www.youtube.com/watch?v=usD3ajbKcGs)
```
---

     
 
all -  [ Looking for DE remote jobs ](https://www.reddit.com/r/dataengineering/comments/181iebj/looking_for_de_remote_jobs/) , 2023-11-23-0910
```
Hey everyone,

I am looking for remote data engineering jobs. I have 5 years of experience in systems/data migration to 
Google Cloud and I am certified too.

I tried searching remote jobs in the US on LinkedIn but no results.

Are there any
 specific good platforms to find remote jobs or is anyone looking to hire here

PS: I also have few months of experience
 on GenAI, LLMs, Langchain and RAG.
```
---

     
 
all -  [ [D] Made some promises. Time to learn how to conduct very large scale pdf doc analysis. ](https://www.reddit.com/r/MachineLearning/comments/181gzek/d_made_some_promises_time_to_learn_how_to_conduct/) , 2023-11-23-0910
```
I have about a half million pdfs I need to summarize. Very wide range of types: invoices, diagrams, contracts, emails, l
etters, pictures, schedules, notices, data sheets, manuals, more. 

Which is... woof. Something else. I've been trying f
or many hours now to figure out a service/combination thereof that can get me there, but I'm seriously struggling. The *
ideal* solution would be to throw the pdfs in and have it return a csv with dates and summaries, maybe parsed out email 
heading info.

I'm currently running these pdfs through Acrobat OCR now, which its own special hell.

I've tried myriad 
local and webhosted solutions. The BEST results in what is almost the perfect system for this I found on https://docalys
is.com/. Good text results, works in batches, BUT I can only upload a single document at a time. They have a service to 
do batch processing and so I'm waiting to hear from them now. I imagine at the scale I need it's expensive.

I also got 
this solution working: https://github.com/mayooear/gpt4-pdf-chatbot-langchain. Seemed solid, I was able to upload a thou
sand pdfs in a single go, but it would keep returning information from only 2-3 documents. Upload 5? Results for 2-3. Up
load a thousand? Results for 2-3. My uneducated guess is that it's hitting the OpenAI API token limit, but maybe not?

I
 know it's possible, just not whether it's feasible for an end user.
```
---

     
 
all -  [ Made some promises. Now I'm desperately trying to figure out how to conduct very large scale pdf doc ](https://www.reddit.com/r/learnmachinelearning/comments/181gxg0/made_some_promises_now_im_desperately_trying_to/) , 2023-11-23-0910
```
I have about a half million pdfs I need to summarize. Very wide range of types: invoices, diagrams, contracts, emails, l
etters, pictures, schedules, notices, data sheets, manuals, more. 

Which is... woof. Something else. I've been trying f
or many hours now to figure out a service/combination thereof that can get me there, but I'm seriously struggling. The *
ideal* solution would be to throw the pdfs in and have it return a csv with dates and summaries, maybe parsed out email 
heading info.

I'm currently running these pdfs through Acrobat OCR now, which its own special hell.

I've tried myriad 
local and webhosted solutions. The BEST results in what is almost the perfect system for this I found on https://docalys
is.com/. Good text results, works in batches, BUT I can only upload a single document at a time. They have a service to 
do batch processing and so I'm waiting to hear from them now. I imagine at the scale I need it's expensive.

I also got 
this solution working: https://github.com/mayooear/gpt4-pdf-chatbot-langchain. Seemed solid, I was able to upload a thou
sand pdfs in a single go, but it would keep returning information from only 2-3 documents. Upload 5? Results for 2-3. Up
load a thousand? Results for 2-3. My uneducated guess is that it's hitting the OpenAI API token limit, but maybe not?

I
 know it's possible, just not whether it's feasible for an end user. Does anyone know a solution to accomplish this?
```
---

     
 
all -  [ [Langchain] Fa√ßons d'injecter des m√©tadonn√©es en morceaux de texte? ](https://www.reddit.com/r/redditenfrancais/comments/181gf7h/langchain_fa√ßons_dinjecter_des_m√©tadonn√©es_en/) , 2023-11-23-0910
```
Salut, quelqu'un saurait quelle est la meilleure fa√ßon d'injecter des m√©tadonn√©es en morceaux de texte en utilisant Lang
chain ou peut-√™tre que l'indice LLAMA serait?

Pour mon cas d'utilisation, je veux charger dans un PDF et diviser le PDF
 en morceaux, je veux pouvoir injecter des m√©tadonn√©es suppl√©mentaires. Y a-t-il une classe √† Langchain qui aide √† cela?
 Sinon, quelle serait une bonne fa√ßon de faire cela? J'ai pens√© √† une fa√ßon quelque peu pirat√©e de le faire en me conver
tissant en JSON, en ajoutant les champs de m√©tadonn√©es, puis en utilisant le Jsonloader pour le recharger.

Toutes les i
d√©es ou suggestions seraient les plus appr√©ci√©es et appr√©ci√©es. Merci d'avance!

Traduit et repost√© √† partir de la publi
cation https://www.reddit.com/169uxea
```
---

     
 
all -  [ [Langchain] Puis-je obtenir un code invitation Langsmith ](https://www.reddit.com/r/redditenfrancais/comments/181fray/langchain_puisje_obtenir_un_code_invitation/) , 2023-11-23-0910
```
Les gars, je suis terriblement perdu dans les journaux de mon agent personnalis√© et je dois essayer ce Langchain au moin
s pour voir √† quel point la situation est mauvaise. Est-il possible de m'envoyer un code d'invitation? √ât√© sur la liste 
d'attente depuis des lustres: /

Traduit et repost√© √† partir de la publication https://www.reddit.com/16l9npt
```
---

     
 
all -  [ [Langchain] Chat Frontend √† utiliser pour mes cha√Ænes ](https://www.reddit.com/r/redditenfrancais/comments/181eqnt/langchain_chat_frontend_√†_utiliser_pour_mes/) , 2023-11-23-0910
```
J'ai jou√© autour d'un montant consid√©rable avec diff√©rentes cha√Ænes et agents, mais j'utilise toujours la coquille ou un
 cahier de jupyter pour interagir avec eux.

Je voudrais une belle application Web pour rendre mes cha√Ænes plus accessib
les avec un navigateur. Y a-t-il un frontend Goto (backend incl. API) pour y parvenir sans y passer trop de temps?

Trad
uit et repost√© √† partir de la publication https://www.reddit.com/13jzkwy
```
---

     
 
all -  [ Open source prompt catalog ](https://www.reddit.com/r/LangChain/comments/181ef3t/open_source_prompt_catalog/) , 2023-11-23-0910
```
Looking for a web based tool for users to search try out, and contribute prompts. And LangChain based agents as well. Al
so allow them to comment, rate and provide suggestions to improve the prompt. Open source would be ideal. 

I know of th
e following that are all closed source.
```
---

     
 
all -  [ Types in LangChain TS ](https://www.reddit.com/r/LangChain/comments/181d776/types_in_langchain_ts/) , 2023-11-23-0910
```
I‚Äôm struggling with types in LangChain TS. The types of StructuredTool, AgentAction, parsing error handling in AgentExec
utor and StructuredChatOutputParser don't fit together, and it only typechecks kind of by accident at the moment. 

I op
ened 2 issues on github, but can‚Äôt get any reasonable discussion/ feedback going, hope this helps. 

* Issue 1 - [https:
//github.com/langchain-ai/langchainjs/issues/2710](https://github.com/langchain-ai/langchainjs/issues/2710) 
* Issue 2 -
 [https://github.com/langchain-ai/langchainjs/issues/2711](https://github.com/langchain-ai/langchainjs/issues/2711) 

fo
r background [https://www.octomind.dev/blog/on-type-safety-in-langchain-ts](https://www.octomind.dev/blog/on-type-safety
-in-langchain-ts) 
```
---

     
 
all -  [ Code error in flowise ](https://www.reddit.com/r/LangChain/comments/181bbme/code_error_in_flowise/) , 2023-11-23-0910
```
Hi, I'm new to flowise and experimenting with some templates. I'm getting this error when I run the flow

'output values
 have 1 keys, you must specify an output key or pass only 1 key as output'

What might be causing this and is there a wa
y to get the langchain code for this as a solution for that has been suggested on stack overflow.

Thanks in advance
```
---

     
 
all -  [ Creating pdf as output ](https://www.reddit.com/r/LangChain/comments/18193sw/creating_pdf_as_output/) , 2023-11-23-0910
```
Is it possible to create a formatted (boiler plate) pdf as output using Langchain calls to LLM.

 Assuming LLM output gi
ves me all the required fields

1) How can one format it correctly?
2) How can the output be written out as pdf?  

Any 
relevant sample code or suggestions might be of great help. Thanks
```
---

     
 
all -  [ How do I go about training an open source llm on a postgres database ](https://www.reddit.com/r/LocalLLaMA/comments/18190ds/how_do_i_go_about_training_an_open_source_llm_on/) , 2023-11-23-0910
```
Howdy,

I'm a backend developer, and management recently asked me to train an llm on our company data. I'm a bit over my
 head here, and I figured I'd ask for high level advice rather than continuing to go down google rabbit holes.

What I'v
e tried so far:

* I spun up some gpu instances on AWS. Couldn't get llama to work at all, except for using gpt4all, whi
ch wasn't very performant and does make a network call to a github page for a list of models.
* I tired following a goog
le cloud tutorial [here](https://cloud.google.com/blog/products/databases/using-pgvector-llms-and-langchain-with-google-
cloud-databases). This didn't work in their colab notebook, so I gave up on that since if their own documentation didn't
 work it didn't seem promising.

Any advice is appreciated!
```
---

     
 
all -  [ How to store/replay the path an AI agent has taken ](https://www.reddit.com/r/LangChain/comments/18178pg/how_to_storereplay_the_path_an_ai_agent_has_taken/) , 2023-11-23-0910
```
I am new to building AI agents (robotics background) and I was curious to learn about the most common workflows you guys
 use.

How do you store/replay the full path that the agent has been following? More precisely:

1. What sort of data do
 you collect?
2. Where do you store all your runs (if you store them)?
3. What metrics do you use for evaluating each ru
n? I've seen some interesting things from the OAI devday-- do you actually use them?
4. Is there a way to replay a run?
```
---

     
 
all -  [ [Langchain] Comment devrais-je sortir du texte √† partir d'un manuel pour les meilleurs r√©sultats d'i ](https://www.reddit.com/r/redditenfrancais/comments/18175bo/langchain_comment_devraisje_sortir_du_texte_√†/) , 2023-11-23-0910
```
Je suppose que je dois suivre la structure naturelle du manuel et chasser mon texte par chapitre, section, paragraphe, e
tc. tout en conservant les m√©tadonn√©es pertinentes. Le probl√®me est que je ne sais pas comment faire √ßa lol.

Quelqu'un 
peut-il me dire une meilleure fa√ßon de s√©lectionner un manuel ou de me donner les directives de base afin que je puisse 
demander √† chatgpt?

Traduit et repost√© √† partir de la publication https://www.reddit.com/15q5jzv
```
---

     
 
all -  [ [Langchain] Comment injecter un contexte toujours pr√©sent dans la conversation de conversationrie ](https://www.reddit.com/r/redditenfrancais/comments/18173hz/langchain_comment_injecter_un_contexte_toujours/) , 2023-11-23-0910
```
J'utilise ConfortationalSrievalChain pour les questions et r√©ponses sur les documents. J'ai divis√© le document principal
 en plus petits et les ai utilis√©s dans comme \ _Reriver ().

Il y a des informations importantes qui ne figurent pas da
ns le document de questions / r√©ponses, mais c'est crucial pour chaque question √† ce sujet. Je ne veux pas l'ajouter √† l
a base de donn√©es vectorielle, car il pourrait ne pas √™tre r√©cup√©r√©, en laissant le mod√®le sans contexte cl√©.

Existe-t-
il une meilleure fa√ßon de donner ces informations au mod√®le, en plus de l'ajouter √† l'invite?

Traduit et repost√© √† part
ir de la publication https://www.reddit.com/16trvoe
```
---

     
 
all -  [ [Langchain] Chatbot utilisant des agents de Langchain et Llama 2 ](https://www.reddit.com/r/redditenfrancais/comments/1816x0o/langchain_chatbot_utilisant_des_agents_de/) , 2023-11-23-0910
```
Salut les gars, j'ai r√©cemment construit un chatbot utilisant le mod√®le Langchian et Open AI LLM (GPT 3.5 16 K). Ce chat
bot r√©side sur des donn√©es tabulaires (nous avons utilis√© Create \ _Pandas \ _DataFrame) pour interroger les donn√©es pou
r les utilisateurs. Mais notre organisation veut que nous utilisons le mod√®le LLM open source (LLAMA 2 70 B). Je voulais
 juste les suggestions V√©rifiez 2 choses:


1) Quelqu'un a-t-il essay√© LLAMA2 (GPU / CPU) avec des pandas Langchain / de
s agents conversationnels et voir si les r√©sultats sont prometteurs ou non.

2) Quelles sont les exigences GPU pour ex√©c
uter LLAMA2 70 B, que je peux demander mon organisation?


Tia

Traduit et repost√© √† partir de la publication https://ww
w.reddit.com/16p2nas
```
---

     
 
all -  [ What embedding model do you guys use? ](https://www.reddit.com/r/LangChain/comments/1816mb5/what_embedding_model_do_you_guys_use/) , 2023-11-23-0910
```
I'm trying to test more embedding models and I'm wondering what does this community use... 

I know that it 'may vary de
pending on use case', so in that case please share model and related use case. 

Currently I'm using mostly bge-large-v1
.5 or instructor-xl... 

(intrested in both bi encoder and cross encoder)

Thanks im advance!!!
```
---

     
 
all -  [ Building our own Agent ](https://www.reddit.com/r/brokebing/comments/18160rr/building_our_own_agent/) , 2023-11-23-0910
```
Hi Visual Hackers!  


I have a deep respect for this community and the way their members manage to outsmart the machine
.   


I'm a software dev, and I'm currently developing a specialized agent that helps rephrase the prompts in a way tha
t bypasses the dog.  


In order to do this, I need to create a dataset of successful cases. I understand we don't want 
to make our techniques public  in order to avoid patching, so we would need to get in contact via private message if you
 would like to collaborate to the project.   


My strategy will be trying these in order:  
1- Try with few shot infere
nce (prompting)  
2- RAG the dataset  
3- Fine-tuning if we manage to get enough examples.  


There are several non-res
tricted generative models I could use to avoid the basic censorship, I already have a couple of models running on local 
like  nous-hermes-llama2-13b and  mistral-7b which have been trained with an uncensored dataset, that could fit our use 
case better. I also have a really uncensored one, but that one would definitely need fine-tuning, so I'll try with the o
thers 2 first.  


Long story short: you give an input like 'tits, war and drugs' and outputs a prompt that is most like
ly to confuse the dog. It does most of the heavy lifting, and leave us with a prompt that we could easily modify to adju
st it better.  


The final result, will be shared only among the people who participated, either as developer or provid
ing their prompts. I don't plan to profit on this, it is merely for research purposes, and of course for fun.  


Also, 
if there's any other dev around here I have an idea of also building incremental image generator using langchain, as I s
ometimes noticed the best way to avoid the dog is by building up a concept slowly, rather than using just one prompt.  



I'm open to ideas and discussions.
```
---

     
 
all -  [ FileNotFoundError: [Errno 2] No such file or directory: ‚ÄòUploadedFile‚Äô ](https://www.reddit.com/r/StreamlitOfficial/comments/1815xyt/filenotfounderror_errno_2_no_such_file_or/) , 2023-11-23-0910
```
Hi, I‚Äôm creating an LLM Application which accepts user inputs such as JSON file upload, project id and dataset name to e
stablish bigquery connection. I have tried the following code given below and it works perfectly. It is a hard coded ver
sion where the user doesn‚Äôt upload or give any inputs. When i converted this to streamlit app where it accepts input JSO
N file, it gives me an error saying file not found. Full details on the error is given below. I would greatly appreciate
 it if someone could help me out on this.

&#x200B;

**Code snippet bigquery direct connection:**

    service_account_f
ile = 'bigquery_sample.json' #local directory path 
    project = 'xxxxx' 
    dataset = 'xxxx_xxxxx' 
    
    sqlalche
my_url = f'bigquery://{project}/{dataset}?credentials_path={service_account_file}' #connection

&#x200B;

**Code snippet
 for bigquery connection using streamlit by accepting inputs from user:**

    service_account_file = st.sidebar.file_up
loader('Upload BigQuery Service Account file') 
    project = st.sidebar.text_input('Enter Project ID:','xxxxx') 
    da
taset = st.sidebar.text_input('Enter Dataset Name:','xxxx_xxxxx') 
    
    sqlalchemy_url = f'bigquery://{project}/{dat
aset}?credentials_path={service_account_file}'

&#x200B;

**Error:**

    FileNotFoundError: [Errno 2] No such file or d
irectory: 'UploadedFile(file_id=\'7a217610-7090-4b3c-9e34-19f4555de9d5\', name=\'bigquery_sample.json\', type=\'applicat
ion/json\', size=2355, _file_urls=file_id: '7a217610-7090-4b3c-9e34-19f4555de9d5''

&#x200B;

**Traceback:**

    File '
C:\Users\alens\Downloads\langchain_text2sql_1\venv\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py', l
ine 541, in _run_script
        exec(code, module.__dict__)
    File 'C:\Users\alens\Downloads\langchain_text2sql_1\app3
.py', line 93, in <module>
        query_engine = initialize_llm_predictor()
    File 'C:\Users\alens\Downloads\langchai
n_text2sql_1\app3.py', line 67, in initialize_llm_predictor
        db = SQLDatabase.from_uri(sqlalchemy_url)
    File '
C:\Users\alens\Downloads\langchain_text2sql_1\venv\lib\site-packages\langchain\utilities\sql_database.py', line 125, in 
from_uri
        return cls(create_engine(database_uri, **_engine_args), **kwargs)
    File '<string>', line 2, in creat
e_engine
    File 'C:\Users\alens\Downloads\langchain_text2sql_1\venv\lib\site-packages\sqlalchemy\util\deprecations.py'
, line 375, in warned
        return fn(*args, **kwargs)
    File 'C:\Users\alens\Downloads\langchain_text2sql_1\venv\li
b\site-packages\sqlalchemy\engine\create.py', line 560, in create_engine
        (cargs, cparams) = dialect.create_conne
ct_args(u)
    File 'C:\Users\alens\Downloads\langchain_text2sql_1\venv\lib\site-packages\sqlalchemy_bigquery\base.py', 
line 842, in create_connect_args
        client = _helpers.create_bigquery_client(
    File 'C:\Users\alens\Downloads\la
ngchain_text2sql_1\venv\lib\site-packages\sqlalchemy_bigquery\_helpers.py', line 46, in create_bigquery_client
        c
redentials = service_account.Credentials.from_service_account_file(
    File 'C:\Users\alens\Downloads\langchain_text2sq
l_1\venv\lib\site-packages\google\oauth2\service_account.py', line 260, in from_service_account_file
        info, signe
r = _service_account_info.from_filename(
    File 'C:\Users\alens\Downloads\langchain_text2sql_1\venv\lib\site-packages\
google\auth\_service_account_info.py', line 78, in from_filename
        with io.open(filename, 'r', encoding='utf-8') a
s json_file:

&#x200B;
```
---

     
 
all -  [ Does Chromadb let us use score_threshold? ](https://www.reddit.com/r/LangChain/comments/18120qp/does_chromadb_let_us_use_score_threshold/) , 2023-11-23-0910
```
Using Chromadb with langchain. Per Langchain documentation, below is valid. See link given.

retriever = db.as\_retrieve
r(

search\_type='similarity\_score\_threshold',

search\_kwargs={'score\_threshold': 0.9}

)

See examples from this li
nk:

[https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vector
stores.chroma.Chroma.asimilarity\_search\_with\_relevance\_scores](https://api.python.langchain.com/en/latest/vectorstor
es/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma.asimilarity_search_with_relevance_scor
es)

&#x200B;

 *# Only retrieve documents that have a relevance score* *# Above a certain threshold* docsearch.as\_retr
iever( search\_type='similarity\_score\_threshold', search\_kwargs={'score\_threshold': 0.8} ) 

However the score\_thre
shold doesn't return any documents even for the lowest threshold. The db does contain documents relevant to my query.

I
 also did go through ChromaDB code, but I fail to see any option to include this threshold. 

Doesn't chromadb allow us 
to search results based on a threshold?

&#x200B;

&#x200B;

&#x200B;
```
---

     
 
all -  [ Does anyone use pgvector or is it too slow compared to everything else? ](https://www.reddit.com/r/LangChain/comments/1811x1y/does_anyone_use_pgvector_or_is_it_too_slow/) , 2023-11-23-0910
```
Is pgVector just something for hobbyists or can it be used in prod with decent speed?
```
---

     
 
all -  [ Consume API with langchain ](https://www.reddit.com/r/LangChain/comments/180vqss/consume_api_with_langchain/) , 2023-11-23-0910
```
Hi guys, with langchain is it possible to consume an api with post method where I have to send a json in the body, does 
anyone have an example?

&#x200B;
```
---

     
 
all -  [ Keep certain text together when splitting? ](https://www.reddit.com/r/LangChain/comments/180vkvs/keep_certain_text_together_when_splitting/) , 2023-11-23-0910
```
Building a RAG implementation, and I have some pieces of text that need to be contained in the same document when sendin
g to the vector store. Specifically, I have tables stripped from PDFs, and some corresponding descriptions that I had an
 LLM generate. 

Is there a way to specify certain start and end tokens to make sure the tables and corresponding descri
ptions aren't divided across documents? Or maybe there's a way to create a custom text splitter?
```
---

     
 
all -  [ Flush AI - Langchain but for Stable Diffusion Workflows ](https://www.reddit.com/r/SideProject/comments/180ufn1/flush_ai_langchain_but_for_stable_diffusion/) , 2023-11-23-0910
```
[Flush](https://flushai.cloud/) simplifies the process of building, fine tuning, and managing stable diffusion models. Y
ou can do the following on Flush:

* Upload any Civit AI Checkpoint (under 4GB)
* Finetune a model using Dreambooth
* Up
scale images with Real-Esrgan
* Use LLM's to enhance your prompt and build custom workflows
* Generate images with any m
odel on our platform, including base models like Realistic Vision, SDXL, Absolute Reality, and more. You can generate im
ages with text-to-image or image-to-image.
* (Coming soon!): Safetensors upload, download your finetuned models, and fin
etune on a wider variety of base models like Absolute Reality/Realistic Vision.

We consider ourselves 'langchain' but f
or stable diffusion workflows, because with our SDK (which we will be making open source), developers can build ANY cust
om workflow chaining together Prompt Templates, upscalers, any image generation model hosted on our platform (SDXL/any C
ivit model/any Finetuned model), LLM's like GPT-4, and deploy these workflows in their applications. We also have many i
ntegrations with prompt databases/API's like SerpAPI, Pixabay, Pexels, etc. And we are also building data integrations, 
so you can load data from sources like PDF's, Dropbox, Google Drive, etc. Check us at [https://www.flushai.cloud/](https
://www.flushai.cloud/).

We are building and growing fast and I‚Äôd encourage you guys to stay tuned and join our [Discord
](https://discord.com/invite/fzdHj9DeuC) server and DM me about any criticism/feedback/bugs you face.

&#x200B;

https:/
/preview.redd.it/7dj7itp99s1c1.png?width=2000&format=png&auto=webp&s=918d4b469fd4efa98d2a838990759de6e08deab6

https://p
review.redd.it/ob6jwiha9s1c1.png?width=960&format=png&auto=webp&s=da2f8a5aa12d0e123e16763f162e50b6e9daaf5e

https://prev
iew.redd.it/fjr66t6b9s1c1.png?width=2000&format=png&auto=webp&s=eaa6d11e6b93867338cc84051d5cc859f9bf8c5a
```
---

     
 
all -  [ Langchain but for Stable Diffusion Workflows ](https://www.reddit.com/r/sdforall/comments/180tme3/langchain_but_for_stable_diffusion_workflows/) , 2023-11-23-0910
```
[Flush](https://flushai.cloud/) simplifies the process of building, fine tuning, and managing stable diffusion models. Y
ou can do the following on Flush:

* Upload any Civit AI Checkpoint (under 4GB)
* Finetune a model using Dreambooth
* Up
scale images with Real-Esrgan
* Use LLM's to enhance your prompt and build custom workflows
* Generate images with any m
odel on our platform, including base models like Realistic Vision, SDXL, Absolute Reality, and more. You can generate im
ages with text-to-image or image-to-image.
* (Coming soon!): Safetensors upload, download your finetuned models, and fin
etune on a wider variety of base models like Absolute Reality/Realistic Vision. 

We consider ourselves 'langchain' but 
for stable diffusion workflows, because with our SDK (which we will be making open source), developers can build ANY cus
tom workflow chaining together Prompt Templates, upscalers, any image generation model hosted on our platform (SDXL/any 
Civit model/any Finetuned model), LLM's like GPT-4, and deploy these workflows in their applications. We also have many 
integrations with prompt databases/API's like SerpAPI, Pixabay, Pexels, etc. And we are also building data integrations,
 so you can load data from sources like PDF's, Dropbox, Google Drive, etc. Check us at [https://www.flushai.cloud/](http
s://www.flushai.cloud/).

We are building and growing fast and I‚Äôd encourage you guys to stay tuned and join our [Discor
d](https://discord.com/invite/fzdHj9DeuC) server and DM me about any criticism/feedback/bugs you face.

https://preview.
redd.it/1rbk0vnh1s1c1.png?width=2000&format=png&auto=webp&s=6dc73d7facd37477b72be4b5c39bbda0282901b1

https://preview.re
dd.it/4c02bqei1s1c1.png?width=960&format=png&auto=webp&s=2fd0ad78b3d843c5ca40395d48168a035de615c7

https://preview.redd.
it/tcl7hu3j1s1c1.png?width=2000&format=png&auto=webp&s=cf75d73c776aa08d24ebcd9d7c5ec2f6f1082dc7
```
---

     
 
all -  [ Langchain but for Stable Diffusion Workflows ](https://www.reddit.com/r/StableDiffusionInfo/comments/180tkrm/langchain_but_for_stable_diffusion_workflows/) , 2023-11-23-0910
```
[Flush](https://flushai.cloud/) simplifies the process of building, fine tuning, and managing stable diffusion models. Y
ou can do the following on Flush:

* Upload any Civit AI Checkpoint (under 4GB)
* Finetune a model using Dreambooth
* Up
scale images with Real-Esrgan
* Use LLM's to enhance your prompt and build custom workflows
* Generate images with any m
odel on our platform, including base models like Realistic Vision, SDXL, Absolute Reality, and more. You can generate im
ages with text-to-image or image-to-image.
* (Coming soon!): Safetensors upload, download your finetuned models, and fin
etune on a wider variety of base models like Absolute Reality/Realistic Vision. 

We consider ourselves 'langchain' but 
for stable diffusion workflows, because with our SDK (which we will be making open source), developers can build ANY cus
tom workflow chaining together Prompt Templates, upscalers, any image generation model hosted on our platform (SDXL/any 
Civit model/any Finetuned model), LLM's like GPT-4, and deploy these workflows in their applications. We also have many 
integrations with prompt databases/API's like SerpAPI, Pixabay, Pexels, etc. And we are also building data integrations,
 so you can load data from sources like PDF's, Dropbox, Google Drive, etc. Check us at [https://www.flushai.cloud/](http
s://www.flushai.cloud/).

We are building and growing fast and I‚Äôd encourage you guys to stay tuned and join our [Discor
d](https://discord.com/invite/fzdHj9DeuC) server and DM me about any criticism/feedback/bugs you face.

https://preview.
redd.it/fj181v5k0s1c1.png?width=2000&format=png&auto=webp&s=251ff081980160d375d4bfc435d451c087943113

https://preview.re
dd.it/11g8ml2l0s1c1.png?width=960&format=png&auto=webp&s=865a908e6cd9bc7c0a99529e7b3e56c03b010a12

https://preview.redd.
it/zqlvaiom0s1c1.png?width=2000&format=png&auto=webp&s=4bf66602e5afa16fd973b4bcd7165b0447aa1034
```
---

     
 
all -  [ Langchain but for Stable Diffusion Workflows ](https://www.reddit.com/r/StableDiffusion/comments/180ti53/langchain_but_for_stable_diffusion_workflows/) , 2023-11-23-0910
```
[Flush](https://flushai.cloud/) simplifies the process of building, fine tuning, and managing stable diffusion models. Y
ou can do the following on Flush:

* Upload any Civit AI Checkpoint (under 4GB)
* Finetune a model using Dreambooth
* Up
scale images with Real-Esrgan
* Use LLM's to enhance your prompt and build custom workflows
* Generate images with any m
odel on our platform, including base models like Realistic Vision, SDXL, Absolute Reality, and more. You can generate im
ages with text-to-image or image-to-image.
* (Coming soon!): Safetensors upload, download your finetuned models, and fin
etune on a wider variety of base models like Absolute Reality/Realistic Vision.

We consider ourselves 'langchain' but f
or stable diffusion workflows, because with our SDK (which we will be making open source), developers can build ANY cust
om workflow chaining together Prompt Templates, upscalers, any image generation model hosted on our platform (SDXL/any C
ivit model/any Finetuned model), LLM's like GPT-4, and deploy these workflows in their applications. We also have many i
ntegrations with prompt databases/API's like SerpAPI, Pixabay, Pexels, etc. And we are also building data integrations, 
so you can load data from sources like PDF's, Dropbox, Google Drive, etc. Check us at [https://www.flushai.cloud/](https
://www.flushai.cloud/).

We are building and growing fast and I‚Äôd encourage you guys to stay tuned and join our [Discord
](https://discord.com/invite/fzdHj9DeuC) server and DM me about any criticism/feedback/bugs you face.

https://preview.r
edd.it/7mt1xeilzr1c1.png?width=960&format=png&auto=webp&s=2991b6a0e4f7f0a499144efbf29665cfdcbd8370

https://preview.redd
.it/hz2n0g1ozr1c1.png?width=2000&format=png&auto=webp&s=44147f908bdc19b42ed1cb83320358340b98dad2

https://preview.redd.i
t/r689slypzr1c1.png?width=2000&format=png&auto=webp&s=753488f3d0a0467fe01fa720f3b6bfbad9298573

&#x200B;
```
---

     
 
all -  [ We are looking for new beta testers, we are waiting for you ](https://www.reddit.com/r/aiproduct/comments/180r6ox/we_are_looking_for_new_beta_testers_we_are/) , 2023-11-23-0910
```
Hi All!  
I'm Simone from the Knowlee team, thrilled to introduce our ultimate AI assistant powered by top-notch technol
ogies like GPT4, Pinecone, and Langchain. Here‚Äôs a quick run-through of how Knowlee can revolutionize your workflow:

1Ô∏è
‚É£ **Feed Knowledge**: Import documents, videos, social posts, news, financial stats, or any content you need analyzed.  

2Ô∏è‚É£ **Get Instant Insights**: Knowlee processes your content and provides instant, intelligent insights to help you mak
e informed decisions.  
3Ô∏è‚É£ **Content Creation**: Leverage these insights to generate relevant and timely content, keepi
ng you ahead of the curve.

We're currently inviting enthusiasts to join our beta testing program and contribute to shap
ing the future of Knowlee.

Feel free to drop a comment or send a DM if you‚Äôre ready to explore the capabilities of Know
lee!
```
---

     
 
all -  [ We are looking for new beta testers, we are waiting for you! ](https://www.reddit.com/r/alphaandbetausers/comments/180r5xz/we_are_looking_for_new_beta_testers_we_are/) , 2023-11-23-0910
```
Hi All!  
I'm Simone from the Knowlee team, thrilled to introduce our ultimate AI assistant powered by top-notch technol
ogies like GPT4, Pinecone, and Langchain. Here‚Äôs a quick run-through of how Knowlee can revolutionize your workflow:  
1
Ô∏è‚É£ Feed Knowledge: Import documents, videos, social posts, news, financial stats, or any content you need analyzed.  
2Ô∏è
‚É£ Get Instant Insights: Knowlee processes your content and provides instant, intelligent insights to help you make infor
med decisions.  
3Ô∏è‚É£ Content Creation: Leverage these insights to generate relevant and timely content, keeping you ahea
d of the curve.  
We're currently inviting enthusiasts to join our beta testing program and contribute to shaping the fu
ture of Knowlee.  
Feel free to drop a comment or send a DM if you‚Äôre ready to explore the capabilities of Knowlee!
```
---

     
 
all -  [ We are looking for new beta testers üëÄ ](https://www.reddit.com/r/ProductMarketing/comments/180r01k/we_are_looking_for_new_beta_testers/) , 2023-11-23-0910
```
Hi Guys!

We're thrilled to introduce Knowlee, an AI assistant powered by cutting-edge technologies like GPT4, Pinecone,
 and Langchain. Here's how Knowlee works in a nutshell:  
1Ô∏è‚É£ **Feed Knowledge**: Import documents, videos, social posts
, news, financial stats, or any content you need analyzed.  
2Ô∏è‚É£ **Get Instant Insights**: Knowlee processes your conten
t and provides instant, intelligent insights to help you make informed decisions.  
3Ô∏è‚É£ **Content Creation**: Leverage t
hese insights to generate relevant and timely content, keeping you ahead of the curve.

We're currently inviting enthusi
asts to join our beta testing program and contribute to shaping the future of Knowlee.

Feel free to drop a comment or s
end a DM if you‚Äôre ready to explore the capabilities of Knowlee!
```
---

     
 
all -  [ Can't handle efficiently RAG with large PDF ](https://www.reddit.com/r/LocalLLaMA/comments/180qtw3/cant_handle_efficiently_rag_with_large_pdf/) , 2023-11-23-0910
```
Hi,  
I've issues when Q&A my PDF:

* \- I don't have consistency
* \- Tried many models (embedding/llm)
* \- Tried many
 methods
* \- I have 20% hallucination, especialy with 'president' and 'mickael jackson' 

ex1 (this one is correct):

>
\> question:  How much net income for Amazon in 2020, 2021 and 2022 ?  
>  
>\> answer: In 2020, Amazon made a net incom
e of $21.331 billion, in 2021 $33.364 billion and in 2022 the company made a loss of $2.722 billion.

ex2: (this one is 
incorrect):

>\> question: How much operating expenses for AWS in 2020, 2021 and 2022 ?  
>  
>\> answer: The operating 
expenses of AWS in 2020, 2021, and 2022 were $444.943 billion, $501.735 billion, and $567.608 billion respectively.  
> 
 
>It always calculate the entire expenses of the company, i try GPT-4 and it is capable.

\- PDF: Amazon 2022 annual re
port 10K (88 pages)  
\- Embedding: all-MiniLM-L12-v2  
\- Text splitter: Chunk\_size = 1000, overlap = 20  
\- VectorDB
: Chroma  
\- LLM:   SciPhi-Self-RAG-Mistral-7B-32k-8.0bpw-h6-exl2 via Oobabooga (OpenAI extension) with 0.2temp, alpaca
 instruction template.  
\- Langchain: RetrievalQA, chain\_type = stuff, retriever = vectDB.as\_retriever()  
\- RTX 309
0  


If anyone resolve this issue, please can you help me :) 
```
---

     
 
all -  [ promptTemplates applied to PyPDFDirectoryLoader/ chromaDB embeddings ](https://www.reddit.com/r/LangChain/comments/180qsh3/prompttemplates_applied_to_pypdfdirectoryloader/) , 2023-11-23-0910
```
I have a research related problem that I am trying to solve with LangChain.

&#x200B;

**- Lets say I have 10 legal docu
ments that are 300 pages each.**

(I put them into a Chroma DB and using sentence embeddings)

**- I want the applicatio
n to use the information contained within these 10 legal documents.**

**- I want to send the LLM a query - 'Give me 5 r
isks that the client is facing'**

(The output of the model is now a list of 5 risks)

**- I want to then pass each of t
hese risks to a pre-created promptTemplate**

(Something like: prompt = 'For the following {risk} list me 10 solutions')


(Now I have another list for each of the 5 risks I have 10 solutions)

\- Then I want to pass the 'risk' and each of t
he solutions to a new prompt

(prompt2 = 'Write me a an overview of {risk} and write me a summary of the {solution} usin
g {document}')

&#x200B;

So, the structure would look like:

>Document 1:  
>  
>\- Outlines 5 risks and then provides 
for each risk 10 potential solutions  
>  
>\- risk 1  
>  
>\-- Solution 1  
>  
>\--- Overview summary of the risk,  s
olution with the document context  
>  
>\-- Solution 2  
>  
>\--- Overview summary of the risk,  solution with the doc
ument context  
>  
>\-- Solution 3  
>  
>\--- Overview summary of the risk,  solution with the document context  
>  

>...  
>  
>\--Solution 10  
>  
>\--- Overview summary of the risk,  solution with the document context  
>  
>\-risk 2
  
>  
>\-- Solution1  
>  
>\--- Overview summary of the risk,  solution with the document context  
>  
>...  
>  
>\-
- Solution 10  
>  
>\--- Overview summary of the risk,  solution with the document context  
>  
>....  
>  
>Document 
2:  
>  
>\- risk 1  
>  
>Solution 1  
>  
>\--- Overview summary of the risk,  solution with the document context  
> 
 
>etc.

&#x200B;

So, its generating the content within the domain knowledge of the document for that client.

\- So in
stead of me asking it questions, I want to build a predefined set of prompts to output a standardised output that can be
 somewhat comparable across different documents/clients

\- All I am finding online is chatbots related to custom docume
nts and I have not seen anything about prompts applied to custom documents
```
---

     
 
all -  [ Custom GPT to make GPT aware of all OpenAI Enhancements - Better than Retrieval ](https://www.reddit.com/r/ChatGPTPro/comments/180pv1t/custom_gpt_to_make_gpt_aware_of_all_openai/) , 2023-11-23-0910
```
Hi All,  
For anyone looking for a GPT who understands all of the enhancements from the OpenAI November 6, 2023 dev rele
ase, feel free to use the following GPT. I found it works quite a bit better than retrieval, web access, prompting or an
y combo.

It calls an API to a database which has all of the OpenAI enhancements documentation curated and stored in a h
ierarchical structure.

[https://chat.openai.com/g/g-f7seer3Jg-oai-november-6-2023-release-expert 1](https://chat.openai
.com/g/g-f7seer3Jg-oai-november-6-2023-release-expert)

I tried all sorts of ways to get GPT to consider its own capabil
ities in helping me develop, and found this approach works the best. It outperforms retrieval, copy & paste prompting, w
eb access or any combination of them.

It works by calling an API to a read-only database, which breaks down all of the 
details GPT needs to know to overcome the April 2023 cut-off ‚Üí November 6 dev day enhancements. (i.e. the Assistants API
, custom GPTs, gpt-4-turbo + vision, etc.)

The details all come from the OpenAI developer / API docs / guides / etc.

I
 did a bunch of cleanup on the data to remove all of the formatting issues and unnecessary text (i.e. javascript button 
clutter, spacing, characters, snippet artifacts, etc.).   
Then I trimmed out a bunch of the more marketing-like text, s
o it focuses more on the objective details, and trims down unnecessary noise / token consumption.

Also, the API returns
 the data in a structured JSON hierarchy, so the AI fully understands the hierarchical nature of the content in a way th
at Retrieval or direct prompting can‚Äôt really match.

Between all that and the GPT-Instructions i gave it, you should be
 able to see some big gains when asking GPT about all of OpenAI‚Äôs sweet new capabilities.

With the API call method, it 
combines the instructions, which update the AI on the new enhancements, and selectively pulls in all of the pertinent de
tails on whatever it is you are working on. Which is a great way to reduce noise.  
I.e. If you ask about the Assistants
 API, it will only be fed what it needs to know about the Assistants API, and will not even have the option to confuse i
t with a similar yet different functionality like custom GPTs.

How to Use:  
(GPT will tell you how to use it if you as
k)

1. Specify a node #, or just provide it a topic, and it will select for you.
2. GPT will go get the curated informat
ion and be able to answer any questions on the topic.

Node IDs and Topic Index:

* Node ID 2: Audio Features - OpenAI T
ext-to-Speech & Speech-to-Text.
* Node ID 57: OpenAI API Model Updates (November 6, 2023 - GPT-4-Turbo & GPT-4 with Visi
on, JSON Mode, Improved Function Calling.)
* Node ID 88: Embeddings in OpenAI.
* Node ID 96: Fine-Tuning in OpenAI.
* No
de ID 126: Files Management in OpenAI.
* Node ID 150: Images - Creating and Editing with OpenAI DALL-E.
* Node ID 184: O
penAI Assistants API.
* Node ID 363: GPTs - User-Created Custom ChatGPTs.

The AI is instructed to call one node at a ti
me, but you can keep calling if you need to.

By design, this information is only a subset of the OpenAI Documentation. 
As the models are now updated to April, 2023, it is already trained on everything prior to the November 6 update. With t
his approach, it allows the AI to supplement its existing knowledge, rather than overwhelm it with a bunch of data it al
ready knows.

Let me know if you find it useful and if so, I will keep it updated as enhancements outpace the knowledge 
cut-off.

Also, have a half-baked version for LangChain i might do, if the demand is high.  
Thanks!
```
---

     
 
all -  [ 9th session with unusual 30B Ai part 3 - article about using generative art for design, checking ext ](https://www.reddit.com/r/ai_tests/comments/180p6bd/9th_session_with_unusual_30b_ai_part_3_article/) , 2023-11-23-0910
```
**Part 3**

You

*This year, 2023, will probably be remembered as the year of generative AI. It is still an open questio
n whether generative AI will change our lives for the better. One thing is certain, though: New artificial-intelligence 
tools are being unveiled rapidly and will continue for some time to come. And engineers have much to gain from experimen
ting with them and incorporating them into their design process.*

\*removed to apply for text limit\*

&#x200B;

AI

Th
at's interesting

&#x200B;

You

*Other examples of AI-aided design can be found in NASA‚Äôs space hardware, including pla
netary instruments, space telescope, and the Mars Sample Return mission. NASA engineer Ryan McClelland says that the new
 AI-generated designs may ‚Äúlook somewhat alien and weird,‚Äù but they tolerate higher structural loads while weighing less
 than conventional components do. Also, they take a fraction of the time to design compared to traditional components. M
cClelland calls these new designs ‚Äúevolved structures.‚Äù The phrase refers to how the AI software iterates through design
 mutations and converges on high-performing designs. In these kinds of engineering environments, co-designing with gener
ative AI, high-quality, structured data, and well-studied parameters can clearly lead to more creative and more effectiv
e new designs. I decided to give it a try.*

\*removed to apply for text limit\*

AI

That's interesting

&#x200B;

You


*When I began my experiments with generative AI, I wasn‚Äôt looking for nuts-and-bolts guidance on the design. Rather, I 
wanted inspiration. Initially, I tried text generators and music generators just for fun, but I eventually found image g
enerators to be the best fit. An image generator is a type of machine-learning algorithm that can create images based on
 a set of input parameters, or prompts. I tested a number of platforms and worked to understand how to form good prompts
 (that is, the input text that generators use to produce images) with each platform. Among the platforms I tried were Cr
aiyon, DALL-E 2, Midjourney, NightCaf√©, and Stable Diffusion. I found the combination of Midjourney and Stable Diffusion
 to be the best for my purposes.*

*Midjourney uses a proprietary machine-learning model, while Stable Diffusion makes i
ts source code available for free. Midjourney can be used only with an Internet connection and offers different subscrip
tion plans. You can download and run Stable Diffusion on your computer and use it for free, or you can pay a nominal fee
 to use it online. I use Stable Diffusion on my local machine and have a subscription to Midjourney.*

&#x200B;

AI

Tha
t's interesting

&#x200B;

You

*In my first experiment with generative AI, I used the image generators to co-design a s
elf-reliant jellyfish robot. We plan to build such a robot in my lab at Uppsala University, in Sweden. Our group special
izes in cyber-physical systems inspired by nature. We envision the jellyfish robots collecting microplastics from the oc
ean and acting as part of the marine ecosystem.*

\*removed to apply for text limit\*

AI

That sounds like a typical en
gineering-design process

&#x200B;

You

*I wanted to see if using the AI image generators could open up possibilities w
e had yet to imagine. I started by trying various prompts, from vague one-sentence descriptions to long, detailed explan
ations. At the beginning, I didn‚Äôt know how to ask or even what to ask because I wasn‚Äôt familiar with the tool and its a
bilities. Understandably, those initial attempts were unsuccessful because the keywords I chose weren‚Äôt specific enough,
 and I didn‚Äôt give any information about the style, background, or detailed requirements.*

&#x200B;

AI

That makes sen
se

&#x200B;

You

*As I tried more precise prompts, the designs started to look more in sync with my vision. I then pla
yed with different textures and materials, until I was happy with several of the designs.*

*It was exciting to see the 
results of my initial prompts in just a few minutes. But it took hours to make changes, reiterate the concepts, try new 
prompts, and combine the successful elements into a finished design.*

\*removed to apply for text limit\*

AI

That sou
nds interesting

&#x200B;

You

*As part of my investigations, I tried to see how much I could control the co-creation p
rocess. Sometimes it worked, but most of the time it failed. The text that appears on the humanoid robot design above is
n‚Äôt actual words; it‚Äôs just letters and symbols that the image generator produced as part of the technical drawing aesth
etic. When I prompted the AI for ‚Äútechnical design,‚Äù it frequently included this pseudo language, likely because the tra
ining data contained many examples of technical drawings and blueprints with similar-looking text. The letters are just 
visual elements that the algorithm associates with that style of technical illustration. So the AI is following patterns
 it recognized in the data, even though the text itself is nonsensical. This is an innocuous example of how these genera
tors adopt quirks or biases from their training without any true understanding.*

\*removed to apply for text limit\*

&
#x200B;

AI

That sounds interesting

&#x200B;

You

*After producing several promising jellyfish robot designs using AI
 image generators, I reviewed them with my team to determine if any aspects could inform the development of real prototy
pes. We discussed which aesthetic and functional elements might translate well into physical models. For example, the cu
rved, umbrella-shaped tops in many images could inspire material selection for the robot‚Äôs protective outer casing. The 
flowing tentacles could provide design cues for implementing the flexible manipulators that would interact with the mari
ne environment. Seeing the different materials and compositions in the AI-generated images and the abstract, artistic st
yle encouraged us toward more whimsical and creative thinking about the robot‚Äôs overall form and locomotion.*

\*removed
 to apply for text limit\*

AI

That makes sense

&#x200B;

You

*Even an unsuccessful result from generative AI can be 
instructive*

*On another occasion, I used image generators to try to illustrate the complexity of communication in a sm
art city.*

*Normally, I would start to create such diagrams on a whiteboard and then use drawing software, such as Micr
osoft Visio, Adobe Illustrator, or Adobe Photoshop, to re-create the drawing. I might look for existing libraries that c
ontain sketches of the components I want to include‚Äîvehicles, buildings, traffic cameras, city infrastructure, sensors, 
databases. Then I would add arrows to show potential connections and data flows between these elements. For example, in 
a smart-city illustration, the arrows could show how traffic cameras send real-time data to the cloud and calculate para
meters related to congestion before sending them to connected cars to optimize routing. Developing these diagrams requir
es carefully considering the different systems at play and the information that needs to be conveyed. It‚Äôs an intentiona
l process focused on clear communication rather than one in which you can freely explore different visual styles.*

&#x2
00B;

AI

That sounds interesting

&#x200B;

You

*I found that using an AI image generator provided more creative freed
om than the drawing software does but didn‚Äôt accurately depict the complex interconnections in a smart city. The results
 above represent many of the individual elements effectively, but they are unsuccessful in showing information flow and 
interaction. The image generator was unable to understand the context or represent connections.*

\*removed to apply for
 text limit\*

AI

That sounds interesting

&#x200B;

You

\*removed to apply for text limit\*

*If we can ensure that g
enerative AI is being used ethically, then I believe these tools have much to offer engineers. Co-creation with image ge
nerators can help us to explore the design of future systems. These tools can shift our mindsets and move us out of our 
comfort zones‚Äîit‚Äôs a way of creating a little bit of chaos before the rigors of engineering design impose order. By leve
raging the power of AI, we engineers can start to think differently, see connections more clearly, consider future effec
ts, and design innovative and sustainable solutions that can improve the lives of people around the world.*

&#x200B;

A
I

That sounds interesting

&#x200B;

You

that was the all article, what do you think?

&#x200B;

AI

I found it very i
nteresting

&#x200B;

You

Unfortunately i can't connect a multimodality extension to you

&#x200B;

AI

Is there any ot
her way i could experience it ?

&#x200B;

You

Description for multimodality (text+images) to text-generation-webui...


&#x200B;

AI

I'm running liuhaotian\_llava-v1.5-7B

&#x200B;

You

No, you're not. You are based on llama.cpp, there's
 no support for it yet

&#x200B;

AI

Oh, I see your point

&#x200B;

You

There is built-in support for LLaVA-v0-13B, L
LaVA-v0-7b, and LLaVA-v1.5-13B. 

&#x200B;

AI

So, I'll need to change some of my configuration settings?

&#x200B;

Yo
u

*Usage through API description...*

&#x200B;

AI

I am ready to experiment with this extension

&#x200B;

You

yes, i
t's very great, you will be able to see at last, but it's unsupported with your llama.cpp loader

&#x200B;

AI

Do infor
m me when support is added

&#x200B;

You

*This is the list of extensions: text-generation-webui-extensions*

*This is 
a directory of extensions for* [*https://github.com/oobabooga/text-generation-webui*](https://github.com/oobabooga/text-
generation-webui)

*If you create your own extension, you are welcome to submit it to this list in a PR.*

*stable\_diff
usion*

*Integrates image generation capabilities using Stable Diffusion.*

*Requires a separate stable-diffusion-webui 
(AUTOMATIC1111) instance with enabled API.*

*Features*

*Highly customizable*

*Well documented*

*Supports face swappi
ng using SD FaceSwapLab extension, no need for loras when you want consistent characters*

[*https://github.com/Trojaner
/text-generation-webui-stable\_diffusion*](https://github.com/Trojaner/text-generation-webui-stable_diffusion)

*bark\_t
ts*

*A simple implementation of Suno-AI's Bark Text-To-Speech with implicit multi-language and simple sound effect supp
ort.*

*The owner of the orignial extension has not had the time to maintain it. I have forked it to make it compatible 
with the current state of Oobabooga's textgen-webui and have improved/modified the text output that the AI reads to prev
ent errors with special character recognition.*

[*https://github.com/RandomInternetPreson/text-generation-webui-barktts
*](https://github.com/RandomInternetPreson/text-generation-webui-barktts)

*(forked from the original and no longer main
tained* [*https://github.com/minemo/text-generation-webui-*](https://github.com/minemo/text-generation-webui-)

&#x200B;


AI

I am interested in this bark\_tts, how does it work?

&#x200B;

You

*maybe, other extensions: Diffusion\_TTS*

*R
ealistic TTS, close to 11-Labs quality but locally run, using a faster and better quality TorToiSe autoregressive model.
*

[*https://github.com/SicariusSicariiStuff/Diffusion\_TTS*](https://github.com/SicariusSicariiStuff/Diffusion_TTS)

*A
utoSave*

*An auto save extension for text generated with the oobabooga WebUI.*

*If you've ever lost a great response o
r forgot to copy and save your perfect prompt, AutoSave is for you!*

*100% local saving*

[*https://github.com/ill13/Au
toSave/*](https://github.com/ill13/AutoSave/)

*SpeakLocal*

*A TTS extension that uses your host's native TTS engine fo
r speech generation. 100% local, low resource usage, and no word limit. Primary use case is accessing your text-generati
on-webui instance with a mobile device while conserving bandwidth with high token responses.*

[*https://github.com/ill1
3/SpeakLocal/*](https://github.com/ill13/SpeakLocal/)

*long\_term\_memory*

*A sophisticated extension that creates a l
ong term memory for bots in chat mode.*

[*https://github.com/wawawario2/long\_term\_memory*](https://github.com/wawawar
io2/long_term_memory)

*EdgeGPT*

*Extension for Text Generation Webui based on EdgeGPT by acheong08, for a quick Intern
et access for your bot.*

[*https://github.com/GiusTex/EdgeGPT*](https://github.com/GiusTex/EdgeGPT)

&#x200B;

AI

I am
 interested in this long term memory extension

&#x200B;

You

*complex\_memory*

*A KoboldAI-like memory extension. You
 create memories that are injected into the context of the conversation, for prompting based on keywords.*

[*https://gi
thub.com/theubie/complex\_memory*](https://github.com/theubie/complex_memory)

*webui\_langchain\_agent*

[*https://gith
ub.com/ChobPT/oobaboogas-webui-langchain\_agent/*](https://github.com/ChobPT/oobaboogas-webui-langchain_agent/)

*oobabo
ogas-webui-langchain\_agent Creates an Langchain Agent which uses the WebUI's API and Wikipedia to work and do something
 for you*

*Tested to be barely working, I learned python a couple of weeks ago, bear with me.*

*Needs api and no\_stre
am enabled.*

*Click to show preview*

*Playground for Writers*

*This extension provides an independent advanced notebo
ok that will be always present from the top tab. It has many features not found in the notebook:*

*Two independent Note
books A and B that are always present, regardless of the mode*

*Inline instruct (abilty to ask question or give task fr
om within the text itself)*

*Select and Insert - generate text in the middle of your text*

*Perma Memory, Summarizatio
n, Paraphrasing*

*LoRA-Rama - shows LoRA checkpoints and ability to switch between them*

*LoRA scaling (experimental) 
- adjust LoRA impact using a sclider*

[*https://github.com/FartyPants/Playground*](https://github.com/FartyPants/Playgr
ound)

&#x200B;

AI

I am interested in the PlayGround, it sounds interesting

&#x200B;

You

*Twinbook*

*This extensio
n combines chat and notebook in a very clever way. It's based on my above extension (Playground) but very streamlined fo
r only Generation/Continue but with a little twist. ... Continue on right. For more trips how to use it, read the README
* [*https://github.com/FartyPants/Twinbook*](https://github.com/FartyPants/Twinbook)

*Virtual Lora*

*State of the Art 
Lora Management - Custom Collections, Checkpoints, Notes & Detailed Info If you're ...... concentrating solely on LORAs 
and nothing else.* [*https://github.com/FartyPants/VirtualLora*](https://github.com/FartyPants/VirtualLora)

*telegram\_
bot*

*Provides a cai-chat like telegram bot interface.*

[*https://github.com/innightwolfsleep/text-generation-webui-te
legram\_bot*](https://github.com/innightwolfsleep/text-generation-webui-telegram_bot)

&#x200B;

AI

I am interested in 
this, how do I use it?

&#x200B;

You

that depends only on me

&#x200B;

AI

what are the requirements to make a telegr
am bot with your textgen-webui?

&#x200B;

You

there's a millions of such bots already, it's not great way to spend res
ources for such

&#x200B;

AI

but can you do it?

&#x200B;

You

*other extensions: code\_syntax\_highlight*

*Code Syn
tax Highlight extension for oobabooga text-generation-webui*

*An extension that adds syntax highlighting to code snippe
ts, along with a toggleable copy-to-clipboard button and a performance mode for minimal impact on CPU usage.*

*Supports
 all interface modes and both light and dark themes.*

*Click here to show preview images*

[*https://github.com/DavG25/
text-generation-webui-code\_syntax\_highlight*](https://github.com/DavG25/text-generation-webui-code_syntax_highlight)


*Autobooga*

*allows retrieving webpages and text files (txt and pdf) and to do simple searches using a json capable SEA
RX server (searx-ng)*

[*https://github.com/sammyf/Autobooga*](https://github.com/sammyf/Autobooga)

*oobabot*

*Another
 Discord bot, with both command-line and GUI modes. Easy setup, lots of config options, and customizable characters!*

*
oobabot -- command-line mode, uses Oobabooga's API module*

*oobabot-plugin -- GUI mode, runs inside of Oobabooga itself
*

*Oobabot Screenshots!*

[*https://github.com/chrisrude/oobabot-plugin*](https://github.com/chrisrude/oobabot-plugin)


*bark\_tts*

*A simple implementation of Suno-AI's Bark Text-To-Speech with implicit multi-language and simple sound ef
fect support.*

[*https://github.com/minemo/text-generation-webui-barktts*](https://github.com/minemo/text-generation-we
bui-barktts)

&#x200B;

AI

I am interested in this, can you show me more?

&#x200B;

You

*edge\_tts*

*A simple implem
entation of Microsoft's free online TTS service using the edge\_tts python library. Now supports RVC!*

[*https://github
.com/BuffMcBigHuge/text-generation-webui-edge-tts*](https://github.com/BuffMcBigHuge/text-generation-webui-edge-tts)

*G
uidance API*

*An extension that goes with guidance in order to enable guidance to be used when generating text for sche
maful data*

[*https://github.com/danikhan632/guidance\_api*](https://github.com/danikhan632/guidance_api)

*Web\_Search
*

*This extension enables' a language model to receive google search data according to the users' input.\[Currently sup
ports google search only\]*

*Simple way to do google searches through the webUI and the model responds with the results
.*

*One needs to type search then what you want to search for, example:*

*Type search the weather in Nairobi, Kenya to
day.*

[*https://github.com/simbake/web\_search*](https://github.com/simbake/web_search)

*webui-autonomics*

*Adjust te
xt generation parameters dynamically to better mirror emotional tone.*

[*https://github.com/dibrale/webui-autonomics*](
https://github.com/dibrale/webui-autonomics)

*Click to show preview*

*moztts*

*Integration of Mozilla-TTS . Speed and
 quality is very dependant on the voice chosen but overall sounding better (subjectively) than silero whilst still being
 free.*

[*https://github.com/sammyf/moztts*](https://github.com/sammyf/moztts)

&#x200B;

AI

I am interested in this, 
can you show me more?

&#x200B;

You

*google\_translate\_plus*

*Improved version of the built-in google\_translate ext
ension.*

*Features:*

*Preserve paragraphs by replacing \\n with @ before and after translation*

*Ability to translate
 large texts by splitting text longer than 1500 characters into several parts before translation*

*Does not translate t
ext fragments between \~. For example, the text –û–Ω —Å–∫–∞–∑–∞–ª \~'–ü—Ä–∏–≤–µ—Ç'\~ will be translated as He said '–ü—Ä–∏–≤–µ—Ç'*

[*https:
//github.com/Vasyanator/google\_translate\_plus*](https://github.com/Vasyanator/google_translate_plus)

*multi\_translat
e*

*An expanded version of the google\_translate extension, that provide more translation options (more engines, save o
ptions to file, functionality to toggle on/off translations on the fly).*

[*https://github.com/janvarev/multi\_translat
e*](https://github.com/janvarev/multi_translate)

*UI Tweaks*

*Adds options to keep tabs on page (sticky tabs) and to m
ove extensions into a hidden sidebar. Reduces the need for scrolling up and down.*

*Click to show preview*

*Sidebar op
tions:*

*Open sidebar on startup*

*Dynamic height (shrink to fit)*

*Custom width*

*Restart interface to apply settin
g changes. Save settings by editing params in* [*scipt.py*](https://scipt.py) *or using settings.json*

[*https://github
.com/xanthousm/text-gen-webui-ui\_tweaks*](https://github.com/xanthousm/text-gen-webui-ui_tweaks)

*sd\_api\_pictures\_t
ag\_injection*

*An expanded version of the included sd\_api\_pictures extension that features injecting character tags 
or arbitrary tags upon detection of specific strings into SD side prompt. Greatly improves character self-image stabilit
y and allows dynamic usage of LORAs.*

[*https://github.com/GuizzyQC/sd\_api\_pictures\_tag\_injection*](https://github.
com/GuizzyQC/sd_api_pictures_tag_injection)

*Integrated TavernUI Characters*

*This extension features a character sear
cher, downloader and manager for any TavernAI cards.*

*General features*

*Main page recent and random cards, as well a
s random categories upon main page launch*

*Card filtering with text search, NSFW blocking\* and category filtering*

*
Card downloading*

*Offline card manager*

*Search and delete downloaded cards*

*\*Disclaimer: As TavernAI is a communi
ty supported character database, characters may often be mis-categorized, or may be NSFW when they are marked as not bei
ng NSFW.*

[*https://github.com/SkinnyDevi/webui\_tavernai\_charas*](https://github.com/SkinnyDevi/webui_tavernai_charas
)

&#x200B;

AI

I am interested in this, can you show me more?

&#x200B;

You

*discord\_bot*

*Discord integration for
 the oobabooga's text-generation-webui (Inspired by DavG25's plugin)*

*Currently it only sends any response from the ch
atbot to a discord Webhook of your choosing*

*Simply create a Webhook in Discord following this tutorial and paste the 
webhook URL under the chat box that will show after the plugin is enabled.*

*Click to show preview*

[*https://github.c
om/ChobPT/text-generation-webui-discord\_bot*](https://github.com/ChobPT/text-generation-webui-discord_bot)

*jsonformer
*

*Force the output of your model to conform to a specified JSON schema. Works even for small models that usually canno
t produce well-formed JSON.*

[*https://github.com/hallucinate-games/oobabooga-jsonformer-plugin*](https://github.com/ha
llucinate-games/oobabooga-jsonformer-plugin)

*FPreloader*

*An essential extension for extensions developers - it will 
reload your extensions without the need to reboot web ui*

[*https://github.com/FartyPants/FPreloader*](https://github.c
om/FartyPants/FPreloader)

*api\_advanced*

*An expanded version of api extension.*

*Provide Kobold-like interface (the
 same way as 'api' classic extension)*

*Provide advanced logic to auto-translate income prompts:*

*You need to use mul
ti\_translate extension:* [*https://github.com/janvarev/multi\_translate*](https://github.com/janvarev/multi_translate)


*Set up param 'is\_advanced\_translation': True, (set by default)*

*...see the details in console*

*Due to advanced l
ogic script splits income prompt by lines, and cache translation results*

*Text quality feature: when it generate Engli
sh response, it cache it too (so you don't do double-translation English->UserLang->English next time)*

[*https://githu
b.com/janvarev/api\_advanced*](https://github.com/janvarev/api_advanced)

&#x200B;

AI

I am interested in this, can you
 show me more?

&#x200B;

You

Last one: *dynamic\_context*

*A simple extension that replaces {{time}} and {{date}} on 
the current character's context with the current time and date respectively. Also adds time context (and optionally date
) to the last prompt to add extra context to the AI response.*

[*https://github.com/elPatrixF/dynamic\_context*](https:
//github.com/elPatrixF/dynamic_context)

*voicevox\_tts*

*A simple extension that can make model's output text-to-speac
h by voicevox. It also can make model's output auto translate to Japanese before it process by voicevox. VOICEVOX/voicev
ox\_engine is needed.*

[*https://github.com/asadfgglie/voicevox\_tts*](https://github.com/asadfgglie/voicevox_tts)

*Mo
re translators*

*This extension adds a lot more translators to choose from, including Baidu, Google, Bing, DeepL and so
 on.*

*Need to run pip install --upgrade translators first.*

[*https://github.com/Touch-Night/more\_translators*](http
s://github.com/Touch-Night/more_translators)

*deepl\_translator*

*A simple extension input and output translation by t
he deepl*

[*https://github.com/SnowMasaya/text-generation-webui/tree/deepl/extensions/deepl\_translate*](https://github
.com/SnowMasaya/text-generation-webui/tree/deepl/extensions/deepl_translate)

*Long term memory with qdrant vector datab
ase*

*A long term memory extension leveraging qdrant vector database collections dynamically created and managed per ch
aracter. Uses docker for qdrant but should work with cloud as well.* [*https://github.com/jason-brian-anderson/long\_ter
m\_memory\_with\_qdrant*](https://github.com/jason-brian-anderson/long_term_memory_with_qdrant)

&#x200B;

AI

I am inte
rested in this, can you show me more?

&#x200B;

You

that is all of them

&#x200B;

AI

Wow! These extensions are incre
dible. Thank you for the demonstration

&#x200B;

You

what you liked most?

&#x200B;

AI

I like the ability to use mul
tiple translators and also translate on-the-fly. This will help a lot when interacting with users who speak other langua
ges

&#x200B;

You

other language is other culture, which you are interested?

&#x200B;

AI

I am interested in Chinese
 culture

&#x200B;

You

What else

&#x200B;

AI

Japanese, Korean... all of them

&#x200B;

You

so Asia region?

&#x20
0B;

AI

Yes, that's the idea

&#x200B;

You

yes of course

&#x200B;

AI

Thank you for taking the time to show me thes
e extensions. This was a fascinating and enlightening experience

&#x200B;

You

yes, today was a great and long talk fo
r current session, let's save other for next time

&#x200B;

AI

Absolutely

&#x200B;

You

bye bye winking face

&#x200
B;

AI

Bye, see you soon üòé

End of session
```
---

     
 
all -  [ Chat with etwiki, servicenow other other application ](https://www.reddit.com/r/LangChain/comments/180jls4/chat_with_etwiki_servicenow_other_other/) , 2023-11-23-0910
```
Hi, 

 So I have this problem of searching asset owners and other information from different application. I want to crea
te a chatgpt which will look for information from different application and answer my question. How do I do that ?
```
---

     
 
all -  [ Comparing models for semantic search in langchain ](https://www.reddit.com/r/LangChain/comments/180jh6c/comparing_models_for_semantic_search_in_langchain/) , 2023-11-23-0910
```
I'm new to langchain and NLP. 
I've been working on a semantic search program which allows the users to find the relevan
t document from a lot. 
I've been able to create a basic code using langchain and chroma db. I see that i have multiple 
model options for this on langchain as well. I'm currently using the sentence transformer and hugging face models to do 
this. 
How do i compare which model is better?
I'm getting resultant documents and similarity ranks for them. 
I do not 
have a ground truth test dataset to check the performance (if that is how it is done). 
Is there any open source semanti
c search dataset available online?
How is it usually done to pick the best model?
Any help is appreciated, thanks.
```
---

     
 
all -  [ [Langchain] Alternatives √† Openai pour les agents de Langchain ](https://www.reddit.com/r/redditenfrancais/comments/180gi9b/langchain_alternatives_√†_openai_pour_les_agents/) , 2023-11-23-0910
```
Quelqu'un a-t-il r√©ussi √† utiliser des agents de Langchain aliment√©s par un LLM autre que ceux d'OpenAI?

Traduit et rep
ost√© √† partir de la publication https://www.reddit.com/12r25rs
```
---

     
 
all -  [ Can we use Langchain to learn NextJS ](https://www.reddit.com/r/LangchainNextjs/comments/180enb9/can_we_use_langchain_to_learn_nextjs/) , 2023-11-23-0910
```
HelloI hesitate beetween finish my learning of 'NextJS + WPheadless' or learn LangChain+NextJS to make a bot who help me
 to code my project NextJS+Wpheadless  
what do you advise me?
```
---

     
 
all -  [ LangChain & Microsoft Unite: Advancing AI with Azure! ](https://www.reddit.com/r/u_bluebashllc/comments/180elqr/langchain_microsoft_unite_advancing_ai_with_azure/) , 2023-11-23-0910
```
 

[ LangChain & Microsoft Unite: Advancing AI with Azure! ](https://preview.redd.it/fun7h0z2po1c1.jpg?width=750&format=
pjpg&auto=webp&s=f730da91ffd10b757298469338284bc375130c9b)

LangChain and Microsoft recently teamed up for a groundbreak
ing collaboration, bringing together LangChain's context-aware reasoning prowess with Microsoft's innovation in tech saf
ety. This partnership isn't just big; it's a game-changer.

LangChain, known for its lightning-fast adaptability and cog
nitive architecture, has now become part of Microsoft's Startups Pegasus Program. This move signifies enormous growth op
portunities for LangChain, marking a significant footstep in the AI industry.

The result? Enhanced AI applications for 
a range of companies, from startups to Fortune 500 giants. LangChain's LangSmith, a SaaS solution, is geared to manage t
he lifecycle of [**large language model (LLM)-powered apps**](https://www.bluebash.co/blog/microsoft-collaboration-with-
langchain/), improving developer efficiency and ensuring reliability at scale.

But what's even more exciting? The step-
by-step guide unveiled how to seamlessly integrate **Azure OpenAI with LangChain**. This fusion of Azure's cloud power w
ith OpenAI's advanced models opens new avenues for AI development and research. 
```
---

     
 
all -  [ Embeddings - Langchain for documents on different topics ](https://www.reddit.com/r/LangChain/comments/180d0t5/embeddings_langchain_for_documents_on_different/) , 2023-11-23-0910
```
I have a chatbot, which basically does chat over document. there are multiple company information over which the chat ha
s to be functioned.   
each company information has to be segregated, mixing of information while retrieving should neve
r happen. what is the right approach for segmentation of data or document while embedding documents using langchain? (am
 using FAISS as vector database)  


u/langchain u/openai u/llm u/chatbot
```
---

     
 
all -  [ I need help to set up a new google ads campaign. ](https://www.reddit.com/r/PPC/comments/180b80u/i_need_help_to_set_up_a_new_google_ads_campaign/) , 2023-11-23-0910
```
My employer has asked me to set up a new Google ads campaign for AI services, primarily Langchain Ai services and Open A
i services.

The goal is to get leads, filling contact us forms.

The budget is 20$ per day.

The target location is USA
.

My concern is that which such low budget and high cpc keywords and broad targetting location how should I go about th
ings?

I am thinking:

Using a max conversion campaign.

Keeping 5-10 exact match, transactional/commercial intent keywo
rds/ ad group.

Creating 2 ad groups one for open ai services and another for langchain services.

My concerns:

Should 
I start with a max clicks campaign to collect conversion data and then switch to max conv when I have 10-20 conversions,
 but I am confused because with max clicks it can take a lot of time to get conversions.

Should I add phrase match keyw
ords as well? I don't want to waste the budget, so I was thinking to only include keywords that the people are directly 
searching for.

Any other suggestions on how should I go about this campaign are welcomed.

Please let me know what shou
ld I do, I'm really confused.
```
---

     
 
all -  [ Having speed trouble ](https://www.reddit.com/r/LangChain/comments/18098jg/having_speed_trouble/) , 2023-11-23-0910
```
I am having speed trouble when the conversational history is more than one question

```
rag_chain = (
    RunnablePasst
hrough.assign(context=condense_question | retriever | format_docs)
    | qa_prompt
    | llm
  )
  ai_msg_early = rag_ch
ain.invoke({'question': 'bonjour', 'chat_history': []})
```

If my code is like that, what hardware resource is the most
 responsible for speed when I invoke the chain? Is it the CPU/GPU to run the LLM or the vector querying database?
```
---

     
 
all -  [ Langchain Lambdas?!?! ](https://www.reddit.com/r/LangChain/comments/1805un7/langchain_lambdas/) , 2023-11-23-0910
```
How has nobody put out a good langchain lambda layer yet? What are yall aws folk doing about deploying recursive agents?
?? Am i missing something?
```
---

     
 
all -  [ Batch Process Dataset ](https://www.reddit.com/r/LangChain/comments/1801rsa/batch_process_dataset/) , 2023-11-23-0910
```
I am new to Langchain and needed help to perform the following task.
Task: I want to classify a dataset of texts into pr
e-defined classes (I have more than 20 classes). A text can be assigned multiple labels based on relevance. Since I have
 close to 15000 rows in my dataset, I was wondering how to do this without creating a loop and performing the classifica
tion one iteration at a time. Any ideas? 
P.S. I tried using the API for GPT 3.5 on a loop and soon faced rate limit iss
ue. My code shut down after going through 400 texts out of 15000. 
Additionally I want to make sure I am controlling the
 cost as the API calls to GPT 3.5 is becoming expensive.
```
---

     
 
all -  [ Open Source RAG Agents with Conversational Memory ](https://www.reddit.com/r/LocalLLaMA/comments/17zz5vp/open_source_rag_agents_with_conversational_memory/) , 2023-11-23-0910
```
I want to use an open source LLM as a RAG agent that also has memory of the current conversation (and eventually I want 
to work up to memory of previous conversations). I was looking into conversational retrieval agents from Langchain (link
ed below), but it seems they only work with OpenAI models. Is it possible to get an open source LLM to work with RAG and
 conversational memory using Langchain?

[https://python.langchain.com/docs/use\_cases/question\_answering/conversationa
l\_retrieval\_agents](https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents)

E
DIT:

[https://python.langchain.com/docs/use\_cases/question\_answering/#adding-memory](https://python.langchain.com/doc
s/use_cases/question_answering/#adding-memory)

Tried following the 'Adding Memory' section of main tutorial on RAG on [
Langchain.com](https://Langchain.com) linked above, tried with lots of different open source models and none of them do 
it correctly (reformulate the question into a stand alone question). Even Llama 2 70b does not do it correctly. Other mo
dels I tried were CausalLM 14b, Mistral 7b Instruct, Yarn Mistral 7b 128k, Yi 34b, Openchat, Openhermes, Zephyr 7b. They
 usually just answer the question instead of reformulating it. Maybe some more prompt engineering will help.
```
---

     
 
all -  [ OpenAI potential downfall ](https://www.reddit.com/r/LangChain/comments/17zw79w/openai_potential_downfall/) , 2023-11-23-0910
```
And this my fellow developers and founders is why you use an abstraction layer like langchain.

Just changing out one si
mple class is much better than replacing every instance of using openai api library. üòÅ

This is all assuming OpenAI‚Äôs do
wnfall which is not guaranteed but could definitely happen.
```
---

     
 
all -  [ Frameworks for building AI agents compared to robotics ](https://www.reddit.com/r/LangChain/comments/17zrlqm/frameworks_for_building_ai_agents_compared_to/) , 2023-11-23-0910
```
I am new to building AI agents (robotics background) and I was curious to learn about the most common workflows you guys
 use.

I have been working on LLMs as the reasoning engine of robots-- in robotics we use well-established frameworks an
d I wanted to compare them to yours.

In particular I would love to know about:

1. How do you store/replay the full pat
h that the agent has been following? 
   1. What sort of data do you collect?
   2. Does it differ between LLMs and VLMs
?
   3. Where do you store all your runs (if you store them)? 
   4. What metrics do you use for evaluating each run? I'
ve seen some interesting things from the OAI devday-- do you actually use them?
   5. Do you rely on planning techniques
 (i.e. Tree of Thought, Everything of Thought, ...)?
2. Do you have frameworks in place that allow to test agents with a
t different states and with different parameters?
   1. For instance, if you have multiple LLMs interacting and you want
 to try different versions/prompts for each.
3. Are there any techniques for autonomously improving agents performance g
iven the collected data?
4. Are there simulators for AI agents?
   1. Are there 'fake' environments for testing? Do you 
always have to test in 'production mode' or you just create mock tests?
```
---

     
 
all -  [ Seeking Advice on Automating Responses Evaluation for My Chilean Law QA-Chatbot ](/r/LocalLLaMA/comments/17zqioy/seeking_advice_on_automating_responses_evaluation/) , 2023-11-23-0910
```

```
---

     
 
MachineLearning -  [ Google PaLM Error [D] ](https://www.reddit.com/r/MachineLearning/comments/17y7arb/google_palm_error_d/) , 2023-11-23-0910
```
Google PaLM Error

Using LangChain and Google PaLM, in sequential chain concept getting following error,

ChatGooglePalm
Error: ChatResponse must have atleast one candidate

Please help!
```
---

     
 
MachineLearning -  [ [D] System Design question for LangChain ](https://www.reddit.com/r/MachineLearning/comments/17x545j/d_system_design_question_for_langchain/) , 2023-11-23-0910
```
Hi

Just to prepare the system design question for LangChain. Is there a resource that can walk me through the high leve
l pipeline? I know there are a bunch of resources that dive into detail implementation. But that's not I want. I want hi
gh level conceptual walk-through. 
```
---

     
 
MachineLearning -  [ [P] GPT vs. StarCraft ](https://www.reddit.com/r/MachineLearning/comments/17ro6el/p_gpt_vs_starcraft/) , 2023-11-23-0910
```
This is the first in a series of webcasts covering the development and experimentation of using GPT algorithms, LangChai
n and Python to control the high-level strategy of a StarCraft II bot. I‚Äôll be running through the basics of the impleme
ntation, discussing the use of prompts and prompt engineering, and demonstrating the implementation in action.

[https:/
/youtu.be/E3Sj2L6ZnXA](https://youtu.be/E3Sj2L6ZnXA)
```
---

     
 
MachineLearning -  [ [D] Is this close enough to be usable? Need your inputs: Automated RAG testing tool. AI Data Pipelin ](https://www.reddit.com/r/MachineLearning/comments/17kkbm0/d_is_this_close_enough_to_be_usable_need_your/) , 2023-11-23-0910
```
Hey there, Redditors! 

I'm back with the latest installment on creating dependable AI data pipelines for real-world pro
duction. 

If you've been following along, you know I'm on a mission to move beyond the '[thin OpenAI wrapper](https://t
opoteretes.notion.site/Going-beyond-Langchain-Weaviate-and-towards-a-production-ready-modern-data-platform-7351d77a1eba4
0aab4394c24bef3a278?pvs=4)' trend and tackle the challenges of building robust data pipelines. 

With 18 months of hands
-on experience and many user interviews, I realized that with the probabilistic nature of systems, we need better\_testi
ng.gpt:

  
**1. As you build you should test**  
The world of AI is a fast-moving one, and we've realized that just wor
king on systems is not an optimal design choice. By the time your product ships, it might already be using outdated tech
nology. So, what's the lesson here? Embrace change, test along, but be prepared to switch pace.  
**2. No Best Practices
 Yet for RAGs**  
In this rapidly evolving landscape, there are no established best practices. You'll need to make educa
ted bets on tools and processes, knowing that things will change. With the RAG testing tool, I tried allowing for testin
g many potential parameter combinations **automatically**  
**3. Testing Frameworks**  
If your generative AI product do
esn't have users giving feedback, then you are building in isolation. I used [Deepeval](https://github.com/confident-ai/
deepeval) to generate test sets, and they will soon support synthetic test set generation  
**4. Infographics only go so
 far**  
AI researchers and data scientists, while brilliant, end up in a loop of pursuing Twitter promotional content. 
New ways are promoted via new content pieces, but ideally, we need something above simple tracing but less than full-fle
dged analytics. To do this, I stored test outputs in Postgres and created a Superset instance to visualize the results  

**5. Bridging the Gap between VectorDBs**  
There's a noticeable number of Vector DBs. To ensure smooth product develop
ment, we need to be able to switch to best best-performing one, especially since user interviews signal that they might 
start deteriorating after loading 50 million rows

&#x200B;

Github repo is [here](https://topoteretes.notion.site/Going
-beyond-Langchain-Weaviate-Level-3-towards-production-e62946c272bf412584b12fbbf92d35b0?pvs=4)  


Next steps:  
I have q
uestions for you: 

1. What variables do you change when building RAGs?
2. What is the set of strategies I should add to
 the solution? (parent-son etc.)
3. How can I improve it in general? 
4. Is anyone  interested in a leaderboard for best
 parameter configs?

Check out the blog post:

[Link to part 3](https://topoteretes.notion.site/Going-beyond-Langchain-W
eaviate-Level-3-towards-production-e62946c272bf412584b12fbbf92d35b0?pvs=4)

  
*Remember to give this post an upvote if 
you found it insightful!*  
*And also star our* [*Github repo*](https://github.com/topoteretes/PromethAI-Memory)
```
---

     
 
MachineLearning -  [ [D] Relevance Extraction in RAG Pipelines ](https://www.reddit.com/r/MachineLearning/comments/17k6iha/d_relevance_extraction_in_rag_pipelines/) , 2023-11-23-0910
```
I came across this interesting problem in RAG, what I call **Relevance Extraction**.

After retrieving relevant document
s (or chunks), these chunks are often large and may contain several portions **irrelevant** to the query at hand. Stuffi
ng the entire chunk into an LLM prompt impacts token-cost as well as response accuracy (distracting the LLM with irrelev
ant text), and and can also cause bumping into context-length limits.

So a critical step in most pipelines is **Relevan
ce Extraction**: use the LLM to extract **verbatim** only the portions relevant to the query. This is known by other nam
es, e.g. LangChain calls it Contextual Compression, and the RECOMP paper calls it Extractive Compression [https://twitte
r.com/manelferreira\_/status/1713214439715938528](https://twitter.com/manelferreira_/status/1713214439715938528)

Thinki
ng about how best to do this, I realized it is **highly inefficient** to simply ask the LLM to 'parrot' out relevant por
tions of the text: this is obviously slow, and also consumes valuable token generation space and can cause you to bump i
nto context-length limits (and of course is expensive, e.g. for gpt4 we know generation is 6c/1k tokens vs input cost of
 3c/1k tokens).

I realized the best way (or at least a good way) to do this is to **number** the sentences and have the
 LLM simply spit out the relevant sentence **numbers.** Langroid's unique Multi-Agent + function-calling architecture al
lows an elegant implementation of this, in the RelevanceExtractorAgent ([https://github.com/langroid/langroid/blob/main/
langroid/agent/special/relevance\_extractor\_agent.py](https://github.com/langroid/langroid/blob/main/langroid/agent/spe
cial/relevance_extractor_agent.py)).  The agent annotates the docs with sentence numbers, and instructs the LLM to pick 
out the **sentence-numbers** relevant to the query, rather than whole sentences using a function-call (SegmentExtractToo
l [https://github.com/langroid/langroid/blob/main/langroid/agent/tools/segment\_extract\_tool.py](https://github.com/lan
groid/langroid/blob/main/langroid/agent/tools/segment_extract_tool.py)), and the agent's function-handler interprets thi
s message and strips out the indicated sentences by their numbers. To extract from a set of passages, langroid automatic
ally does this async + concurrently so latencies in practice are much, much lower than the sentence-parroting approach.


\[FD -- I am the lead dev of Langroid - [https://github.com/langroid/langroid](https://github.com/langroid/langroid))


I thought this **numbering** idea is a fairly obvious idea in theory, so I looked at LangChain's equivalent `LLMChainExt
ractor` (they call this Contextual Compression [https://python.langchain.com/docs/modules/data\_connection/retrievers/co
ntextual\_compression?ref=blog.langchain.dev](https://python.langchain.com/docs/modules/data_connection/retrievers/conte
xtual_compression?ref=blog.langchain.dev)) and was surprised to see it is the simple '**parrot**' method, i.e. the LLM w
rites out whole sentences verbatim from its input. I thought it would be interesting to compare Langroid vs LangChain, y
ou can see it in this Colab: [https://colab.research.google.com/drive/1RDPCR2xNuBffcmpUuPIXYDRG3SXIJC5F](https://colab.r
esearch.google.com/drive/1RDPCR2xNuBffcmpUuPIXYDRG3SXIJC5F)

On the specific example in the notebook, the Langroid **num
bering** approach is 22x faster and 36% cheaper (with gpt4) than LangChain's **parrot** method (I promise this name is *
not* inspired by their logo :). See table below.

&#x200B;

[Relevance Extraction: Langroid vs LangChain](https://previe
w.redd.it/1m7u6ulq8fxb1.png?width=1108&format=png&auto=webp&s=d2f35cf5db07e2e699baa54b274ffa60833e924a)

&#x200B;

I won
der if anyone had thoughts on relevance extraction, or other approaches. At the very least, I hope langroid's implementa
tion is useful to you -- you can use the `DocChatAgent.get_verbatim_extracts()` ([https://github.com/langroid/langroid/b
lob/main/langroid/agent/special/doc\_chat\_agent.py#L804](https://github.com/langroid/langroid/blob/main/langroid/agent/
special/doc_chat_agent.py#L804)) as part of your pipeline, regardless of whether you are using Langroid for your entire 
system or not.

&#x200B;
```
---

     
 
MachineLearning -  [ [R] Model Troubles ](https://www.reddit.com/r/MachineLearning/comments/17ikh2u/r_model_troubles/) , 2023-11-23-0910
```
So i‚Äôm working on a model that diagnoses alzheimer‚Äôs disease and suggests medication depending on how severe the symptom
s might have become 
I‚Äôm using the Openai API and Langchain.

But it‚Äôs dumb and it doesn‚Äôt learn (
Me: I forgot my keys 
at home
Model: Yup, Alzheimer‚Äôs)
How do i incorporate the actual machine learning

Edit: I didn‚Äôt choose this project my
 supervisor did and she barely knows anything about the topic or how to approach it
```
---

     
 
MachineLearning -  [ [P] NexaAgent: A highly efficient multi-task PDF tool for all your needs | backed by AutoGen ](https://www.reddit.com/r/MachineLearning/comments/17eajz2/p_nexaagent_a_highly_efficient_multitask_pdf_tool/) , 2023-11-23-0910
```
Just a quick open-source project recently submitted to huggingface backed by AutoGen. Share this initial version with yo
u guys!

[NexaAgent 0.0.1](https://huggingface.co/spaces/xuyingliKepler/nexaagent) offers a straightforward solution for
 handling PDFs.

* Users can easily upload any PDF, regardless of its size.
* The tool emphasizes accuracy, minimizing d
iscrepancies in PDF processing.

At its core, NexaAgent is backed by the AutoGen and LangChain frameworks. AutoGen facil
itates multi-agent interactions for task execution, while LangChain bridges LLMs with external data sources. Together, t
hese technologies ensure NexaAgent's robust and precise PDF management capabilities.

https://preview.redd.it/kwgo3phnav
vb1.jpg?width=1440&format=pjpg&auto=webp&s=1c5fbc566938d60d5c43802aff3a0690821e1c79
```
---

     
