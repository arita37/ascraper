 
all -  [ Consent in Crisis (NeurIPS 2024) Paper Summary via DeepDive ](https://www.reddit.com/r/learnmachinelearning/comments/1fwpt7r/consent_in_crisis_neurips_2024_paper_summary_via/) , 2024-10-06-0914
```
[https://www.youtube.com/watch?v=1I-ABssKrps](https://www.youtube.com/watch?v=1I-ABssKrps)
```
---

     
 
all -  [ LASR Labs (technical AIS research programme) applications open until Oct 27th ](https://www.reddit.com/r/ControlProblem/comments/1fw5k1d/lasr_labs_technical_ais_research_programme/) , 2024-10-06-0914
```
ðŸš¨**LASR Labs: Spring research programme in AI Safety** ðŸš¨

**When:** Apply by October 27th. Programme runs 10th February-
 9th May.Â 

**Where:** London 

**Details & Application:** [https://www.lesswrong.com/posts/SDatnjKNyTDGvtCEH/lasr-labs-
spring-2025-applications-are-open](https://www.lesswrong.com/posts/SDatnjKNyTDGvtCEH/lasr-labs-spring-2025-applications-
are-open)Â 



**What is it?**Â 

A full-time, 13 week paid (Â£11k stipend) research programme for people interested in car
eers in technical AI safety. Write a paper as part of a small team with supervision from an experienced researcher. **Pa
st alumni have gone on to Open AI dangerous capability evals team, UK AI Safety Institute or continued working with thei
r supervisors. In 2023, 4 out of 5 groups had papers accepted to workshops or conferences (ICLR, NeurIPS).**  



**Who 
should apply?**Â 

Weâ€™re looking for candidates with \~2 years experience in relevant postgraduate programmes or industry
 roles (Physics, Math or CS PhD, Software engineering, Machine learning, etc). You might be a good fit if youâ€™re excited
 about:

* Producing empirical work, in an academic style
* Working closely in a small team




```
---

     
 
all -  [ [D] Option to make NeurIPS rejected paper reviews public? ](https://www.reddit.com/r/MachineLearning/comments/1fvy0n4/d_option_to_make_neurips_rejected_paper_reviews/) , 2024-10-06-0914
```
The decision notification e-mail from NeurIPS mentioned that we would be offered the option to opt in to publicly releas
ing reviews for a rejected paper and that instructions would follow in a few days.

It's been over a week and we have no
t yet received any e-mail nor is there any author task to opt in. Since last year this e-mail came only 3 days after the
 notification I'm wondering if there was some issue and if no1 has received the e-mail yet?


```
---

     
 
all -  [ [R] Announcing the first series of Liquid Foundation Models (LFMs) â€“ a new generation of generative  ](https://www.reddit.com/r/MachineLearning/comments/1fvgo7o/r_announcing_the_first_series_of_liquid/) , 2024-10-06-0914
```
https://www.liquid.ai/liquid-foundation-models

https://www.liquid.ai/blog/liquid-neural-networks-research

https://x.co
m/LiquidAI_/status/1840768716784697688

https://x.com/teortaxesTex/status/1840897331773755476

'We announce the first se
ries of Liquid Foundation Models (LFMs), a new generation of generative AI models built from first principles.

Our 1B, 
3B, and 40B LFMs achieve state-of-the-art performance in terms of quality at each scale, while maintaining a smaller mem
ory footprint and more efficient inference.'

'LFM-1B performs well on public benchmarks in the 1B category, making it t
he new state-of-the-art model at this size. This is the first time a non-GPT architecture significantly outperforms tran
sformer-based models.

LFM-3B delivers incredible performance for its size. It positions itself as first place among 3B 
parameter transformers, hybrids, and RNN models, but also outperforms the previous generation of 7B and 13B models. It i
s also on par with Phi-3.5-mini on multiple benchmarks, while being 18.4% smaller. LFM-3B is the ideal choice for mobile
 and other edge text-based applications.

LFM-40B offers a new balance between model size and output quality. It leverag
es 12B activated parameters at use. Its performance is comparable to models larger than itself, while its MoE architectu
re enables higher throughput and deployment on more cost-effective hardware.

LFMs are large neural networks built with 
computational units deeply rooted in the theory of dynamical systems, signal processing, and numerical linear algebra.


LFMs are Memory efficient LFMs have a reduced memory footprint compared to transformer architectures. This is particular
ly true for long inputs, where the KV cache in transformer-based LLMs grows linearly with sequence length.

LFMs truly e
xploit their context length: In this preview release, we have optimized our models to deliver a best-in-class 32k token 
context length, pushing the boundaries of efficiency for our size. This was confirmed by the RULER benchmark.

LFMs adva
nce the Pareto frontier of large AI models via new algorithmic advances we designed at Liquid: 
 
Algorithms to enhance 
knowledge capacity, multi-step reasoning, and long-context recall in models + algorithms for efficient training and infe
rence.

We built the foundations of a new design space for computational units, enabling customization to different moda
lities and hardware requirements.

What Language LFMs are good at today:
General and expert knowledge,
Mathematics and l
ogical reasoning,
Efficient and effective long-context tasks,
A primary language of English, with secondary multilingual
 capabilities in Spanish, French, German, Chinese, Arabic, Japanese, and Korean.

What Language LFMs are not good at tod
ay:
Zero-shot code tasks,
Precise numerical calculations,
Time-sensitive information,
Counting râ€™s in the word â€œStrawber
ryâ€!,
Human preference optimization techniques have not yet been applied to our models, extensively.'

'We invented liqu
id neural networks, a class of brain-inspired systems that can stay adaptable and robust to changes even after training 
[R. Hasani, PhD Thesis] [Lechner et al. Nature MI, 2020] [pdf] (2016-2020). We then analytically and experimentally show
ed they are universal approximators [Hasani et al. AAAI, 2021], expressive continuous-time machine learning systems for 
sequential data [Hasani et al. AAAI, 2021] [Hasani et al. Nature MI, 2022], parameter efficient in learning new skills [
Lechner et al. Nature MI, 2020] [pdf], causal and interpretable [Vorbach et al. NeurIPS, 2021] [Chahine et al. Science R
obotics 2023] [pdf], and when linearized they can efficiently model very long-term dependencies in sequential data [Hasa
ni et al. ICLR 2023].

In addition, we developed classes of nonlinear neural differential equation sequence models [Mass
aroli et al. NeurIPS 2021] and generalized them to graphs [Poli et al. DLGMA 2020]. We scaled and optimized continuous-t
ime models using hybrid numerical methods [Poli et al. NeurIPS 2020], parallel-in-time schemes [Massaroli et al. NeurIPS
 2020], and achieved state-of-the-art in control and forecasting tasks [Massaroli et al. SIAM Journal] [Poli et al. Neur
IPS 2021][Massaroli et al. IEEE Control Systems Letters]. The team released one of the most comprehensive open-source li
braries for neural differential equations [Poli et al. 2021 TorchDyn], used today in various applications for generative
 modeling with diffusion, and prediction.

We proposed the first efficient parallel scan-based linear state space archit
ecture [Smith et al. ICLR 2023], and state-of-the-art time series state-space models based on rational functions [Parnic
hkun et al. ICML 2024]. We also introduced the first-time generative state space architectures for time series [Zhou et 
al. ICML 2023], and state space architectures for videos [Smith et al. NeurIPS 2024]

We proposed a new framework for ne
ural operators [Poli et al. NeurIPS 2022], outperforming approaches such as Fourier Neural Operators in solving differen
tial equations and prediction tasks.

Our team has co-invented deep signal processing architectures such as Hyena [Poli 
et al. ICML 2023] [Massaroli et al. NeurIPS 2023], HyenaDNA [Nguyen et al. NeurIPS 2023], and StripedHyena that efficien
tly scale to long context. Evo [Nguyen et al. 2024], based on StripedHyena, is a DNA foundation model that generalizes a
cross DNA, RNA, and proteins and is capable of generative design of new CRISPR systems.

We were the first to scale lang
uage models based on both deep signal processing and state space layers [link], and have performed the most extensive sc
aling laws analysis on beyond-transformer architectures to date [Poli et al. ICML 2024], with new model variants that ou
tperform existing open-source alternatives. 

The team is behind many of the best open-source LLM finetunes, and merges 
[Maxime Lebonne, link].

Last but not least, our teamâ€™s research has contributed to pioneering work in graph neural netw
orks and geometric deep learning-based models [Lim et al. ICLR 2024], defining new measures for interpretability in neur
al networks [Wang et al. CoRL 2023], and the state-of-the-art dataset distillation algorithms [Loo et al. ICML 2023].'
```
---

     
 
all -  [ Bringing Learning to Rank to Reddit - LTR modeling ](https://www.reddit.com/r/RedditEng/comments/1ft1tkw/bringing_learning_to_rank_to_reddit_ltr_modeling/) , 2024-10-06-0914
```
*Written by Sahand Akbari.*

In the previous series of articles in the learning to rank series, we looked at how we set 
up the [training data](https://www.reddit.com/r/RedditEng/comments/191nhka/bringing_learning_to_rank_to_reddit_search_go
als/) for the ranking model, how we did [feature engineering](https://www.reddit.com/r/RedditEng/comments/191nhka/bringi
ng_learning_to_rank_to_reddit_search_goals/), and [optimized our Solr clusters](https://www.reddit.com/r/RedditEng/comme
nts/1efartq/bringing_learning_to_rank_to_reddit_search/) to efficiently run LTR at scale. In this post we will look at l
earning to rank ML modeling, specifically how to create an effective objective function.Â 

To recap, imagine we have the
 following training data for a given query.

|Query|Post ID|Post Title|F1: Terms matching post title|F2: Terms matching 
posts body text|F3: Votes|Engagement Grade|
|:-|:-|:-|:-|:-|:-|:-|
|Cat memes|p1|Funny cat memes|2|1|30|0.9|
|Cat memes|
p2|Cat memes ?|2|2|1|0.5|
|Cat memes|p3|Best wireless headphones|0|0|100|0|

For simplicity, imagine our features in our
 data are defined per each query-post pair and they are:

* F1: Terms in the query matching the post title
* F2: Terms i
n the query matching the post body
* F3: number of votes for this post

Engagement grade is our label per query-post pai
r. It represents our estimation of how relevant the post is for the given query. Letâ€™s say itâ€™s a value between 0 and 1 
where 1 means the post is highly relevant and 0 means itâ€™s completely irrelevant. Imagine we calculate the engagement gr
ade by looking at the past week's data for posts redditors have interacted with and discarding posts with no user intera
ction. We also add some irrelevant posts by randomly sampling a post id for a given query (i.e [negative sampling](https
://www.reddit.com/r/RedditEng/comments/191nhka/bringing_learning_to_rank_to_reddit_search_goals/)). The last row in the 
table above is a negative sample. Given this data, we define an engagement-based grade as our labels: click through rate
 (CTR) for each query-post pair defined by ratio of total number of clicks on the post for the given query divided by to
tal number of times redditors viewed that specific query-post pair.

Now that we have our features and labels ready, we 
can start training the LTR model. The goal of an LTR model is to predict a relevance score for each query-post pair such
 that more relevant posts are ranked higher than less relevant posts. Since we donâ€™t know the â€œtrue relevanceâ€ of a post
, we approximate the true relevance with our engagement grade.

One approach to predicting a relevance score for each qu
ery-post is to train a supervised model which takes as input the features and learns to predict the engagement grade dir
ectly.Â  In other words, we train a model so that its predictions are as close as possible to the engagement grade. Weâ€™ll
 look closer at how that can be done. But first, letâ€™s review a few concepts regarding supervised learning. If you alrea
dy know how supervised learning and gradient descent work, feel free to skip to the next section.

# Machine Learning cr
ash course â€“ Supervised Learning and Gradient Descent

Imagine we have `d` features ordered in a vector (array) `x = [x1
, x2, â€¦, xd]`and a label `g`(grade).Â 

Also for simplicity imagine that our model is a linear model that takes the input
 `x` and predicts `y` as output:

https://preview.redd.it/947okib4ezrd1.png?width=1096&format=png&auto=webp&s=9dc8a5656a
a9ff520b42179259284c7273ca82e4

We want to penalize the model when `y` is different from `g`. So we define a Loss functi
on that measures that difference. An example loss function is squared error loss `(y-g)^2`. The closer `y` is to `g` the
 smaller the loss is.Â 

In training, we donâ€™t have just one sample `(x, g)` but several thousands (or millions) of sampl
es. Our goal is to change the weights `w` in a way that makes the loss function over all samples as small as possible.


In the case of our simple problem and loss function we can have a [closed-form solution](https://en.wikipedia.org/wiki/C
losed-form_expression) to this optimization problem, however for more complex loss functions and for practical reasons s
uch as training on large amounts of data, there might not be an efficient closed-form solution. As long as the loss func
tion is end-to-end differentiable and has other desired mathematical properties, one general way of solving this optimiz
ation problem is using [stochastic gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) where we make a ser
ies of small changes to weights `w` of the model. These changes are determined by the negative of the gradient of the lo
ss function `L`. In other words, we take a series of small steps in the direction that minimizes `L`. This direction is 
approximated at each step by taking the negative gradient of `L` with respect to `w` on a small subset of our dataset.Â 


At the end of training, we have found a `w` that minimizes our Loss function to an acceptable degree, which means that 
our predictions `y` are as close as possible to our labels `g` as measured by `L`. If some conditions hold, and weâ€™ve tr
ained a model that has learned true patterns in the data rather than the noise in the data, we'll be able to generalize 
these predictions. In other words, weâ€™ll be able to predict with reasonable accuracy on unseen data (samples not in our 
training data).

One thing to remember here is that the choice of weights `w` or more generally the model architecture (
we could have a more complex model with millions or billions of weights) allows us to determine how to get from inputs t
o the predictions. And the choice of loss function `L` allows us to determine what (objective) we want to optimize and h
ow we define an accurate prediction with respect to our labels.Â 

# Learning to rank loss functions

Now that we got tha
t out of the way, letâ€™s discuss choices of architecture and loss. For simplicity, we assume we have a linear model. A li
near model is chosen only for demonstration and we can use any other type of model (in our framework, it can be any end 
to end differentiable model since we are using stochastic gradient descent as our optimization algorithm).

https://prev
iew.redd.it/xb09p119fzrd1.png?width=1096&format=png&auto=webp&s=a4914f2e67883df40b1fc5d75ad45287f895faa4

An example los
s function is `(y-g)^2`. The closer `y` is to `g` on average, the smaller the loss is. This is called a pointwise loss f
unction, because it is defined for a single query-document sample.Â 

While these types of loss functions allow our model
 output to approximate the exact labels values (grades), this is not our primary concern in ranking. Our goal is to pred
ict scores that produce the correct *rankings* regardless of the exact value of the *scores* (model predictions). For th
is reason, [learning to rank](https://en.wikipedia.org/wiki/Learning_to_rank) differs from classification and regression
 tasks which aim to approximate the label values directly. For the example data above, for the query â€œcat memesâ€, the ra
nking produced by the labels is \[p1 - p2 - p3\]. An Ideal LTR loss function should penalize the predictions that produc
e rankings that differ from the ranking above and reward the predictions that result in similar rankings.

*Side Note: U
sually in Machine learning models, loss functions express the â€œlossâ€ or â€œcostâ€ of making predictions, where cost of maki
ng the right predictions is zero. So lower values of loss mean better predictions and we aim to minimize the loss.*

*Pa
irwise* loss functions allow us to express the correctness of the ranking between a pair of documents for a given query 
by comparing the rankings produced by the model with rankings produced by the labels given a pair of documents. In the d
ata above for example, p1 should be ranked higher than p2 as its engagement grade is higher. If our model prediction is 
consistent, i.e. the predicted score for p1 is higher than p2, we donâ€™t penalize the model. On the other hand, if p1â€™s s
core is higher than p2, the loss function assigns a penalty.

https://preview.redd.it/dp3ohw2nfzrd1.png?width=940&format
=png&auto=webp&s=0e7d3eca8ce5d981bb68e98c405daaac08f99d75

Loss for a given query `q` is defined as the sum of pairwise 
losses for all pairs of documents `i,j`.

`1(g_i > g_j)` is an indicator function. It evaluates to 1 when `g_i > g_j` an
d to 0 otherwise. This means that if the grade of document `i` is larger than the grade of document `j`, the contributio
n of `i,j` to loss is equal to `max(0, 1 - (y_i - y_j)).` In other words, if `g_i > g_j`, loss decreases as `(y_i - y_j)
` increases because our model is ranking document `i` higher than document `j`. Loss increases when the model prediction
 for document `j` is higher than document `i`.Â 

One downside of using pairwise loss is the increase in computational co
mplexity relative to pointwise solutions. For each query, we need to calculate the pairwise loss for distinct document p
airs. For a query with `D` corresponding posts, the computation complexity is `O(D^2)` while for a pointwise solution it
 is `O(D)`. In practice, we usually choose a predefined number of document pairs rather than calculating the loss for al
l possible pairs.

In summary, we calculate how much the pairwise difference of our model scores for a pair of documents
 matches the relative ranking of the documents by labels (which one is better according to our grades). Then we sum the 
loss for all such pairs to get the loss for the query. The loss of a given dataset of queries can be defined as the aggr
egation of loss for each queries.Â 

Having defined the loss function `L` and our model `f(x)`, our optimization algorith
m (stochastic gradient descent) finds the optimal weights of the model (`w` and `b`)Â  that minimizes the loss for a set 
of queries and corresponding documents.Â 

In addition to pointwise and pairwise ranking loss functions, there's another 
category known as *listwise*. Listwise ranking loss functions assess the entire ranked list, assigning non-zero loss to 
any permutation that deviates from the ideal order. Loss increases with the degree of divergence.Â 

These functions prov
ide the most accurate formulation of the ranking problem, however, to compute a loss based on order of the ranked list, 
the list needs to be sorted. Sorting is a non-differentiable and non-[convex](https://en.wikipedia.org/wiki/Convex_funct
ion) function. This makes the gradient based optimization methods a non-viable solution. [Many studies](http://icml2008.
cs.helsinki.fi/papers/167.pdf) have sought to create approximate listwise losses by either [directly](https://proceeding
s.neurips.cc/paper/2021/file/b5200c6107fc3d41d19a2b66835c3974-Paper.pdf) approximating sorting with a differentiable fun
ction or by defining an [approximate loss](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/SoftRankW
sdm08Submitted.pdf) that penalizes deviations from the ideal permutation order. The other challenge with listwise approa
ches is computationally complexity as these approaches need to maintain a model of permutation distribution which is fac
torial in nature. In practice, there is usually a tradeoff between degree of approximation and computational complexity.


For learning to rank at Reddit Search, we used a weighted pairwise loss called [LambdaRank](https://www.microsoft.com/
en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf). The shortcoming of the pairwise hinge loss function defin
ed above is that different pairs of documents are treated the same whereas in search ranking we usually care more about 
higher ranked documents. LambdaRank defines a pairwise weight (i.e. LambdaWeight), dependent on positions of the documen
ts, to assign an importance weight for each comparison. Our pairwise hinge loss with lambda weight becomes:Â 

https://pr
eview.redd.it/a70xg8f6hzrd1.png?width=1036&format=png&auto=webp&s=5f383fc396bd1328027b458ba20a41336df3b3e2

There are di
fferent ways to define the importance of comparisons. We use [NDCG lambda weight](https://www.tensorflow.org/ranking/api
_docs/python/tfr/keras/losses/NDCGLambdaWeight) which calculates a weight proportionate to the degree of change in [NDCG
](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) after a swap is made in the comparison.

*Side Note: We stil
l need to sort the ranking list in order to calculate the LambdaWeight and since sorting is not a differentiable operati
on, we must calculate the LambdaWeight component without gradients. In tensorflow, we can use* [*tf.stop\_gradient*](htt
ps://github.com/tensorflow/ranking/blob/c46cede726fd453e0aaa6097871d23dc8e465bdc/tensorflow_ranking/python/losses_impl.p
y#L882) *to achieve this.*

One question that remains: how did we choose `f(x)`? We opted for a dense neural network (i.
e. multi-layer perceptron). Solr supports the Dense Neural network architecture in the [Solr LTR plugin](https://solr.ap
ache.org/docs/8_7_0/solr-ltr/org/apache/solr/ltr/model/NeuralNetworkModel.html) and we used [tensorflow-ranking](https:/
/www.tensorflow.org/ranking) for training the ranker and exporting to the Solr LTR format. Practically, this allowed us 
to use the tensorflow ecosystem for training and experimentation and running LTR at scale within Solr. While gradient bo
osted trees such as [LambdaMart](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)
 are popular architectures for learning to rank, using end-to-end differentiable neural networks allows us to have a mor
e extensible architecture by enabling only minimal modifications to the optimization algorithm (i.e. stochastic gradient
 descent) when adding new differentiable components to the model (such as semantic embeddings).Â Â Â 

We have our model! S
o how do we use it?Â 

Imagine the user searches for â€œdog memesâ€. We have never seen this query and corresponding documen
ts in our training data. This means that we donâ€™t have any engagement grades. Our model trained by the Pairwise loss, ca
n now predict scores for each query - document pair.Â  Sorting the model scores in a descending order will result in a ra
nking of documents that will be returned to the user.Â 



|Query|Post ID|Post Title|F1: Terms matching post title|F2: Te
rms matching posts body|F3: Votes|Engagement Grade|Model Predicted Score|
|:-|:-|:-|:-|:-|:-|:-|:-|
|dog memes|p1|Funny 
dog memes|2|1|30|?|10.5|
|dog memes|p2|Dog memes|2|2|1|?|3.2|
|dog memes|p3|Best restaurant in town?|0|0|100|?|0.1|

# C
onclusion

In this post, we explored how learning-to-rank (LTR) objectives can be used to train a ranking model for sear
ch results. We examined various LTR loss functions and discussed how we structure training data to train a ranking model
 for Reddit Search. A good model produces rankings that put relevant documents at the top. How can we measure if a model
 is predicting good rankings? We would need to define what â€œgoodâ€ means and how to measure better rankings. This is some
thing we aim to discuss in a future blog post. So stay tuned!
```
---

     
 
all -  [ MSCS FALL'25 Profile evaluation ](https://www.reddit.com/r/MSCS/comments/1ft08a6/mscs_fall25_profile_evaluation/) , 2024-10-06-0914
```
# Country:

- From Pakistan

# Undergraduate University:

- Not good ranked  
- CGPA: 3.43 / 4.0  
- No Undergrad Thesis
, a Final year project  
- GRE (Not given, want to apply to unis which do not require GRE)

# Industry Experience:

- 3+
 years total as a ML Engineer  
- 1 internship at a startup  
- working in a silicon valley startup as DeepLearning Engi
neer for the past year  
- AI Fellow at PI School of AI

# Research Experience:

- Research Collaborator at MIT Media La
b, 1 paper published at NeurIPS, 1 under-review at ICLR. (I am not leading author in both)  
- AI Research/Pre-doctoral 
fellow at Fatima Fellowship (On-going, probably will publish my work by Dec)

# Considering the following UNIs as they d
o not require GRE in MS:

- umass amherst  
- john hopkins  
- Virginia Tech  
- Texas A&M  
- ohio state  
- uni of min
nisota  
- Darmouth  
- UNC Walmington  
- NorthEastern  
- NorthWetern
```
---

     
 
all -  [ [D] Resources for staying updated on recent papers ](https://www.reddit.com/r/MachineLearning/comments/1fsx8q2/d_resources_for_staying_updated_on_recent_papers/) , 2024-10-06-0914
```
Hello, Iâ€™m looking for time-saving ways to stay updated on the latest research papers from conferences like CVPR, ECCV, 
NeurIPS, ICML, and journals like TPAMI. I know these conferences/journals publish cutting-edge work, but keeping track o
f all the new papers gets overwhelming at times. Iâ€™m interested in resources that summarize or highlight the most signif
icant papers, like blogs, newsletters, or curated lists. Does anyone know of any:

1. blogs or newsletters that regularl
y cover the latest papers from these conferences and journals
2. twitter discussions, subreddits, medium blogs, or perso
nal websites run by researchers who highlight or summarize key papers (I've heard about paperswithcode and 2-minute pape
rs but are they quick with such newly published papers?)
3. curated paper repositories (github or any websites) where pe
ople organize papers based on recent conferences/journals?

Iâ€™m particularly interested in resources that focus on compu
ter vision, neural network architectures, and their optimization. Iâ€™d appreciate any suggestions or tips. Thanks in adva
nce!
```
---

     
 
all -  [ [R] optimizing transformers ](https://www.reddit.com/r/MachineLearning/comments/1fsgz5i/r_optimizing_transformers/) , 2024-10-06-0914
```
Hello, Iâ€™m currently aiming to work on optimizing transformer models, specifically in multi-view images and/or cross-att
ention networks. I've noticed that cross-attention layers add up a lot of parameters, which can slow down the training p
rocess. Iâ€™m exploring ways to reduce the computational complexity to increase the speed (for now and subsequently withou
t sacrificing too much performance sometime later). I'm starting to look into:

1. low-rank matrix factorization - Iâ€™ve 
been reading about how it can be applied to reduce the size of the projection matrices (e.g., the projq, projk, projv in
 cross-attention). Does anyone have experience using low-rank factorization specifically in cross-attention mechanisms?

2. other param reduction techniques - Aside from low-rank factorization, are there other methods I could explore for red
ucing the number of parameters in transformer models, like sparsity and pruningâ€”do you have recommendations or experienc
es with these?
3. overcoming redundancy in multi-view scenarios - Given the multi-view nature of my problem, I suspect t
hereâ€™s some redundancy in how cross-attention processes the different views. Has anyone worked on reducing redundancy ac
ross views in transformer-based networks? What techniques worked best for you?

Iâ€™m starting to look into CVPR, NEURIPS,
 ECCV, etc, but any insights, advise, experiences, or papers you can share would be greatly appreciated! Thanks in advan
ce!
```
---

     
 
all -  [ äººå¤§é™„é«˜ä¸­ç”Ÿä¸­NeurIPSï¼Œå…¥é€‰é«˜ä¸­èµ›é“Spotlightï¼Œé¡¶ä¼šçœŸå·åˆ°ä¸­å­¦äº† ](https://www.reddit.com/r/real_China_irl/comments/1fs0z7a/äººå¤§é™„é«˜ä¸­ç”Ÿä¸­neuripså…¥é€‰é«˜ä¸­èµ›é“spotlighté¡¶ä¼šçœŸå·åˆ°ä¸­å­¦äº†/) , 2024-10-06-0914
```
NeurIPS 2024æ”¾æ¦œï¼Œäººå¤§é™„ä¸­æœ‰é«˜ä¸­ç”Ÿä¸€ä½œå…¥é€‰ã€‚

ä»Šå¹´ï¼ŒNeurIPSçŽ‡å…ˆæŠŠAIé¡¶ä¼šå·åˆ°äº†é«˜ä¸­é‡Œï¼Œæ­£å¼é¢å‘é«˜ä¸­ç”Ÿå¾é›†è®ºæ–‡ï¼Œè¿˜ä¸ºæ­¤ä¸“é—¨è®¾ç½®äº†é«˜ä¸­ç”Ÿèµ›é“ï¼ˆHigh School Projects Trackï¼‰ã€‚

çŽ°åœ¨ç»“æžœç»ˆäºŽå‡ºç‚‰ï¼ŒåŒ—
äº¬å¤§å­¦è®¡ç®—æœºå­¦é™¢çš„å¼ é“­æ•™æŽˆåˆ†äº«äº†ä¸€åˆ™å…¥å›´æ¶ˆæ¯ï¼š

äººå¤§é™„ä¸­å´æ‚ ï¼Œæœ‰ä¸€ç¯‡ä¸€ä½œè®ºæ–‡å…¥é€‰è¯¥èµ›é“ï¼Œè¿˜è¢«é€‰ä¸ºäº†Spotlight Projectã€‚



è®ºæ–‡é¢˜ä¸ºã€ŠVision-Brailleï¼šAn End-to-End Tool for Chine
se Braille Image-to-Text Translationã€‹ï¼Œæå‡ºäº†ä¸€ç§ä¸­æ–‡ç›²æ–‡å›¾åƒåˆ°æ–‡æœ¬çš„ç«¯åˆ°ç«¯ç¿»è¯‘å·¥å…·ã€‚

æ®å¼ é“­æ•™æŽˆä»‹ç»ï¼Œå´æ‚ åœ¨2022å¹´é«˜ä¸€åŠ å…¥å¥¹çš„è¯¾é¢˜ç»„æ—¶ï¼Œå°±æå‡ºäº†è¿™ä¸ªé¡¹ç›®çš„æƒ³æ³•ã€‚



# ç«¯åˆ°ç«¯ä¸­æ–‡ç›²æ–‡å›¾åƒåˆ°æ–‡æœ¬
ç¿»è¯‘å·¥å…·

å…·ä½“æ¥è¯´ï¼Œè¯¥é¡¹ç›®åŸºäºŽè°·æ­Œçš„mT5æ¨¡åž‹ï¼Œé‡‡ç”¨Curriculum Learningï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰æ–¹æ³•å¾®è°ƒå‡ºäº†ä¸€ä¸ªç›²æ–‡ç¿»è¯‘æ¨¡åž‹ã€‚



å…¶ä¸­çš„éš¾ç‚¹ä¸»è¦åŒ…æ‹¬å‡ ä¸ªæ–¹é¢ï¼š

ç¼ºå°‘æ•°æ®é›†ï¼šä¸­æ–‡ç›²æ–‡ç¿»è¯‘æ•°æ®é›†éžå¸¸ç¨€ç¼ºï¼Œæ•°æ®çš„é‡‡é›†ä¹Ÿæ¯”è¾ƒå›°éš¾ï¼Œéœ€è¦
è€—è´¹å¤§é‡äººåŠ›ã€‚

ç›²æ–‡æ•°æ®çš„ç‰¹æ®Šæ€§ï¼šç›²æ–‡é€šè¿‡æœ€å¤šä¸‰ä¸ªå•å…ƒæ ¼æ¥è¡¨ç¤ºæ¯ä¸ªæ±‰å­—çš„å‘éŸ³ï¼Œå³å£°æ¯ã€éŸµæ¯å’ŒéŸ³è°ƒã€‚ä½†åœ¨å®žé™…ä½¿ç”¨ä¸­ï¼Œç›²æ–‡ä½¿ç”¨è€…é€šå¸¸ä¼šçœç•¥å¤§éƒ¨åˆ†å£°è°ƒç¬¦å·ï¼Œè¿™ç»™ç›²æ–‡ç¿»è¯‘å¸¦æ¥äº†æŒ‘æˆ˜ã€‚

åŒéŸ³å­—æ··æ·†ï¼šä¸­æ–‡ä¸­å­˜åœ¨å¤§é‡åŒéŸ³å­—ï¼Œå¹¶ä¸”ç”±äºŽå£°è°ƒç¬¦å·ç»å¸¸è¢«çœç•¥
ï¼ŒåŒéŸ³å­—çš„åŒºåˆ†å˜å¾—æ›´åŠ å›°éš¾ã€‚

ä¸ºæ­¤ï¼Œè®ºæ–‡ä½œè€…ä»¬é¦–å…ˆæž„å»ºäº†ä¸€ç»„ä¸­æ–‡-ç›²æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬Chinese-Braille-Full-Toneã€Chinese-Braille-No-Toneå’ŒChinese-Braille-10per-Toneã€‚


ä½œè€…ä»ŽèŽ±æ¯”é”¡æ•°æ®é›†ä¸­æ”¶é›†äº†100ä¸‡ä¸ªä¸åŒçš„ä¸­æ–‡å¥å­ï¼Œä½¿ç”¨ä¸­æ–‡ç›²æ–‡åœ¨çº¿å¹³å°æä¾›çš„å·¥å…·ï¼Œå°†æ”¶é›†åˆ°çš„ä¸­æ–‡å¥å­è½¬æ¢ä¸ºâ€œå…¨éŸ³â€ç›²æ–‡ã€‚

è€ŒåŽï¼Œä¸ºäº†æ¨¡æ‹ŸçœŸå®žä¸–ç•Œä¸­ç›²æ–‡ä½¿ç”¨è€…çœç•¥å£°è°ƒçš„æƒ…å†µï¼Œä½œè€…è¯†åˆ«å‡ºè¿™äº›ç›²æ–‡ä¸­ä»£è¡¨å£°è°ƒçš„éƒ¨åˆ†ï¼Œå¹¶éšæœºåŽ»é™¤äº†å…¶ä¸­90%çš„å£°
è°ƒï¼Œåˆ›å»ºChinese-Braille-10per-Toneä»¥åæ˜ çŽ°å®žä¸–ç•Œä¸­ä¸­æ–‡ç›²æ–‡çš„ä½¿ç”¨æƒ…å†µã€‚

æ•°æ®æŒ‰ç…§8:1:1çš„æ¯”ä¾‹è¢«åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚



è®­ç»ƒæ–¹æ³•æ–¹é¢ï¼Œä½œè€…ä½¿ç”¨RetinaNetæ¥æ‰§è¡Œç›²æ–‡OCRä»»åŠ¡ï¼Œå°†ç›²æ–‡å›¾åƒè½¬æ¢
ä¸ºæ•°å­—ç›²æ–‡å­—ç¬¦ã€‚

æŽ¥ç€ï¼Œé‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥â€”â€”å³ä»Žç®€å•åˆ°å¤æ‚åœ°å®‰æŽ’è®­ç»ƒä»»åŠ¡ï¼Œåˆ†ä¸‰ä¸ªé˜¶æ®µå¾®è°ƒäº†å¤šè¯­è¨€Transformeræ¨¡åž‹mT5ï¼š

ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨Chinese-Braille-Full-Toneæ•°æ®é›†ä½œä¸ºè®­ç»ƒçš„ç®€å•éƒ¨åˆ†ï¼Œè®©æ¨¡åž‹å­¦ä¹ åŸºæœ¬
çš„ç¿»è¯‘è§„åˆ™ã€‚è¿™ä¸ªæ•°æ®é›†ä¸­çš„ç›²æ–‡åŒ…å«å®Œæ•´çš„å£°è°ƒä¿¡æ¯ã€‚

ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨Chinese-Braille-No-Toneæ•°æ®é›†ï¼Œè®©æ¨¡åž‹åœ¨æ²¡æœ‰å£°è°ƒä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå­¦ä¼šæ ¹æ®ä¸Šä¸‹æ–‡çŒœæµ‹æ­£ç¡®çš„ä¸­æ–‡å­—ç¬¦ã€‚

ç¬¬ä¸‰é˜¶æ®µï¼šä½¿ç”¨Chinese-Braille-10
per-Toneæ•°æ®é›†ï¼Œè®©æ¨¡åž‹æ›´å¥½åœ°é€‚åº”å®žé™…åº”ç”¨åœºæ™¯ã€‚

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šï¼Œè¯¥æ¨¡åž‹çš„BLEUå¾—åˆ†åˆ†åˆ«è¾¾åˆ°äº†62.4å’Œ62.3ï¼Œæ˜¾è‘—æé«˜äº†ç›²æ–‡ç¿»è¯‘çš„å‡†ç¡®åº¦ã€‚

è®ºæ–‡ä½œè€…å·²ç»æ”¾å‡ºäº†é¡¹ç›®Demoï¼Œæ•ˆæžœæ˜¯é…±å©¶çš„ï¼Œæ„Ÿå…´è¶£çš„å°ä¼™ä¼´ä»¬å¯ä»¥
æˆ³æ–‡æœ«é“¾æŽ¥è‡ªè¡Œæµ‹è¯•ï¼š



ï¼ˆæ­£ç¡®ç­”æ¡ˆï¼šä¸è¿‡ï¼Œå¯¹äºŽè‡ªå·±å¤–å‘çš„æ€§æ ¼ï¼ŒåŸƒæ‰˜å¥¥è¯´ï¼Œâ€œè¿™å°±æ˜¯çœŸå®žçš„æˆ‘ï¼Œæˆ‘ä¸ä¼šä¸ºæ­¤æ”¹å˜ã€‚ï¼‰

è¯¥é¡¹ç›®æ˜¯åœ¨å´æ‚ é«˜ä¸‰æ—¶å®Œæˆã€‚å¼ é“­æ•™æŽˆé€éœ²ï¼Œä»–ç›®å‰å·²è¿›å…¥åº·å¥ˆå°”å¤§å­¦å°±è¯»è®¡ç®—æœºå’Œç”Ÿç‰©åŒ»è¯å·¥ç¨‹ä¸“ä¸šã€‚

è®ºæ–‡è‡´è°¢ä¸­æåˆ°ï¼Œå´æ‚ ä¸»è¦æ˜¯åœ¨
å¼ é“­æ•™æŽˆåšå£«ç”Ÿã€è®ºæ–‡ç¬¬äºŒä½œè€…è¢é‡Žçš„æŒ‡å¯¼ä¸‹å®Œæˆäº†è¿™é¡¹ç ”ç©¶ã€‚

å¼ é“­ï¼ŒåŒ—äº¬å¤§å­¦è®¡ç®—æœºå­¦é™¢æ•™æŽˆï¼Œåšå£«ç”Ÿå¯¼å¸ˆï¼Œç ”ç©¶é¢†åŸŸåŒ…æ‹¬æ–‡æœ¬æŒ–æŽ˜ã€çŸ¥è¯†å›¾è°±ã€å›¾ç¥žç»ç½‘ç»œå’Œè®¡ç®—æœºæ•™è‚²ç ”ç©¶ç­‰ã€‚å¥¹åˆä½œå‘è¡¨çš„ç§‘ç ”å­¦æœ¯è®ºæ–‡æ›¾èŽ·ICML 2014æœ€ä½³è®ºæ–‡ã€ICDM 2022æœ€
ä½³è®ºæ–‡æåç­‰è£èª‰ã€‚Google Scholaræ˜¾ç¤ºï¼Œå¥¹çš„è®ºæ–‡å¼•ç”¨é‡æŽ¥è¿‘2ä¸‡ï¼ŒhæŒ‡æ•°ä¸º48ã€‚

# NeurIPSé«˜ä¸­ç”Ÿèµ›é“

NeurIPSæ˜¯ä»Šå¹´åˆšè®¾çš„â€œé«˜ä¸­ç”Ÿèµ›é“â€ï¼Œä¸»è¦å¾é›†â€œæœºå™¨å­¦ä¹ çš„ç¤¾ä¼šå½±å“â€æ–¹å‘çš„è®ºæ–‡ã€‚



å…¬å‘Šæ˜¯è¿™æ ·å†™çš„ï¼š

>


è¯¦ç»†æ¥è¯´ï¼Œå°±æ˜¯å…è®¸é«˜ä¸­ç”Ÿä»¬æ‰¾å¤–éƒ¨å¯¼å¸ˆæ¥åˆä½œå®Œæˆé¡¹ç›®ï¼Œä½†å¿…é¡»æŠŠå¯¼å¸ˆä»¥åŠåˆä½œè€…çš„è´¡çŒ®ï¼Œå’Œé«˜ä¸­ç”Ÿä½œè€…çš„è´¡çŒ®åŒºåˆ†å¼€æ¥ã€‚

å…¬å‘Šä¸­è¿˜è§„å®šäº†ï¼Œä½œè€…éœ€è¦æäº¤é«˜ä¸­åœ¨è¯»è¯æ˜Žï¼Œæ‰€æœ‰è¡¥å……ææ–™å‡åº”å®Œå…¨ç”±ä½œè€…å®Œæˆï¼ŒåŒ…æ‹¬è§†é¢‘ã€Demoã€æµ·æŠ¥ã€ç½‘ç«™æˆ–æºä»£ç ã€‚

å€¼å¾—
ä¸€æçš„æ˜¯ï¼Œå…¶ä»–é¡¶ä¼šä¹Ÿæœ‰ç§¯æžæŽ¥è§¦å’Œå½±å“é«˜ä¸­ç”Ÿçš„è¶‹åŠ¿ã€‚
```
---

     
 
all -  [ Merit of high level research publication ](https://www.reddit.com/r/ApplyingToCollege/comments/1fr97j4/merit_of_high_level_research_publication/) , 2024-10-06-0914
```
How much will a first-author **workshop** paper acceptance (not main conference) at a prestigious venue like NeurIPS (1s
t one): [https://scholar.google.com/citations?view\_op=top\_venues&hl=en&vq=eng\_artificialintelligence](https://scholar
.google.com/citations?view_op=top_venues&hl=en&vq=eng_artificialintelligence) boost an Asian CS kids chances at top coll
eges?

Would this be considered on the same tier as ISEF or something of the sort. Is this enough to somewhat guarantee 
admission into a top 10?

Thanks
```
---

     
 
all -  [ Post-PhD Education for Quant ](https://www.reddit.com/r/quantfinance/comments/1fpewtu/postphd_education_for_quant/) , 2024-10-06-0914
```
Please don't downvote. I already asked before in r/quant but you have to do that in a weekly thread and it's often hard 
to get any answers. (I've never gotten a reply to date.) There doesn't seem to be rules against this kind of post in thi
s sub though (and there are a couple of others).

This is in the US. Feel free to continue for details, or skip to **TL;
DR**.

I have a theoretical physics PhD and I did mostly heavy mathematical research with little to no programming invol
ved. I learned on my own, but due to immigration constraints, couldn't get internships because my advisor was the 'indus
try sucks' type and wouldn't approve anything that wasn't academic. So wasn't able to intern throughout PhD, and when I 
graduated in the middle of COVID, my student VISA timeline was working against me and I had to go for a post-doc. (Note:
 publication record is not super strong because it was heavy theory work, and the academic post-doc was just one year. S
o I got like 3 or 4 papers in strong journals, but not the 10+ papers in ICML, NeurIPS, etc. More of the classical 'I pu
blished good papers in good research journals', which takes time.)

I managed to then secure a fixed-term position as a 
research scientist doing ML with applications to in finance at a good tech company (like those PhD programmes at banks) 
that was basically an industry post-doc (think like Microsoft/Meta/Apple ML Post-Doc but in quant finance).

That positi
on ran out last year, and since I was still under immigration constraints, and the market being what it was, the best I 
could secure was a position as a Data Scientist at a consulting firm. I did got some buyside quant interviews, but they 
were either looking for strong quant devs or seasoned QRs and wouldn't consider me for fresh grad / early career PhD pos
itions. The sellside ones I did get seemed to expect far more tech experience despite the positions being labeled 'ML sc
ientist' or 'research scientist'.

My concern here is the branding aspect: PhD was not from a target school, not in the 
more desired majors (applied math, stats, EE), and my experience so far seems to not help either despite it fairly techn
ical ML research in quant finance.

This branding issue seems to be US-specific to me because I have a friend who went t
o London instead to do post-doc and then had no issue getting into a Tier 2 multi-strat via the typical math + LC interv
iew pipeline. His same firm would not consider me at all on the US side, even though we went to the same grad school, he
's got even less of a publication record, and we have the same competitive background (in terms of physics olympiads, GP
As, etc).

This has gotten me to the point of considering doing a short but intense specialized QF masters at a target s
chool, either in the US (Baruch MFE being my primary target) or something like the MSc in QF at Erasmus or MSc in QF at 
ETH Zurich, in Europe. (EUR examples based on duration, cost, and reputation.) Now that I finally got to a point were my
 Green Card got through, if itâ€™s a European Masters, I'd obviously prefer to do something like that and go right back to
 the US, but I'm open to other options.

Obviously the clear flaw is that it still indicates some sort of weak profile b
ecause the obvious question would be 'why masters after PhD?' but the markets in the US vs Europe have been very differe
nt over the past 4 years (perhaps a bit more) with the typical QR hiring really favoring a more solid CS background, in 
my opinion. At some point, I'd think that's a gap you can't convincingly cover via applied research experience.

My ques
tion is: what are my options if I still want to pursue some sort of quant finance career (whether its buyside or sellsid
e)? (I'm open to all sorts of roles, including data scientist roles and the like, as long as modeling is an important co
mponent, but I'm really interested in the quant finance industry specifically.)

**TL;DR:**

Background:

* Theoretical 
Physics PhD (US, non-target)
* 1 year academic post-doc
* 2 years industry post-doc
   * Post-doc course and lack of int
ernship experience due to immigration constraints and lack of PhD advisor assistance with approvals for industry interns
hips
* Currently working as a data scientist in a consulting firm
* Very strong math background, classical ML (statistic
al learning) modeling experience
* Reasonably well-versed in Python programming (built libraries entirely from scratch a
s part of research and applied research work; worked with teams)
* Now finally have US permanent residency, and can expl
ore more options

Main problem:

* Seem to be suffering from a branding issue given career trajectory so far and lack of
 industry connections
* Not being considered for fresh PhD roles anywhere in the US (but friends with 95% similar profil
es not having this issue elsewhere in Europe)
* Not sure how to re-align and pursue a QR/QR-adjacent (read modeling-orie
nted role) career

Options considered:

* Intensive Masters in QF at Target School (US, Europe) to secure a good QR/QR-a
djacent position and spring back to the US
   * Concerned about the perception of 'Masters after PhD' (more tolerated in
 other industries, seems to be heavily frowned upon in quant finance)
* Willing to also pursue quant trading trajectory 
if it makes sense

Seeking options and guidance, flexible on roles to pursue as long as they're reasonably modeling-orie
nted as opposed to SWE-heavy (quant dev and the like)
```
---

     
 
all -  [ LEGO Meets AI: BricksRL Accepted at NeurIPS 2024! ](https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/) , 2024-10-06-0914
```
We're excited to share that our paper on BricksRL, a library of RL algorithms that can be trained and deployed on afford
able, custom LEGO robots, has been accepted at NeurIPS 2024 as a spotlight paper!

As AI and machine learning continue t
o make waves, we believe it's essential to make reliable and affordable education tools available to the community. Not 
everyone has access to hundreds of GPUs, and understanding how ML works in practice can be challenging.

That's why we'v
e been working on BricksRL, a collaboration between Universitat Pompeu Fabra and PyTorch. Our goal is to provide a fun a
nd engaging way for people to learn about AI, ML, robotics, and PyTorch, while maintaining high standards of correctness
 and robustness.

BricksRL is based on Pybricks and can be deployed on many different LEGO hubs. We hope it will empower
 labs worldwide to prototype ideas affordably without requiring expensive robots.

  
Check out our website:Â [https://br
icksrl.github.io/ProjectPage/](https://bricksrl.github.io/ProjectPage/)

The library is open-sourced under an MIT licens
e on GitHub:Â [https://github.com/BricksRL/bricksrl/](https://github.com/BricksRL/bricksrl/)

Read our paper:Â [https://ar
xiv.org/abs/2406.17490](https://arxiv.org/abs/2406.17490)

Watch the robots in action:Â [https://www.youtube.com/watch?v=
k\_Vb30ZSatk&t=10s](https://www.youtube.com/watch?v=k_Vb30ZSatk&t=10s)

We're working on some exciting follow-up project
s, so stay tuned!

See you in Vancouver

https://preview.redd.it/1ghfs9t9l0rd1.jpg?width=2006&format=pjpg&auto=webp&s=86
8867adcd52825bd4ee719513a454527d017307


```
---

     
 
all -  [ [D] NeurIPS 2024 Review Question  ](https://www.reddit.com/r/MachineLearning/comments/1fpa7ua/d_neurips_2024_review_question/) , 2024-10-06-0914
```
My initial reviewers addressed some weaknesses & concerns, but these were resolved in my rebuttals. They acknowledged an
d raised their score. 

My paper was ultimately rejected because the program chair introduced new weaknesses that are a 
result of misreading the paper, if these were stated in the original reviews, this would easily be resolved. Is there an
ything I can do to fix this program chair review?
```
---

     
 
all -  [ [D] - NeurIPS 2024 Decisions ](https://www.reddit.com/r/MachineLearning/comments/1foky4r/d_neurips_2024_decisions/) , 2024-10-06-0914
```
Hey everyone! Just a heads up that the NeurIPS 2024 decisions notification is set for September 26, 2024, at 3:00 AM CES
T. I thought itâ€™d be cool to create a thread where we can talk about it.
```
---

     
 
all -  [ Should I go for a masters, professional masters, or PhD? ](https://www.reddit.com/r/gradadmissions/comments/1foc03f/should_i_go_for_a_masters_professional_masters_or/) , 2024-10-06-0914
```
My goal with graduate school is to set myself up to launch a company that produces a system of swarm robots that coopera
te to efficiently assemble orbital infrastructure; I believe the space industry is in the process of taking off and such
 a system will be necessary to scale it.

  
In my undergrad, I worked on 4 major research projects.

* One on efficient
 open set multimodal 3d mapping; under review at NeurIPS MAR workshop
* One on improving the performance of reinforcemen
t learning for knowledge graph query answering; accepted to IEEE TASL journal and resulted in a patent
* One on creating
 a universal testing platform for cooperative autonomous driving; accepted to VTC 2024 conference
* One on using evoluti
onary computation to improve the performance of a neurosymbolic planning architecture with the goal of creating a natura
l language iterface for a symbolic planning system; worked on this at CMU over the summer but am still collecting result
s. Likely to submit to ICRA or IROS down the road

and one major robotics project that was just for fun; I built an auto
nomous drone that can navigate to GPS coordinates while avoiding obstacles with a depth camera. I also have a startup wh
ere we're building models to translate natural language into formatted API calls in under a second, but with all the res
earch work we've been in the beta phase for a while (though we are eyeing an aquihire from an interested local firm).

 
 
I know my future company will be a deep tech company and be heavily research-oriented in its inception, but since it i
s still a company I know I'll need more business knowhow to make it successful. I've heard that MBAs teach people how to
 be effective managers at large businesses and from my own experience I know a very different skillset is required for s
tartups, so I'm left deciding between a dual research-based MS/PhD and MBA OR pursuing a professional masters focused on
 entrepreneurship (see CMU's MSAII or MRSD programs) and getting out into the market faster. CMU, MIT, and Stanford alum
s in particular, what do you think best aligns with my goals? Thank you for any feedback you can offer!
```
---

     
 
all -  [ Post-Doc Position in Intersection of LLMs/Reasoning/Data at Stanford Scaling Intelligence Lab ](https://www.reddit.com/r/CompSocial/comments/1fnnziy/postdoc_position_in_intersection_of/) , 2024-10-06-0914
```
Azalia Mirhoseini (CS) and Amin Saberi (Math) are jointly seeking a Post-Doc to join the [Scaling Intelligence Lab](http
s://scalingintelligence.stanford.edu/pubs/) at Stanford, which focuses on the development of 'scalable and self-improvin
g AI systems and methodologies towards the goal of AGI.' 

The post-doc researcher would work with both professors to co
ntribute to cutting-edge research at the intersection of language models, data, and reasoning. From the call:

>The post
doc will be expected to help define the research questions of interest, and lead both empirical and methodological resea
rch efforts towards publication, working together with student collaborators. Teaching is not required as part of this p
osition.

>Required Qualifications:Â 

>\* Strong mathematical background, including expertise in one or more of the foll
owing areas: machine learning, statistics, and algorithms.

>\* Ph.D. (or expected completion by Fall 2024) in computer 
science, statistics, operations research, or related fields

>\* Prior experience working with data, including expertise
 with computational methodsÂ 

>\* Prior experience building ML systems, designing and running experiments in PyTorch or 
JAX

>\* Strong publication record in top machine learning conferences (e.g. NeurIPS, ICML, ICLR). A strong background i
n theory is a plus. Â  

To learn more about the role and how to apply, visit: [https://docs.google.com/document/d/1SBfvF
hLF4hSseTBybXRKJeRFMxqw4ahQ9f4Cf5Vbl7I/edit](https://docs.google.com/document/d/1SBfvFhLF4hSseTBybXRKJeRFMxqw4ahQ9f4Cf5V
bl7I/edit)
```
---

     
 
all -  [ Looking at quant jobs from unconventional path ](https://www.reddit.com/r/FinancialCareers/comments/1fnefe2/looking_at_quant_jobs_from_unconventional_path/) , 2024-10-06-0914
```
Hi folks!

I am very new to quant/fintech so please forgive me for my naivety and all the mistakes I make. I was doing a
n MBBS (equivalent to a US MD) from India at one of the top med schools in the country and was so done with the people a
nd culture that I dropped out recently and started a degree in Data Science at another top university in India (although
 my program is not considered competitive at all, I couldn't go via normal route because it usually takes years of prepa
ration \~6-7 years to get into the traditional program). For now, I have \~4/4 gpa.

I have been doing a lot of neurosci
ence and have several publications concerning clinical data analyses (2), experimental neuroscience (1), one on dimensio
nality reduction algo, submitting to neurips workshops this year (3), on a topic based on physics and neuroscience prese
nting to neuroscience conference, math neuro conferences, a few other research experiences using neuroscience datasets, 
etc. This year I am also joining a lab in Germany for research. I was previously a SURF at Caltech. And the list goes on
 in the direction that to me seems very tangential to quant jobs.

For as long as I can remember I have been in love wit
h neuroscience and machine learning and now I have begun to look towards more quantitative and complex ideas above neuro
science and my first thought is either research in quantum computing (QML) or quant research jobs.

I am probably being 
super crazy thinking of even doing this, but I was wondering whether I have any chance of getting an interview at let's 
say, the Jane Street internship program, next year while I focus more on getting research in ML/math this year. Or shoul
d I do something else with my life?

Thanks for all the insight!
```
---

     
 
all -  [ Summaries Of Research Papers We Read ](https://www.reddit.com/r/deeplearning/comments/1fl4bzm/summaries_of_research_papers_we_read/) , 2024-10-06-0914
```
The Vision Language Group at IIT Roorkee has curated a repository of comprehensive summaries for deep learning research 
papers from top-tier conferences like NeurIPS, CVPR, ICCV, ICML from 2016 to 2024. These summaries aim to provide a conc
ise understanding of influential papers in fields such as computer vision, natural language processing, and machine lear
ning. The collection is constantly growing, with new summaries added frequently. Here are a few notable examples:

- **D
reamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**, CVPR'23  
  [DreamBooth Summary](
https://github.com/vlgiitr/papers_we_read/blob/master/summaries/DreamBooth.md)

- **Segment Anything**, ICCV'23  
  [Seg
ment Anything Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Segment_Anything.md)

- **An Imag
e is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion**, ICCV'23  
  [Textual Inversion Su
mmary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Textual_inversion.md)

- **Photorealistic Text-to
-Image Diffusion Models with Deep Language Understanding**, NIPS'22  
  [Photorealistic Diffusion Summary](https://githu
b.com/vlgiitr/papers_we_read/blob/master/summaries/imagen.md)

- **An Image is Worth 16x16 Words: Transformers for Image
 Recognition at Scale**, ICLR'21  
  [Vision Transformer Summary](https://github.com/vlgiitr/papers_we_read/blob/master/
summaries/Vision_Transformer.md)

- **Big Bird: Transformers for Longer Sequences**, NIPS'20  
  [Big Bird Transformers 
Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Big_Bird_Transformers.md)

The repository invit
es contributions from the community. If you find the summaries helpful, you are encouraged to submit your own summaries 
for research papers. The team aims to regularly update the collection with summaries of papers from upcoming conferences
 and key topics in deep learning and AI. 

You can access the full repository and contribute here:  
[Vision Language Gr
oup Paper Summaries](https://github.com/vlgiitr/papers_we_read)

By contributing, you'll help make advanced research mor
e accessible to both beginners and experts in the field.
```
---

     
 
all -  [ [R] Some Research Papers We Read ](https://www.reddit.com/r/MachineLearning/comments/1fl4bi0/r_some_research_papers_we_read/) , 2024-10-06-0914
```
The Vision Language Group at IIT Roorkee has curated a repository of comprehensive summaries for deep learning research 
papers from top-tier conferences like NeurIPS, CVPR, ICCV, ICML from 2016 to 2024. These summaries aim to provide a conc
ise understanding of influential papers in fields such as computer vision, natural language processing, and machine lear
ning. The collection is constantly growing, with new summaries added frequently. Here are a few notable examples:



- \
*\*DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\*\*, CVPR'23  

  \[DreamBooth S
ummary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/DreamBooth.md)



- \*\*Segment Anything\*\*,
 ICCV'23  

  \[Segment Anything Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Segment\_An
ything.md)



- \*\*An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\*\*, ICCV
'23  

  \[Textual Inversion Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Textual\_invers
ion.md)



- \*\*Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\*\*, NIPS'22  

  \[Phot
orealistic Diffusion Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/imagen.md)



- \*\*An 
Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\*\*, ICLR'21  

  \[Vision Transformer Summary\]
(https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Vision\_Transformer.md)



- \*\*Big Bird: Transforme
rs for Longer Sequences\*\*, NIPS'20  

  \[Big Bird Transformers Summary\](https://github.com/vlgiitr/papers\_we\_read/
blob/master/summaries/Big\_Bird\_Transformers.md)



The repository invites contributions from the community. If you fin
d the summaries helpful, you are encouraged to submit your own summaries for research papers. The team aims to regularly
 update the collection with summaries of papers from upcoming conferences and key topics in deep learning and AI. 



Yo
u can access the full repository and contribute here:  

\[Vision Language Group Paper Summaries\](https://github.com/vl
giitr/papers\_we\_read)



By contributing, you'll help make advanced research more accessible to both beginners and exp
erts in the field.
```
---

     
 
all -  [ Summaries of some Research Papers we read! ](https://www.reddit.com/r/neuralnetworks/comments/1fl4al2/summaries_of_some_research_papers_we_read/) , 2024-10-06-0914
```
The Vision Language Group at IIT Roorkee has curated a repository of comprehensive summaries for deep learning research 
papers from top-tier conferences like NeurIPS, CVPR, ICCV, ICML from 2016 to 2024. These summaries aim to provide a conc
ise understanding of influential papers in fields such as computer vision, natural language processing, and machine lear
ning. The collection is constantly growing, with new summaries added frequently. Here are a few notable examples:

* **D
reamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**, CVPR'23 [DreamBooth Summary](http
s://github.com/vlgiitr/papers_we_read/blob/master/summaries/DreamBooth.md)
* **Segment Anything**, ICCV'23 [Segment Anyt
hing Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Segment_Anything.md)
* **An Image is Worth
 One Word: Personalizing Text-to-Image Generation using Textual Inversion**, ICCV'23 [Textual Inversion Summary](https:/
/github.com/vlgiitr/papers_we_read/blob/master/summaries/Textual_inversion.md)
* **Photorealistic Text-to-Image Diffusio
n Models with Deep Language Understanding**, NIPS'22 [Photorealistic Diffusion Summary](https://github.com/vlgiitr/paper
s_we_read/blob/master/summaries/imagen.md)
* **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scal
e**, ICLR'21 [Vision Transformer Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Vision_Transfo
rmer.md)
* **Big Bird: Transformers for Longer Sequences**, NIPS'20 [Big Bird Transformers Summary](https://github.com/v
lgiitr/papers_we_read/blob/master/summaries/Big_Bird_Transformers.md)

The repository invites contributions from the com
munity. If you find the summaries helpful, you are encouraged to submit your own summaries for research papers. The team
 aims to regularly update the collection with summaries of papers from upcoming conferences and key topics in deep learn
ing and AI.

You can access the full repository and contribute here:  
[Vision Language Group Paper Summaries](https://g
ithub.com/vlgiitr/papers_we_read)

By contributing, you'll help make advanced research more accessible to both beginners
 and experts in the field.
```
---

     
 
all -  [ Comprehensive Summaries of Paper We Read ](https://www.reddit.com/r/u_vlg_iitr/comments/1fl48qg/comprehensive_summaries_of_paper_we_read/) , 2024-10-06-0914
```
**The Vision Language Group at IIT Roorkee** has put together an awesome repository of **comprehensive summaries** for d
eep learning papers from top conferences like **NeurIPS, CVPR, ICCV, ICML (2016-2024)**. These summaries break down key 
papers in computer vision, NLP, and machine learningâ€”perfect if you want to stay updated without diving deep into the fu
ll papers. Here are a few highlights:

* **DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Gen
eration**, CVPR'23 ðŸ‘‰ [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/DreamBooth.md)
* **Segmen
t Anything**, ICCV'23 ðŸ‘‰ [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Segment_Anything.md)
*
 **An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion**, ICCV'23 ðŸ‘‰ [Summary](htt
ps://github.com/vlgiitr/papers_we_read/blob/master/summaries/Textual_inversion.md)
* **Photorealistic Text-to-Image Diff
usion Models with Deep Language Understanding**, NIPS'22 ðŸ‘‰ [Summary](https://github.com/vlgiitr/papers_we_read/blob/mast
er/summaries/imagen.md)
* **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**, ICLR'21 ðŸ‘‰ [Sum
mary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Vision_Transformer.md)
* **Big Bird: Transformers 
for Longer Sequences**, NIPS'20 ðŸ‘‰ [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Big_Bird_Tra
nsformers.md)

If you find these summaries useful, feel free to contribute your own! The repo is constantly being update
d with new papers from major conferences, so it's a great way to keep up with the latest in AI and deep learning.

ðŸ“‚ **C
heck out the full repo and contribute here**  
[Vision Language Group Paper Summaries](https://github.com/vlgiitr/papers
_we_read)

Happy reading! ðŸŽ‰
```
---

     
 
all -  [ [R] Erasing the Invisible: A Stress-Test Challenge for Image Watermarks (NeurIPS 2024 Competition) ](https://www.reddit.com/r/MachineLearning/comments/1fk90gj/r_erasing_the_invisible_a_stresstest_challenge/) , 2024-10-06-0914
```
We're excited to announce the NeurIPS competition '**Erasing the Invisible: A Stress-Test Challenge for Image Watermarks
**' running fromÂ **September 16 to November 5**. This is your chance to test your skills in a cutting-edge domain and wi
n a share of our $6000 prize pool!

# Competition Overview

This competition is divided intoÂ **two tracks: Black Box Tra
ck and Beige Box Track**. It aims to validate the robustness of image watermarks under varying visibility conditions and
 attacker knowledge. Competitors will attempt to remove invisible watermarks while maintaining the quality of the images
. Evaluations will be based on two criteria: the effectiveness of watermark removal and the preservation of image qualit
y.

# ðŸ”— Important Dates:

â–¶ï¸Â **Submission phase:**Â Sep 16 - Nov 5  
â–¶ï¸Â **Registration and submissions close:**Â Nov 5  
â–¶
ï¸Â **Winning team announcement:**Â Nov 20

# ðŸŒ More Info & Registration:

â–¶ï¸Â **Website:**Â [http://erasinginvisible.github.
io](http://erasinginvisible.github.io/)  
â–¶ï¸Â **Hosted on Codabench:**  
â©Â **Beige-Box Track:**Â [codabench.org/competitio
ns/3821](https://codabench.org/competitions/3821/)  
â©Â **Black-Box Track:**Â [codabench.org/competitions/3857](https://co
dabench.org/competitions/3857/)

# ðŸ’¡ Why Participate?

* **Test your skills**Â in a real-world, cutting-edge domain.
* **
Validate watermark robustness**Â under various conditions.
* **Collaborate**Â with a global community of researchers and p
ractitioners.
* **Earn your share of $6000**Â (and counting as more sponsors join)!

# ðŸ’° Prize Pool: $6000 (and growing!)


Want toÂ **sponsor**Â the competition? Reach out to us at:  
ðŸ“§Â [erasinginvisible@googlegroups.com](mailto:erasinginvisib
le@googlegroups.com)Â orÂ [furongh@umd.edu](mailto:furongh@umd.edu)
```
---

     
 
all -  [ How to get into CS/AI related research and get a paper published in a top international publication  ](https://www.reddit.com/r/Indian_Academia/comments/1fjy5bt/how_to_get_into_csai_related_research_and_get_a/) , 2024-10-06-0914
```
Qualifications: B. Tech. CSE (Tier-3 private college)   
YOE: 1  
Summary: I want to know how I can contribute to resear
ch and get papers published in the top international publications like ICML, NeurIPS, ICCV, CVPR etc. My fundamentals in
 subjects like Deep Learning and Computer Vision are quite strong, and I was wondering how I can get into research, and 
what the process looks like. I am guessing I need to talk to professors at some of the top institutes like IIITs/IITs? H
ow do I start, I'd really appreciate some feedback regarding this.
```
---

     
 
all -  [ [Call for papers] Safe Generative AI Workshop at NeurIPS 2024 ](https://groups.google.com/g/ml-news/c/URCyANyWxGA) , 2024-10-06-0914
```

```
---

     
 
all -  [ [R] submitting to neurips and coling at the same time ](https://www.reddit.com/r/MachineLearning/comments/1fiivv5/r_submitting_to_neurips_and_coling_at_the_same/) , 2024-10-06-0914
```
Would I be able to submit to both neurips solar and coling 2025? Colingâ€™s policy is no journals or conferences but solar
 is a workshop and it allows dual submission.
```
---

     
 
all -  [ Navigating UCSD as a freshman ](https://www.reddit.com/r/u_TrainingResolution12/comments/1fg4ars/navigating_ucsd_as_a_freshman/) , 2024-10-06-0914
```
Hey. It is the time of the year where everyone's excited for our upcoming session. Welcome to UCSD and I hope you are do
ing great. As a senior, I have been exposed to various handy tools to navigate college life. Here are my recommendations


1. **Networking system**-Â **Discord, Slack, Instagram webpage**Â for theÂ **clubs and classes**Â you are interested in.Â *
*Piazza**Â good for reference to asking questions to the instructional team+class.Â **Cold email and Office hours**Â have w
orked well in finding research jobs.Â **Linkedin**Â is great for having your online profile up top, highly recommend a pro
fessional photograph.Â **X/Twitter**Â for following your latest research if you're into EE/AI/related fields.Â **Geisel, Cl
asses, Intramural Sports and Greek Rush**Â are experiences that got me some good exposure to social aspects of campus (no
t in a frat but made friends). For jobsÂ **Handshake and Linkedin**Â is enough if you have a good portfolio going on. Look
 at accepted alumnis portfolio based on your goals and see what jobs fit your traits best.Â **Resident life events**Â too 
are lit. Become a IA/SI leader. You'll have fun lol.
2. **Offerup and Facebook Marketplace**Â are great for furniture and
 rentals. If you're a business nerd like me, you could also practice your negotiation skills there without anything at s
take (LOL!). Other good things I found wereÂ **Styl App**Â andÂ **Poshmark + thrift stores and stockx**Â if you're looking t
o up your fashion game.
3. **Notetaking**/**Information management-**Â A HIGHLY debated topic amongst students because pe
n and paper models are outdated. I usedÂ **Apple Notes**Â combined with P**erplexity/Claude/GPT-4**Â for notetaking. The wo
rkflow I follow isÂ **lecture notes (typed or handwritten + screenshot)-> pdf textbook into perplexity/GPT and prompt to 
follow textbook-> get key concepts and insights per lesson-> work on problems and use LLMs to see how and where each min
i concept applies-> understand doubtful concepts in office hours and mail TAs/post on piazza-> reflect and add summary t
o notes-> hold group study sessions to debate tougher questions and teach what you mastered to other students.**

The wh
ole process takes about 1 hour a day to 2 max per subject- benchmark-> math 142a. Perplexity is useful for getting searc
h and images/videos directly from your notes and interacting with content on a deeper level, GPT is great for summary an
d analysis. I strongly believe we should interface LLMs with learning and conventional instruction. Never rely on LLMs f
or answers. Never cheat. libgen,Â **JSTOR, arXiv, nature science journal, neurIPS, internet archive**Â are some excellent 
sources for articles. Geisel has some articles too that can be access with campusÂ **VPNs.**Â My GPA went from a 3.2 to 4.
0 with straight As thanks to this note taking system. It is like my second brain. I also met a few cool people this way,
 we learnt a lot together.

4.Â **Hangouts**- The most subjective thing by far. Not a local. I intern full time rn so I b
arely have time. The one activity i did choose was fitness and beaches because I can't keep up with studies otherwise. I
 go to 24 fitness or main gym. Ice rink close by or the mt soledad drive is a cool date idea for reference. Cool tidepoo
ls nearby at LJ shores you can search instagram reels idk i randomly stumbled. There's some good cafes all around town g
o on yelp. Surf spots are windansea and couple others. Convoy easily on top for asian food. Crazy bakeries and sweet sho
ps all around little italy. Hillcrest welfare wednesdays you can do cool stuff. Indian food is good mira mesa side. Mexi
can at Old town and near TJ. SDSU side has lit parties if you want go make friends there and get invited. PB amazing for
 food and party. Downtown has great museums. Seaworld and zoo, sd science center amazing. La Jolla Playhouse is cool. Ch
e cafe is awesome. Gaslamp gastropubs go hard. SD padres games great hangouts you will make lots of friends. MMA PPV eve
nts too. Weekend getaway for non locals they have a secret place at OB which I know lot of you will try goofy stuff. Rem
ember if it takes a toll on your body and affects mental health then it is not worth it.

Ah thank god for speech to tex
t. Have fun. Went through 4 years and a lot of memories.
```
---

     
 
all -  [ [D] Updated Paper submission [NeurIPS 2024 Workshop] ](https://www.reddit.com/r/MachineLearning/comments/1fex05d/d_updated_paper_submission_neurips_2024_workshop/) , 2024-10-06-0914
```
Hey, everyone.  
Sorry for asking a noob question.  
So basically we have submitted a paper at a workshop of NeurIPS 202
4. This is our first work during our undergrad. After submission, we received an email the next day regarding a margin i
ssue that needed to be fixed, or our submission would be rejected. Which we fixed \[a very unintentional error\] and try
ing to submit it since then  but in the Openreview, it keeps saying that the invitation submission has expired. So is th
ere any deadline we have to maintain for this kind of scenario. The main review will be given in the next month. We have
 tried to contact them, but we are not getting any response.
```
---

     
 
all -  [ [D] Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise ](https://www.reddit.com/r/MachineLearning/comments/1fec2jq/d_cold_diffusion_inverting_arbitrary_image/) , 2024-10-06-0914
```
Hi everyone, 

The point of this post is not to blame the authors, I'm just very surprised by the review process.

I jus
t stumbled upon this paper. While I find the ideas somewhat interesting, I found the overall results and justifications 
to be very weak.   
It was a clear reject from ICLR2022, mainly for a lack of any theoretical justifications. [https://o
penreview.net/forum?id=slHNW9yRie0](https://openreview.net/forum?id=slHNW9yRie0)  
The exact same paper is resubmitted a
t NeurIPS2023 and I kid you not, the thing is accepted for a poster. [https://openreview.net/forum?id=XH3ArccntI](https:
//openreview.net/forum?id=XH3ArccntI)

I don't really get how it could have made it through the review process of NeurIP
S. The whole thing is very preliminary and is basically just consisting of experiments.  
It even llack citations of oth
er very closely related work such as *Generative Modelling With Inverse Heat Dissipation* [https://arxiv.org/abs/2206.13
397](https://arxiv.org/abs/2206.13397) which is basically their 'blurring diffusion' but with theoretical background and
 better results (which was accepted to ICLR2023)...

I thought NeurIPS was on the same level as ICLR, but now it seems t
o me sometimes papers just get randomly accepted.

So I was wondering, if anyone had an opinion on this, or if you have 
encountered other similar cases ? 
```
---

     
 
all -  [ Derivation of the upper bound of the average regret of online-to-batch conversion in H-smoothness ](https://www.reddit.com/r/mathematics/comments/1fbn724/derivation_of_the_upper_bound_of_the_average/) , 2024-10-06-0914
```
I've been studying a \[paper\]\[1\] (Smoothness, Low-Noise and Fast Rates) on the impact of smoothness on the convergenc
e rate of online-to-batch conversion, specifically Theorem 2, which provides a bound on the average regret in the contex
t of online convex optimization. The paper claims that this theorem can be proved using Lemma 3.1 in the original paper 
and Theorem 1 from \[this thesis\]\[2\] (Online Learning: Theory, Algorithms, and Applications). However, I'm struggling
 to see the connection between these results. Could someone help clarify how Lemma 3.1 and Theorem 1 from the thesis are
 used to prove Theorem 2 in the paper?

\*\*Lemma 3.1:\*\* For an $H$-smooth non-negative function $f : W \\rightarrow \
\mathbb{R}$, for all $w \\in W$:

$$

\\|\\nabla f(w)\\|\^\* \\leq \\sqrt{4H f(w)}

$$

\*\*Theorem 1:\*\* Under the sam
e conditions as Lemma 1. Assume that a constant $L$ exists such that for all $t$, the function $f\_t$ is $L$-self-bounde
d for the norm $\\|\\cdot\\|$. Let $U\_1$ and $U\_2$ be two positive scalars and set $c = L + \\sqrt{L\^2 + \\frac{LU\_2
}{U\_1}}$. Then, for any $u \\in S$ that satisfies $f(u) \\leq U\_1$ and $\\sum\_{t=1}\^{T} g\_t(u) \\leq U\_2$, we have
,

$$

R(u, T) = \\sum\_{t=1}\^{T} g\_t(w\_t) - \\sum\_{t=1}\^{T} g\_t(u) \\leq 2\\sqrt{LU\_1U\_2} + 4LU\_1.

$$

\*\*Th
eorem 2:\*\* For any $B \\in \\mathbb{R}$ and $\\overline{L\^\*}$, if we use stepsize

$$

\\eta = \\frac{1}{H B\^2 + \\
sqrt{H\^2 B\^4 + H B\^2 n \\overline{L\^\*}}}

$$

for the Mirror Descent algorithm, then for any instance sequence

$z\
_1, \\ldots, z\_n \\in \\mathcal{Z}$, the average regret w.r.t. any

$w\^\* \\in W$ such that $F(w\^\*) \\leq B\^2$ and 
$\\frac{1}{n} \\sum\_{j=1}\^{n} \\ell(w\^\*, z\_j) \\leq \\overline{L\^\*}$

is bounded by:

$$

\\frac{1}{n} \\sum\_{i=
1}\^{n} \\ell(w\_i, z\_i) - \\frac{1}{n} \\sum\_{i=1}\^{n} \\ell(w\^\*, z\_i) \\leq \\frac{4H B\^2}{n} + \\frac{2 \\sqrt
{H B\^2 \\overline{L\^\*}}}{n}

$$

I found the proof of Theorem 1 a bit confusing as well, particularly because it does
n't clearly explain the relationship between $U\_1, U\_2$, and $c$ in that specific manner.

\[1\]: [https://proceedings
.neurips.cc/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf](https://proceedings.neurips.cc/paper/2010/file/7
6cf99d3614e23eabab16fb27e944bf9-Paper.pdf)

\[2\]: [https://home.ttic.edu/\~shai/papers/ShalevThesis07.pdf](https://home
.ttic.edu/~shai/papers/ShalevThesis07.pdf)

For the original post, check [convex optimization - Derivation of the upper 
bound of the average regret of online-to-batch conversion in H-smoothness - Mathematics Stack Exchange](https://math.sta
ckexchange.com/questions/4966362/derivation-of-the-upper-bound-of-the-average-regret-of-online-to-batch-conversio)
```
---

     
