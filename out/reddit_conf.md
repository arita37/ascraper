 
all -  [ [D] Expectation from Machine Learning Engineering jobs ](https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/) , 2024-11-19-0913
```
Hey everyone,

I‚Äôve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up
 a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems 
to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, whi
ch are apparently more in demand than research scientist positions.

I‚Äôm curious about the difference between these two 
roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master‚Äôs degree is often pr
eferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are you
r thoughts?
```
---

     
 
all -  [ [D] NeurIPS 24‚Äô Attendance without tickets ](https://www.reddit.com/r/MachineLearning/comments/1gt9r5b/d_neurips_24_attendance_without_tickets/) , 2024-11-19-0913
```
Does anyone know, for past conferences, whether there were people checking badges at the entrance of the convention cent
er or at the workshop. (cause I wasn‚Äôt able to get a ticket so far via lottery this year, and I am wondering whether I c
ould just walk in or something)
```
---

     
 
all -  [ On the current state of robotics (ig?) (from a CSE perspective) (For all my homies out there) ](https://www.reddit.com/r/Btechtards/comments/1gsjxio/on_the_current_state_of_robotics_ig_from_a_cse/) , 2024-11-19-0913
```
Follow up on¬†[https://www.reddit.com/r/Btechtards/comments/1gseqvq/cs\_roadmap\_for\_all\_my\_1st\_year\_homes\_out\_the
re/](https://www.reddit.com/r/Btechtards/comments/1gseqvq/cs_roadmap_for_all_my_1st_year_homes_out_there/)

Background -
 CSE 4th year, T1 (idk much about the electronics aspects of robotics and I kinda do computer vision not robotics)

\---
Profs---

I have like a research-ish approach to robotics cause of labs and stuff. Here's how I judge research -¬†[https:
//csrankings.org/#/index?all&us](https://csrankings.org/#/index?all&us)¬†(only considers the good conferences)

As for Ro
botics and CV in India -¬†[https://csrankings.org/#/index?vision&robotics&in](https://csrankings.org/#/index?vision&robot
ics&in)

You get the usual research heavy unis (IIIT H, IISc, IIT K).

Let's go from a professor perspective, here are t
he absolute beasts in India (in no particular order) (CV + robotics):

1. K Madhava Krishna (IIIT H):¬†[https://scholar.g
oogle.co.in/citations?user=QDuPGHwAAAAJ&hl=en](https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en)
2. C V Ja
wahar (IIIT H):¬†[https://scholar.google.com/citations?user=U9dH-DoAAAAJ&hl=en](https://scholar.google.com/citations?user
=U9dH-DoAAAAJ&hl=en)
3. R. Venkatesh Babu (IISc):¬†[https://cds.iisc.ac.in/faculty/venky](https://cds.iisc.ac.in/faculty/
venky)
4. Shishir N Y Kolathaya (IISc):¬†[https://www.shishirny.com/](https://www.shishirny.com/)
5. Indranil Saha (IIT K
):¬†[https://scholar.google.com/citations?user=F6QSFGkAAAAJ&hl=en](https://scholar.google.com/citations?user=F6QSFGkAAAAJ
&hl=en)
6. Avinash Sharma (IIT J):¬†[https://3dcomputervision.github.io/](https://3dcomputervision.github.io/)
7. Chetan 
Arora (IIT D):¬†[https://www.cse.iitd.ac.in/\~chetan/](https://www.cse.iitd.ac.in/~chetan/)

(Lmk if I should add any)

H
ere's how I would have started:

1. Look up their research and try to make sense out of it
2. Look at their top 5 newest
 papers and top 5 papers and see if you understand the abstract. If you don't relentelessly use Perplexity and get infor
mation.

Now you know the SOTA in India for robotics and CV. Then, look at these international profs (trying to add 5 in
 no order that have diverse research interests)

1. [https://www.cs.cmu.edu/\~./choset/](https://www.cs.cmu.edu/~./chose
t/)
2. [https://people.eecs.berkeley.edu/\~svlevine/](https://people.eecs.berkeley.edu/~svlevine/)¬†(absolute fucking god
 imo)
3. [https://animesh.garg.tech/](https://animesh.garg.tech/)
4. [https://research.qut.edu.au/qcr/people/michael-mil
ford](https://research.qut.edu.au/qcr/people/michael-milford)
5. [https://people.eecs.berkeley.edu/\~anca/](https://peop
le.eecs.berkeley.edu/~anca/)¬†(HCI stuff but I consider her robotics)

Good, now you know what's happening in academia. S
ince industry stems from academic esp. in robotics, you also know what will be happening there 5 years down the line. Lo
ok at cool stuff from Boston Dynamics (duh), Allen Institue for AI, Honda Research, etc. as well. Some pretty amazing Ch
inese and Israeli companies exist as well.

\---Starting with robotics---

I'm a sucker for mobile robotics -

1. [https
://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN\_](https://www.youtube.com/playlist?list=PLgnQpQtFTOG
QrZ4O5QzbIHgl3b1JHimN_)
2. [https://www.youtube.com/playlist?list=PLgnQpQtFTOGQJXx-x0t23RmRbjp\_yMb4v](https://www.youtu
be.com/playlist?list=PLgnQpQtFTOGQJXx-x0t23RmRbjp_yMb4v)
3. [https://www.youtube.com/playlist?list=PLgnQpQtFTOGQh\_J16IM
wDlji18SWQ2PZ6](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQh_J16IMwDlji18SWQ2PZ6)

All by C. Stachniss.

Then, q
uite literally do any course on robot manipulation and dynamics.

Then, start OpenCV - the docs are beautiful. Use them.
 Use Python. Learn PyTorch as well! (docs work).

Learn ROS using the docs (or any playlist tbh, all are pretty good).


Tie eveyrthing together by building a CV + Robotics pipeline using ROS simulations such as camera calibration, SLAM pipe
line, etc.

\---Going ahead---

Take a top-tier robotics paper (one that is not too math-y but more ML-y) and read their
 codebases. Literally just google any of the papers from the conferences listed below and choose one that sounds interes
ting (and has the codebase available).

Then, and I cannot emphasize this further, write your own implementation. It mig
ht take weeks (and ik the difficulties involved vis-a-vis hardware or GPUs - just simulate / do on a lower scale) but it
'll be worth it.

My fav repos are

1. [https://github.com/amaralibey/MixVPR](https://github.com/amaralibey/MixVPR)
2. [
https://github.com/robodhruv/visualnav-transformer](https://github.com/robodhruv/visualnav-transformer)¬†(all three of th
e codebases)
3. [https://github.com/PRBonn/kiss-icp](https://github.com/PRBonn/kiss-icp)

(Again, lmk if you have any ad
ditions to this list)

Robotics honestly just diversifies at this point. Choose a direction that interests you (SLAM, ha
rdware, optimizations, vision, HCI, etc.)

\---Jobs?---

Join academia or a research lab (none in India unfortunately). 
You can cold-email profs asking for research internships or assistantships - that works sometimes. CAIR in Blr is also a
mazing.

Industry - some super cool stuff in India as well rn (minuszero, Swaayatt Robots, a bunch of drone companies, e
tc.). Nvidia also has some cool roles as well.

\---Should you do it?---

Idk. I'm an undergrad. This is what I've done 
and what some PhDs and MS people told me. Ask people on LinkedIn / Twitter to figure out whether robotics (and which par
t of robotics specifically) is what you want.

\---Conferences---

Robotics - ICRA, IROS, CORL, RSS

CV - CVPR, ICCV, EC
CV, BMVC, WACV (people also submit CV stuff to ICLR, NeurIPS, ICML, etc)


```
---

     
 
all -  [ [R] Convolutional Differentiable Logic Gate Networks ](https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/) , 2024-11-19-0913
```
Abstract

With the increasing inference cost of machine learning models, there is a growing interest in models with fast
 and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxatio
n was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference on
ly requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware
 and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR p
ooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and util
izing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, whic
h improves over the SOTA while being 29√ó smaller.

  
Accepted at Neurips 2024, 'SOTA' here means comparable approaches.
 I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. C
urious what others think?
```
---

     
 
all -  [ How is the ACL Conference? ](https://www.reddit.com/r/deeplearning/comments/1gs7he7/how_is_the_acl_conference/) , 2024-11-19-0913
```
Hello, I know it's a very noob question but I was wondering what the reputation of ACL is in the field. I have been writ
ing my first paper and my mentor recommended that I aim for the ACL deadline, I just wanted to know how prestigious it w
as relative to bigger conferences like NeurIPS, ICML, ICLR, etc.

Also, purely hypothetical, but what weight does an ACL
 acceptance hold for getting a summer internship/research? I'm an undergrad and I'm kind of cooked with my summer intern
ship prospects, so I was wondering if it would help in any regard.
```
---

     
 
all -  [ [D] Neurips 2024 Hotel Roommate Search ](https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/) , 2024-11-19-0913
```
The hotels around the venue for Neurips 2024 are pretty expensive, and I'm looking for a roommate to split the cost with
 (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room f
or Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. I
f anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post wit
h your research group or other attendees that you know.

If you are unsure about rooming with a complete stranger, you c
an get to know me a little bit through my personal website (https://mtcrawshaw.github.io/), which has links to my google
 scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. 
Just a grad student trying to make conferences affordable! Thanks.
```
---

     
 
all -  [ Need an Advice ](https://www.reddit.com/r/PhD/comments/1grg73d/need_an_advice/) , 2024-11-19-0913
```
Hi everyone, I‚Äôm a first-year computer science PhD student in Europe from Asia, going into my second year soon. I wanted
 to ask for some advice on what I should do. First off, I‚Äôm an international student from a Southeast Asian country, and
 right now I‚Äôm really struggling with the lab environment.

First, my professor requires all PhD students to work in the
 lab from 9 to 5 every weekday, no exceptions except for weekends. We‚Äôre only allowed to take time off when the universi
ty is officially closed. Second, I found out from previous PhD students that my professor insists on a strict policy of 
‚Äúequal credit‚Äù in publications, meaning that even if I do all the work for a paper‚Äîfrom analysis to programming, writing
, and revisions‚Äîmy name won‚Äôt be listed as the first author because authorship order is strictly alphabetical.

Third, s
ome of us in the lab (we‚Äôre all international students) aren‚Äôt allowed to submit our papers to conferences, even big one
s like ICML or NeurIPS. My professor only wants our publications in journals, even though conferences are important for 
PhD students to network and get feedback from experts in the field.

Lastly, and perhaps the most difficult part for me,
 is that I‚Äôm not allowed to collaborate with anyone outside the lab. I‚Äôm not even allowed to discuss my project or seek 
advice from people outside the lab group. This restriction makes me feel isolated, and for the past three months, I‚Äôve h
ad recurring nightmares and panic attacks before going into the lab. I reached out to the PhD board to ask if I could tr
ansfer to a different lab, but they said it‚Äôs impossible.

I‚Äôm really at a loss here. Should I stick it out in this lab 
for the next 2-3 years, knowing I won‚Äôt have the chance to publish as the primary author and that, when I graduate, I‚Äôll
 probably have no network beyond the people in this lab?
```
---

     
 
all -  [ [D] Looking for a project partner who's published in top conferences [cvpr, neurips, wacv, iccv, etc ](https://www.reddit.com/r/computervision/comments/1gpfcpk/d_looking_for_a_project_partner_whos_published_in/) , 2024-11-19-0913
```
Hello y'all. Deep into my master's degree, I am in a dire need of a mentor/partner for my research partner. Some of the 
professors at the academia who claim to specialize in the field of computer vision/ai doesnt know how to clone an existi
ng model from github or provide gpu alternatives and solutions who doesnt have fancy things to speed up the process. 

s
o if you do feel the same way and is interested to work on some cool research gap leading to a publication. drop a comme
nt on what excites you most. thankss.
```
---

     
 
all -  [ GitHub - hutaiHang/ToMe: [NeurIPS 2024] Token Merging for Training-Free Semantic Binding in Text-to- ](https://github.com/hutaihang/ToMe) , 2024-11-19-0913
```

```
---

     
 
all -  [ [D] How to Choose an AI-Focused Master's Program? ](https://www.reddit.com/r/computervision/comments/1gpba3y/d_how_to_choose_an_aifocused_masters_program/) , 2024-11-19-0913
```
I'm currently applying for AI-focused Master's programs, and I could really use some advice. I love working in computer 
vision, and I think I‚Äôm genuinely passionate about research. I presented my first paper at an affinity workshop at ICML,
 and I‚Äôll be attending NeurIPS as a workshop presenter. This experience has been a blast, and I'm hoping to continue dow
n this path.

Right now, I'm feeling overwhelmed by all the options and the looming deadlines. The only program I‚Äôm trul
y excited about is at UvA (University of Amsterdam). But I know I need to consider more options to keep my career moving
 forward.

Here‚Äôs what I'm interested in:

* **Self-Supervised Learning (SSL):** I have experience in this area and woul
d love to deepen my expertise.
* **Video Understanding and GNNs:** These are becoming my newest interests, and I‚Äôd love 
to join a program where I can explore these topics.
* **Research-oriented environments:** I‚Äôm currently collaborating wi
th a professor and have found that I really enjoy the collaborative, exploratory nature of research.

The problem? I don
‚Äôt want to settle for a program that doesn‚Äôt align with these interests or doesn‚Äôt offer strong mentorship and research 
opportunities. I‚Äôm also worried I might be *too* picky, which is making the process even more stressful. I‚Äôd love to hea
r from anyone who‚Äôs been in a similar position:

1. **How did you prioritize which programs to apply to?**
2. **Did you 
find a strategy that helped you balance your interests with program options?**
3. **Any advice on picking a program that
 will help with a long-term research-focused career?**

Thanks so much for any insights you can share!
```
---

     
 
all -  [ [D] NeurIPS After Dark Networking Event ](https://www.reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/) , 2024-11-19-0913
```
Just got an email about an official ticketed after dark NeurIPS networking event - this will be my first time attending/
presenting, wondering if these events are worth going to. More generally, also interested in hearing about how to make t
he most of my time attending.
```
---

     
 
all -  [ NeuroAI - NeurIPS Workshop (Vancouver, Dec 15) ](https://www.reddit.com/r/corticallabs/comments/1gp8lg5/neuroai_neurips_workshop_vancouver_dec_15/) , 2024-11-19-0913
```
Hey all, just wanted to make the announcement that some of the Cortical Labs team will be in Vancouver for NeurIPS and C
TO Dave will be publishing the beta API spec for community feedback.

  
[https://neuroai-workshop.github.io](https://ne
uroai-workshop.github.io)
```
---

     
 
all -  [ Anyone's paper got selected for NeurIPS and are planning to go to Vancouver from Bengaluru??? ](https://www.reddit.com/r/Bengaluru/comments/1go3w7w/anyones_paper_got_selected_for_neurips_and_are/) , 2024-11-19-0913
```
If anyone is traveling, i really need your help in planning mine. 
```
---

     
 
all -  [ [R] Classic GNNs (GCNs, GraphSAGEs, GATs) are Strong Baselines on Node Classification ](https://www.reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/) , 2024-11-19-0913
```
We‚Äôre excited to share our recent paper '[\[NeurIPS 2024\] Classic GNNs are Strong Baselines: Reassessing GNNs for Node 
Classification](https://arxiv.org/pdf/2406.08993).'

In this study, we conduct a thorough review of classic GNNs for nod
e classification tasks. Our findings suggest that the superior performance often reported by state-of-the-art graph lear
ning models may be due to suboptimal hyperparameter configurations in classic GNNs. By fine-tuning these hyperparameters
, we show that classic GNNs outperform the latest models on 17 out of 18 widely used node classification datasets.

Code
:¬†[https://github.com/LUOyk1999/tunedGNN](https://t.co/QeNSn2D9CN)  
Arxiv:¬†[https://arxiv.org/abs/2406.08993](https://t
.co/MD4mVTnHk8)

If you find our work interesting, we‚Äôd greatly appreciate a ‚≠êÔ∏è on GitHub!
```
---

     
 
all -  [ CV Sugession ](https://www.reddit.com/r/gradadmissions/comments/1gnbr8g/cv_sugession/) , 2024-11-19-0913
```
I  tried to publish research papers twice‚Äîfirst at NeurIPS and recently at ICVGIP‚Äîbut I got rejected both times ü•≤.

Now,
 I am thinking of adding a section to my CV called ‚ÄúAppendix: Research Work Sample since I don‚Äôt have any published pape
rs yet. Should I include these papers and label them as ‚Äúsubmitted‚Äù or ‚Äúsubmitted to conf __‚Äù?

I would really appreciat
e your advice.
```
---

     
 
all -  [ Ok, for real how do I rank? ](https://www.reddit.com/r/eb_1a/comments/1gn2zo7/ok_for_real_how_do_i_rank/) , 2024-11-19-0913
```
I was pretty certain that my pathway to green card was gonna be smooth‚Ä¶ until the Trump victory. I‚Äôm gearing up for EB1A
 but worried that the extra scrutiny during his term will close that door for me. Here are the stats.

- FAANG ML Engine
er with MSc
- Some media coverage of my work
- 4 papers, 3 preprints, 1 industrial demo, 1 thesis, first author on all b
ut 2; some are top-tier like CVPR and ACL
- 450+ citations
- have served as reviewer for about 50 manuscripts; all for t
he top tier conferences (CVPR, NeurIPS, ICML, ICLR)

Am I toast or can I gun for EB1A?
```
---

     
 
all -  [ [R] Most Time Series Anomaly Detection results are meaningless (two short videos explain why) ](https://www.reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/) , 2024-11-19-0913
```
Dear Colleagues

Time Series Anomaly Detection (TSAD) is hot right now, with dozens of ¬†papers each year in NeurIPS, SIG
KDD, ICML, PVLDB etc.

However, I claim that much of the published results are meaningless, because the uncertainty of t
he ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.

I have made
 two 90-second-long videos that make this clear in a visual and intuitive way:

¬†1)¬†¬†¬†¬†¬† Why Most Time Series Anomaly De
tection Results are Meaningless (Dodgers)

[https://www.youtube.com/watch?v=iRN5oVNvZwk&ab\_channel=EamonnKeogh](https:/
/www.youtube.com/watch?v=iRN5oVNvZwk&ab_channel=EamonnKeogh)

¬†¬†2)¬†¬†¬†¬†¬† Why Most Time Series Anomaly Detection Results a
re Meaningless (AnnGun)

[https://www.youtube.com/watch?v=3gH-65RCBDs&ab\_channel=EamonnKeogh](https://www.youtube.com/w
atch?v=3gH-65RCBDs&ab_channel=EamonnKeogh)

As always, corrections and comments welcome.

Eamonn

¬†EDIT: To be clear, my
 point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, 
we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of 
highly cited papers)

  


For a review of most of the commonly used TSAD datasets, see this file:

[https://www.dropbox
.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0](
https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyj
splu0u1t7ugg&dl=0)
```
---

     
 
all -  [ Boss wants me in Vancouver for neurips conference. Please take my 1 tix, 5 day pass. Selling at cost ](https://www.reddit.com/r/Wonderfruit/comments/1gmvwe0/boss_wants_me_in_vancouver_for_neurips_conference/) , 2024-11-19-0913
```
I have one 5 day pass, bought it during early bird. Can change name during pre-registration. I want to make no profit fr
om this. Heck, I‚Äôll give you a discount. But I need to talk to you up front, and we need to figure out a payment situati
on. 

Best case scenario is that you‚Äôre in the United States.

Reply to this and I will reach out.
```
---

     
 
all -  [ Alaa Lab at UC Berkeley / UCSF Seeking PhD Students in ML/AI for Healthcare ](https://www.reddit.com/r/CompSocial/comments/1gmh3q6/alaa_lab_at_uc_berkeley_ucsf_seeking_phd_students/) , 2024-11-19-0913
```
Prof. Ahmad Alaa, who leads a [joint lab](https://alaalab.berkeley.edu/home) at UC Berkeley and UCSF is seeking PhD appl
icants interested in working at the intersection of ML/AI and Healthcare. They call out the following focus areas, with 
example papers:

* **Track 1:**¬†**Machine Learning Theory, Statistics and Causal Inference** (Example papers: [NeurIPS 2
024](https://arxiv.org/abs/2402.07307), [NeurIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/94ab02
a30b0e4a692a42ccd0b4c55399-Abstract-Conference.html), [AISTATS 2023](https://proceedings.mlr.press/v206/alaa23a.html))
*
 **Track 2: Large Vision and Language Models for Medicine**¬†(Example papers: [NeurIPS 2024 - 1](https://arxiv.org/abs/24
03.00177), [NeurIPS 2024 - 2](https://arxiv.org/pdf/2405.19567), [ICML 2024](https://arxiv.org/pdf/2406.05396),¬†[ICLR 20
24](https://arxiv.org/abs/2310.00390), [NeurIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2b1d1e5
affe5fdb70372cd90dd8afd49-Abstract-Conference.html))
* **Track 3: Applied¬†Machine Learning for Cardiology**¬†(Example pap
ers: [Nature Machine Intelligence 2021](https://www.nature.com/articles/s42256-021-00353-8),¬†[PLOS 2019](https://journal
s.plos.org/plosone/article?id=10.1371/journal.pone.0213653))

To learn more and connect with Dr. Alaa prior to submittin
g a PhD application, check out this Google Form: [https://docs.google.com/forms/d/e/1FAIpQLScgiULXsOJjsnK2y9av10ztg-gGCL
hCX\_eybpwHxwYv-ZmJmA/viewform](https://docs.google.com/forms/d/e/1FAIpQLScgiULXsOJjsnK2y9av10ztg-gGCLhCX_eybpwHxwYv-ZmJ
mA/viewform)
```
---

     
 
all -  [ CAN I GET INTO HARVARD + UC's ](https://www.reddit.com/r/chanceme/comments/1gm5icr/can_i_get_into_harvard_ucs/) , 2024-11-19-0913
```
**Demographics**:

* low ranking HS, 250 ish grad class
* Asian
* Hook: I play chess
* CS BA or BS

**GPA**: 3.7 W(good 
reasons why so low just trust)

No rank in my HS

7 APs, 8 Tests

SAT - 1500 composite

**Awards**:

* AP scholar with h
onors
* honor roll
* Top 100 nationally ranked chess players In age groups for the past 4 years
* USCF candidate master

* Won an international/national tournament + state champ in chess

**ECs:**

* High School Chess league president - 20+ 
schools, 100+ participants, $1k+ raised
* 1st author to Novel Ai paper - published and submitted to conferences like Neu
rips + COLING, available on arXiv
* Chess Club president - 3 peat champion in regional league, top 5 teams in State
* DE
CA - 3x state qualifier
* Motorola Solutions Intern - made a REST API for one of their apps in prod
* Paid Chess coach -
 Apart of non-profit group for underprivileged youth in chicago(not from there did remote)
* Volunteer Chess Coach - vol
unteered apart of local chess academy, 200 ish hours over the 4 years
* Wrestling - Varsity
* SASA(south asian student a
ssociation) treasurer - raised 10k from sponsors and events, provided scholarships for the first year to south asian stu
dents
* Inspirit AI scholars program(free) - Made Chess bot with GPT 4o capable of playing at an expert level

**Persona
l Statement:**¬†8.5/10 (not insanely good, but everyone who reads it likes it, and reviewers can't find problems with it,
 so conservative 7.5, but 9.5/10 liberally)

**Colleges**:  
Reach: HARVARD, UMICH, CMU, UMD, UWM, NEU,  UCLA, Cal, UCSD


Target: Ohio State, Penn State, Purdue, IU, Vtech, UMass Amherst

Safety: UPitt, RIT, Bentley, Rutgers

  

```
---

     
 
all -  [ Question about EB2 NIW and re-election of President Trump ](https://www.reddit.com/r/USCIS/comments/1glc0s5/question_about_eb2_niw_and_reelection_of/) , 2024-11-19-0913
```
Hey, I have a question about possible effects of the recent re-election on my application as an Iranian citizen (male) w
ho recently filed their I140. I'm wondering if I should apply for premium processing or not.

# About my profile:

I'm a
 PhD student (started in January 2024) in the states studying deep learning (theory). I have three first-author publishe
d works on deep learning theory that align well with my research interests and what I'm working on right now.:One at Neu
rIPS 2022, 26 citations.One at ICML 2023, 16 citations.One at ICML 2024, 4 citations.I have another submission that is n
ot published yet on which I'm the second author.I've received a masters in CS from a top-tier university in Canada (UBC)
.

# My Concern:

I've filed my I140 on October 20th this year. As an Iranian citizen, I‚Äôm worried about the possibility
 of my application being affected by the re-election of President Trump. Because of that, I‚Äôm considering applying for p
remium processing to get a decision on my I140 before potential new laws/orders come into effect.¬†From talking to friend
s I‚Äôve heard that there are possibilities that:

* The approval bar goes higher
* The processing time slows down
* etc


2805$ is not nothing for me, as I‚Äôm a PhD student. I can pay it, but it‚Äôs not easy on me.

I‚Äôm wondering if I should app
ly for PP nevertheless, or if the chances of my application getting affected by the re-election are slim. Any advice wou
ld be appreciated! Thanks.
```
---

     
 
all -  [ Question about EB2 NIW and re-election of President Trump ](https://www.reddit.com/r/EB2_NIW/comments/1glbyrt/question_about_eb2_niw_and_reelection_of/) , 2024-11-19-0913
```
Hey, I have a question about possible effects of the recent re-election on my application as an Iranian citizen (male) w
ho recently filed their I140. I'm wondering if I should apply for premium processing or not.

# About my profile:

I'm a
 PhD student (started in January 2024) in the states studying deep learning (theory). I have three first-author publishe
d works on deep learning theory that align well with my research interests and what I'm working on right now.:One at Neu
rIPS 2022, 26 citations.One at ICML 2023, 16 citations.One at ICML 2024, 4 citations.I have another submission that is n
ot published yet on which I'm the second author.I've received a masters in CS from a top-tier university in Canada (UBC)
.

# My Concern:

I've filed my I140 on October 20th this year. As an Iranian citizen, I‚Äôm worried about the possibility
 of my application being affected by the re-election of President Trump. Because of that, I‚Äôm considering applying for p
remium processing to get a decision on my I140 before potential new laws/orders come into effect.¬†From talking to friend
s I‚Äôve heard that there are possibilities that:

* The approval bar goes higher
* The processing time slows down
* etc


2805$ is not nothing for me, as I‚Äôm a PhD student. I can pay it, but it‚Äôs not easy on me. 

I‚Äôm wondering if I should ap
ply for PP nevertheless, or if the chances of my application getting affected by the re-election are slim. Any advice wo
uld be appreciated! Thanks.
```
---

     
 
all -  [ Ethics in AI ](https://www.reddit.com/r/ArtificialInteligence/comments/1gkndse/ethics_in_ai/) , 2024-11-19-0913
```
What are some good learning/certificate opportunities to enter the AI ethics space? Are there other volunteer opportunit
ies to do ethics reviews on papers besides NeurIPS (and what are the requirements to become an ethics reviewer?) 
```
---

     
 
all -  [ Is implementing famous research papers in ML worthy of writing in a resume? ](https://www.reddit.com/r/Btechtards/comments/1gjym1x/is_implementing_famous_research_papers_in_ml/) , 2024-11-19-0913
```
Title. I am currently in my 3rd year of BTech in CSE. I‚Äôve been interested in ML/DL for quite some time now. And I was t
hinking of doing this. Is it okay to put them in resume for applying to research/industry internships?

Papers that I am
 thinking of implementing in no particular order:

- [Attention Is All You Need](https://proceedings.neurips.cc/paper_fi
les/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- [Hand Written Digit Recognition Using Back Propagation
 Neural Network](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf)
- [An IMage 
is worth 16x16 words](https://openreview.net/pdf?id=YicbFdNTTy) Vision Transformer
- [LoRA- Low-Rank Adaptation Method f
or LLMs](https://openreview.net/pdf?id=nZeVKeeFYf9)
- [RAG paper from 2020](https://arxiv.org/pdf/2005.11401)
- [ESRGAN]
(https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversar
ial_Networks_ECCVW_2018_paper.pdf)
```
---

     
 
all -  [ Do I have to upload a poster and a video recording for my accepted paper in NeurIPS 2024 to be publi ](https://www.reddit.com/r/learnmachinelearning/comments/1gjx8bv/do_i_have_to_upload_a_poster_and_a_video/) , 2024-11-19-0913
```
I registered the virtual pass of NeurIPS 2024. Do I have to upload a poster and a video recording for my accepted paper 
in NeurIPS 2024 to be published? The emails or the instructions do not make any clarification about this. 
```
---

     
 
all -  [ [D] Looking for Research Internship in Applied RL & Robotics ](https://www.reddit.com/r/MachineLearning/comments/1giq0e8/d_looking_for_research_internship_in_applied_rl/) , 2024-11-19-0913
```
I am a PhD candidate at Mila, working on reinforcement learning for different robotic applications (worked on applicatio
ns like excavator automation, physics-based character animation, and autonomous driving). I'm currently seeking a summer
 research internship for 2025, and I'm really interested in any roles that focus on applied RL or embodied AI.

Here‚Äôs a
 bit about my research journey so far:

* **Automatic Reward Modeling**: Developed methods for deriving reward functions
 from expert demonstration for excavator automation in Vortex Simulator. (Presented at the NeurIPS RL for Real-life Appl
ications workshop.)
* **Sample-Efficient RL**: Improved sample efficiency on the Atari benchmark through transformer-bas
ed discrete world modeling. (ICML 2024)
* **Compositional Motion Priors for Multi-Task RL**: I'm currently working on mu
lti-task learning for robotic locomotion with compositional motion priors, using Isaac Gym.
* **RL for Autonomous Drivin
g**: Designed a curriculum learning method for autonomous driving on the CARLA simulator, eliminating the need for compl
ex reward shaping. (Inria research student).

I‚Äôm also exploring the use of Diffusion Models alongside RL for stable, di
verse control strategies.

If anyone knows of relevant openings or has any advice on places that may value applied RL re
search, I‚Äôd really appreciate it.

Thank you so much for any leads or suggestions!

*My CV and more details are on my*¬†h
ttps://pranaval.github.io/*.*
```
---

     
 
all -  [ Looking for Research Internship in Applied RL & Robotics ](https://www.reddit.com/r/reinforcementlearning/comments/1gipwq6/looking_for_research_internship_in_applied_rl/) , 2024-11-19-0913
```
I am a PhD candidate at Mila, working on reinforcement learning for different robotic applications (worked on applicatio
ns like excavator automation, physics-based character animation, and autonomous driving). I'm currently seeking a summer
 research internship for 2025, and I'm really interested in any roles that focus on applied RL or embodied AI.

Here‚Äôs a
 bit about my research journey so far:

* **Automatic Reward Modeling**: Developed methods for deriving reward functions
 from expert demonstration for excavator automation in Vortex Simulator. (Presented at the NeurIPS RL for Real-life Appl
ications workshop.)
* **Sample-Efficient RL**: Improved sample efficiency on the Atari benchmark through transformer-bas
ed discrete world modeling. (ICML 2024)
* **Compositional Motion Priors for Multi-Task RL**: I'm currently working on mu
lti-task learning for robotic locomotion with compositional motion priors, using Isaac Gym.
* **RL for Autonomous Drivin
g**: Designed a curriculum learning method for autonomous driving on the CARLA simulator, eliminating the need for compl
ex reward shaping. (Inria research student).

I‚Äôm also exploring the use of Diffusion Models alongside RL for stable, di
verse control strategies.

If anyone knows of relevant openings or has any advice on places that may value applied RL re
search, I‚Äôd really appreciate it.

Thank you so much for any leads or suggestions!

*My CV and more details are on my* *
https://pranaval.github.io/.*
```
---

     
 
all -  [ [D] Publishing in NeurIPS, ICML, ICLR as an Early Researcher: Any Advice? ](https://www.reddit.com/r/MachineLearning/comments/1gip4cf/d_publishing_in_neurips_icml_iclr_as_an_early/) , 2024-11-19-0913
```
I'm currently pursuing a master's degree, and my goal is to publish a paper in one of the top AI/ML venues, like NeurIPS
, ICML, or ICLR, before I finish my program. I'm studying at a Federal University in Brazil, which is well-regarded loca
lly but doesn‚Äôt have much international recognition. My research lab is somewhat unstructured‚Äîwe mainly share computatio
nal resources but don‚Äôt have collaborative or large-scale projects. Because of this, I don‚Äôt have an ongoing project I c
an join for guidance or support.

Additionally, my supervisor‚Äôs research focus is more on applied machine learning in ch
emistry, so he doesn‚Äôt have experience publishing in these top conferences. This means I don‚Äôt have direct mentorship on
 the publishing process specific to these venues. To give some context, NeurIPS's call for papers is expected around May
 2025, so I still have some time but want to prepare as thoroughly as possible.

I‚Äôd really appreciate any advice on how
 to increase my chances of getting published in these venues. For example, I‚Äôve heard that it helps to cite potential re
viewers in your work. Any tips on how to navigate the process, write in a way that aligns with these conferences, or und
erstand what reviewers might be looking for would be helpful. I‚Äôd also like advice on handling rejection, like potential
 backup venues to consider if my paper isn‚Äôt accepted.
```
---

     
 
all -  [ Visa rejected for No Proper reason ](https://www.reddit.com/r/CanadaVisitorVisa/comments/1ghdzz6/visa_rejected_for_no_proper_reason/) , 2024-11-19-0913
```
I was supposed to go for NeurIPS conference from India and I had Invitation letter as well as cover letter from my compa
ny. Yet they have rejected with absolutely no logical reasons. How to appeal this? 
```
---

     
 
all -  [ [R] QTIP: Quantization with Trellises and Incoherence Processing ](https://www.reddit.com/r/MachineLearning/comments/1ggyj3l/r_qtip_quantization_with_trellises_and/) , 2024-11-19-0913
```
We're pleased to introduce QTIP, a new LLM quantization algorithm that uses trellis coded quantization and incoherence p
rocessing to achieve a state of the art combination of speed and quantization quality.

Paper (NeurIPS 2024 Spotlight):¬†
[https://arxiv.org/pdf/2406.11235](https://arxiv.org/pdf/2406.11235)

Codebase + inference kernels:¬†[https://github.com/
Cornell-RelaxML/qtip](https://github.com/Cornell-RelaxML/qtip)

Prequantized models (including 2 Bit 405B Instruct):¬†[ht
tps://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803](https://huggingface.co/collecti
ons/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803)

QTIP has significantly better quality over QuIP# while bein
g just as fast. QTIP is also on par with or better than PV-Tuning while being much faster (\~2-3x).


```
---

     
 
all -  [ New Quantization Method -- QTIP: Quantization with Trellises and Incoherence Processing ](https://www.reddit.com/r/LocalLLaMA/comments/1ggwrx6/new_quantization_method_qtip_quantization_with/) , 2024-11-19-0913
```
We're pleased to introduce QTIP, a new LLM quantization algorithm that uses trellis coded quantization and incoherence p
rocessing to achieve a state of the art combination of speed and quantization quality.

Paper (NeurIPS 2024 Spotlight):¬†
[https://arxiv.org/pdf/2406.11235](https://arxiv.org/pdf/2406.11235)

Codebase + inference kernels:¬†[https://github.com/
Cornell-RelaxML/qtip](https://github.com/Cornell-RelaxML/qtip)

Prequantized models (including 2 Bit 405B Instruct):¬†[ht
tps://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803](https://huggingface.co/collecti
ons/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803)

QTIP has significantly better quality over QuIP# while bein
g just as fast. QTIP is also on par with or better than PV-Tuning while being much faster (\~2-3x).

[2 Bit 405B Instruc
t running pipelined on 2 GPUs. The inference backend uses torch.compile and HF so this should be much faster on somethin
g like llama.cpp.](https://reddit.com/link/1ggwrx6/video/rz8ghv5fc8yd1/player)
```
---

     
 
all -  [ Chance a Chess Fiend With Below Average Grades but Good EC's and SAT ](https://www.reddit.com/r/chanceme/comments/1ggukfh/chance_a_chess_fiend_with_below_average_grades/) , 2024-11-19-0913
```
**Demographics**:

* mid to low comp HS, 250 ish grad class
* Asian
* I play chess?
* CS BA or BS

**GPA**: 3.7 W(good r
easons why so low just trust)

No rank

7 APs, 8 Tests

SAT - 1510 composite

**Awards**:

* AP scholar with honors
* ho
nor roll
* Top 100 nationally ranked chess players In age groups for the past 4 years
* USCF candidate master
* Won an i
nternational/national tournament + state champs in chess

\*\*ECs(\*\*n·ªët **ordered properly):**

* High School Chess le
ague president - 20+ schools, 100+ participants, $1k+ raised
* 1st author to Novel Ai paper - published and submitted to
 conferences like Neurips + COLING, available on arXiv
* Chess Club president - 3 peat champion in regional league, top 
5 teams in State
* DECA - 3x state qualifier
* Motorola Solutions Intern - made a REST API for one of their apps in prod

* Paid Chess coach - Apart of non-profit group for underprivileged youth in chicago(not from there did remote)
* Volunt
eer Chess Coach - volunteered apart of local chess academy, 200 ish hours over the 4 years
* Wrestling - Varsity
* SASA(
south asian student association) treasurer - raised 10k from sponsors and events, provided scholarships for the first ye
ar to south asian students
* Inspirit AI scholars program(free) - Made Chess bot with GPT 4o capable of playing at an ex
pert level

**Personal Statement:** 7.5/10 (not insanely good, but everyone who reads it likes it, and reviewers can't f
ind problems with it, so conservative 7.5)

**Colleges**:  
Umass, UMD, UW Madison, BU, NEU, Ohio State, Penn State, Pur
due, IU, Vtech, UPitt, RIT, Bentley, Rutgers
```
---

     
 
all -  [ [D] Is TMLR good enough to consider as an alternative to A* conferences? ](https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/) , 2024-11-19-0913
```
Hi there, I am a current PhD student in Artificial Intelligence working on Multi-Armed Bandits. More recently, I have co
mpleted one of my works on the intersection of Bandits and LLMs and was wondering for a suitable venue for publication.


The closest conference I see is ICML having deadline of 31st January which is about two months from now, therefore was 
wondering about a suitable alternate venue. While previous reddit threads (a year back) claim that TMLR is better than A
AAI, IJCAI and similar conferences but falls way short compared to ICML, NeurIPS, ICLR, etc, I was wondering if it's sti
ll true. 

Does the ML community still considers TMLR to be a potential place to submit it, given that the deadline for 
the closest conference is too far?
```
---

     
 
all -  [ Neural network recognizer for hand-written zip code digits (1988): 'with a high-performance preproce ](https://www.reddit.com/r/mlscaling/comments/1ggr0j4/neural_network_recognizer_for_handwritten_zip/) , 2024-11-19-0913
```
This paper was published just before LeNet-1. Notable features:

* 18 hand-designed kernels (??).
* An early bitter less
on? 'In the early phases of the project, we found that neural network methods gave rather mediocre results. Later, with 
a high-performance preprocessor, plus a large training database, we found that a layered network gave the best results, 
surpassing even Parzen Windows.'
   * 'Several different classifiers were tried, including Parzen Windows, K nearest nei
ghbors, highly customized layered networks, expert systems, matrix associators, fea ture spins, and adaptive resonance. 
We performed preliminary studies to identify the most promising methods. We determined that the top three methods in thi
s list were significantly better suited to our task than the others, and we performed systematic comparisons only among 
those three \[Parzen Windows, KNN, neural networks\].'
* Nevermind, seems they didn't take the bitter lesson. 'Our metho
ds include low-precision and analog processing, massively parallel computation, extraction of biologically-motivated fea
tures, and learning from examples. We feel that this is, therefore, a fine example of a Neural Information Processing Sy
stem. We emphasize that old-fashioned engineering, classical pattern recognition, and the latest learning-from-examples 
methods were all absolutely necessary. Without the careful engineering, a direct adaptive network attack would not succe
ed, but by the same token, without learning from a very large database, it would have been excruciating to engineer a su
fficiently accurate representation of the probability space.'

Denker, John, et al. '[Neural network recognizer for hand
-written zip code digits](https://proceedings.neurips.cc/paper/1988/hash/a97da629b098b75c294dffdc3e463904-Abstract.html)
.'¬†*Advances in neural information processing systems*¬†1 (1988).
```
---

     
 
all -  [ Florence-2-as-a-Judge ](https://www.reddit.com/r/LocalLLaMA/comments/1gfv0me/florence2asajudge/) , 2024-11-19-0913
```
I learned about Judge Distillation from slide 14 in [this deck](https://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.
pdf) describing how Phi-2 researchers scaled their data quality filter to a large synthetic dataset.

I'm planning to sc
ale up data synthesis for the [OpenSpaces dataset](https://huggingface.co/datasets/remyxai/OpenSpaces) and have found I 
can use [SpaceLLaVA](https://huggingface.co/remyxai/SpaceLLaVA) in VLM-as-a-Judge with [prometheus-vision](https://githu
b.com/prometheus-eval/prometheus-vision). Check out the [SpaceJudge Dataset](https://huggingface.co/datasets/salma-remyx
/SpaceJudgeDataset) to see the an assessment of a small split.

Now, I'm fine-tuning Florence-2 on this dataset, introdu
cing the new <JUDGE> task to help filter out low-quality synthetic samples. Here's the [experiment collection](https://h
uggingface.co/collections/salma-remyx/vlm-judge-distillation-671fc8fe1925c49630307a82).

Will discuss some of this at OD
SC West tomorrow, let's connect!


```
---

     
 
all -  [ [D]ended up with a poster in NuerIPS-24 ](https://www.reddit.com/r/MachineLearning/comments/1gdxef5/dended_up_with_a_poster_in_nuerips24/) , 2024-11-19-0913
```
I have a poster in NuerIPS this year through the  journal track(MLRC) along with the main conference papers.I didnt expe
ct this to happen so i hadnt planned/researched about the expenses/funding prior.I already had my visa and conference re
gistration arranged but have no clue about further proceedings of Nuerips and how to fund it(i am an UG junior).If you h
ave already attended NeurIPS before please pour your ideas and experiences.
```
---

     
 
all -  [ Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CVPR,  ](https://www.reddit.com/r/computervision/comments/1gd6812/looking_for_collaborations_on_ongoing/) , 2024-11-19-0913
```
Hey everyone,

Our group,¬†**Vision and Language Group, IIT Roorkee,**¬†recently got three workshop papers accepted at Neu
rIPS workshops! üöÄ We‚Äôve also set up a website üëâ¬†[VLG](https://vlgiitr.github.io/), featuring other publications we‚Äôve wo
rked on, so our group is steadily building a portfolio in ML and AI research. Right now, we‚Äôre collaborating on several 
work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have even
 more ideas we‚Äôre excited about. Still, a few of our main limitations have been access to proper guidance and funding fo
r GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interested i
n working together, we‚Äôd love to explore intersections in our fields of interest and any new ideas you might bring to th
e table!

If you have resources available or are interested in discussing potential collaborations, please feel free to 
reach out! Looking forward to connecting and building something impactful together! Here is the link for our Open Slack 
üëâ¬†[Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)
```
---

     
 
all -  [ Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CVPR,  ](https://www.reddit.com/r/neuralnetworks/comments/1gd64zq/looking_for_collaborations_on_ongoing/) , 2024-11-19-0913
```
# Hey everyone,

Our group,¬†**Vision and Language Group, IIT Roorkee,**¬†recently got three workshop papers accepted at N
eurIPS workshops! üöÄ We‚Äôve also set up a website üëâ¬†[VLG](https://vlgiitr.github.io/), featuring other publications we‚Äôve 
worked on, so our group is steadily building a portfolio in ML and AI research. Right now, we‚Äôre collaborating on severa
l work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have ev
en more ideas we‚Äôre excited about. Still, a few of our main limitations have been access to proper guidance and funding 
for GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interested
 in working together, we‚Äôd love to explore intersections in our fields of interest and any new ideas you might bring to 
the table!

If you have resources available or are interested in discussing potential collaborations, please feel free t
o reach out! Looking forward to connecting and building something impactful together! Here is the link for our Open Slac
k üëâ¬†[Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)
```
---

     
 
all -  [ Some General Rules for Uni Selection Fall'25 ](https://www.reddit.com/r/MSCS/comments/1gcs53j/some_general_rules_for_uni_selection_fall25/) , 2024-11-19-0913
```
1. If you dont have 9+ GPA from tier 3 / 8.7 GPA from NIT / 8.5 GPA from IIT, do not apply to any of UC's MSCS (except U
C Riverside and UC Santa Cruz) .
2. Do not apply to Ivy Leagues MSCS if you don't have 9.5+ GPA from tier 3/ 9+ GPA from
 Tier 2.
3. If you are not from IIT NIT, do not apply to TAMU unless you have 9.5+ GPA.
4. The earlier you apply to NEU 
with a 8+ GPA/100+ TOEFL, the higher your chances of getting admit  for MSCS (Mostly full by Nov end). After that you wi
ll be offered alternate campuses.
5. Dont apply to Stony Brook MSCS if you dont have 8.5+ GPA, 315+ GRE, 167+ Quants. Yo
u can try MSDS though.
6. Dont apply to UMASS if you dont have 315+ GRE.
7. If you have 2+ years of work ex, try MCS in 
UC's, especially UCI, and TAMU.
8. Dont apply to USC if you cant afford extreme tuition fees and 1000$ deposit.
9. Use [
admits.fyi](http://admits.fyi), [https://gpa.eng.uci.edu/](https://gpa.eng.uci.edu/) to get estimates.
10. Conferences d
ont matter, no matter how many, unless they are NeurIPS, ICCV, CVPR etc. Especially conferences and journals like IJET, 
IJSER, IJRASET etc. can have negative impact on your profile. IEEE Access is another common Journal, its good, but wont 
create much of impact on your profile.
11. MSDS is much more gettable than MSCS.
12. You dont need to send you GRE and T
OEFL scores before application in most cases. You can upload unofficial copy, and send official scores after you get an 
admit. Dont waste money.
13. Work ex wont create much of a difference for MSCS, but def creates a great diff when it com
es to getting a job.
14. Prefer East coast for finance, West Coast for tech jobs ( Location matters more than rankings).
 For pure research, prefer reputation over anything else.
15. SJSU MSCS is best uni for getting a job in tech for those 
who have low gpa, low scores and financial issues. Be aware you need CS undergrad degree background to get MSCS at SJSU.

16. Purdue took very few people last year as they had funding issues. Purdue was harder than UCSD to get in.
17. GPA ma
tters more than anything else. 9 GPA tier 3 = 8.7 GPA tier 2 = 8.5 GPA tier 1.
18. [https://github.com/SimplifyJobs/Summ
er2025-Internships](https://github.com/SimplifyJobs/Summer2025-Internships),  [https://github.com/cvrve/Summer2025-Inter
nships](https://github.com/cvrve/Summer2025-Internships) . Get an Idea of what things are currently in demand in the US,
 dont just go for any branch or any specialization because you have interest in it.
19. Rankings along with minimum idea
l gpa according to tier 3 MIT, Stanford (9.9 GPA) > CMU, UC Berkeley, UIUC (9.8) > GATECH, UT Austin, UWash (9.6)> UCSD,
 Columbia, CalTech, UCLA, UPenn (9.5)> UWM, UMCP, Purdue (9.4)> UMass, TAMU, UCI, UCSB (9.3)> UCD, USC, NYU Courant (9.1
)> Stony Brook, Penn State, Virginia Tech (8.7\~9) > Rutgers, NEU, NYU Tandon (8.5) >  NCSU, BU (8.5)
20. Dont go for Co
nsultations, waste of money.

Anymore questions, feel free to ask.
```
---

     
 
all -  [ 'Foundations for Machine Learning' ](https://www.reddit.com/r/learnmachinelearning/comments/1gcc2fc/foundations_for_machine_learning/) , 2024-11-19-0913
```


https://preview.redd.it/pea5gbvrr0xd1.png?width=1280&format=png&auto=webp&s=452854ecdc3e378b5b4a5b57dbbf145cf093b551


In 2022, I graduated with a PhD in Mechanical Engineering from MIT. 



Although a big component of my research was pure
ly hands-on experiments, my exposure to foundational graduate-level ML courses at MIT, research courses, and Scientific 
Machine Learning via Julia gave me the confidence of a Machine Learning researcher. 



I incorporated ML into my resear
ch, and it solved a problem that is otherwise difficult to solve theoretically or experimentally. Now I have co-authored
 multiple AI-ML research papers and two of them are accepted to the upcoming NeurIPS workshop. 



Behind all of this ef
fort, there is the confidence that stems from knowing what happens underneath the ML algorithms.



Most of the online c
ourses have little emphasis on fundamentals. People are so used to spending time on toy Kaggle projects. Very few people
 I know can build a neural network from scratch or explain what happens behind them.



For the last 4 months, I have be
en working to launch a new course titled on 'Foundations for Machine Learning.' This will be a 45-hour course with \~65 
lectures. I will be hosting all lectures on this playlist: [https://www.youtube.com/playlist?list=PLPTV0NXA\_ZSiLI0ZfZYb
HM2FPHKIuMW6K](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiLI0ZfZYbHM2FPHKIuMW6K)



My singular goal with this c
ourse is to teach you the entire foundations required to learn ML from scratch.



There are no prerequisites. If you ha
ve basic logical thinking capability and a willingness to dedicate time, consistently, you can follow this course.



I 
have split the course into 5 modules.



1) First I will cover the 4 mathematical pillars of ML: Linear Algebra, Probabi
lity, Statistics, and Calculus.



2) In the second module I cover the basic programming fundamentals for a complete beg
inner. I will teach you Python from scratch and it's some of the most important packages for ML including NumPy and PyTo
rch.



3) In the 3rd module we will learn about optimization and gradient descent. I wanted to dedicate an entire modul
e to optimization because when you actually build ML models, you will be spending a lot of time on optimization.



4) I
n the 4th module, I will give you an overview of the AI landscape. What happened from 2010-2020 and what does it look li
ke from 2020-2030? This overview will help you understand overall where ML, DL, NLP, CV, and GenAI are heading.



5) In
 the final module, I will cover the most important 2 steps you will have to master as a Data Scientist or ML engineer: p
rocessing data and communication via storytelling. I will teach you some of the most powerful preprocessing and visualiz
ation techniques.



I have already published the first lecture. Check out here. I am sure you will enjoy and learn a lo
t: [https://youtu.be/C8hEa2qb46k?si=7dRHM6EZwlUBDC5C](https://youtu.be/C8hEa2qb46k?si=7dRHM6EZwlUBDC5C)
```
---

     
 
all -  [ [R] Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CV ](https://www.reddit.com/r/u_vlg_iitr/comments/1gc2yzd/r_looking_for_collaborations_on_ongoing/) , 2024-11-19-0913
```
Hey everyone,

Our group, **Vision and Language Group, IIT Roorkee,** recently got three workshop papers accepted at Neu
rIPS workshops! üöÄ We‚Äôve also set up a website üëâ [VLG](https://vlgiitr.github.io/), featuring other publications we‚Äôve wo
rked on, so our group is steadily building a portfolio in ML and AI research. Right now, we‚Äôre collaborating on several 
work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have even
 more ideas we‚Äôre excited about. Still, few of our main limitations have been access to proper guidance along with fundi
ng for GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interes
ted in working together, we‚Äôd love to explore intersections in our fields of interest and any new ideas you might bring 
to the table! 

If you have resources available or are interested in discussing potential collaborations, please feel fr
ee to reach out! Looking forward to connecting and building something impactful together! Here, is the link for our Open
 Slack üëâ [Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)
```
---

     
 
all -  [ Paper summaries for some of our papers that recently got accepted in NeurIPS ](https://www.reddit.com/r/learnmachinelearning/comments/1gb78i9/paper_summaries_for_some_of_our_papers_that/) , 2024-11-19-0913
```
Hey everyone, here is the list of papers by our groups that got accepted recently in NeurIPS 2024; It is a proud moment 
for us as an all-UG group; all the papers were published without any external support from the academia; here is a summa
ry of our papers. We hope this inspires others to pursue AI and look into research as a perspective where we can work to
gether, and all you require is the right guidance (not even necessarily a PhD or a professor). If you find these papers 
useful and want to working/collabrating with us, feel free to connect with us!

* Give me a hint: Can LLMs take a hint t
o solve math problems? üëâ¬†[Arxiv link](https://arxiv.org/abs/2410.05915)
   * We propose improving LLM performance on adv
anced math problems using 'hints,' inspired by human pedagogy. We also test the model's robustness to incorrect hints. O
ur approach is evaluated on various LLMs using diverse problems from the MATH dataset, comparing it with one-shot, few-s
hot, and chain of thought prompting.
* Attention Shift: Steering AI Away from Unsafe Content üëâ¬†[Arxiv link](https://arxi
v.org/abs/2410.04447)
   * This study explores methods to restrict unsafe content in generative models. We propose a nov
el training-free approach using attention reweighing to remove unsafe concepts during inference. Our method is compared 
to existing techniques, evaluated on direct and adversarial jailbreak prompts. We also discuss potential causes, limitat
ions, and broader implications.
* Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright P
rotection in Images üëâ¬†[Arxiv link](https://arxiv.org/abs/2406.12592v1)
   * This paper extends the study of concept abla
tion in pre-trained models, as introduced by Kumari et al. (2022). We reproduce results from various concept ablation te
chniques and propose a novel variant, 'trademark ablation,' to address branded elements in model outputs. We also analyz
e the model's limitations, behavior under ablation leakage prompts, and performance degradation on unrelated concepts.


**The Vision Language Group at IIT Roorkee**¬†has compiled an excellent repository of¬†**comprehensive summaries**¬†for dee
p learning papers from top conferences like¬†**NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down ke
y papers in computer vision, NLP, and machine learning‚Äîperfect if you want to stay updated without diving deep into the 
full papers.
```
---

     
 
MachineLearning -  [ New to Research - Need Info on Publications [R][D] ](https://www.reddit.com/r/MachineLearning/comments/1gaa3o8/new_to_research_need_info_on_publications_rd/) , 2024-11-19-0913
```
I have been writing and publishing a few papers/journals in the field of AI for the last two years now, but I am really 
not sure what the best journals and conferences are. In my case, I usually write a paper, and my professor, based on the
 content of the paper, submits it to that conference/journal.

So would like to understand some info from this sub.

\->
 What are some really good journals/conferences where you can publish a paper? (How are the journals /conferences ranked
, is there a way to check? I heard ICML, NeurIPS are the top conferences in this field)

\-> What are the best publisher
s?

\-> What are sci Q1, Q2 journals and A\*  journals?

I have a paper that I am writing now which is in the field of m
edicine (In the speech domain), can anyone suggest to me, what the best Journals/Conferences in this field are?

Sorry, 
if these are some basic questions, (I only know about the publishers: IEEEXplore, Springer, Elseveir and used to think i
f it's Scopus-indexed, it is a good conference/journal).
```
---

     
 
MachineLearning -  [ [D] Responses to false accusations of plagiarism for Gaunt Tensor Product paper ](https://www.reddit.com/r/MachineLearning/comments/1ga12d8/d_responses_to_false_accusations_of_plagiarism/) , 2024-11-19-0913
```
I‚Äôm posting this on behalf of the authors of the paper. The first author tried to make a post about this, but the post g
ot removed for some reason. The author reached out to me because I was one of the people defending them, so see below fo
r the author writeup about the accusations.

**TL;DR**: We're the authors of the Gaunt Tensor Product paper, and we want
 to directly address the false plagiarism accusations against our work. Our main contribution, a new perspective on tens
or products of irreducible representations (irreps) in machine learning and equivariant neural networks, is novel and or
iginal. The claimed 'similarity' are actually algorithms from elementary math and CS courses, and are not the main contr
ibution of our work: our independent implementation is clear if you look at our code, which is quite different because w
e had a completely different application area in mind. On the other hand, our core contributions, including establishing
 the connection between tensor products of irreps and integrals of products of spherical harmonics and various design pa
radigms of equivariant operations, are completely omitted. There is an oversight of citation due to the gap between fiel
ds (machine learning vs. graphics), but this is not plagiarism, and now that we know about this, we are updating the pap
er with the citation and discussion accordingly. This is similar situation to areas such as neural ODEs, where the origi
nal ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until mu
ch later. The anonymous accuser is selectively replying, omitting key details, and controlling the narrative.

**More de
tails below**:

We are the authors of \['Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor
 Products'\](https://openreview.net/forum?id=mhyQXJ6JsK) . We are creating a new post to clearly outline our responses t
o the false accusations of plagiarism that we've received for the Gaunt Tensor Product paper in another thread. While we
 have replied on that thread, the anonymous OP of that thread is selectively replying and omitting a lot of information 
from our responses, and we don't think it is fair that they single-handedly control the narrative. Note that we never go
t any emails or posts on OpenReview from the author, who has instead decided to anonymously post on here.

Firstly, we w
ould like to comprehensively respond to the false accusations again:

\- **The contributions of our work**: as emphasize
d in our paper, our main contribution in this work is the new perspective on tensor products of irreps, which is novel a
nd original to the machine learning community (Equations 3 and 4). The whole Section 3.1 elaborates on how to establish 
the connection between tensor products of irreps and integrals of products of spherical harmonics. Although the OP claim
s 'However, it is important to note that this derivation accounts for less than one page of the nine-page paper.', the f
act is that our establishment and derivation are based on a series of rigorous deductions with many efforts on building 
a solid mathematical foundation including group theory and quantum mechanics (please refer to Appendix A.1-A.7, page 16-
28), which is not straightforward and trivial to obtain. Without these efforts, we cannot establish such connections, le
t alone the efficient algorithm. In the context of equivariant machine learning, this derivation presents significance t
o refresh the understanding of basic equivariant operations, which cannot be omitted.

\- **The similarities of the effi
cient algorithms between our work and FSHP work**: Firstly, we would like to apologize that we did not cite the FSHP wor
k in our submission, which is unintentional and due to the gap between these two communities (we are from the ML communi
ty, and they are from graphics, and the paper was not known to us until recently). We will update the arXiv version of o
ur paper asap by adding a discussion paragraph to carefully discuss the FSHP work and our work. **On the other hand, we 
also would like to clarify that there does not exist any plagiarism behavior of the FFT algorithm**: after we figure out
 the relation between the tensor product of irreps and integrals of the product of three spherical harmonics, it is rath
er natural to connect it with products of spherical functions. Moreover, there exist classical results for efficient com
putation of products of spherical functions, i.e., Convolution Theorem and FFT, which involve elementary knowledge that 
can be learned in several undergraduate classes: (1) change of basis, which can be learned in linear algebra and signal 
processing and is used in both paper to connect spherical harmonics and Fourier basis; (2) FFT, which is commonly taught
 in signal processing and numerical computation classes and is used for acceleration. Due to the basicness of these math
ematical tools, both works follow the standard way to formalize and present, which leads to similarity. As we said, this
 cannot be misrepresented as plagiarism because we independently worked on this, and did not know about the other work u
ntil later because of the different communities. This is similar to work in areas such as neural ODEs, where the origina
l ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until much
 later.

\- **The differences in implementation**: it is noteworthy that, as a work for the equivariant machine learning
 community, it is not enough to simply propose an approach just for the tensor product operation. What we really care ab
out is the various design paradigms of equivariant operations, which are built upon tensor products. In Section 3.3, we 
categorize these paradigms into three classes in terms of their different characteristics and applied range. For each cl
ass of equivariant operations, we carefully specialize our approach by combining their properties and considering the re
strictions. For example, for the equivariant convolution, we figure out that we can further leverage eSCN/EquiformerV2's
 findings to achieve further acceleration; for the equivariant many-body interactions, a divide-and-conquer approach is 
natural, which is also generally taught in various CS courses and projects. There also exist different instantiation str
ategies in modern equivariant networks when applying these classes of operations, please refer to the Discussion paragra
ph in Section 3.3 and Appendix C. Simply proposing the efficient approach for tensor products is not feasible to these m
entioned points. Without these additional efforts and contributions, the efficient algorithm is not practical to be used
 for the equivariant machine learning community,  which cannot be omitted.

\- There is quite a lot of literature in the
 last few decades in the graphics community on this, and this is another general point is that work on the graphics comm
unity on efficient algorithms is not heard of and/or undercited in the rotationally equivariant neural networks communit
y, when these algorithms pop up in a lot of equivariant NN work. Additionally, this graphics paper is not in the field o
f ML, and this algorithm is being applied to a completely different area, which is why we did not see it originally and 
had an independent formalization. Perhaps an analogy here is that there are papers applying Transformers to different ar
eas like vision instead of language, but this shouldn't be 'plagiarism' at all. Likewise, neural ODEs shouldn't be consi
dered plagiarism of traditional ODE solvers simply because they are using the same method (and indeed, some of the origi
nal ideas of neural ODEs were in engineering papers from the '90s, and not discovered/cited in ML papers until later bec
ause of the different communities). One user on this thread also put it well that the concepts here like FFT are quite w
ell-known: 'After skimming, my impression is that those are well known results from textbooks and signal processing cour
ses that nobody bother to cite anymore. I could be wrong.'

\- The implementation in the GTP paper is fairly different f
rom the FSHP paper and was implemented independently because we derived our implementation based on being motivated by o
ur specific application area of ML for molecular modeling: their code is in C++, doesn't support efficient computations 
for lower rotation orders (L), and is not made for use with irreducible representations. This should be clear when you s
ee the code.

\- The main purpose of the Equiformerv2 experiment with the self-mix layer was a proof-of-concept to show 
that such a self-mix layer can be implemented because of the Gaunt Tensor Product formulation. Without this formulation 
(and using the more standard Clebsch-Gordan Tensor Product), it would have been very slow to add this layer (and not gre
at from a memory usage perspective). This can be made more clear in the arXiv version.

Secondly, we would like to point
 out that the anonymous OP of that thread is selectively replying to posts, and omitting a lot of information (including
 in how they are updating their own thread, they do not include all of the details of our responses). To us, the posts a
lso seem LLM generated but you should draw your own conclusions. We also posted this new topic because the authors respo
nses on the original thread are all folded, which cannot be directly seen by new readers.

Finally, we appreciate that m
any people have been commenting on the thread to defend us. These types of anonymous, sensational claims can have seriou
s implications and to post anonymously on Reddit before emailing us or posting on OpenReview is really problematic. We h
ope that you all read these threads carefully before jumping to conclusions.
```
---

     
 
MachineLearning -  [ [R] Molecular Topological Profile (MOLTOP) - Simple and Strong Baseline for Molecular Graph Classifi ](https://www.reddit.com/r/MachineLearning/comments/1g7gj4m/r_molecular_topological_profile_moltop_simple_and/) , 2024-11-19-0913
```
Accepted at ECAI 2024 conference, ArXiv: [https://arxiv.org/abs/2407.12136](https://arxiv.org/abs/2407.12136)

Some high
lights:

- simple feature engineering on graphs

- it outperforms GROVER (NeurIPS 2020) and GraphMVP (ICLR 2024) on Mole
culeNet, and all models on peptide function prediction on LRGB (e.g. SAN+RWSE graph transformer)

- no hyperparameters t
o tune, takes seconds on most datasets

- surprisingly powerful in distinguishing non-isomorphic graphs

In short, if yo
u need a good and simple baseline for molecular graph classification, MOLTOP may be a good choice. We designed it not to
 be the best, but to be fast, easy-to-use, and also give strong results on average.

Abstract:

>We revisit the effectiv
eness of topological descriptors for molecular graph classification and design a simple, yet strong baseline. We demonst
rate that a simple approach to feature engineering - employing histogram aggregation of edge descriptors and one-hot enc
oding for atomic numbers and bond types - when combined with a Random Forest classifier, can establish a strong baseline
 for Graph Neural Networks (GNNs). The novel algorithm, Molecular Topological Profile (MOLTOP), integrates Edge Betweenn
ess Centrality, Adjusted Rand Index and SCAN Structural Similarity score. This approach proves to be remarkably competit
ive when compared to modern GNNs, while also being simple, fast, low-variance and hyperparameter-free. Our approach is r
igorously tested on MoleculeNet datasets using fair evaluation protocol provided by Open Graph Benchmark. We additionall
y show out-of-domain generation capabilities on peptide classification task from Long Range Graph Benchmark. The evaluat
ions across eleven benchmark datasets reveal MOLTOP's strong discriminative capabilities, surpassing the¬†1-WL test and e
ven¬†3We revisit the effectiveness of topological descriptors for molecular graph classification and design a simple, yet
 strong baseline. We demonstrate that a simple approach to feature engineering - employing histogram aggregation of edge
 descriptors and one-hot encoding for atomic numbers and bond types - when combined with a Random Forest classifier, can
 establish a strong baseline for Graph Neural Networks (GNNs). The novel algorithm, Molecular Topological Profile (MOLTO
P), integrates Edge Betweenness Centrality, Adjusted Rand Index and SCAN Structural Similarity score. This approach prov
es to be remarkably competitive when compared to modern GNNs, while also being simple, fast, low-variance and hyperparam
eter-free. Our approach is rigorously tested on MoleculeNet datasets using fair evaluation protocol provided by Open Gra
ph Benchmark. We additionally show out-of-domain generation capabilities on peptide classification task from Long Range 
Graph Benchmark. The evaluations across eleven benchmark datasets reveal MOLTOP's strong discriminative capabilities, su
rpassing the¬†1-WL test and even¬†3-WL test for some classes of graphs. Our conclusion is that descriptor-based baselines,
 such as the one we propose, are still crucial for accurately assessing advancements in the GNN domain.

Happy to discus
s / answer any questions in the comments.
```
---

     
 
MachineLearning -  [ [D] Why do PhD Students in the US seem like overpowered final bosses  ](https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/) , 2024-11-19-0913
```
Hello,

I'm a PhD student in a European university, working on AI/ML/CV ..etc. my PhD is 4 years. The first year I liter
ally just spent learning how to actually do research, teaching one course to learn how things work...etc. Second year, I
 published my first publication as a co-author in CVPR. By third year, I can manage research projects, I understand how 
to do grants applications, how funding works, the politics of it all ...etc. I added to my CV, 2 publications, one journ
al and another conference as first author. I'm very involved in industry and I also write a lot of production grade code
 in regard to AI, systems architecture, backend, cloud, deployment, etc for companies that have contracts with my lab.


The issue is when I see PhD students similar to me in the US, they be having 10 publications, 5 of them 1st author, all 
of them are either CVPR, ICML, ICLR, NeurIPS ...etc. I don't understand, do these people not sleep ? How are they able t
o achieve this crazy amount of work and still have 3 publications every year in A\* journals ?

I don't think these peop
le are smarter than I, usually I get ideas and I look up if something exists, and I can see that something was just publ
ished by some PhD student in Stanford or DeepMind ..etc like 1 month ago, So I can see that my reasoning isn't late in r
egard to SOTA. but the concepts that you would need to grasp to just have one of those publications + the effort and the
 time you need to invest and the resources to get everything done, wouldn't be possible for 2\~3 months project. How is 
it possible for these people to do this ?

Thank you !
```
---

     
