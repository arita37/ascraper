 
all -  [ Ilya Sutskever NeurIPS talk [video] ](https://www.youtube.com/watch?v=1yvBqasHLZs) , 2024-12-15-0916
```

```
---

     
 
all -  [ A little bit of drama: Pre-training is only over if you have no imagination - Logan Kilpatrick ](https://www.reddit.com/r/singularity/comments/1he9tsn/a_little_bit_of_drama_pretraining_is_only_over_if/) , 2024-12-15-0916
```
https://x.com/OfficialLoganK/status/1868002617311596552?t=uNazJ-3HPuWlBrXGagkAag&s=19

It's a slow saturday so why not s
hitpost a little.
This is in response to Ilya Sutskever's talk during NeurIPS 2024.
```
---

     
 
all -  [ Trending Topic: [D] What happened at NeurIPS? ](https://www.reddit.com/r/aesdr/comments/1he8sf1/trending_topic_d_what_happened_at_neurips/) , 2024-12-15-0916
```
Discussion: [D] What happened at NeurIPS?

Read more: https://i.redd.it/k0q9frsuir6e1.jpeg
```
---

     
 
all -  [ Shocked at the implicit racial bias at NeurIPS yesterday ](https://www.reddit.com/r/LocalLLaMA/comments/1he89rx/shocked_at_the_implicit_racial_bias_at_neurips/) , 2024-12-15-0916
```
[Rosalind Picard's Keynote on Friday at NeurIPS](https://preview.redd.it/iksntl5equ6e1.jpg?width=1179&format=pjpg&auto=w
ebp&s=153dfdf19591c696e176065623ec4612c83510f5)


```
---

     
 
all -  [ An MIT professor delivered a racist presentation targeting Chinese students as the keynote speaker a ](https://x.com/drjingjing2026/status/1867737326895632894) , 2024-12-15-0916
```
At NeurIPS 2024, an MIT professor delivered a keynote address that addressed unethical behavior in academia. During the 
speech, the professor referenced a Chinese student in a manner that was not only unnecessary but also highly racist. (Th
e following is her slide):

This is dishonest and unethical: Using Al to generate fabricated results, fake data, and pub
lications containing these, AND presenting them as real. 

“I did it to make my paper results look better. Nobody at my 
school taught us morals or values.' 

- Excuse given by Chinese student
who is now expelled from top university. 

NOTE:
 Most Chinese who I know are honest and morally upright.
```
---

     
 
all -  [ Trending Topic: [D] The winner of the NeurIPS 2024 Best Paper Award sabotaged the other teams ](https://www.reddit.com/r/aesdr/comments/1he5ht5/trending_topic_d_the_winner_of_the_neurips_2024/) , 2024-12-15-0916
```
Discussion: [D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams

Read more: https://www.reddi
t.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/
```
---

     
 
all -  [ A Perfect Storm for AI Inference TPU will be new king  ](https://www.reddit.com/r/Bard/comments/1he4e14/a_perfect_storm_for_ai_inference_tpu_will_be_new/) , 2024-12-15-0916
```
Ilya Sutskever's recent bombshell at NeurIPS – [https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-op
enai-model-data-training](https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-traini
ng) that we've reached 'peak data' and the era of pre-training as we know it is ending – has sent ripples through the AI
 world. His vision of a future dominated by 'agentic,' reasoning AI, capable of learning from limited data, sets the sta
ge for a fundamental shift in how we develop and deploy artificial intelligence. This shift, it turns out, might just be
 the perfect storm for the rise of the TPU and Broadcom's latest chip announcement could be the catalyst. [https://www.b
roadcom.com/company/news/product-releases/62691](https://www.broadcom.com/company/news/product-releases/62691)

**Why TP
Us are Poised to Shine in a Post-'Peak Data' World:**

1. **Inference, Inference, Inference:** Sutskever's emphasis on a
 future where AI is smarter, not just bigger, puts the spotlight squarely on **inference**. This is where AI applies its
 learned knowledge to make predictions and decisions in real-world scenarios. And this is precisely where TPUs have a di
stinct advantage.
2. **Enter Broadcom: The 3.5D XDSiP and the TPU Advantage:** Broadcom's new 3.5D chip isn't just anoth
er incremental improvement; it's a potential game-changer, especially for TPUs. Its innovative design, featuring vertica
l die stacking and face-to-face interconnects, directly addresses the key challenges of inference:
   * **Latency Killer
:** By drastically reducing the distance data needs to travel, Broadcom's chip minimizes latency, enabling the rapid-fir
e calculations that TPUs are built for. This is crucial for real-time inference applications.
   * **Power Saver:** The 
3.5D architecture slashes power consumption, a critical factor for deploying TPUs in data centers and edge devices where
 energy efficiency is paramount.
   * **Density Champion:** The compact form factor allows for denser packing of TPUs, p
aving the way for more powerful and efficient inference systems.

The potential rise of TPUs, spurred by the need for mo
re efficient inference and enabled by innovations like Broadcom's, could trigger a paradigm shift, compelling all major 
players in the AI field to develop their own specialized chips and hardware solutions to remain competitive in this rapi
dly evolving landscape. This may be the dawn of the age of custom AI silicon, and potentially the beginning of the TPU e
ra.
```
---

     
 
all -  [ Ilya Sutskever, cofondateur et ancien directeur scientifique d'OpenAI, a fait une rare apparition pu ](https://www.reddit.com/r/actutech/comments/1hdxdjs/ilya_sutskever_cofondateur_et_ancien_directeur/) , 2024-12-15-0916
```
Il a notamment affirmé que le pré-entraînement des modèles tel que nous le connaissons va inévitablement prendre fin, co
mparant les données à un 'combustible fossile' limité. Selon lui, nous avons atteint un pic des données disponibles, car
 il n'existe qu'un seul internet.  
  
Pour l'avenir, il prédit que les prochaines générations d'IA seront plus 'agentiq
ues' et capables de raisonner véritablement, contrairement aux systèmes actuels qui se contentent principalement de reco
nnaître des motifs. Ces systèmes deviendront plus imprévisibles à mesure qu'ils développeront leur capacité.  
  
Il a é
galement établi un parallèle intéressant entre l'évolution de l'IA et la biologie évolutive, suggérant que l'IA pourrait
 découvrir de nouvelles approches de mise à l'échelle, tout comme l'évolution a trouvé un nouveau modèle pour le cerveau
 des hominidés.

https://preview.redd.it/2ly7b8fejr6e1.jpg?width=960&format=pjpg&auto=webp&s=ae6cfe8241a9fc37bf648d189d8
2908a8624094c


```
---

     
 
all -  [ [D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams ](https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/) , 2024-12-15-0916
```
Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the
 other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his col
leagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.git
hub.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is
 asserted and if this is true this would be nice to confirm.
```
---

     
 
all -  [ Feels good to see Mr.X getting noted ](https://i.redd.it/oexht6c8rg6e1.jpeg) , 2024-12-15-0916
```
Link: https://x.com/elonmusk/status/1866797259968614885?s=46
```
---

     
 
all -  [ and now we know why Elon named his stupid AI 'Grok'  ](https://i.redd.it/j0pk4yr54f6e1.png) , 2024-12-15-0916
```
also why does the chart go all the way back to 1991
```
---

     
 
all -  [ 
New framework for quantifying uncertainty in LLMs: Semantic Density ](https://www.reddit.com/r/airesearch/comments/1hc6fez/new_framework_for_quantifying_uncertainty_in_llms/) , 2024-12-15-0916
```
Can we trust LLMs in high-stakes decisions? Cognizant AI Research Lab introduces Semantic Density, a scalable framework 
to quantify response-specific uncertainty without retraining. Tested on state-of-the-art models, it outperforms existing
 methods on benchmarks. Presented at NeurIPS 2024—let’s discuss: [https://medium.com/@evolutionmlmail/quantifying-uncert
ainty-in-llms-with-semantic-density-ff0e58836416](https://medium.com/@evolutionmlmail/quantifying-uncertainty-in-llms-wi
th-semantic-density-ff0e58836416)


```
---

     
 
all -  [ How well-informed Elon Musk is when he makes a statement ](https://www.reddit.com/r/EnoughMuskSpam/comments/1hc415l/how_wellinformed_elon_musk_is_when_he_makes_a/) , 2024-12-15-0916
```
https://preview.redd.it/ekphsq71aa6e1.png?width=798&format=png&auto=webp&s=025884971824e9394aa64d398c70b1d44da62083


```
---

     
 
all -  [ Can research projects be replacement for internships in North America? ](https://www.reddit.com/r/csMajors/comments/1hc391a/can_research_projects_be_replacement_for/) , 2024-12-15-0916
```
I'm not a computer science major but do have extensive technical background (I'm in cognitive science with a computer sc
ience minor.) I've completed a research project for credit under a big name in cognitive science and it involved CNNs. I
 aim to refine the project so that I can submit it to a journal or a good conference (NeurIPS maybe!) I've also done thi
s unpaid internship thing from back home under a research organization my father knows very well and it too was technica
l in nature. I presented the research at a big psychology conference. I'm also volunteering at a computational psychiatr
y lab as a technical reviewer.

That being said, how much are research projects worth when finding full-time jobs, even 
the ones that are published or presented at conferences? I'm graduating next May. I haven't gotten any internships throu
ghout my undergrad, only got technical assessments without any interviews.

Edit: My GPA is bad because of long-term men
tal health issues
```
---

     
 
all -  [ [D] How to make friends and network at NeurIPS? ](https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/) , 2024-12-15-0916
```
I’m attending NeurIPS for the first time and it’s quite overwhelming seeing the amount of people and so many recruiters.
 I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is
 here.

I didn’t really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach
 others who are in big groups of people and (2) I’m feeling strong imposter syndrome and under-qualified for the jobs re
cruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other 
students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to
 potentially meet up and have a chat? I’m a 3rd year PhD student from the UK, but from Vancouver myself so know lots of 
stuff going on in the area. Cheers!
```
---

     
 
all -  [ NeurIPS 2024: What Matters When Building Vision Language Models ](https://www.reddit.com/r/computervision/comments/1hb2zk0/neurips_2024_what_matters_when_building_vision/) , 2024-12-15-0916
```
Check out [Harpreet Sahota’s](https://www.linkedin.com/in/harpreetsahota204/) conversation with [Hugo Laurençon](https:/
/www.linkedin.com/in/hugo-lauren%C3%A7on/) of Sorbonne Université and Hugging Face about his NeurIPS 2024 paper, “What M
atters When Building Vision Language Models.”

* [Complete paper presentation and Q&A on YouTube](https://www.youtube.co
m/watch?v=OfbsZeBBFrg)
* [Research paper on arXiv](https://arxiv.org/abs/2405.02246)

  
Preview video below:

https://r
eddit.com/link/1hb2zk0/video/9ebds5l7716e1/player


```
---

     
 
all -  [ NeurIPS is starting tomorrow but I’m too young to go there😭😭😭 ](https://www.reddit.com/r/teenagers/comments/1haqs1i/neurips_is_starting_tomorrow_but_im_too_young_to/) , 2024-12-15-0916
```
It’s right where I live as well
```
---

     
 
all -  [ [R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effe ](https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/) , 2024-12-15-0916
```
We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**: [https://arx
iv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**: [https://github.com/trungtrinh44/DAMP](https://gi
thub.com/trungtrinh44/DAMP)  
**DAMP** (Data augmentation via multiplicative perturbations) is a simple yet effective ap
proach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augme
ntation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness withou
t compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Found
ation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations
, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires onl
y random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while
 being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transforme
rs from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet wit
hout complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly imp
roves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corrupti
on error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58
.83% baseline)

**Why DAMP?** Unlike traditional approaches that rely on complex data augmentation pipelines or computat
ionally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustnes
s. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing 
techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over sta
ndard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical disc
ussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond co
mputer vision!
```
---

     
 
all -  [ NeurIPS 2024 - Creating SPIQA: Addressing the Limitations of Existing Datasets for Scientific VQA ](https://www.reddit.com/r/computervision/comments/1ha9cup/neurips_2024_creating_spiqa_addressing_the/) , 2024-12-15-0916
```
Check out [Harpreet Sahota](https://www.linkedin.com/in/harpreetsahota204/)’s conversation with [Shraman Pramanick](http
s://www.linkedin.com/in/shramanpramanick/) of Johns Hopkins University and Meta AI about his NeurIPS 2024 paper, “Creati
ng SPIQA: Addressing the Limitations of Existing Datasets for Scientific VQA.”

* [Complete paper presentation and Q&A o
n YouTube](https://youtu.be/p56kAbdYtUg)
* [Research paper on arXiv](https://arxiv.org/abs/2407.09413)

Preview video:


https://reddit.com/link/1ha9cup/video/z1vatdr5ot5e1/player

  

```
---

     
 
all -  [ NeurIPS 2024 - No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Mul ](https://www.reddit.com/r/computervision/comments/1h9q0x1/neurips_2024_no_zeroshot_without_exponential_data/) , 2024-12-15-0916
```
Check out [Harpreet Sahota](https://www.linkedin.com/in/harpreetsahota204/)’s conversation with [Vishaal Udandarao](http
s://www.linkedin.com/in/vishaal-udandarao/) of the University of Tübingen and Cambridge about his NeurIPS 2024 paper, “N
o 'Zero-Shot' Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.”

* [Comp
lete paper presentation and Q&A on YouTube](https://youtu.be/_4cQJmz2u1c)
* [Research paper on arXiv](https://arxiv.org/
abs/2404.04125)

Preview video:

https://reddit.com/link/1h9q0x1/video/pcw40i25ao5e1/player
```
---

     
 
all -  [ NeurIPS 2024: A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis ](https://www.reddit.com/r/computervision/comments/1h82qz6/neurips_2024_a_textbook_remedy_for_domain_shifts/) , 2024-12-15-0916
```
Check out [Harpreet Sahota’s](https://www.linkedin.com/in/harpreetsahota204/) conversation with [Yue Yang](https://www.l
inkedin.com/in/yue-yang-4b2075174) of the University of Pennsylvania and AI2 about his NeurIPS 2024 paper, “A Textbook R
emedy for Domain Shifts: Knowledge Priors for Medical Image Analysis.”

* [Complete paper presentation and Q&A on YouTub
e](https://youtu.be/VjJWOQHgsmk)
* [Research paper on arXiv](https://arxiv.org/abs/2405.14839)

Video preview below:

ht
tps://reddit.com/link/1h82qz6/video/lintlyfuo85e1/player
```
---

     
 
all -  [ NeurlPS 2024: NaturalBench - Evaluating Vision-Language Models on Natural Adversarial Samples ](https://www.reddit.com/r/computervision/comments/1h7f4k2/neurlps_2024_naturalbench_evaluating/) , 2024-12-15-0916
```
Check out [Harpreet Sahota](https://www.linkedin.com/in/harpreetsahota204/)’s conversation with [Zhiqiu Lin](https://www
.linkedin.com/in/zhiqiu-lin-b49ba7126/) of Carnegie Mellon University about his NeurIPS 2024 paper, “NaturalBench: Evalu
ating Vision-Language Models on Natural Adversarial Samples.”

* [Complete interview and discussion on YouTube](https://
youtu.be/eFfdo4vZIic)
* [Research paper on arXiv](https://youtu.be/eFfdo4vZIic)

Video preview below:

https://reddit.co
m/link/1h7f4k2/video/6mw2ahngi25e1/player

  

```
---

     
 
all -  [ Former Intern Sabotages ByteDance’s AI Training, Faces ¥8 Million Lawsuit, Yet Wins NeurIPS 2024 Bes ](https://www.reddit.com/r/LocalLLaMA/comments/1h6i1m9/former_intern_sabotages_bytedances_ai_training/) , 2024-12-15-0916
```
In October 2024, media outlets reported that 'ByteDance's large-scale model training was attacked by an intern,' and onl
ine sources claimed that 'over 8,000 GPUs were involved, resulting in losses exceeding tens of millions of dollars.' Byt
eDance later issued an official statement to clarify the situation, confirming that a serious infraction did occur invol
ving an intern. The intern in question was terminated by the company in August 2024.

[ByteDance Official Statement](htt
ps://preview.redd.it/uvwtbcqyhu4e1.png?width=785&format=png&auto=webp&s=3b9040d2ac1f03c5c0c2d279b61704c4997a438d)

ByteD
ance's lawsuit against former intern Tian for tampering with code and attacking the company's internal model training ha
s been officially accepted by the Haidian District People's Court in Beijing. ByteDance is requesting the court to order
 Tian to compensate the company for damages amounting to ¥8 million, along with reasonable expenses of ¥20,000, and to p
ublicly issue an apology.

Recently, the intern who maliciously attacked ByteDance's training cluster, Keyu Tian, receiv
ed the NeurIPS 2024 Best Paper Award. [Paper](https://arxiv.org/abs/2404.02905) [Github](https://github.com/FoundationVi
sion/VAR)

https://preview.redd.it/4hfxwbkcju4e1.png?width=1802&format=png&auto=webp&s=ec79a9a1b49bbd1e329c6b7560e74a415
907dff1

Coincidentally, this award-winning paper was the result of his collaboration with the team during his internshi
p in ByteDance's Commercialization Technology Department.
```
---

     
 
all -  [ NeurIPS 2024 - A Label is Worth a Thousand Images in Dataset Distillation ](https://www.reddit.com/r/computervision/comments/1h6hx3p/neurips_2024_a_label_is_worth_a_thousand_images/) , 2024-12-15-0916
```
https://reddit.com/link/1h6hx3p/video/k7wh8qlfiu4e1/player

Check out [Harpreet Sahota’s](https://www.linkedin.com/in/ha
rpreetsahota204/) conversation with [Sunny Qin](https://www.linkedin.com/in/sunnytqin/) of Harvard University about her 
NeurIPS 2024 paper, 'A Label is Worth a Thousand Images in Dataset Distillation.”

https://i.redd.it/mg9isc68iu4e1.gif


* [Complete interview and discussion on YouTube](https://www.youtube.com/watch?v=9TdDJ9J-VZI)
* [Research Paper](https:/
/arxiv.org/abs/2406.10485)
```
---

     
 
all -  [ I have reasons to believe that Recursion (RXRX) will became quite popular in the next month. ](https://www.reddit.com/r/wallstreetbets/comments/1h1volv/i_have_reasons_to_believe_that_recursion_rxrx/) , 2024-12-15-0916
```
I believe that in the future, drugs will be highly customisable based on the patience’s health history. Based on your ph
ysiology, syndromes, and genetics, you may receive a drug that is well-suited for you and only you. 

How can you do tha
t? First and foremost, you need data, huge amounts of it. We all know how generative and predictive models had advanced 
in the last year. It wasn’t in fact, until the launch of AlphaFold (by Google, whose team was recently awarded with the 
Chemistry Nobel Prize), that AI drug discovery became prominent. This open source model is used for molecular discovery.
 Again, would be nice if a company could:

1. Generate proprietary synthetic, good quality molecular data using models l
ike AlphaFold.
2. Using this data to train models for drug discovery, reducing pipelines costs and times up to 50%.
3. E
ventually, with the possibility of bringing the first AI-aided drug to the market.

First two points have been achieved,
 and the company is Recursion. We may know them because NVIDIA invested 50m in them. Why then are at ATL? I think the an
swer is time. We all know there is no room for patience when it comes to money sometimes. Training and bringing such res
ults may take years.

However, I think another catalyst is coming. On 9. December, they will host a seminar for new read
outs in one of their most well-known drugs in development, CDK7, for advance solid tumours (an inhibitor, which are curr
ently none approved by the FDA).

Now, I am not saying that they will cure cancer - that’s BS. But over the years conver
ging to novel oncological solutions using AI? This is not the only drug they have (other 9 are in development).

They ha
ve more than 60 petabytes of data.
They combined forces with Exscientia recently, forming probably the most important po
werhouse of AI-drug research.
They are extremely active in the research field (see their presence in the upcoming NeuRIP
S conference) or their new open dataset for Quantum Computing (OpenQDC).

I started investing in IONQ in 2021 for a simi
lar impression. Now I am getting the same vibes with this. I feel that a small catalyst will put this to fly, although t
he real potencial will come in the next 5-10 years. If they can bring the first AI drug to the market,  this implodes.


Of course, no financial advice. I’m long 800 shares and loading as much as I possibly can.
```
---

     
 
all -  [ can I attend neurips as an enthusiast?  ](https://www.reddit.com/r/MLQuestions/comments/1h1kw31/can_i_attend_neurips_as_an_enthusiast/) , 2024-12-15-0916
```
neurips is coming to my hometown, can I just go? I want to hunt down all the recruiters lol 
```
---

     
 
all -  [ [0 YoE, Unemployed, Machine Learning Research Intern, Serbia] ](https://www.reddit.com/r/resumes/comments/1h0jscl/0_yoe_unemployed_machine_learning_research_intern/) , 2024-12-15-0916
```
&#x200B;

[CV](https://preview.redd.it/hv5mq2zkma3e1.png?width=5100&format=png&auto=webp&s=5afe5f21d9ec107c2f5d6cafffd7e
e872ebaa606)

Hello, I hope you are doing well!

I am a second year undergraduate student seeking to land a machine lear
ning research intern position next summer. The CV up there is shrunk to be 1-page length and contains only the most mach
ine learning-related stuff I have created in the past year.  
I am curious in what ways could this CV be bettered and de
tails provided, so I can increase my chances. I have already gotten plenty of industry offers, but I would like to maxim
ize my chances for research positions.

I am very much so aware that, after the second year of undergraduate studies, it
 is very hard to land one, but in case it happens, it can kickstart my career a lot.

I am also interested whether I sho
uld add 2 work in progress papers that I am aiming to publish in the following months - one is related to graph neural n
etworks in medicine, while the other is related to graph neural networks and categorical deep learning.

Thank you in ad
vance - every advice is helpful and appreciated!
```
---

     
 
all -  [ MS Thesis project at IBM Research-Zurich ](https://www.reddit.com/r/ethz/comments/1gzk140/ms_thesis_project_at_ibm_researchzurich/) , 2024-12-15-0916
```
Dear MS Students,

We have an opportunity for an MS Thesis project at IBM Research-Zurich.  

**Project description:** D
espite the breakthrough made by large language models (LLMs), they struggle with high-level reasoning tasks requiring de
liberate thinking and problem-solving skills. Particularly, the pretrained state-of-the-art Transformer language models 
fail at compositional generalization, multi-step deductive reasoning, and analogical reasoning \[1, 2, 3\]. As a potenti
al alternative, neuro-symbolic AI seeks complementary approaches that beneficially combine deep learning advancements wi
th symbolic computations to endorse their strengths and supplement their weaknesses. Key challenges in neuro-symbolic AI
 involve the potentially exponential time required to perform probabilistic inference and the difficulty in learning new
 symbolic programs. Our latest research results addressed these challenges by performing analogical reasoning over distr
ibuted representations \[4,5\]. In this project, the main objective is to develop methods that reduce the computational 
bottleneck in general neuro-symbolic AI systems, while maintaining learning rules/programs that exhibit out-of-distribut
ion generalization, flexibility, and interpretability. Other inputs or directions are welcomed.

**Requirements:** Stron
g motivation and self-drive. Strong analytical and problem-solving skills. Concrete knowledge in deep learning, or a sol
id background in machine learning. Experience with TensorFlow or PyTorch frameworks. Expertise with LLMs is an advantage
.

**Some administrative information:**  
o Earliest start date: Feb 2025  
o Duration: 6 months  
o Pay: None (prohibit
ed from ETH)

The thesis will be performed at the IBM Research-Zurich in Rüschlikon. If you are interested in this chall
enging position on an exciting new topic, please send your most recent curriculum vitae including a transcript of BS and
 MS grades by email to: Dr. Michael Hersche ([her@zurich.ibm.com](mailto:her@zurich.ibm.com)) and Dr. Abbas Rahimi ([abr
@zurich.ibm.com](mailto:abr@zurich.ibm.com))

\[1\] N. Dziri et al., ‘Faith and Fate: Limits of Transformers on Composit
ionality’, *Advances in* *Neural Information Processing Systems (NeurIPS),* 2023.

\[2\] J. Thomm et al., ‘Limits of Tra
nsformer Language Models on Learning to Compose Algorithms’, *Advances in* *Neural Information Processing Systems (NeurI
PS),* 2024.

\[3\] X. Chen, et al., ‘Premise Order Matters in Reasoning with Large Language Models’, *ICML*, 2024.

\[4\
] M. Hersche, et al., ‘A Neuro-Vector-Symbolic Architecture for Solving Raven’s Progressive Matrices’, *Nature Machine I
ntelligence*, 2023.

\[5\] G. Camposampiero, et al., ‘Towards Learning Abductive Reasoning using VSA Distributed Represe
ntations’, *International Conference on Neural-Symbolic Learning and Reasoning (NeSy*), 2024.
```
---

     
 
all -  [ Should i even try for my chances or is it just a waste of time applying with poor grades . Can my SO ](https://www.reddit.com/r/Indians_StudyAbroad/comments/1gzec5d/should_i_even_try_for_my_chances_or_is_it_just_a/) , 2024-12-15-0916
```
I have completed my undergrad from Osmania University (VCE) in electronics and communications ( a tier 2 ish college in 
India )

I have a very low GPA ( 3.18 / 4 ) , I have graduated in 2024 and am aspiring to apply for fall 2026 .

I have 
had severe health issues during my undergrad and did a lot of community outreach programs in my sophomore year , My grad
es were good in my junior year but it was just downhill after that , I was suffering from many health issues which spoil
ed my grades but got recovered in final year and had good grades.

After my health recovered I worked like hell and alre
ady published 4 papers in IEEE conferences and 2 in smaller conferences

**I am currently working at AT&T as senior asso
ciate software engineer with the field being machine learning . I have applied to CVPR 2025 conference and am sure of ge
tting my paper published there , I have 3 more papers ready for ICML and NeurIPS as well , these are the top most confer
ences in ML in the entire world . I am sure of having at least 15 paers published by end of 2025 ( 5 are Q1 level ) . I 
have 3 patents on my name and aspire to get more . It took me sleepless nights during my final year to accomplish all of
 this after my health recovered**

My\_qualifications :

2 research internships , strong LOR's from top international un
iversities where I worked with a professor , aiming to do at least 2 more internships at labs by end of 2025

Can I conv
ince the admission committee that my health affected my grades and can the other strong part of backup my grades part in
 my SOP or is it just a waste of time applying ??

dream target univs - Caltech MS EE ( ML track ) , Stanford MS EE ( ML
 track ) , Stanford MS CS

I recently read this [https://www-cs.stanford.edu/\~rkarthik/DAGAP.pdf](https://www-cs.stanfo
rd.edu/~rkarthik/DAGAP.pdf) where the writer says low GPA will be immediately rejected . Should i even try building my p
rofile or give up hope ?

Stanford and Caltech have been my dream univs since many years and life seems meaningless with
out them , I really need serious advice if i have to give up hope and change my path or still build profile . i have one
 year of time to apply .
```
---

     
 
all -  [ Should i even try ? ](https://www.reddit.com/r/gradadmissions/comments/1gze93d/should_i_even_try/) , 2024-12-15-0916
```
I have completed my undergrad from Osmania University (VCE) in electronics and communications ( a tier 2 ish college in 
India ) 



I have a very low GPA ( 3.18 ) , I have graduated in 2024 and am aspiring to apply for fall 2026 .

 I have 
had severe health issues during my undergrad and did a lot of community outreach programs in my sophomore year , My grad
es were good in my junior year but it was just downhill after that , I was suffering from many health issues which spoil
ed my grades but got recovered in final year and had good grades. 

After my health recovered I worked like hell and alr
eady published 4 papers in IEEE conferences and 2 in smaller conferences

**I am currently working at AT&T as senior ass
ociate software engineer with the field being machine learning . I have applied to CVPR 2025 conference and am sure of g
etting my paper published there , I have 3 more papers ready for ICML and NeurIPS as well , these are the top most confe
rences in ML in the entire world . I am sure of having at least 18 papers published by end of 2025 ( 5 are Q1 level ) . 
I have 3 patents on my name and aspire to get more . It took me sleepless nights during my final year to accomplish all 
of this after my health recovered**



other profile aspects -

2 research internships , strong LOR's from top internati
onal universities where I worked with a professor , aiming to do at least 2 more internships at labs by end of 2025

Can
 I convince the admission committee that my health affected my grades and can the other strong part of backup my grades 
part in my SOP or is it just a waste of time applying ??

dream target univs - Caltech MS EE ( ML track ) , Stanford MS 
EE ( ML track ) , Stanford MS CS

I recently read this [https://www-cs.stanford.edu/\~rkarthik/DAGAP.pdf](https://www-cs
.stanford.edu/~rkarthik/DAGAP.pdf) where the writer says low GPA will be immediately rejected . Should i even try buildi
ng my profile or give up hope ?

Stanford and Caltech have been my dream univs since many years and life seems meaningle
ss without them , I really need serious advice if i have to give up hope and change my path or still build profile . i h
ave one year of time to apply .
```
---

     
 
all -  [ Should I even apply for a PhD? ](https://www.reddit.com/r/PhD/comments/1gy1xp7/should_i_even_apply_for_a_phd/) , 2024-12-15-0916
```
Hi all

So I (23M, Indian) had the dream to pursue a MS+PhD in CS (AI) in the US (No masters, I just have a Bachelors de
gree in Electronics Engineering). I was aiming to get into universities like UCSD, John Hopkins, UIUC.

To make it happe
n, I applied to Indian professors and worked with them. Have spent over 1 year working in research, while managing my so
ftware engineering job in parallel. Got one paper published in ICPR 2024, and one in a small conference, not a big deal.
 I have managed to gather 3 Letters of Recommendation after working.

However, from some sample SOPs on the net, I see t
hat the applicants for these colleges have already 1st author publications in top-tier conferences like AAAI, NeurIPS et
c.

In this scenario, should I even apply? I feel like I have no chance to compete with these people. Am I aiming for to
o high? What would you suggest?

Thank you everyone.

  
Edit 1

Thank you everyone. I got overwhelmed by everything and
 became tensed. I will curate and apply. Whatever happens next, will happen. Let me do my part at least. thanks!
```
---

     
 
MachineLearning -  [ [D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 p ](https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/) , 2024-12-15-0916
```
At NeurIPS 2024 I found a paper that got accepted that positions its main contribution in the form of “Existing algorith
ms for X ignore Y. We adapt algorithm Z for X to account for Y”.

On OpenReview I see that the reviewers in particular p
raised the novelty of the work, and recognised Y as an important aspect that had been ignored in the field of X.

Now th
e interesting bit: co-authors and I published a paper in Springer’s Machine Learning journal in 2023 that also proposes 
an algorithm for X that account for Y. We were also not the first to study the problem setting of X with Y: our paper’s 
related work section discusses 4 papers that have all proposed algorithms for X that account for Y. One is even from Neu
rIPS (2017), and the oldest one dates back to 2012 (an AAAI paper).

The authors of this 2024 NeurIPS paper completely m
issed all this prior literature and believed they were the first, and so did all the reviewers.

This week I e-mailed th
e authors of this NeurIPS 2024 paper and they acknowledged that these works (mine + the 4 others) indeed were all workin
g on the same problem setting, mentioned that they were unaware of all these works, and acknowledged that they can no lo
nger claim novelty of the problem setting.

NeurIPS allows updating the camera ready paper after the conference, and the
 authors promised to use this opportunity to incorporate those related works and modify their contribution statements to
 no longer claim novelty of a first solution of X with Y.

At the one hand, it makes me happy that our work will get cre
dited appropriately.

At the other hand I have my doubts about the ethics of severely modifying contribution statements 
post-review. The authors will no longer claim novelty, but the reviewers in particular praised this novelty, which makes
 me uncertain whether reviewers would have recommended acceptance had they known that this paper will ultimately no long
er be able to claim the novelty that it claimed to have in the reviewed version.

Moreover this makes me wonder about th
e experimental section. Almost surely, reviewers would have demanded comparison to those 5 prior works as baselines. Thi
s paper did not compare against baselines, which will have seemed reasonable to a reviewer who reviewed this work under 
the assumption that the problem setting was completely novel and no prior methods exist that could function as a baselin
e.

Asking the group here about any thoughts on how such cases should get resolved:
- should the paper be retracted?
- s
hould the area chair / program committee be informed? who may or may not take action
- should the paper just get updated
 by authors in the way that was promised, and that is it?
- something else?

I redacted X, Y and Z in order to not publi
cly shame the authors, as they have engaged with my e-mails and I am convinced that there is no foul play and they truly
 were unaware of those works.
```
---

     
 
MachineLearning -  [ [R] Agentic AI test suites. All the test environments used to benchmark BALROG. ](https://www.reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/) , 2024-12-15-0916
```
# BabyAI

+ Purpose is to facilitate research on *grounded language learning.*  The current domain of BabyAI is a 2D gri
dworld in which synthetic natural-looking instructions (e.g. “put the red ball next to the box on your left”) require th
e agent to navigate the world including unlocking doors) and move objects to specified locations.  

https://openreview.
net/forum?id=rJeXCo0cYX

----

# Crafter

+ Crafter features randomly generated 2D worlds where the player needs to fora
ge for food and water, find shelter to sleep, defend against monsters, collect materials, and build tools.

https://gith
ub.com/danijar/crafter?tab=readme-ov-file


----

# TextWorld

+ Microsoft TextWorld is an open-source, extensible engin
e that both generates and simulates text games. You can use it to train reinforcement learning (RL) agents to learn skil
ls such as language understanding and grounding, combined with sequential decision making.

https://www.microsoft.com/en
-us/research/project/textworld/

https://github.com/microsoft/TextWorld

https://arxiv.org/pdf/1806.11532

----

# Baba 
is AI 

+ Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine 
those rules and objectives.    We test three ***state-of-the-art multi-modal large language models (OpenAI GPT-4o, Googl
e Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically*** when generalization requires that the rul
es of the game must be manipulated and combined. 


https://github.com/nacloos/baba-is-ai

https://arxiv.org/abs/2407.13
729

----

# MiniHack

+ MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforc
ement Learning (RL).   The motivation behind MiniHack is to be able to perform RL experiments in a controlled setting wh
ile being able to increasingly scale the complexity of the tasks.

https://github.com/facebookresearch/minihack

https:/
/minihack.readthedocs.io/en/latest/


----

# NetHack

+ NetHack is an attractive research platform as it contains hundr
eds of enemy and object types, has complex and stochastic environment dynamics, and has a clearly defined goal (descend 
the dungeon, retrieve an amulet, and ascend) which can be achieved in a diverse set of ways. The game is considered one 
of the hardest in the world1, with winning episodes lasting 100,000s of steps, and a permadeath setting that starts agen
ts at the beginning in a whole new world if they die in the dungeon. NetHack is even difficult to master for human playe
rs who often rely on external knowledge.


https://proceedings.neurips.cc/paper_files/paper/2023/file/764ba7236fb6374301
4fafbd87dd4f0e-Paper-Conference.pdf

https://github.com/upiterbarg/hihack

https://arxiv.org/pdf/2203.11889

https://www
.youtube.com/watch?v=8L8LiQ-cIWA
```
---

     
 
MachineLearning -  [ [D] PhD in RL/ML Theory or LLM ](https://www.reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/) , 2024-12-15-0916
```
Hi guys,

I'm at a crossroads in my academic journey and would appreciate the community's insights. I'm trying to decide
 between pursuing a PhD focused on reinforcement learning/ML theory versus specializing in large language models with mo
re experimental/applied research (these are the only two offers I had).

# Key considerations are the following:

# Rese
arch Impact

* RL/ML Theory: Foundational work that could advance the field's mathematical understanding
* LLMs: Direct 
applications in today's most transformative AI systems

# Job Prospects

* Theory: Academia, research labs, potentially 
more limited industry roles
* LLMs: High industry demand, active research area in both academia and industry

# Long-ter
m Relevance

* Theory: Core principles likely to remain valuable regardless of specific technologies
* LLMs: Currently r
evolutionary but uncertain long-term trajectory

Personal background

* I'm an international student and about to finish
 my master program in US, so I no longer has enough time before making the final decision. I used to research in ml theo
ry, but did not end up with a real top conference publication in theory. I personally doubt if I have enough mathematica
l background to pursue a successful PhD in this area (e.g., at least publish 2 theory papers a year on ICML/NeurIPS/ICLR
/COLT/AISTATS). At the same time, I am personally doubting if theory works indeed advance the ML/AI community, as many p
apers are just proving vacuous bounds or propose some new algorithms that themselves cannot even implement or experiment
ally tested.
* I also used to research in more applied ml, with one aaai paper. My personal concerns is that I'm not fas
t at implementation and coding, the most strategic ability for a successful applied ML researcher. After we entered the 
LLM era, the pacing or applied ML research (especially in LLM and CV) becomes so fast. It's like competitive programming
 in research community (well, also the #GPUs competition).
```
---

     
 
MachineLearning -  [ [D] Expectation from Machine Learning Engineering jobs ](https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/) , 2024-12-15-0916
```
Hey everyone,

I’ve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up
 a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems 
to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, whi
ch are apparently more in demand than research scientist positions.

I’m curious about the difference between these two 
roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master’s degree is often pr
eferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are you
r thoughts?
```
---

     
 
MachineLearning -  [ [R] Convolutional Differentiable Logic Gate Networks ](https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/) , 2024-12-15-0916
```
Abstract

With the increasing inference cost of machine learning models, there is a growing interest in models with fast
 and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxatio
n was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference on
ly requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware
 and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR p
ooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and util
izing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, whic
h improves over the SOTA while being 29× smaller.

  
Accepted at Neurips 2024, 'SOTA' here means comparable approaches.
 I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. C
urious what others think?
```
---

     
 
MachineLearning -  [ [D] Neurips 2024 Hotel Roommate Search ](https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/) , 2024-12-15-0916
```
The hotels around the venue for Neurips 2024 are pretty expensive, and I'm looking for a roommate to split the cost with
 (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room f
or Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. I
f anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post wit
h your research group or other attendees that you know.

If you are unsure about rooming with a complete stranger, you c
an get to know me a little bit through my personal website (https://mtcrawshaw.github.io/), which has links to my google
 scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. 
Just a grad student trying to make conferences affordable! Thanks.
```
---

     
 
deeplearning -  [ How is the ACL Conference? ](https://www.reddit.com/r/deeplearning/comments/1gs7he7/how_is_the_acl_conference/) , 2024-12-15-0916
```
Hello, I know it's a very noob question but I was wondering what the reputation of ACL is in the field. I have been writ
ing my first paper and my mentor recommended that I aim for the ACL deadline, I just wanted to know how prestigious it w
as relative to bigger conferences like NeurIPS, ICML, ICLR, etc.

Also, purely hypothetical, but what weight does an ACL
 acceptance hold for getting a summer internship/research? I'm an undergrad and I'm kind of cooked with my summer intern
ship prospects, so I was wondering if it would help in any regard.
```
---

     
