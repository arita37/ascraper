 
all -  [ Tough job market advice on how to proceed? ](https://www.reddit.com/r/computervision/comments/1atbsty/tough_job_market_advice_on_how_to_proceed/) , 2024-02-18-0910
```
Fortunately I am currently employeed but searching for new opportunities.

I have a phd 5 publications and 4 patents. No
 Neurips/Cvpr papers though. I have 5 years experience in industry and some time before that doing ML/CV throughout grad
uate school. 

Over the last two months I have probably interviewed with 10 companies to varying degrees the best one I 
went six rounds that they said I aced 5/6 and they finally rejected because of coding I didn’t write an optimal solution
. 

Every place I talk and interview with what they ask about Varies so much and I always feel like I’m preparing for th
e wrong things. So how do I prep and what should I prep?

My last question/comment is how do I break into a new field me
aning if I worked in detection/tracking how do I move to 3D recon? I feel like with the job market as it is people aren’
t willing to hire people who have ability/tenacity they want someone who is 100% up to speed and if they aren’t they are
 out.



Edit: I’m not looking to make such a drastic jump from detection/tracking to 3d recon so much more tangential f
ields but the idea I think remains the same
```
---

     
 
all -  [ Antis are right: image generative AI is fundamentally limited ](https://www.reddit.com/r/aiwars/comments/1at4xj0/antis_are_right_image_generative_ai_is/) , 2024-02-18-0910
```
Since SORA was teased, I've heard a lot of pro-AI folks dragging up old comments from anti-AI folks saying that AI is dy
ing and will always be terrible, and this is entirely fair. But it's important to also remember that some of the critici
sms of generative AI are very true.

First, let me clarify my background. I'm a creative on the side, but my primary car
eer has been in the software industry for the past 35+ years. I first worked with neural networks in the 1980s and thoug
h I've never been on the front-lines of AI development myself, I've worked for AI companies and companies that have done
 AI work. I've had to deal with many issues surrounding AI software for large chunks of my career.

Okay, my credentials
 being out of the way, generative AI as we currently use it is mostly based on two primary things:

1. Transformers^1
2.
 Diffusion models^2

These two advances that were made fairly recently have revolutionized the field, and given us a mas
sive step forward in the state of the art. But it is important to remember how limited these things are.

AI currently h
as no capacity for self-awareness, generating and utilizing long-term memory, emotional empathy or truly self-directed g
oal setting. These are features of human brains that, as far as we are aware, are not merely features of training weight
s in neural networks, and so, while these artificial neural networks can learn in a way very similar to human brains, th
ere is a vast gulf in total capabilities.

I've long speculated that we are 2-3 major breakthroughs (on par with transfo
rmers) away from true parity with human minds, and that that process will probably take between 10 and 50 years. The gen
erative AI that we have today will not be replacing humans because it fundamentally lacks many of the capabilities of hu
mans in the workplace. It can't relate to others socially (which requires empathy); it can't effectively manage anyone (
which requires long-term autonomous planning); it can't maintain a consistent context beyond its token limit (which requ
ires long-term memory); etc.

So when you see claims that AI will just replace artists or 'paper pushers' or any other r
ole, stop and ask yourself if what is being suggested requires these additional capabilities. The ability to create disc
onnected, but very pretty pictures/videos is not the same as being able to engage in the interactive process of creating
 art. Nor is the ability to pass multiple choice or even essay tests sufficient to manage a team of programmers, and the
se capabilities will not magically appear in current AI models any more than the internal combustion engine magically ap
peared in ever-improving wagon wheels.

----

^1 Vaswani, Ashish, et al. 'Attention is all you need.' Advances in neural
 information processing systems 30 (2017). https://arxiv.org/abs/1706.03762
^2 Dhariwal, Prafulla, and Alexander Nichol.
 'Diffusion models beat gans on image synthesis.' Advances in neural information processing systems 34 (2021): 8780-8794
. https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf
```
---

     
 
all -  [ Research Conferences Guide! ](https://www.reddit.com/r/research/comments/1apl0c4/research_conferences_guide/) , 2024-02-18-0910
```
Hi I have these research conferences that are of my interest! (Area: Applied AI and Social NLP)

 

* AI:Neurips, ICML ,
 AAAI, IJCAI; Only NLP: ICLR, EMNLP, ACL; 

&#x200B;

Is there any good digests, newsletters, or RSS feeds I can follow 
them? What is the general norm of keeping up to date with them! ) If you know any cool websites that consolidate that wo
uld be much appreciated!

&#x200B;
```
---

     
 
all -  [ RA/Research Internships (post MEng, pre-doc) ](https://www.reddit.com/r/gradadmissions/comments/1aoa9b0/raresearch_internships_post_meng_predoc/) , 2024-02-18-0910
```
Hi!

I am an Information Engineering student at Cambridge (UK) (BA,MEng). I realised late (early 3rd year) that I wanted
 to pursue a PhD in ML. After this, I did a MLR internship with a top prof (> 70 H-Index) at Caltech, where I got three 
workshop papers (1 first author, two NeurIPS workshops). The issue is that this work was super low-hanging fruit, and ve
ry applied. To get into a T10 ML PhD in the US (or even in the UK) atm, I am sure I will need more research experience (
conference paper(s)) and another strong rec letter. As such, I decided to not apply for PhDs in this cycle - I am taking
 a year out to \*hopefully\* RA, and apply for the next cycle (2025 entry).

I am aware that there are not many (or any?
) programs in the US/UK/Europe that provide official RA-ships/positions for students who have graduated. Ad-hoc collabor
ations and cold emailing are always possible - I will resort to this when it comes to it - but I was wondering if anyone
 has been in a similar position or have come across opportunities/ insights that might be useful in my situation. Genera
l advice (or criticism) is also super appreciated.

P.S. My MEng project will ideally go to some conference, and I have 
a good academic record (top 3% of my class of 250+ with 9 academic awards/scholarships). I am interested in symbolic rea
soning/robustness of large models. I am also more fundamental work, in statistical guarantees (inc. conformal prediction
) for adversarial robustness, fairness and so on. Super happy to share my profile/cv/personal website via DM.

Thanks in
 advance :)
```
---

     
 
all -  [ Research Assistantships (Pre-Doc, Post-MEng) ](https://www.reddit.com/r/learnmachinelearning/comments/1aoa5er/research_assistantships_predoc_postmeng/) , 2024-02-18-0910
```
Hi!

I am an Information Engineering student at Cambridge (UK) (BA,MEng). I realised late (early 3rd year) that I wanted
 to pursue a PhD in ML. After this, I did a MLR internship with a top prof (> 70 H-Index) at Caltech, where I got three 
workshop papers (1 first author, two NeurIPS workshops). The issue is that this work was **super low-hanging fruit**, an
d very applied. To get into a T10 ML PhD in the US (or even in the UK) atm, I am sure I will need more research experien
ce (conference paper(s)) and another strong rec letter. As such, I decided to not apply for PhDs in this cycle - I am ta
king a year out to **\*hopefully\*** RA, and apply for the next cycle (2025 entry).

I am aware that there are not many 
**(or any?)** programs in the US/UK/Europe that provide official RA-ships/positions for students who have graduated. Ad-
hoc collaborations and cold emailing are always possible - I will resort to this when it comes to it - but I was wonderi
ng if anyone has been in a similar position or have come across opportunities/ insights that might be useful in my situa
tion. General advice (or criticism) is also super appreciated.

P.S. My MEng project will ideally go to some conference,
 and I have a good academic record (top 3% of my class of 250+ with 9 academic awards/scholarships). I am interested in 
symbolic reasoning/robustness of large models. I am also more fundamental work, in statistical guarantees (inc. conforma
l prediction) for adversarial robustness, fairness and so on. Super happy to share my profile/cv/personal website via DM
.

Thanks in advance :)
```
---

     
 
all -  [ Wat betekent 'maandelijks de testknop indrukken' in de meterkast? ](https://i.redd.it/9gkltksaaxhc1.png) , 2024-02-18-0910
```

```
---

     
 
all -  [ Faith and Fate: Limits of Transformers on Compositionality [R] ](https://www.reddit.com/r/MachineLearning/comments/1amzb52/faith_and_fate_limits_of_transformers_on/) , 2024-02-18-0910
```
Edit: Kevin Murphy,  Francois Chollet, Vitaly Kurin and others recommended this paper (some very highly)

https://arxiv.
org/abs/2305.18654 (Presented at NeurIPS in December)

**Abstract:**

Transformer large language models (LLMs) have spar
ked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models 
simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or d
o they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of th
ese models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a clas
sic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps 
into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of co
mplexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transform
er LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, with
out necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical a
rguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidl
y decay with increased task complexity.

---

Kevin Murphy's summary: 'I like this paper. They prove that transformers a
re guaranteed to suffer from compounding errors when doing long reasoning chains (as @ylecun has argued), and much appar
ent 'success' is just due to unreliable pattern matching / shortcut learning.'
```
---

     
 
all -  [ [D] concerns about the series of works in reflexion(self-adjustment)-powered LLM agent ](https://www.reddit.com/r/MachineLearning/comments/1am3ior/d_concerns_about_the_series_of_works_in/) , 2024-02-18-0910
```
we see tons of works in LLM-based agent which can perform tasks on web applications such as webshop, [webarena](https://
github.com/web-arena-x/webarena),  [agentbench](https://github.com/THUDM/AgentBench/tree/main)etc...

also, we can find 
following works on reflexion-based agent which intakes the feedbacks and errors from previous trials from the interactio
ns with the environment. the typical work is  `Reflexion: Language Agents with Verbal Reinforcement Learning`

within ea
ch trial, the agent, or say, llm, digests the prompt which contains not only history from current trial but also the sys
tem info or feedbacks or error messages from previous trials. The feedbacks could come from system setting or from anoth
er more powerful LLM that can act as a super judge to give feedbacks.

anyway, I do not think this is RL since there is 
no learning process for the agent, but a concat of prompt.

My primary concern is that is this label leakage ? The agent
 get feedbacks from the environment and with more trials, of course, the agent should have a more clear approach to the 
final answer. So what is the point ?

I see a post which shares my same concern:  [noahshinn/reflexion: \[NeurIPS 2023\]
 Reflexion: Language Agents with Verbal Reinforcement Learning (github.com)](https://github.com/noahshinn/reflexion/issu
es/27)

&#x200B;

Would like to hear from you in view of academic and industry.

&#x200B;

&#x200B;

&#x200B;

&#x200B;
```
---

     
 
all -  [ [R] Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning ](https://www.reddit.com/r/MachineLearning/comments/1am1v5f/r_long_is_more_for_alignment_a_simple_but/) , 2024-02-18-0910
```
**Title**: Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning

**Paper**: [http
s://arxiv.org/abs/2402.04833](https://arxiv.org/abs/2402.04833)

**Abstract**: There is a consensus that instruction fin
e-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-
of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a qual
ity scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from s
tandard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while 
remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-th
e-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a lig
htweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to
 obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no ext
ra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not s
imply due to GPT-4's preference for longer responses, thus ruling out any artificial improvement. In conclusion, our fin
dings suggest that fine-tuning on the longest instructions should be the default baseline for any research on instructio
n fine-tuning.
```
---

     
 
all -  [ [R] Persistent homology and topological data analysis helped robust detection of AI-generated texts ](https://www.reddit.com/r/MachineLearning/comments/1aky8xt/r_persistent_homology_and_topological_data/) , 2024-02-18-0910
```
The main idea is that text data can be presented as points in some high-dimensional space. It can be assumed that the da
taset fits into some surface in it. The problem is that such a surface may have fractal characteristics, so complex math
ematics is required.

The authors of a new study have developed a method for determining the fractal dimension of such a
 surface (researchers called it the internal dimension), which is based on the concept of persistent homologies. Briefly
, the main thing is that this dimension differs for human and machine texts with a high degree of reliability, which can
 be used in practice. 

&#x200B;

[ ](https://preview.redd.it/8r71lx9jh4hc1.png?width=1091&format=png&auto=webp&s=3077d2
1099e4c86a881ab297f22467f338e3c593)

*Real and artificial texts have different intrinsic dimension*

It is noteworthy th
at the internal dimensions are different for texts in different languages - from 7 ± 1 for Chinese to 10 ± 1 for Italian
 - but reliable discrimination is achieved in all of them.

The code and data are available on [GitHub](https://github.c
om/ArGintum/GPTID), and details of the study can be found in the [article](https://openreview.net/forum?id=8uOZ0kNji6) p
ublished in the proceedings of the NeurIPS 2023 conference.
```
---

     
 
all -  [ IS THIS A GOOD ROADMAP TO LEARN PYTHON? ](https://www.reddit.com/r/learnpython/comments/1ak8v9p/is_this_a_good_roadmap_to_learn_python/) , 2024-02-18-0910
```
. Python Basics:  
Resources:  
'Python Crash Course' by Eric Matthes  
'Automate the Boring Stuff with Python' by Al
 Sweigart  
Codecademy's Python course  
2. Mathematics for Machine Learning:  
Linear Algebra, Calculus, Probability
 & Statistics  
Resources:  
'Linear Algebra Done Right' by Sheldon Axler  
'Introduction to Probability' by Joseph K
. Blitzstein and Jessica Hwang  
'Deep Learning' by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (for deeper unde
rstanding)  
3. Machine Learning Fundamentals:  
Understand supervised and unsupervised learning algorithms, model eva
luation, and cross-validation.  
Resources:  
'Introduction to Machine Learning with Python' by Andreas C. Müller & Sa
rah Guido  
Andrew Ng's Machine Learning course on Coursera  
4. Deep Learning:  
Learn neural networks, deep learnin
g architectures, and frameworks like TensorFlow and PyTorch.  
Resources:  
'Deep Learning' by Ian Goodfellow, Yoshua 
Bengio, and Aaron Courville  
Fast.ai's Practical Deep Learning for Coders course  
'Hands-On Machine Learning with Sc
ikit-Learn, Keras, and TensorFlow' by Aurélien Géron  
5. Natural Language Processing (NLP):  
Study text processing, 
sentiment analysis, named entity recognition, and language modeling.  
Resources:  
Natural Language Processing Specia
lization on Coursera by Deeplearning.ai  
'Natural Language Processing with Python' by Steven Bird, Ewan Klein, and Edw
ard Loper  
6. Computer Vision:  
Explore image processing, object detection, and convolutional neural networks (CNNs)
.  
Resources:  
'Computer Vision: Algorithms and Applications' by Richard Szeliski  
Convolutional Neural Networks S
pecialization on Coursera by Deeplearning.ai  
7. Reinforcement Learning:  
Learn about Markov Decision Processes, Q-l
earning, and policy gradients.  
Resources:  
'Reinforcement Learning: An Introduction' by Richard S. Sutton and Andre
w G. Barto  
Reinforcement Learning Specialization on Coursera by Deeplearning.ai  
8. Projects and Hands-On Practice:
  
Apply your knowledge through projects on platforms like Kaggle or building your own projects.  
Resources:  
Kaggl
e competitions and datasets  
GitHub repositories for inspiration and collaboration  
9. Stay Updated and Networking:
  
Follow research papers, attend conferences, and participate in online forums and communities.  
Resources:  
Arxiv.
org for research papers  
Conferences like NeurIPS, ICML  
Reddit communities (r/MachineLearning, r/learnmachinelearni
ng)  
LinkedIn groups  
10. Advanced Topics:  
Delve into specialized areas like GANs, recommendation systems, time s
eries analysis, etc.  
Resources:  
Books, research papers, and specialized courses on platforms like Coursera, Udacit
y, and edX.
```
---

     
 
all -  [ IS THIS A GOOD ROADMAP FOR MASCHINE LEARNING? ](https://www.reddit.com/r/learnmachinelearning/comments/1ak8qxi/is_this_a_good_roadmap_for_maschine_learning/) , 2024-02-18-0910
```
 

### . Python Basics:

* **Resources:**
   * 'Python Crash Course' by Eric Matthes
   * 'Automate the Boring Stuff wit
h Python' by Al Sweigart
   * Codecademy's Python course

### 2. Mathematics for Machine Learning:

* Linear Algebra, Ca
lculus, Probability & Statistics
* **Resources:**
   * 'Linear Algebra Done Right' by Sheldon Axler
   * 'Introduction t
o Probability' by Joseph K. Blitzstein and Jessica Hwang
   * 'Deep Learning' by Ian Goodfellow, Yoshua Bengio, and Aaro
n Courville (for deeper understanding)

### 3. Machine Learning Fundamentals:

* Understand supervised and unsupervised 
learning algorithms, model evaluation, and cross-validation.
* **Resources:**
   * 'Introduction to Machine Learning wit
h Python' by Andreas C. Müller & Sarah Guido
   * Andrew Ng's Machine Learning course on Coursera

### 4. Deep Learning:


* Learn neural networks, deep learning architectures, and frameworks like TensorFlow and PyTorch.
* **Resources:**
   
* 'Deep Learning' by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
   * Fast.ai's Practical Deep Learning for Coder
s course
   * 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow' by Aurélien Géron

### 5. Natural Lan
guage Processing (NLP):

* Study text processing, sentiment analysis, named entity recognition, and language modeling.
*
 **Resources:**
   * Natural Language Processing Specialization on Coursera by Deeplearning.ai
   * 'Natural Language Pr
ocessing with Python' by Steven Bird, Ewan Klein, and Edward Loper

### 6. Computer Vision:

* Explore image processing,
 object detection, and convolutional neural networks (CNNs).
* **Resources:**
   * 'Computer Vision: Algorithms and Appl
ications' by Richard Szeliski
   * Convolutional Neural Networks Specialization on Coursera by Deeplearning.ai

### 7. R
einforcement Learning:

* Learn about Markov Decision Processes, Q-learning, and policy gradients.
* **Resources:**
   *
 'Reinforcement Learning: An Introduction' by Richard S. Sutton and Andrew G. Barto
   * Reinforcement Learning Speciali
zation on Coursera by Deeplearning.ai

### 8. Projects and Hands-On Practice:

* Apply your knowledge through projects o
n platforms like Kaggle or building your own projects.
* **Resources:**
   * Kaggle competitions and datasets
   * GitHu
b repositories for inspiration and collaboration

### 9. Stay Updated and Networking:

* Follow research papers, attend 
conferences, and participate in online forums and communities.
* **Resources:**
   * Arxiv.org for research papers
   * 
Conferences like NeurIPS, ICML
   * Reddit communities (r/MachineLearning, r/learnmachinelearning)
   * LinkedIn groups


### 10. Advanced Topics:

* Delve into specialized areas like GANs, recommendation systems, time series analysis, etc.

* **Resources:**
   * Books, research papers, and specialized courses on platforms like Coursera, Udacity, and edX.
```
---

     
 
all -  [ Cape to Carthage: documentary about an all African, female-led AI research team rising against the o ](https://www.reddit.com/r/MachineLearning/comments/1ajkh13/cape_to_carthage_documentary_about_an_all_african/) , 2024-02-18-0910
```
In the world of AI, Africa has a reputation for being a missing continent. Follow an underdog, female-led, all-African r
esearch team as they compete with tech giants and top universities for a spot at the top international AI research confe
rence NeurIPS in a bid to change history.

Watch the 30 minute documentary [here](https://decisiveagents.com/capetocarth
age/).
```
---

     
 
all -  [ Actor Critic with q-function approximation not converging ](https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/) , 2024-02-18-0910
```
Recently I have been trying to implement the actor critic described in this [paper](https://proceedings.neurips.cc/paper
/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).

However, when using the cart pole v1 environment the agent 
learns a little of the behavior but then sorta falls apart. Any ideas about incorrect implementation or alternative crit
ic features would be much appreciated.

I have also been playing with hyperparameters but no combination has worked well
 for me.

[Code](https://drive.google.com/file/d/1CeZgjkrJH5biy9zeA7S9JjYm753kVNzl/view?usp=sharing)

&#x200B;

Edit: I 
have realized that in the score function I should not take the sum of the outputs and instead should calculate the gradi
ent for each action individually in a for loop. However, the agent still has poor performance.

Edit2: For the score fun
ction I use the gradients for all of the parameters in the model and performance has increased. I have also realized I m
isunderstood the time variance described in the paper and have refactored the code to implement it correctly.  However, 
there are still issues with convergence and the model becoming overconfident causing nan values.
```
---

     
 
all -  [ [D] Publishing Negative Results ](https://www.reddit.com/r/MachineLearning/comments/1aikp5f/d_publishing_negative_results/) , 2024-02-18-0910
```
I‘ve been working on a ML research project, and unfortunately, the results don‘t align with my hypothesis. I‘ve gotten n
egative results.

While disheartening, I believe there‘s great value in sharing these results as the hypothesis itself r
elies on a sensible theoretical foundation, and it‘s not a priori evident that the results would have been negative.

So
, my question is, can negative results be published at top ML conferences (NeurIPS/ICLR/ICML/…)? Have any of you faced s
imilar situations? How did you navigate this? Did your efforts to publish negatice results at prestigious conferences pr
ove successful?
```
---

     
 
all -  [ AI alignment prize suggestion: Introduce AI Safety concepts into the ML community ](https://www.reddit.com/r/AIsafetyideas/comments/1aiglw9/ai_alignment_prize_suggestion_introduce_ai_safety/) , 2024-02-18-0910
```
Recently, there have been several papers published at top ML conferences that introduced concepts from the AI safety com
munity into the broader ML community. Such papers often define a problem, explain why it matters, sometimes formalise it
, often include extensive experiments to showcase the problem, sometimes include some initial suggestions for remedies. 
Such papers are useful in several ways: they popularise AI alignment concepts, pave the way for further research,  and d
emonstrate that researchers can do alignment research while also publishing in top venues. A great example would be Opti
mal Policies Tend To Seek Power, published in NeurIPS. Future Fund could advertise prizes for any paper that gets publis
hed in a top ML/NLP/Computer Vision conference (from ML, that would be NeurIPS, ICML, and ICLR) and introduces a key con
cept of AI alignment. 
```
---

     
 
all -  [ [D] questions on ICML 2024 submission timeline ](https://www.reddit.com/r/MachineLearning/comments/1ahxe7t/d_questions_on_icml_2024_submission_timeline/) , 2024-02-18-0910
```
Hello all!

Since it's the first time I am submitting to ICML:

1. is it known when the reviews will be released? in neu
rips and iclr, there was info in the call for papers but I couldn't find sth in this year's icml deadline
2. how much ti
me are we given for the author response? is it as long as it is for iclr?
3. will we be able to upload a new draft or wi
ll the replies be only given by text?
4. can we interact with reviewers during the rebuttal or is it just a one-way, sin
gle-time author response?

thanks!
```
---

     
 
all -  [ Picked up all these Tapes today! ](https://i.redd.it/pcbd29avx1gc1.jpeg) , 2024-02-18-0910
```
Found this whole lot at VV for the grand total of $5.99! Lots of them still have the original Sony store tags on them!
```
---

     
 
all -  [ Academic journal or conference for AI safety ](https://www.reddit.com/r/AIsafetyideas/comments/1agmkg5/academic_journal_or_conference_for_ai_safety/) , 2024-02-18-0910
```
To help boost the prestige of safety research, leading to more people starting the career.

Since academic ML mostly pub
lishes at conferences, this could be a conference instead.

&#x200B;

AI Safety Academic Conference

Technical AI Safety


&#x200B;

The idea is to fund and provide logistical/admin support for a reasonably large AI safety conference along t
he lines of Neurips etc. Academic conferences provide several benefits: 1) Potentially increasing the prestige of an are
a and boosting the career capital of  people who get accepted papers. 2) Networking and sharing ideas, 3)  Providing fee
dback on submitted papers and highlighting important/useful papers.  This conference would be unusual in that the work s
ubmitted shares approximately the same concrete goal (avoiding risks from powerful AI).  While traditional  conferences 
might focus on scientific novelty and complicated/'cool' papers, this conference could have a particular focus on things
 like reproducibility or correctness of empirical results, peer support and mentorship, non-traditional research mediums
 (e.g. blog posts/notebooks) , and encouraging authors to have a plausible story for why their work is actually reducing
 risks from AI.
```
---

     
 
all -  [ Research Advances in Transformer Time Series Forecasting Models ](https://www.reddit.com/r/deeplearning/comments/1ag4xfp/research_advances_in_transformer_time_series/) , 2024-02-18-0910
```
Just published a new article describing [recent advances in the deep learning for time series](https://medium.com/deep-d
ata-science/advances-in-deep-learning-for-time-series-forecasting-classification-winter-2024-a3fd31b875b0) forecasting a
nd classification space. Specifically, discussed new research at Neurips and ICML and how it compared to baseline method
s like DLinear. I also critiqued some problematic and flawed papers such as TimeGPT. 
```
---

     
 
all -  [ How much am I worth? ~Big Tech Specialty Research Scientist ](https://www.reddit.com/r/Salary/comments/1afrju3/how_much_am_i_worth_big_tech_specialty_research/) , 2024-02-18-0910
```
**Face value experience:**  
I have a PhD in mathematical optimization, with expertise in signal processing, with public
ized innovations in both theoretical (ICML) and applied (NeurIPS) machine learning. The latter publication within the la
st 2 years. I have worked in govt contractor-like roles for the last 4-6 years, but for small scientific firms. 1-2 year
s as a Senior Scientist, and next (currently) as a 'Research Professor' (95% applied research, 5% mentoring PhD candidat
es). Intentional vagueness for anonymity. I have never worked in big tech except for a 5 month software engineering inte
rnship (also related to ML).

**The Position:**  
I have reason to believe that a big-tech team is about to call me with
 an offer. It is a very specialized, secretive lab of about 20 people within one of the famous 'big tech' companies (ove
r 80k employees) with a very high reputation in the ML field (top 3 depending who you ask). The lab is trying to build a
 product with sensors that use biometric signals that have never been commercialized before and requires signal-processi
ng-tailored ML (learning invariances at the sensor level). 

At the onsite, I came to really like the team and am convin
ced this is my dream job. The position is 'research scientist' at the technical lead level, so I would be doing research
 but also setting the research direction for 2-3 other. I have experience leading a team of 3 and interfacing with many 
tech leads to deploy real world systems, but not for very long, so this will be a promotion for me. I'm not sure how muc
h they realize that.

**The Issue:**  
From my understanding, it is literally expected that I negotiate. This goes again
st my nature (which is to thank them and say yes), so I'm anxiously searching podcasts and youtube videos for help under
standing what I'm worth and what I should ask for/ how I should ask for it. But I'm not a software engineer or a general
ized data scientist -- it's got this specialization aspect that may or may not be (but seems) important. I do not have a
ny other offers (I'm not even applying anywhere else ATM, a friend of a friend on the team reached out to me about apply
ing).

**Other Random Factors That May Not Matter:**.

*  I have the highest US security clearance an employer could hop
e for a potential employee to have. No clue if this would be of use to the specific team--probably not. Not if anyone in
 the company would care but other big tech famously pay >50k bonuses to maintain the required lifestyle.
* My current sa
lary is between 175k-200k, I get so much PTO that I can hardly spend it all and it never expires, with VERY flexible wor
k arrangement and the best job security you can hope for. In my view the job security and flexibility is comparatively g
oing away (although it will be good for long-term resume) and I doubt I'll get nearly as much time off. In general I hav
e it pretty laid back and I think this will increase my stress levels across the board which should be compensated for.


**TL;DR:**  
How much should I be aiming for? How should I distribute the target compensation across salary, sign on bo
nus, stock, and wahtever else... (Ive never gotten a big tech offer)
```
---

     
 
all -  [ Regret bounds in reinforcement learning ](https://www.reddit.com/r/reinforcementlearning/comments/1aeiexo/regret_bounds_in_reinforcement_learning/) , 2024-02-18-0910
```
I’ve been away from reading theoretical reinforcement learning papers for a couple of years and was getting curious on h
ow the field has progressed since then. Last time I checked, there was a paper that claimed that they closed the upper a
nd lower bounds of the regret in MDP… where a mistake was discovered in the proof. What happened since then?

Edit: I th
ink it was this one (https://proceedings.neurips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html) if s
omeone can point to a follow up paper, I’d really appreciate it!
```
---

     
 
all -  [ [R] Thoughts about ML theory papers in conferences like International Symposium on Information Theor ](https://www.reddit.com/r/MachineLearning/comments/1abwmal/r_thoughts_about_ml_theory_papers_in_conferences/) , 2024-02-18-0910
```
I have published a few papers in conferences like the International Symposium on Information Theory (ISIT) and Allerton.
 However, when I apply for internship positions, the applications sometimes ask about the number of published papers in 
conferences like Neurips, ICML, ICLR, etc.

Although by any standards, my research papers are 'good' (at least in my opi
nion). However, I feel that I'm not targeting the right conferences. My advisor has also published a lot in these confer
ences, and I would say s/he likes to 'play safe' and avoids taking any risks at these big venues.
```
---

     
 
all -  [ Acceptance rate of workshops in conferences [D] ](https://www.reddit.com/r/MachineLearning/comments/19do6qn/acceptance_rate_of_workshops_in_conferences_d/) , 2024-02-18-0910
```
From the Internet I easily found the acceptance rate of conferences but what is the acceptance rate of workshops conduct
ed in conferences like AISTATS/CVPR/Neurips/ICML? 
```
---

     
 
all -  [ What Bodies Think About: Bioelectric Computation Outside the Nervous System - NeurIPS 2018 ](https://youtu.be/RjD1aLm4Thg?si=j1-jVO--H2lGHaUf) , 2024-02-18-0910
```
One of the best lectures I’ve ever watched! This might sound boring because it’s presented that way, but this has the po
tential to to enlighten you!
```
---

     
 
all -  [ I read through the NeurIPS 2023 Abstracts and wrote about it ](https://alexzhang13.github.io/blog/2024/neurips2023) , 2024-02-18-0910
```
I made this resource that I think might be quite useful here, especially for those looking to find some new, relevant wo
rks to read or use for their own projects. It discusses the content from roughly 300 papers, but the topics broadly pert
ain to all of NeurIPS 2023. Happy reading!
```
---

     
 
all -  [ Advancements in machine learning for machine learning ](https://www.reddit.com/r/worldTechnology/comments/19c2sch/advancements_in_machine_learning_for_machine/) , 2024-02-18-0910
```
With the recent and accelerated advances in machine learning (ML), machines can understand natural language, engage in c
onversations, draw images, create videos and more. Modern ML models are programmed and trained using ML programming fram
eworks, such as TensorFlow, JAX, PyTorch, among many others. These libraries provide high-level instructions to ML pract
itioners, such as linear algebra operations (e.g., matrix multiplication, convolution, etc.) and neural network layers (
e.g., 2D convolution layers, transformer layers). Importantly, practitioners need not worry about how to make their mode
ls run efficiently on hardware because an ML framework will automatically optimize the user's model through an underlyin
g compiler. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on
 heuristics to solve complex optimization problems, often resulting in suboptimal performance.

In this blog post, we pr
esent exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Pr
ior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting 
better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small 
sub-programs, such as basic blocks or kernels. We introduce “TpuGraphs: A Performance Prediction Dataset on Large Tensor
 Computational Graphs” (presented at NeurIPS 2023), which we recently released to fuel more research in ML for program o
ptimization. We hosted a Kaggle competition on the dataset, which recently completed with 792 participants on 616 teams 
from 66 countries. Furthermore, in “Learning Large Graph Property Prediction via Graph Segment Training”, we cover a nov
el method to scale graph neural network (GNN) training to handle large programs represented as graphs. The technique bot
h enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the 
model.

# ML compilers

ML compilers are software routines that convert user-written programs (here, mathematical instru
ctions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML 
program can be represented as a computation graph, where a node represents a tensor operation (such as matrix multiplica
tion), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimiz
ation problems, including graph-level and kernel-level optimizations. A graph-level optimization requires the context of
 the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization tra
nsforms one kernel (a fused subgraph) at a time, independently of other kernels.

&#x200B;

[ Important optimizations in
 ML compilers include graph-level and kernel-level optimizations. ](https://preview.redd.it/n6hs3zdyhsdc1.png?width=1999
&format=png&auto=webp&s=afa7e9a80f5d73c94c1692eb45612f51d7bdfe11)

To provide a concrete example, imagine a matrix (2D t
ensor):

&#x200B;

[matrix](https://preview.redd.it/q7m0npe3isdc1.png?width=1999&format=png&auto=webp&s=56382ec9b2462d71
e48efcd3fc38405219b046c2)

It can be stored in computer memory as \[A B C a b c\] or \[A a B b C c\], known as row- and 
column-major memory layout, respectively. One important ML compiler optimization is to assign memory layouts to all inte
rmediate tensors in the program. The figure below shows two different layout configurations for the same program. Let’s 
assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual opera
tor. However, this layout configuration requires the compiler to insert a copy operation to transform the memory layout 
between the add and convolution operations. On the other hand, the right-hand side configuration might be less efficient
 for each individual operator, but it doesn’t require the additional memory transformation. The layout assignment optimi
zation has to trade off between local computation efficiency and layout transformation overhead.

&#x200B;

![img](2r9mj
oz7isdc1 ' A node represents a tensor operator, annotated with its output tensor shape [n0, n1, ...], where ni is the si
ze of dimension i. Layout {d0, d1, ...} represents minor-to-major ordering in memory. Applied configurations are highlig
hted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inp
uts and outputs of influential operators (i.e., convolution and reshape). A copy operator is inserted when there is a la
yout mismatch.
 ')

If the compiler makes optimal choices, significant speedups can be made. For example, we have seen u
p to a 32% speedup when choosing an optimal layout configuration over the default compiler’s configuration in the XLA be
nchmark suite.

# TpuGraphs dataset

Given the above, we aim to improve ML model efficiency by improving the ML compiler
. Specifically, it can be very effective to equip the compiler with a learned cost model that takes in an input program 
and compiler configuration and then outputs the predicted runtime of the program.

&#x200B;

With this motivation, we re
lease TpuGraphs, a dataset for learning cost models for programs running on Google’s custom Tensor Processing Units (TPU
s). The dataset targets two XLA compiler configurations: layout (generalization of row- and column-major ordering, from 
matrices, to higher dimension tensors) and tiling (configurations of tile sizes). We provide download instructions and s
tarter code on the TpuGraphs GitHub. Each example in the dataset contains a computational graph of an ML workload, a com
pilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the data
set are collected from open-source ML programs, featuring popular model architectures, e.g., ResNet, EfficientNet, Mask 
R-CNN, and Transformer. The dataset provides 25× more graphs than the largest (earlier) graph property prediction datase
t (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction da
tasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction ta
sk on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality.

&#x200B
;

[ Scale of TpuGraphs compared to other graph property prediction datasets. ](https://preview.redd.it/ebfs36lcisdc1.pn
g?width=2868&format=png&auto=webp&s=0e6e2e1f39ebd839df0cb979e4ad2c142b7e676b)

 We provide baseline learned cost models 
with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represent
ed as a graph. Node features, shown in blue below, consist of two parts. The first part is an *opcode id*, the most impo
rtant information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id t
o an *opcode embedding* via an embedding lookup table. The opcode embedding is then concatenated with the second part, t
he rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed
-size embedding of the graph using a simple graph pooling reduction (i.e., sum and mean). The resulting graph embedding 
is then linearly transformed into the final scalar output by a feedforward layer. 

&#x200B;

[ Our baseline learned cos
t model employs a GNN since programs can be naturally represented as graphs. ](https://preview.redd.it/jdcalhjgisdc1.png
?width=2284&format=png&auto=webp&s=f253742e4262ad004fa7a6bc6b3dea31c15c9d5c)

Furthermore we present Graph Segment Train
ing (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases wh
ere the prediction task is on the entire-graph (i.e., graph-level prediction). Unlike scaling training for node- or edge
-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs c
an contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN m
odel is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large
 graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a 
random subset of segments is selected to update the model; embeddings for the remaining segments are produced without sa
ving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to gen
erate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the histor
ical embedding table to efficiently obtain graph segments’ embeddings and segment dropout to mitigate the staleness from
 historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×.

&#x200B;

[ Compari
ng Full Graph Training \(typical method\) vs Graph Segment Training \(our proposed method\). ](https://preview.redd.it/l
us2m3ikisdc1.png?width=790&format=png&auto=webp&s=eb8ad9e466ef062ef171b24f409b6c4eea2c5346)

&#x200B;

[Advancements in 
machine learning for machine learning](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html)
```
---

     
