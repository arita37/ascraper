 
all -  [ Stanford MS/CS 2024 - REJECTED ](https://www.reddit.com/r/gradadmissions/comments/1bhwguc/stanford_mscs_2024_rejected/) , 2024-03-19-0910
```
I unfortunately got denied from Online Stanford MS in CS Today. I was fairly confident in my application. I had 4.00/4.0
0 GPA (ranked 1st in my graduation class from decent US college), ML Engineer at FAANG, published a paper in NeurIPS. My
 references include Dean of Engineering, My college Prof (Oxford grad) and my manager from work - MIT (PhD). I had alrea
dy got rejected 2 years ago, since then I made a publication switched to an ML oriented role and tailored my referrals a
ccordingly. It makes sense that a lot of people flooded in to AI track this year but I really want to get a sense of oth
er profiles who got admitted this year.

Maybe next year I will try again :/
```
---

     
 
all -  [ [D] papers that only evaluate on cifar10 ](https://www.reddit.com/r/MachineLearning/comments/1bhp54a/d_papers_that_only_evaluate_on_cifar10/) , 2024-03-19-0910
```
Hi everyone!

While reviewing for NeurIPS 2024, one of the things I keep noticing is that a lot of papers only evaluate 
on datasets of very small size, like Cifar-10. his feels weird to me: I consider Cifar10 to be a toy-dataset and testbed
 for my methods, not something I'd use to show that my method actually works/is relevant in practice. So my first intuit
ion is always 'this approach probably does not scale to larger datasets'. I mean, ImageNet is 12 years old now, and I've
 personally been giving results on imagenet for my papers since ~8 years. most computer vision applications I know requi
re larger resolutions than just 32x32. it's also my impression that almost all of the 'good' papers I read have results 
on larger scale data. But given how often I encounter this situation, I have to wonder: am  I just working in a very pri
vileged environment, or are the manuscript-authors just lazy? How much faith do you have in papers that only evaluate on
 MNIST and CIFAR10?
```
---

     
 
all -  [ Unveiling the Complexity of Visual Recognition: MIT's Study Challenges AI Perception ](https://www.reddit.com/r/AIforBiz/comments/1bhnhu7/unveiling_the_complexity_of_visual_recognition/) , 2024-03-19-0910
```
In a groundbreaking study from the Massachusetts Institute of Technology's Computer Science and Artificial Intelligence 
Laboratory ([MIT CSAIL](https://www.csail.mit.edu/node/2873)), researchers have introduced a novel perspective on the ca
pabilities and limitations of artificial intelligence (AI) in image recognition. This research, spearheaded by MIT PhD s
tudent David Mayo and his team, explores the intricacies of how AI systems perceive and interpret visual data, challengi
ng our preconceived notions about machine vision and its comparison to human perceptual abilities.

&#x200B;

Introducin
g Minimum Viewing Time (MVT)

At the heart of this study is the development of a new metric termed 'Minimum Viewing Time
' (MVT), designed to assess the complexity of image recognition tasks. MVT measures the time required for humans to accu
rately identify an image, offering insights into the challenges faced by AI in processing complex visual information. Th
is metric serves as a bridge to understand the gap between human and machine vision, particularly in recognizing images 
that demand more extended viewing times for accurate identification.

&#x200B;

https://preview.redd.it/5568ybcsemnc1.pn
g?width=1024&format=png&auto=webp&s=8f087df66da0acc8ecce87614a16417d31355a3d

The research reveals that while AI models 
exhibit high performance on standard datasets, they falter when confronted with images that are challenging for humans, 
indicating significant disparities in AI's ability to process complex visual stimuli. By comparing AI's performance on b
oth straightforward and intricate images, the MIT CSAIL team has illuminated the limitations of current AI models in mir
roring human-like perception.

&#x200B;

Bridging AI and Human Vision

The implications of this study extend far beyond 
academic discourse, bearing significant potential for enhancing AI applications in critical fields such as healthcare. I
n areas where precise interpretation of medical images is paramount, advancing AI's visual recognition capabilities coul
d lead to breakthroughs in diagnostic accuracy and patient care. By delving into the neurological foundations of visual 
recognition, the research aims to unravel the mechanisms through which our brains efficiently decode the visual world, p
aving the way for AI systems that more closely resemble human perceptual processes.

&#x200B;

https://preview.redd.it/b
g1at8ceemnc1.png?width=1125&format=png&auto=webp&s=b3f5cb669e7c060caaa7c2c37d1d07fe52d3147f

This endeavor is not merely
 a scientific milestone but a crucial step toward developing more sophisticated and dependable AI vision systems capable
 of navigating the complexities inherent in real-world visual recognition tasks. The study challenges the prevailing bel
ief that AI systems have outstripped human vision, highlighting the necessity for more realistic benchmarks and equitabl
e comparisons between machine and human perception.

&#x200B;

A Call to Action for the AI Community

As the findings ar
e presented at the 2023 Conference on Neural Information Processing Systems (NeurIPS), the study serves as a clarion cal
l to the AI research community. It urges a reevaluation of the methodologies employed in training and testing AI models,
 advocating for approaches that ensure AI systems are not merely proficient in executing simple tasks but are genuinely 
equipped to tackle the subtleties and intricacies of real-world visual understanding.

&#x200B;

This research opens new
 pathways for AI exploration, emphasizing the importance of preparing AI systems for the complexities of visual recognit
ion they will encounter outside laboratory settings. By fostering a more nuanced understanding of the parallels and dive
rgences between AI and human vision, the study lays the groundwork for future innovations in AI technology.

&#x200B;

T
he Path Forward: Realizing Human-like AI Vision Systems

The MIT CSAIL team's research underscores a critical juncture i
n AI development, highlighting the need for AI systems that can process visual information with the same depth and nuanc
e as human observers. Achieving this level of sophistication in AI vision requires a concerted effort to develop algorit
hms that not only mimic but also understand the underlying principles of human perception.

&#x200B;

To bridge the gap 
between AI and human vision, future research must focus on integrating insights from neuroscience and cognitive psycholo
gy into AI models. This interdisciplinary approach will be instrumental in creating AI systems that can interpret the wo
rld around them as effectively as humans do, with the potential to revolutionize industries reliant on visual recognitio
n, from autonomous vehicles to environmental monitoring.

&#x200B;

Moreover, this study emphasizes the ethical consider
ations in AI development, advocating for transparency, fairness, and accountability in AI vision systems. As AI continue
s to evolve, ensuring that these technologies serve the greater good while respecting human values and rights remains pa
ramount.

&#x200B;

Embracing the Complexity of Visual Recognition

The MIT CSAIL team's study is a testament to the ong
oing journey of discovery and innovation in AI research. By challenging existing perceptions of AI's capabilities in vis
ual recognition, the research not only sheds light on the current limitations of AI models but also charts a course for 
future advancements that more closely align with human perceptual abilities.

&#x200B;

As we look to the future, the qu
est to develop AI systems with human-like vision remains a formidable challenge, yet one that holds immense promise for 
transforming our interaction with technology and the world. This study not only contributes to our understanding of AI a
nd human vision but also inspires a renewed commitment to pushing the boundaries of what AI can achieve, ensuring that t
he next generation of AI systems is ready to meet the complexities of the visual world head-on.

&#x200B;

The groundbre
aking findings from MIT's CSAIL team on the Minimum Viewing Time (MVT) for image recognition have profound implications 
for the business world, particularly in sectors where visual data plays a crucial role. For industries such as retail, s
ecurity, and automotive, where accurate and swift image recognition is paramount, the insights into AI's visual processi
ng challenges can drive the development of more advanced AI models. These enhanced models can significantly improve the 
efficiency and reliability of tasks like inventory management through visual stock tracking, facial recognition systems 
for enhanced security measures, and autonomous vehicle navigation systems that better mimic human perceptual processes. 
By understanding and narrowing the gap between AI and human vision, [Use of AI in businesses](https://neuralpit.com) to 
not only streamline operations but also open new avenues for innovation and service enhancement, ultimately leading to a
 competitive edge in the rapidly evolving digital marketplace.
```
---

     
 
all -  [ Towards General-Purpose In-Context Learning Agents ](https://www.reddit.com/r/AcceleratingAI/comments/1bfbbvj/towards_generalpurpose_incontext_learning_agents/) , 2024-03-19-0910
```
**Paper**: [https://openreview.net/forum?id=eDZJTdUsfe](https://openreview.net/forum?id=eDZJTdUsfe)

**Talk and slides**
: [https://neurips.cc/virtual/2023/79880](https://neurips.cc/virtual/2023/79880)

**Blog post**: [http://louiskirsch.com
/glas](http://louiskirsch.com/glas)

**Abstract**:

>Reinforcement Learning (RL) algorithms are usually hand-crafted, dr
iven  by the research and engineering of humans. An alternative approach is to  automate this research process via meta-
learning. A particularly  ambitious objective is to automatically discover new RL algorithms from  scratch that use in-c
ontext learning to learn-how-to-learn entirely from  data while also generalizing to a wide range of environments. Those
 RL  algorithms are implemented entirely in neural networks, by conditioning  on previous experience from the environmen
t, without any explicit  optimization-based routine at meta-test time. To achieve generalization,  this requires a broad
 task distribution of diverse and challenging  environments. Our Transformer-based **Generally Learning Agents** (**GLAs
**) are  an important first step in this direction. Our GLAs are meta-trained  using supervised learning techniques on a
n offline dataset with  experiences from RL environments that is augmented with random  projections to generate task div
ersity. During meta-testing our agents  perform in-context meta-RL on entirely different robotic control  problems such 
as Reacher, Cartpole, or HalfCheetah that were not in the  meta-training distribution.
```
---

     
 
all -  [ 2 yoe in research labs only in computer vision. What should be my next steps? ](https://www.reddit.com/r/cscareerquestionsEU/comments/1bdslud/2_yoe_in_research_labs_only_in_computer_vision/) , 2024-03-19-0910
```
Hi,
Based in France. I have a masters where I did 2 internships in a good research lab and then started working as resea
rch engineer in a public lab for ~2 years. They only offer temporary contracts and there is a limit to how long you can 
work with these temporary contracts in France (you can try for the few research scientists positions but jts if you have
 phd/post doc plus good publiching track and experience, so i need to move on).
I didn't go for a phd and don't want to 
now since I felt I didn't have the right motivation and didn't want to do it for the sake of it. Didn't see myself as a 
researcher either publishing all my life. Research Engineer is a sweet spot.

From internships/job, I've so far co-autho
red 4 papers in computer vision conferences (neurips, iccv, cvpr, miccai) in the topics of 3d reconstruction and human m
odeling. But that's pretty much the achievement which is probably meaningless for jobs in the industry. And thing is, I 
feel my skill stack being also limited to research workflow might be little interesting to the industry (practical exper
ience with only python vs c++ for example). 

And i feel i should switch to industry (instead of another contract in a r
esearch lab) since honestly, even though these papers might seem nothing, they take a lot of effort and work and the who
le publishing cycle drains you mentally. And the compensation you get is really low I feel in these public labs (32k bru
t).

When I look around for jobs, I see mostly poorly defined descriptions but titled data scientist or mainly MLops. No
thing specifically related to what ive done being needed. I have 7 months left on my contract. What should I start learn
ing/doing or what could be an ideal focus for job search with such profile? Id like to stay in France and even in the sa
me city near lyon since im a foreigner and i feel more settled due to having made friends and understanding the language
. Your insights would be helpful. I feel very lost.
```
---

     
 
all -  [ ML Internships aren't supposed to be this difficult to get - Rant ](https://www.reddit.com/r/cscareerquestions/comments/1bd940o/ml_internships_arent_supposed_to_be_this/) , 2024-03-19-0910
```
As an international master's student, I'm on the verge of quitting my search as my internship hunt for the past 8 months
 has practically given 0 offers. I've applied to over 600 postings (every single ML/DS posting that has come). I did get
 3-4 callbacks but they never converted to offers despite the interviews going well. All I can say is that the competiti
on is unbelievably high, and the companies seem to be taking just 1 intern per team or none. Every position I am interes
ted in requires a Ph.D. candidate or papers in conferences like ICML, NeurIPS, or CVPR on the exact niche area the team 
works on. When there's a lack of jobs and an abundant supply of candidates, they can get choosy. But these are internshi
ps we are talking about, things meant to be entry-level positions.

Is it actually supposed to be this difficult? The wo
rst part is that all the callbacks I got so far are through direct contact with the hiring manager. None of the regular 
applications(even with referrals) gave me anything. As a final plea, if any of my fellow Redditors here have leads I'd r
eally appreciate some help.
```
---

     
 
all -  [ America: The Country With The Most Innovation ](https://i.redd.it/mbye6uerusnc1.jpeg) , 2024-03-19-0910
```

```
---

     
 
all -  [ People with no top-tier ML papers, where are you working at? ](https://www.reddit.com/r/reinforcementlearning/comments/1b1suv1/people_with_no_toptier_ml_papers_where_are_you/) , 2024-03-19-0910
```
I am graduating soon, and my Ph.D. research is about RL algorithms and their applications.  
However, I failed to publis
h papers in top-tier ML conferences (NeurIPS, ICLR, ICML).   
But with several papers in my domain, how can I get hired 
for an RL-related job?  
I have interviewed a handful of mobile and e-commerce (RecSys) companies, all failed.  


I don
't want to do a postdoc and I am not interested in anything related to academia.   


Please let me know if there are an
y opportunities in startups, or other positions I have not explored yet.
```
---

     
 
all -  [ Postdoc requirements ](https://www.reddit.com/r/PhD/comments/1b1i5vr/postdoc_requirements/) , 2024-03-19-0910
```
Hi colleagues,

In some of the postdoc or industrial research scientist positions i see requirements like this:  


* St
rong publications record at top tier venues (CVPR, ICCV, ECCV, Siggraph, etc.)
* We are looking for candidates with a st
rong track record at top-tier computer vision and machine learning venues, such as CVPR, ICCV, ECCV, and NeurIPS.
* Firs
t-authored publications at peer-reviewed conferences (e.g. CVPR. ECCV, ICCV, NeurIPS, and SIGGRAPH).

In case one joins 
robotics related PhD on vision and perception and would focus on publications on IROS, ICRA(these 2 are top robotics con
ference, which have vision section), will these publications be considered for above mentioned positions? Or only the me
ntioned top-tier computer vision only conferences are accepted?  
I do understand that robotics conferences are a little
 less competitive, but still they are top-tier and peer-reviewed. 
```
---

     
 
all -  [ Help! ](https://www.reddit.com/gallery/1az9fpc) , 2024-03-19-0910
```
I have stubborn texture and always get pimples on my chin area that just won’t go away anyone have tips or products that
 they use?TIA included what I use now thank you!!
```
---

     
 
all -  [ [R] 'Generative Models: What do they know? Do they know things? Let's find out!'. Quote from paper:  ](https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/) , 2024-03-19-0910
```
[Paper](https://arxiv.org/abs/2311.17137). [Project website](https://intrinsic-lora.github.io/). I am not affiliated wit
h the authors.

Abstract:

>Generative models have been shown to be capable of synthesizing highly detailed and realisti
c images. It is natural to suspect that they implicitly learn to model some image intrinsics such as surface normals, de
pth, or shadows. In this paper, we present compelling evidence that generative models indeed internally produce high-qua
lity scene intrinsic maps. We introduce Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms any
 generative model into a scene intrinsic predictor, capable of extracting intrinsic scene maps directly from the origina
l generator network without needing additional decoders or fully fine-tuning the original network. Our method employs a 
Low-Rank Adaptation (LoRA) of key feature maps, with newly learned parameters that make up less than 0.6% of the total p
arameters in the generative model. Optimized with a small set of labeled images, our model-agnostic approach adapts to v
arious generative architectures, including Diffusion models, GANs, and Autoregressive models. We show that the scene int
rinsic maps produced by our method compare well with, and in some cases surpass those generated by leading supervised te
chniques.

A figure from the paper:

https://preview.redd.it/uid7hrhcmckc1.jpg?width=722&format=pjpg&auto=webp&s=db5b3a9
9a80d229f48c78d63449445800769f3e3

Quotes from the paper:

>In this paper, our goal is to understand the underlying know
ledge present in all types of generative models. We employ Low-Rank Adaptation (LoRA) as a unified approach to extract s
cene intrinsic maps — namely, normals, depth, albedo, and shading — from different types of generative  models. Our meth
od, which we have named as INTRINSIC LORA (I-LORA), is general and applicable to diffusion-based models, StyleGAN-based 
models, and autoregressive generative models. Importantly, the additional weight parameters introduced by LoRA constitut
e less than 0.6% of the total weights of the pretrained generative model, serving as a form of feature modulation that e
nables easier extraction of latent scene intrinsics. By altering these minimal parameters and using as few as 250 labele
d images, we successfully extract these scene intrinsics.  
>  
>Why is this an important question? Our motivation is th
ree-fold. First, it is scientifically interesting to understand whether the increasingly realistic generations of large-
scale text-to-image models are correlated with a better understanding of the physical world, emerging purely from applyi
ng a generative objective on a large scale. Second, rooted in the saying 'vision is inverse graphics' – if these models 
capture scene intrinsics when generating images, we may want to leverage them for (real) image understanding. Finally, a
nalysis of what current models do or do not capture may lead to further improvements in their quality.

&#x200B;

>For s
urface normals, the images highlight the models’ ability to infer surface orientations and contours. The depth maps disp
lay the perceived distances within the images, with warmer colors indicating closer objects and cooler colors representi
ng further ones. Albedo maps isolate the intrinsic colors of the subjects, removing the influence of lighting and shadow
. Finally, the shading maps capture the interplay of light and surface, showing how light affects the appearance of diff
erent facial features.

&#x200B;

>We find consistent, compelling evidence that generative models implicitly learn physi
cal scene intrinsics, allowing tiny LoRA adaptors to extract this information with minimal fine-tuning on labeled data. 
More powerful generative models produce more accurate scene intrinsics, strengthening our hypothesis that learning this 
information is a natural byproduct of learning to generate images well. Finally, across various generative models and th
e self-supervised DINOv2, scene intrinsics exist in their encodings resonating with fundamental 'scene characteristics' 
as defined by Barrow and Tenenbaum.

[Twitter thread about paper from one of the authors](https://twitter.com/anand_bhat
tad/status/1730230190159135175).

From paper [StyleGAN knows Normal, Depth, Albedo, and More](https://arxiv.org/abs/2306
.00987) ([newer version PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/e7407ab5e89c405d28ff6807ffec594a
-Paper-Conference.pdf)) ([Twitter thread about paper)](https://twitter.com/anand_bhattad/status/1664798414318518274):

>
Barrow and Tenenbaum, in an immensely influential paper of 1978, defined the term 'intrinsic image' as 'characteristics 
– such as range, orientation, reflectance and incident illumination – of the surface element visible at each point of th
e image'. Maps of such properties as (at least) depth, normal, albedo, and shading form different types of intrinsic ima
ges. The importance of the idea is recognized in computer vision – where one attempts to recover intrinsics from images 
– and in computer graphics – where these and other properties are used to generate images using models rooted in physics
.

The 1978 paper mentioned in the previous paragraph: [Recovering intrinsic scene characteristics](https://www.sri.com/
publication/computer-vision-pubs/2d-3d-reasoning-and-augmented-reality-pubs/recovering-intrinsic-scene-characteristics-f
rom-images/):

>Abstract  
>  
>We suggest that an appropriate role of early visual processing is to describe a scene in
 terms of intrinsic (veridical) characteristics – such as range, orientation, reflectance, and incident illumination – o
f the surface element visible at each point in the image. Support for this idea comes from three sources: the obvious ut
ility of intrinsic characteristics for higher-level scene analysis; the apparent ability of humans, to determine these c
haracteristics, regardless of viewing conditions or familiarity with the scene, and a theoretical argument, that such a 
description is obtainable, by a non-cognitive and non-purposive process, at least, for simple scene domains. The central
 problem in recovering intrinsic scene characteristics is that the information is confounded in the original light-inten
sity image: a single intensity value encodes all of the characteristics of the corresponding scene point. Recovery depen
ds on exploiting constraints, derived from assumptions about the nature of the scene and the physics of the imaging proc
ess.

Language model GPT-4 Turbo explained normals, depth, albedo, and shading as follows:

>Normals: Imagine you have a
 smooth rubber ball with little arrows sticking out of it, pointing directly away from the surface. Each one of these li
ttle arrows is called a “normal.” In the world of 3D graphics and images, normals are used to describe how surfaces are 
oriented in relation to a light source. Knowing which way these arrows (normals) point tells the computer how light shou
ld hit objects and how it will make them look—whether shiny, flat, bumpy, etc.  
>  
>Depth: When you look at a scene, t
hings that are close to you seem larger and more detailed, and things far away seem smaller and less clear. Depth is all
 about how far away objects are from the viewpoint (like from a camera or your eyes). When computers understand depth, t
hey can create a 3D effect, make things look more realistic, and know which objects are in front of or behind others.  

>  
>Albedo: Have you ever painted a room in your house? Before the colorful paint goes on, there’s a base coat, usually
 white or gray. This base coat is sort of what albedo is about. It’s the basic, true color of a surface without any tric
ks of light or shadow messing with it. When looking at an apple, you know it’s red, right? That red color, regardless of
 whether you’re looking at it in bright sunshine or under a dim light, is the apple’s albedo.  
>  
>Shading: Think abou
t drawing a picture of a ball and then coloring it in to make it look real. You would darken one side to show that it’s 
farther from the light, and lighten the other side where the light shines on it. This play with light and dark, with dif
ferent tones, is what gives the ball a rounded, 3-dimensional look on the paper. Shading in images helps show how light 
and shadows fall on the surfaces of objects, giving them depth and shape so they don’t look flat.  
>  
>So, in the pape
r, the challenge they were addressing was how to get a computer to figure out these aspects—normals, depth, albedo, and 
shading—from a 2D image, which would help it understand a scene in 3D, much like the way we see the world with our own e
yes.
```
---

     
 
all -  [ Amorphous Fortress Online ](https://www.reddit.com/r/alife/comments/1axeoqq/amorphous_fortress_online/) , 2024-03-19-0910
```
Hi everyone!

I'd like to introduce a research project my team and I have been working on that's inspired by the Sims an
d Dwarf Fortress: [Amorphous Fortress Online](http://amorphous-fortress.xyz/index.php). It's an open-ended multi-agent s
imulation / game engine where you can design FSM-based AI that interact with each other in a small environment.

It's st
ill a work in development and the site has a user guide to help you get familiar with the interface and a feedback form 
to leave comments and report bugs. So far, we've published some research papers at a [ALIFE 2023](https://arxiv.org/pdf/
2306.13169.pdf) workshop and in a [NeurIPS 2023 workshop](https://arxiv.org/pdf/2312.02231.pdf) based on our Python vers
ion of the [engine](https://github.com/dipikarajesh18/amorphous-fortress).

Check out the [promo video](https://www.yout
ube.com/watch?v=ANoQkIgOa6c) and come design some fortresses!

&#x200B;

[Amorphous Fortress Online Promo](https://reddi
t.com/link/1axeoqq/video/4exdgqd9q6kc1/player)
```
---

     
 
all -  [ Is there any trick to help peg-in-hole tasks converge? ](https://www.reddit.com/r/reinforcementlearning/comments/1aw8399/is_there_any_trick_to_help_peginhole_tasks/) , 2024-03-19-0910
```
Hi!

I'm starting with a simple peg-in-hole task but it's hard to converge whether using dense or sparse reward.

For th
e sparse reward, the trick of random goal position is used in this [paper](https://proceedings.neurips.cc/paper_files/pa
per/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html) to help converge. Is there any **smart** trick that has be
en used to help converge for peg-in-hole tasks?

BTW, are there any recommended open-source repos regarding peg-in-hole 
tasks?

Any help would be appreciated! Thanks!
```
---

     
 
all -  [ [HIRING] Research Scholar (Technical Research) @ Centre for the Governance of AI in Oxford, UK (Hybr ](https://www.reddit.com/r/london_forhire/comments/1avlszu/hiring_research_scholar_technical_research_centre/) , 2024-03-19-0910
```
GovAI was founded to help humanity navigate the transition to a world with advanced AI. Our first research agenda, publi
shed in 2018, helped define and shape the nascent field of AI governance. Our team and affiliate community possess exper
tise in a wide variety of domains, including AI regulation, responsible development practices, compute governance, AI co
mpany corporate governance, US-China relations, and AI progress forecasting.  
GovAI researchers — particularly those wo
rking within our Policy Team — have closely advised decision makers in government, industry, and civil society. Our rese
archers have also published in top peer-reviewed journals and conferences, including International Organization, NeurIPS
, and Science. Our alumni have gone on to roles in government, in both the US and UK; top AI companies, including DeepMi
nd, OpenAI, and Anthropic; top think tanks, including the Centre for Security and Emerging Technology and RAND; and top 
universities, including the University of Oxford and the University of Cambridge.  
Although we are based in Oxford, Uni
ted Kingdom — and currently have an especially large UK policy focus — we also have team members in the United States an
d European Union.  
‍  
About the Role  
Research Scholar is a one-year visiting position. It is designed to support the
 career development of AI governance researchers and practitioners — as well as to offer them an opportunity to do high-
impact work.  
As a Research Scholar, you will have freedom to pursue a wide range of styles of work. This could include
 conducting policy research, social science research, or technical research; engaging with and advising policymakers; or
 launching and managing applied projects.   
For example, past and present Scholars have used the role to:  
produce an 
influential report on the benefits and risks of open-source AI;  
conduct technical research into questions that bear on
 compute governance;  
take part in the UK policy-making process as a part-time secondee in the UK government; and  
lau
nch a new organisation to facilitate international AI governance dialogues.  
Over the course of the year, you will also
 deepen your understanding of the field, connect with a network of experts, and build your skills and professional profi
le, all while working within an institutional home that offers both flexibility and support.  
You will receive research
 supervision from a member of the GovAI team or network. The frequency of supervisor meetings and feedback will vary dep
ending on supervisor availability, although once-a-week or once-every-two-weeks supervision meetings are typical. There 
will also be a number of additional opportunities for Research Scholars to receive feedback, including internal work-in-
progress seminars. You will receive further support from Emma Bluemke, GovAI's Research Manager.  
Some Research Scholar
s may also — depending on the focus of their work — take part in GovAI’s Policy Team, which is led by Markus Anderljung.
 Members of the GovAI Policy Team do an especially large amount of policy engagement and coordinate their work more subs
tantially. They also have additional team meetings and retreats. While Policy Team members retain significant freedom to
 choose projects, there is also an expectation that a meaningful portion of their work will fit into the team’s joint pr
iorities.

**Read more / apply:** [**https://ai-jobs.net/job/139016-research-scholar-technical-research/**](https://ai-j
obs.net/job/139016-research-scholar-technical-research/)

&#x200B;
```
---

     
 
all -  [ Do a Master Thesis that can into NeurIPS? ](https://www.reddit.com/r/careerguidance/comments/1avj9ch/do_a_master_thesis_that_can_into_neurips/) , 2024-03-19-0910
```
I'm in the process of deciding on my masters thesis in Data Science. The professor that I have been communicating with o
ffered me to join a team that would submit their research to NeurIPS. On the other hand, the topic of the research does 
not perfectly align with my interests (i.e., Quantitative Finance).

Should I pursue a thesis that would put a NeurIPS c
onference in my CV, or do a topic that is more related to my field of interest? Would like to know people's opinions and
 experiences, thanks!
```
---

     
 
all -  [ What conferences do you attend? Or wish you could? ](https://www.reddit.com/r/cscareerquestions/comments/1ati510/what_conferences_do_you_attend_or_wish_you_could/) , 2024-03-19-0910
```
I didn’t have the opportunity to attend many conferences during my undergrad because they were usually expensive and req
uired traveling. But I’ve recently started working as a backend SWE and definitely want to start attending more conferen
ces this year to learn more and (if in-person) expand my professional network.

I’m trying to create a list across all c
ategories, like general engineering (e.g. IEEE), languages (cppcon), tools/frameworks (Apache, Confluent KafkaCon, Cassa
ndraCon), ML (NeurIPS, CVPR, ICML), etc. But it’s sort of a “you don’t know what you don’t know” situation of not knowin
g if I’m missing major conferences within a category (or even missing categories as a whole).

So, I just wanted to hear
 from people which ones they’ve attended/would like to attend (regardless of location/virtual or not)
```
---

     
 
all -  [ Antis are right: image generative AI is fundamentally limited ](https://www.reddit.com/r/aiwars/comments/1at4xj0/antis_are_right_image_generative_ai_is/) , 2024-03-19-0910
```
Since SORA was teased, I've heard a lot of pro-AI folks dragging up old comments from anti-AI folks saying that AI is dy
ing and will always be terrible, and this is entirely fair. But it's important to also remember that some of the critici
sms of generative AI are very true.

First, let me clarify my background. I'm a creative on the side, but my primary car
eer has been in the software industry for the past 35+ years. I first worked with neural networks in the 1980s and thoug
h I've never been on the front-lines of AI development myself, I've worked for AI companies and companies that have done
 AI work. I've had to deal with many issues surrounding AI software for large chunks of my career.

Okay, my credentials
 being out of the way, generative AI as we currently use it is mostly based on two primary things:

1. Transformers^1
2.
 Diffusion models^2

These two advances that were made fairly recently have revolutionized the field, and given us a mas
sive step forward in the state of the art. But it is important to remember how limited these things are.

AI currently h
as no capacity for self-awareness, generating and utilizing long-term memory, emotional empathy or truly self-directed g
oal setting. These are features of human brains that, as far as we are aware, are not merely features of training weight
s in neural networks, and so, while these artificial neural networks can learn in a way very similar to human brains, th
ere is a vast gulf in total capabilities.

I've long speculated that we are 2-3 major breakthroughs (on par with transfo
rmers) away from true parity with human minds, and that that process will probably take between 10 and 50 years. The gen
erative AI that we have today will not be replacing humans because it fundamentally lacks many of the capabilities of hu
mans in the workplace. It can't relate to others socially (which requires empathy); it can't effectively manage anyone (
which requires long-term autonomous planning); it can't maintain a consistent context beyond its token limit (which requ
ires long-term memory); etc.

So when you see claims that AI will just replace artists or 'paper pushers' or any other r
ole, stop and ask yourself if what is being suggested requires these additional capabilities. The ability to create disc
onnected, but very pretty pictures/videos is not the same as being able to engage in the interactive process of creating
 art. Nor is the ability to pass multiple choice or even essay tests sufficient to manage a team of programmers, and the
se capabilities will not magically appear in current AI models any more than the internal combustion engine magically ap
peared in ever-improving wagon wheels.

----

^1 Vaswani, Ashish, et al. 'Attention is all you need.' Advances in neural
 information processing systems 30 (2017). https://arxiv.org/abs/1706.03762
^2 Dhariwal, Prafulla, and Alexander Nichol.
 'Diffusion models beat gans on image synthesis.' Advances in neural information processing systems 34 (2021): 8780-8794
. https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf
```
---

     
