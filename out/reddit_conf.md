 
MachineLearning -  [ [D] Dress code at NEURIPS as a workshop presenter ](https://www.reddit.com/r/MachineLearning/comments/18halvl/d_dress_code_at_neurips_as_a_workshop_presenter/) , 2023-12-15-0910
```
Pretty much what the title says.
```
---

     
 
MachineLearning -  [ [D] Authors in NeurIPS and ICML and similar venues - How advanced is your mathematics background ? ](https://www.reddit.com/r/MachineLearning/comments/18g85lx/d_authors_in_neurips_and_icml_and_similar_venues/) , 2023-12-15-0910
```
Hello,

I'm trying to understand what sort of background preparation in mathematics is sufficient to conduct publishable
 research in top venues.

Is calculus, linear algebra, engineering level probability & statistics and optimization suffi
cient ?

By **sufficient**, I mean the *bare minimum* you must know to get by and *you learn more things as you encounte
r them* or solve problems (**learning while doing**) without having to first prepare requisite background and then dive 
in.


My **main problem** is - at what point do I say let's directly jump into research and figure out things I don't kn
ow on the fly VS first learn the necessary background and then proceed. I'm familiar - not strong but have taken those c
ourses in undergrad but have largely forgotten many important concepts.

The **traditional** approach in mathematics (as
 suggested by MathOverflow folks) involves nothing short of solving textbook exercises in Math for mastery. This is larg
ely impractical/unfeasible and will take forever stalling me from actual research progress.

A **parallel** question is 
also - *how do you guys learn the math you don't know ? Do you actually solve textbook problems like folks from Math dep
artments ? Or just understand high level picture and key central idea without focusing on proofs or a dozen theorems sur
rounding it ?*

Any help from authors from ML venues is greatly appreciated. 

Please post only if you are an ML researc
her or from academia :).

Thank you very much !
```
---

     
 
MachineLearning -  [ [R] Add and Thin: Diffusion for Temporal Point Processes ](https://www.reddit.com/r/MachineLearning/comments/18f53oy/r_add_and_thin_diffusion_for_temporal_point/) , 2023-12-15-0910
```
Paper: [https://arxiv.org/abs/2311.01139](https://arxiv.org/abs/2311.01139) Code: [https://github.com/davecasp/add-thin]
(https://github.com/davecasp/add-thin)

https://preview.redd.it/ydhlwfte9h5c1.jpg?width=2544&format=pjpg&auto=webp&s=1c1
1a4a1f8cab2626e46546d589c35afb5e02fea

Generative diffusion models are all the rage, but it is unclear how they could be
 applied to sequences of varying numbers of events, e.g. tweets, reddit comments or taxi trips. We present a diffusion a
pproach that turns any sequence into a noise sequence, a sample from a homogeneous Poisson process. Conversely, our mode
l transforms such noise sequence samples iteratively into samples from any target data distribution by deleting events t
hat it classifies as noise and proposing new events from a learned (conditional) intensity distribution. Experimentally,
 we were able to demonstrate excellent forecasting performance on various benchmark datasets.

I am looking forward to d
iscuss our method here on reddit. If you happen to be at NeurIPS, you can also meet /u/davidluedke at poster #602 on Tue
sday 10:45am-12:45pm!
```
---

     
 
MachineLearning -  [ [N] Full-report of 2 great NeurIPS papers ](https://www.reddit.com/r/MachineLearning/comments/18eejuc/n_fullreport_of_2_great_neurips_papers/) , 2023-12-15-0910
```
2 great NeurIPS accepted papers here, on pages 2 and 32:

[https://www.rsipvision.com/ComputerVisionNews-2023December/2/
](https://www.rsipvision.com/ComputerVisionNews-2023December/2/)

Enjoy!

https://preview.redd.it/x4xk78kv2a5c1.png?widt
h=970&format=png&auto=webp&s=79bb0f481765d86e54d218bb6f601d906b5f42de
```
---

     
 
MachineLearning -  [ [D] A genuine and honest discussion on Collusion Ring(s) ](https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/) , 2023-12-15-0910
```
Dear fellow NeurIPS rejects. As your deep learning, reinforcement learning, graph neural networks, and deep learning the
ory people fly off to New Orleans and you realize that you are left behind.

&#x200B;

I invite you to join me in this g
roup therapy discussion, where our topic of the day is Collusion Rings.

&#x200B;

I suppose that.... the first question
 is do they actually exist, and what is the extent of their penetration into the machine learning academic community? As
 someone who struggled many years to have their first paper published, it's my anecdotal evidence that machine learning 
is much more about marching to the beat of the drummer, where the drummer is certainly someone who is a fan of deep lear
ning.

&#x200B;

As someone struggling, still, to get another paper published, my anecdotal observation is the drum beat
ing has gotten even more fierce over the last few years.

&#x200B;

As someone who has had many, many conversations with
 others whom are also marginalized, our anecdotal data pools not quite to a dataset, but a filtration which is not i.i.d
., but certainly indicates actively farming deep learning citations is the better choice for our careers.

&#x200B;

As 
someone currently reviewing for ICLR/AAAI/AISTATS. My anecdotal evidence is the reviewer coordination is with secret han
dshakes, keywords, citations, reference lists, topics, arxiv preprints, and shibboleths.

&#x200B;

I hope you find the 
bravery to share your experience as one who is on the inside looking out, or on the outside looking in.

&#x200B;

As a 
beacon of hope, I remind you to read [The Revolution Hasn't Happened Yet](https://medium.com/@mijordan3/artificial-intel
ligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7) by Michael Jordan.

&#x200B;

As a final question to ponder. Is 
the deep learning collusion ring already collapsing, and will it collapse further?
```
---

     
 
MachineLearning -  [ [D] Place to print poster at NeurIPS 2023 ](https://www.reddit.com/r/MachineLearning/comments/18cn7v1/d_place_to_print_poster_at_neurips_2023/) , 2023-12-15-0910
```
I need to present a poster at NeurIPS 2023. However, I couldn't print it in my university due to some issues. Is there a
 place near the convention center that provides quick poster printing?  TIA
```
---

     
 
MachineLearning -  [ [R] RETVec: Resilient and Efficient Text Vectorizer ](https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/) , 2023-12-15-0910
```
Happy Friday,

Really happy to share that the code and model for RETVec our new SOTA robust text tokenizer for classific
ation is available on Github [here](https://github.com/google-research/retvec/) and the NeurIPS paper [here](https://arx
iv.org/abs/2302.09207).  We also provide native support for TFLite and for the web via a TFJS. Hope you will find it use
ful for your research. If you would like to give it a try we have a get started [notebook](https://github.com/google-res
earch/retvec/blob/main/notebooks/train_retvec_model_tf.ipynb).

Let us know if you have any questions.

&#x200B;
```
---

     
 
MachineLearning -  [ [D] NeurIPS Climbing Club (Gradient Ascent?) ](https://www.reddit.com/r/MachineLearning/comments/187fbdy/d_neurips_climbing_club_gradient_ascent/) , 2023-12-15-0910
```
I'll be attending NeurIPS 2023 this year and won't know anyone. I'm an avid climber and there is a nice bouldering gym c
lose to the conference center - would anyone else fancy a climb over the week? Reply below so we can see how many people
 and I can setup a meetup page or something. I'm a final year PhD from the UK working in video understanding.
```
---

     
 
MachineLearning -  [ [D] NeurIPS 2023 Institutions Ranking ](https://www.reddit.com/gallery/185pdax) , 2023-12-15-0910
```

```
---

     
 
MachineLearning -  [ [R] Rethinking Open'sAI's Q-Learning : Insights from the Award-Winning 'Non-delusional Q-learning' P ](https://www.reddit.com/r/MachineLearning/comments/182bz42/r_rethinking_opensais_qlearning_insights_from_the/) , 2023-12-15-0910
```
OpenAI's approach to Q-Learning has been drawing significant attention recently.

However, there's a fundamental issue i
n the way Q-learning is typically implemented in deep learning and neural network environments. This concern is highligh
ted in the award-winning paper 'Non-delusional Q-learning,' presented at NeurIPS.

The paper suggests a fundamental flaw
 in the blind application of Q-learning updates to deep neural networks. It points out that such updates can create a se
lf-contradictory scenario where improving the network for the current batch of data inadvertently makes it less effectiv
e for other batches. This is akin to a situation in supervised learning where optimizing a network for a specific set of
 data may degrade its performance on other datasets.

For more insights, the full paper can be accessed here: [Non-delus
ional Q-learning Paper](https://papers.nips.cc/paper_files/paper/2018/hash/5fd0245f6c9ddbdf3eff0f505975b6a7-Abstract.htm
l)(Follow up ICML paper: [Practical Non-delusional-Q Learning](https://www.cs.toronto.edu/~cebly/Papers/CONQUR_ICML_2020
_camera_ready.pdf) )

I'm curious about others' views on this topic. What do you think about the implications of these f
indings for the future of Q-learning in deep learning environments?
```
---

     
 
MachineLearning -  [ [D] [NeurIPS] Do I have to buy a ticket to attend? I am a first author of a NeurIPS workshop apper ](https://www.reddit.com/r/MachineLearning/comments/180ant3/d_neurips_do_i_have_to_buy_a_ticket_to_attend_i/) , 2023-12-15-0910
```
I am an undergrad student and I am a first author of a NeurIPS workshop paper accepted this year. Do I have to buy the t
icket for both the main conference and the workshop session to attend? I would like to attend both. Or can I just buy th
e main conference ticket? I am not particularly interested in attending workshop sessions other than for presenting my w
ork
```
---

     
 
MachineLearning -  [ [D] Complimentary NeurIPS passes for reviewers ](https://www.reddit.com/r/MachineLearning/comments/1805b5m/d_complimentary_neurips_passes_for_reviewers/) , 2023-12-15-0910
```
I received an email today that I can claim a free NeurIPS registration, apparently because of my service as a reviewer. 
Did all reviewers get free passes or is this a lottery system and/or based on review quality? I didn’t plan on attending
, but now I’m actually tempted to go. Wish I’d known earlier, flights are pretty expensive now.
```
---

     
 
MachineLearning -  [ [D] How to find academic ML competitions ](https://www.reddit.com/r/MachineLearning/comments/17y2r9u/d_how_to_find_academic_ml_competitions/) , 2023-12-15-0910
```
There are websites like this keeping track of ML conference deadlines like this https://aideadlin.es/?sub=ML,CV,CG,NLP,R
O,SP,DM,AP,KR

Are there any such websites keeping track of ML competitions . Specifically competitions as part of ML co
nferences like Neurips, CVPR and so on.
```
---

     
 
MachineLearning -  [ [R] Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning ](https://www.reddit.com/r/MachineLearning/comments/17x2ovh/r_neural_mmo_20_a_massively_multitask_addition_to/) , 2023-12-15-0910
```
**Paper**: [https://arxiv.org/abs/2311.03736](https://arxiv.org/abs/2311.03736)

**Project page**: [https://neuralmmo.gi
thub.io](https://neuralmmo.github.io)

**GitHub**: [https://github.com/neuralmmo](https://github.com/neuralmmo)

**Abstr
act**:

>Neural MMO 2.0 is a massively multi-agent environment for reinforcement  learning research. The key feature of 
this new version is a flexible  task system that allows users to define a broad range of objectives and  reward signals.
 We challenge researchers to train agents capable of  generalizing to tasks, maps, and opponents never seen during train
ing.  Neural MMO features procedurally generated maps with 128 agents in the  standard setting and support for up to. Ve
rsion 2.0 is a complete  rewrite of its predecessor with three-fold improved performance and  compatibility with CleanRL
. We release the platform as free and  open-source software with comprehensive documentation available at [this http URL
](http://neuralmmo.github.io/)  and an active community Discord. To spark initial research on this new  platform, we are
 concurrently running a competition at NeurIPS 2023.

https://preview.redd.it/8pvecr133t0c1.png?width=901&format=png&aut
o=webp&s=f9bf7f063c1b9950d40ad2a3b04bdc7f0780413f

&#x200B;
```
---

     
 
MachineLearning -  [ [D] Which option — trying to publish papers in top conferences individually or expanding knowledge i ](https://www.reddit.com/r/MachineLearning/comments/17ur25g/d_which_option_trying_to_publish_papers_in_top/) , 2023-12-15-0910
```
I recently began my career as a Machine Learning Engineer, marking my first job in the industry. A few months ago, I com
pleted an MPhil program specializing in Environmental Engineering, with a research focus on applying ML in that field. T
hroughout my job search, I frequently encounter questions about my academic background. Additionally, utilizing ML techn
ologies has become increasingly convenient with the availability of user-friendly APIs.

&#x200B;

To enhance my competi
tiveness in the industry, I am looking to take proactive steps. I have been actively staying updated on ML technologies 
and trends, and now I am considering the following options:

1. As an independent researcher, try to publish papers in t
op conferences in the field like CVPR and NeurIPS.

2. Expand my knowledge in data engineering and MLops and work on end
-to-end personal projects.

&#x200B;

I understand that both of these options require a significant investment of time a
nd energy. Therefore, I wonder which option would be more worthwhile in terms of my time and effort. It is important to 
note that I am interested in working in the industry and do not intend to pursue a career in academia.
```
---

     
