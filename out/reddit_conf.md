 
all -  [ Am I Making the Right Choice? Masters in ML, Research Lab Experience, and Building Things That Matte ](https://www.reddit.com/r/learnmachinelearning/comments/1hgng0o/am_i_making_the_right_choice_masters_in_ml/) , 2024-12-18-0914
```
I‚Äôm currently pursuing my master‚Äôs in machine learning, and I love building things ‚Äî that‚Äôs how I understand concepts be
st. But my first semester hit me with a tough realization: I joined a research lab way too early, and it just wasn‚Äôt the
 right fit for me.

The lab‚Äôs environment felt off. The code was sloppy, results were rushed, and even the smallest net-
positive outcome led to a question: ‚ÄúWhich journal should we target?‚Äù Maybe this is how it works in many research labs ‚Äî
 I don‚Äôt know, this was my first experience. But the emphasis on quick publication, without deeper exploration or clean 
fundamentals, didn‚Äôt sit well with me.

For context, I was working with LLMs. What surprised me is how many papers get p
ublished even when they‚Äôre essentially hacks ‚Äî a lot of prompt engineering, and observations that openly admit ‚ÄúWe don‚Äôt
 know why this works, but it does.‚Äù I respect research, but it started to feel‚Ä¶ unfulfilling. I wasn‚Äôt enjoying it, and 
I had no time to work on projects of my own.

Now I‚Äôm planning to quit the lab. But here‚Äôs where I‚Äôm conflicted: it seem
s like most ML jobs require a strong research profile ‚Äî X papers in NeurIPS, ICML, etc. Part of me wonders if I should s
tick with it, keep my head down, and publish papers just to ‚Äúcheck the box.‚Äù But then I remember why I‚Äôm here in the fir
st place: I genuinely enjoy ML, especially when I‚Äôm building things that matter, not just chasing publications.

To thos
e who‚Äôve been down this road: Am I sabotaging my career prospects by walking away from research so early? Is it better t
o focus on building meaningful projects, even if they don‚Äôt come with a DOI? Or am I missing something about the value o
f sticking it out in the lab?
```
---

     
 
all -  [ Legal Tech‚Äôs Data Dilemma: Trust, Betrayal, and Competition. ](https://www.reddit.com/r/legaltech/comments/1hgmxc4/legal_techs_data_dilemma_trust_betrayal_and/) , 2024-12-18-0914
```
Ilya Sutskever, co-founder of OpenAI, recently highlighted a critical issue at the NeurIPS 2024 conference: the AI indus
try is facing a data scarcity problem, often referred to as 'peak data.' Despite advancements in computing power, the av
ailability of high-quality training data is becoming a bottleneck for AI development. Sutskever emphasized that syntheti
c data, while a potential solution, does not fully address this challenge.

In this landscape, companies promising not t
o mine your data face immense pressure to break that pledge. The competitive advantage of leveraging vast, real-world da
tasets is simply too great to ignore. Discarding millions of dollars‚Äô worth of high-quality data‚Äîdata that could refine 
models, boost performance, and outpace competitors‚Äîis a hard sell for any profit-driven firm.

And here lies the uncomfo
rtable truth: no amount of compliance paperwork, signed audits, or certifications can fully guarantee your data‚Äôs safety
. Unless you examine production code directly, there‚Äôs no way to ensure that your data isn‚Äôt being anonymized and quietl
y used to train systems. Unlike static cloud storage, generative AI operates on a completely different scale. Its rapid 
feedback loops and massive bandwidth allow companies to quickly organize and refine reinforcement-learning-grade dataset
s‚Äîeven with anonymized or de-identified data.

We‚Äôre decisively moving from the compute era to the data era of AI, where
 success is no longer about the size of your GPU cluster but the quality of your post-training data. In this new paradig
m, aligning models with the correct data is essential‚Äîplacing tools for data curation, human supervision, and evaluation
 at the heart of AI development.

The legal tech industry must take heed: make sure you own your AI. AI in the cloud is 
not aligned with you‚Äîit‚Äôs aligned with the company that owns it. To protect sensitive data and retain control, on-premis
e solutions and transparent practices are no longer optional‚Äîthey are imperative.

[NeurIPS 2024 conference](https://pre
view.redd.it/k32axcw1lh7e1.jpg?width=2048&format=pjpg&auto=webp&s=3887497b862f3190153705674c943c94697a7bd3)


```
---

     
 
all -  [ ChanceMe : Asian Male CS üôèüôèüôè ](https://www.reddit.com/r/chanceme/comments/1hgkgzi/chanceme_asian_male_cs/) , 2024-12-18-0914
```
I think its a strong application but holy shit my GPA is eating at my confidence rn, please chanceme would be much appre
ciated

Currently a junior - any senior related stuff is likely predictions - chanceme as such

**Demographics:**¬†  
Mal
e, Indian, Southern USA, Looking at top CS programs, potentially recruited for d2 level swim, upper middle class income


**Intended major(s):**

CS w a focus on AI/ML, EECS

**Academics:**

* **SAT:**¬†1600 superscore
* **Class rank:**¬†top 2
0% atleast, most likely top 15%
* **UW/W GPA: 3.9/6.05 on a 6.7 scale**
   * My school grades by semester, so i have 63 
total semester grades of which 6 are B's.
* **Coursework:**¬†
   * AP Human geo, Discrete Math
   * AP physics, AP CSA, A
P Precalc, AP World history, Linear Algebra
   * AP Lang, AP Physics C:Mech, AP Calc AB, AP Stats, APUSH, AP CSP
   * AP
 Lit, AP Physics C: EM, AP Calc BC, Multivariable Calc & Differential EQ, AP Macro/Micro, AP Gov
      * 5's on all exce
pt a 4 on AP human geo
* **Awards:**
   * USACO Plat
   * USAJMO Qual
   * ISEF grand award
   * USAPhO Medallist (Not g
old)
   * Published research in a major CS conference (Neurips level but not neurips)
   * Deciding between All state ja
zz saxophone or a business related award,

**Extracurriculars:**

* AI/ML startup - 5 figure revenue, interviewed by Y c
ombinator, abt 15k users
* Software/CSE Intern at a major company working in lawtech
* Nonprofit that provides olympiad 
tutoring and classes for free, about 500 hours taught total, 15 volunteers, personally taught about 100 hours early on
*
 d2/3 swim, some minor schools reaching out, 25 second 50 free
* Math competition club president - major club with about
 100 members, schools ranked pretty high nationally in this one competition
* Speech and debate congressional debate com
petitor, finaled some pretty big tournaments and have accumulated 15 bids to TOC
* AI ML publication - if not in awards 
ill mention here, did prompting research and technical research with a PhD at berkeley - through cold emailing
* Patent 
for a novel bio-based material, considering commercializing but probably not
* Jazz saxophone player for abt 8 years, pl
ay a lot for fun and am decent at it, considering putting it on like a portfolio
* Youtube channel with 1.1 million life
time views and about 5k subscribers, where I post music and education content

**Schools:**

* Basically the top 20 CS s
chools, preference on MIT/CMU/Stanford

LOR

10/10 - research teacher

10/10 - Math teacher

7/10 - Saxophone tutor

Ess
ays  
Common app - 8/10 - not an amazing writer but fairly good feeling about it  
Supps - didnt write yet - re: current
 junior

**H**ad a shitty GPA because close family member died first semester of sophomore year, so I got some B's, and 
got into some disciplinary trouble around that time with the school, nothing major, no suspension etc. all the classes w
ith B's were A+ next semester.

I feel decently confident, but im fairly worried about my GPA and how it measures up her
e, I go to a rich private school full of tryhards :()

Would really appreciate yall's thoughts on this :D
```
---

     
 
all -  [ This is such a great take, and I would expect no less from the creator of such an pivotal platform. ](https://i.redd.it/9wqttkix7g7e1.jpeg) , 2024-12-18-0914
```

```
---

     
 
all -  [ Advice for CS PhD ](https://www.reddit.com/r/gradadmissions/comments/1hg2ecz/advice_for_cs_phd/) , 2024-12-18-0914
```
First year masters student at a mid-ranked US university. I would like to apply to PhD CS programs next fall. Current GP
A: 3.2/4 (fucked up really bad, I have two more quarters to catch up, so best case 3.7/4)

I have one first author A* wo
rkshop paper and one third author NeurIPS poster paper, as well as one rejected submission to CoRL. I plan to apply to O
NLY 5 groups in the world, since I am very particular about my research interests. All these groups are in Stanford, Ber
keley or MIT. 

I want to know this: I‚Äôm willing to do anything it takes to do a PhD only from these groups. I am willin
g to spend the next year (and possibly even 2 more years after that doing a research-y job or Research Assistantship und
er a prof), but I only want to work at one of these groups.

Here is the deal: To the best of my knowledge, I have NEVER
 (literally , never) seen someone in that gpa range get in to these schools. The purpose of this post is to ask people i
f I have a shot at Stanford/Berkeley/MIT‚Äôs CS PhD program, given that i‚Äôm willing to conduct research for 2-3 more years
 before applying, and hope to have a few more first authored publications at good venues? I want to know if this is a mi
sconception I‚Äôve been carrying or if it is actually the case. 

Please note that this post is specifically only for the 
3 schools mentioned above, so kindly do not come at me saying that I am over-estimating the profiles that get in to thes
e schools. 

TLDR: I want to know what exactly it takes to overcome a low undergrad AND masters GPA to specifically get 
accepted into the CS PhD program at Stanford/MIT/Berkeley (only these schools). 


```
---

     
 
all -  [ So what do you all think of Rosalind Picard‚Äôs racist remarks at NeurIPS? ](https://www.reddit.com/r/mit/comments/1hg1v53/so_what_do_you_all_think_of_rosalind_picards/) , 2024-12-18-0914
```
Is she going to get away with it because of her decorated career and influences in the AI community?


```
---

     
 
all -  [ Valence & Recursion Sweep Awards at Foundation Models for Science Workshop at NeurIPS ](https://www.reddit.com/r/RecursionPharma/comments/1hfo8d6/valence_recursion_sweep_awards_at_foundation/) , 2024-12-18-0914
```
https://preview.redd.it/0jg8zf1sv87e1.png?width=1200&format=png&auto=webp&s=951c8af14784f575ef01a613f15a8c80e2f73d17

Th
is past weekend at NeurIPS, Valence Labs and Recursion won first, second and third place awards at the Foundation Models
 for Science workshop. These foundation models offer breakthroughs in leveraging machine learning to better model the in
tersection of biology and chemistry necessary to improve and scale AI drug discovery. 

**First place** was awarded to a
 paper on MolPhenix, a foundation model that can predict the effect of any given molecule and concentration pair on phen
otypic cell assays and cell morphology by integrating phenomics data with chemistry data. Over the past decade, Recursio
n has generated billions of phenomics images through automated, high-throughput experiments. Paired with new phenomics f
oundation models like Phenom-1, Recursion can extract meaningful representations from these high-dimensional images to b
uild Maps of Biology, allowing them to navigate which molecules and genes map to the same space of morphological changes
. MolPhenix mines that rich data and delivers 10X improvement over previous methods ‚Äì from 7.9% to 77.3% on the Top 1% r
ecall of active molecules. **Paper:** [https://www.arxiv.org/abs/2409.08302](https://www.arxiv.org/abs/2409.08302?fbclid
=IwZXh0bgNhZW0CMTAAAR1DNiyBVjVqVLuNtS9J5NV1MFqG6gxWIEMc8VxgmY_h1F61pVC1QMYBx5E_aem_eMEc0w9oQCwp4X5KhTwjSA)

**Second pla
ce** was awarded to a paper on Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as
 building blocks to explore drug-like chemical space more comprehensively. The model uses an unsupervised pre-training a
pproach using offline drug-like molecule datasets, conditioning A-GFNs on inexpensive yet informative molecular descript
ors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve 
as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. **
Paper:** [https://arxiv.org/abs/2409.09702](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F2409.09702%3F
fbclid%3DIwZXh0bgNhZW0CMTAAAR1TFhx9hYXN7h0fUw0yz-jj1XbKnG5VVmeGbhBjXq3D_mI-0EaoAFSuPdM_aem_DSvWO2X4lUCyArxe25tAxg&h=AT3p
MSPJmPbXaJMGgs_or31Oprx-0d6DEJfLPfDpG7eaT2mbKba97Q38wnlIRWtiiybJk6-ebgL6mKUxVGSjRbxBmELjxYpjXNjdHgL29VeAnS-hGjDCqAKKMQMj
w7aG1g&__tn__=-UK-R&c[0]=AT0iM8paHD3SnWn3-5OATLDMYPKlw0l87qqsKblkdsjHHaCw_MGxpfhRaSEvxsy0AUorpVBjv6c2ceWmePe-Jl-BcrAc3Ma
QXhUG9VtgoQ9q8Jm8kZAX4gjgN8BWRSb9QeIYE2XwWCLl8bqvpU0AKz5RAGcDYz5s2BukDaagpBsjOI07tAnlVJ-4_HtQZ-b4sEweKQ) 

**Third place
** was awarded to a paper on the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter 
ViT-G/8 MAE trained on over 8 billion microscopy images that achieves a 60% improvement in linear separability of geneti
c perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate co
nsistency benchmarks. **Paper:** [https://arxiv.org/abs/2411.02572](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.o
rg%2Fabs%2F2411.02572%3Ffbclid%3DIwZXh0bgNhZW0CMTAAAR1TFhx9hYXN7h0fUw0yz-jj1XbKnG5VVmeGbhBjXq3D_mI-0EaoAFSuPdM_aem_DSvWO
2X4lUCyArxe25tAxg&h=AT2mulqFETtdCih5m7gG70wHri8uodEz6hXHak0aAKc6GwUbm4BJnRDlDjN6L8dYBc_aXrweThNM8xEMPZ5GqovHoSWE3w2YETVv
k99BY8TWSX6EyOpJw5-OvDXcHdj2fw&__tn__=-UK-R&c[0]=AT0iM8paHD3SnWn3-5OATLDMYPKlw0l87qqsKblkdsjHHaCw_MGxpfhRaSEvxsy0AUorpVB
jv6c2ceWmePe-Jl-BcrAc3MaQXhUG9VtgoQ9q8Jm8kZAX4gjgN8BWRSb9QeIYE2XwWCLl8bqvpU0AKz5RAGcDYz5s2BukDaagpBsjOI07tAnlVJ-4_HtQZ-b
4sEweKQ)

\#NeurIPS #NeurIPS2024 #ML
```
---

     
 
all -  [ NeurIPS conference in Vancouver draws 16,000 AI researchers $META ](https://www.reddit.com/r/alertscreener/comments/1hfitr5/neurips_conference_in_vancouver_draws_16000_ai/) , 2024-12-18-0914
```
With major companies like Meta $META, Alphabet $GOOGL, and Microsoft $MSFT showcasing their latest AI advancements and p
roducts.

### Follow [@alertscreener](https://x.com/intent/user?screen_name=alertscreener) for more
```
---

     
 
all -  [ Last Evening in Vancouver: Must-See Experiences Before Heading Back? ](https://www.reddit.com/r/askvan/comments/1hf3fah/last_evening_in_vancouver_mustsee_experiences/) , 2024-12-18-0914
```
I've been here for a week attending NeurIPS 2024 and will be heading back Toronto tomorrow. I only have this evening lef
t‚Äîwhat's the must-see or unique experience I shouldn‚Äôt miss to avoid regretting it later
```
---

     
 
all -  [ Last Evening in Vancouver: Must-See Experiences Before Heading Back? ](https://www.reddit.com/r/canadatravel/comments/1hf37do/last_evening_in_vancouver_mustsee_experiences/) , 2024-12-18-0914
```
I've been here for a week attending NeurIPS 2024 and will be heading back Toronto tomorrow. I only have this evening lef
t‚Äîwhat's the must-see or unique experience I shouldn‚Äôt miss to avoid regretting it later


```
---

     
 
all -  [ [D] Are We Okay With This? Questionable Poster Behavior at NeurIPS ](https://www.reddit.com/r/MachineLearning/comments/1heo36q/d_are_we_okay_with_this_questionable_poster/) , 2024-12-18-0914
```
This was my first year at NeurIPS. It‚Äôs inspiring to see so much cutting-edge research being presented, but something tr
oubling caught my attention during the poster sessions that I feel compelled to share, especially given [the recent inci
dent with Rosalind Picard](https://www.reddit.com/r/MachineLearning/comments/1hdxbru/d_what_happened_at_neurips/).

Gett
ing a paper accepted at NeurIPS is a huge achievement. Each poster spot represents so much hard work and is highly covet
ed.

I saw two posters that *shouldn‚Äôt* have been there, and it has left me wondering about the exploitation of these sp
aces.

**Illegal Poster #1:** [Generative Boba](https://x.com/BoyuanChen0/status/1778565953627775453). This was a ‚Äúcute,
 look at me‚Äù poster, but it also featured a QR code linking to the creator‚Äôs X/Twitter. While the poster itself was plac
ed on a side wall in the exhibition hall and not in an official poster spot (when I saw it anyway), it still felt odd. W
hy did they make this poster? Was this about sparking joy, or gaining attention and followers?

[Illegal Poster #1: Gene
rative Boba.](https://preview.redd.it/u3vfvszkoy6e1.jpg?width=3363&format=pjpg&auto=webp&s=8c09ddda45e0ac002223dadf0eac4
165bfdc0433)

**Illegal Poster #2:** [Benchmarkthing](https://x.com/xdotli/status/1867823150068535797)**.** This was far
 more concerning. It blatantly promoted a new AI startup, mentioning funding by a prominent figure in our field, Jeff De
an. Unlike the boba poster, this could visually pass as a real NeurIPS poster. Probably most passersby didn‚Äôt give it a 
second thought, but the poster's presenter (who is also the company‚Äôs founder) was essentially promoting his new startup
, sometimes to a significant audience size AND across *multiple* poster sessions. This feels deceptive and exploitative 
‚Äî gaming the trust of the community to cheatingly gain visibility in a sacred academic space.

[Illegal Poster #2: Bench
markthing.](https://preview.redd.it/qn7vpos4py6e1.jpg?width=2646&format=pjpg&auto=webp&s=4cfd1aa535bdf74cdb57ba8e44f1fa8
13b9d28a7)

A different type of gaming involves authors putting up their poster at unused spots while leaving a sign in 
their formally assigned location that says ‚ÄúSee poster at #{better spot}‚Äù. If the authors for the unused spot arrived, t
hey‚Äôd just move their poster back ‚Äî but if not, they would presumably revel in the extra attention from being located, f
or example, closer to the hall‚Äôs entrance with more foot traffic.

Relocating posters still seems problematic, but at le
ast the posters *belong* at the conference. On the other hand, I feel much more strongly that unauthorized posters for p
ersonal or commercial promotion hurts the integrity of the space, disrespects the presenters whose posters truly belong 
there, and undermines the conference overall.

Questions for the community:

1. Should there be stricter policies or bet
ter enforcement for poster sessions?
2. How do we differentiate between minor gaming (e.g. relocating posters) and outri
ght exploitation (e.g. unauthorized posters)?
3. Is it fair to tolerate some flexibility as long as the intentions are l
ighthearted or still academic?¬†
4. How do we address these behaviors moving forward? Should there be consequences?
```
---

     
 
all -  [ A little bit of drama: Pre-training is only over if you have no imagination - Logan Kilpatrick ](https://www.reddit.com/r/singularity/comments/1he9tsn/a_little_bit_of_drama_pretraining_is_only_over_if/) , 2024-12-18-0914
```
https://x.com/OfficialLoganK/status/1868002617311596552?t=uNazJ-3HPuWlBrXGagkAag&s=19

It's a slow saturday so why not s
hitpost a little.
This is in response to Ilya Sutskever's talk during NeurIPS 2024.
```
---

     
 
all -  [ A Perfect Storm for AI Inference TPU will be new king  ](https://www.reddit.com/r/Bard/comments/1he4e14/a_perfect_storm_for_ai_inference_tpu_will_be_new/) , 2024-12-18-0914
```
Ilya Sutskever's recent bombshell at NeurIPS ‚Äì [https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-op
enai-model-data-training](https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-traini
ng) that we've reached 'peak data' and the era of pre-training as we know it is ending ‚Äì has sent ripples through the AI
 world. His vision of a future dominated by 'agentic,' reasoning AI, capable of learning from limited data, sets the sta
ge for a fundamental shift in how we develop and deploy artificial intelligence. This shift, it turns out, might just be
 the perfect storm for the rise of the TPU and Broadcom's latest chip announcement could be the catalyst.¬†[https://www.b
roadcom.com/company/news/product-releases/62691](https://www.broadcom.com/company/news/product-releases/62691)

**Why TP
Us are Poised to Shine in a Post-'Peak Data' World:**

1. **Inference, Inference, Inference:**¬†Sutskever's emphasis on a
 future where AI is smarter, not just bigger, puts the spotlight squarely on¬†**inference**. This is where AI applies its
 learned knowledge to make predictions and decisions in real-world scenarios. And this is precisely where TPUs have a di
stinct advantage.
2. **Enter Broadcom: The 3.5D XDSiP and the TPU Advantage:**¬†Broadcom's new 3.5D chip isn't just anoth
er incremental improvement; it's a potential game-changer, especially for TPUs. Its innovative design, featuring vertica
l die stacking and face-to-face interconnects, directly addresses the key challenges of inference:
   * **Latency Killer
:**¬†By drastically reducing the distance data needs to travel, Broadcom's chip minimizes latency, enabling the rapid-fir
e calculations that TPUs are built for. This is crucial for real-time inference applications.
   * **Power Saver:**¬†The 
3.5D architecture slashes power consumption, a critical factor for deploying TPUs in data centers and edge devices where
 energy efficiency is paramount.
   * **Density Champion:**¬†The compact form factor allows for denser packing of TPUs, p
aving the way for more powerful and efficient inference systems.

The potential rise of TPUs, spurred by the need for mo
re efficient inference and enabled by innovations like Broadcom's, could trigger a paradigm shift, compelling all major 
players in the AI field to develop their own specialized chips and hardware solutions to remain competitive in this rapi
dly evolving landscape. This may be the dawn of the age of custom AI silicon, and potentially the beginning of the TPU e
ra.
```
---

     
 
all -  [ Ilya Sutskever, cofondateur et ancien directeur scientifique d'OpenAI, a fait une rare apparition pu ](https://www.reddit.com/r/actutech/comments/1hdxdjs/ilya_sutskever_cofondateur_et_ancien_directeur/) , 2024-12-18-0914
```
Il a notamment affirm√© que le pr√©-entra√Ænement des mod√®les tel que nous le connaissons va in√©vitablement prendre fin, co
mparant les donn√©es √† un 'combustible fossile' limit√©. Selon lui, nous avons atteint un pic des donn√©es disponibles, car
 il n'existe qu'un seul internet.  
  
Pour l'avenir, il pr√©dit que les prochaines g√©n√©rations d'IA seront plus 'agentiq
ues' et capables de raisonner v√©ritablement, contrairement aux syst√®mes actuels qui se contentent principalement de reco
nna√Ætre des motifs. Ces syst√®mes deviendront plus impr√©visibles √† mesure qu'ils d√©velopperont leur capacit√©.  
  
Il a √©
galement √©tabli un parall√®le int√©ressant entre l'√©volution de l'IA et la biologie √©volutive, sugg√©rant que l'IA pourrait
 d√©couvrir de nouvelles approches de mise √† l'√©chelle, tout comme l'√©volution a trouv√© un nouveau mod√®le pour le cerveau
 des hominid√©s.

https://preview.redd.it/2ly7b8fejr6e1.jpg?width=960&format=pjpg&auto=webp&s=ae6cfe8241a9fc37bf648d189d8
2908a8624094c


```
---

     
 
all -  [ [D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams ](https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/) , 2024-12-18-0914
```
Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the
 other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his col
leagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.git
hub.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is
 asserted and if this is true this would be nice to confirm.
```
---

     
 
all -  [ Feels good to see Mr.X getting noted ](https://i.redd.it/oexht6c8rg6e1.jpeg) , 2024-12-18-0914
```
Link: https://x.com/elonmusk/status/1866797259968614885?s=46
```
---

     
 
all -  [ and now we know why Elon named his stupid AI 'Grok'  ](https://i.redd.it/j0pk4yr54f6e1.png) , 2024-12-18-0914
```
also why does the chart go all the way back to 1991
```
---

     
 
all -  [ 
New framework for quantifying uncertainty in LLMs: Semantic Density ](https://www.reddit.com/r/airesearch/comments/1hc6fez/new_framework_for_quantifying_uncertainty_in_llms/) , 2024-12-18-0914
```
Can we trust LLMs in high-stakes decisions? Cognizant AI Research Lab introduces¬†Semantic Density, a scalable framework 
to quantify response-specific uncertainty without retraining. Tested on state-of-the-art models, it outperforms existing
 methods on benchmarks. Presented at NeurIPS 2024‚Äîlet‚Äôs discuss: [https://medium.com/@evolutionmlmail/quantifying-uncert
ainty-in-llms-with-semantic-density-ff0e58836416](https://medium.com/@evolutionmlmail/quantifying-uncertainty-in-llms-wi
th-semantic-density-ff0e58836416)


```
---

     
 
all -  [ How well-informed Elon Musk is when he makes a statement ](https://www.reddit.com/r/EnoughMuskSpam/comments/1hc415l/how_wellinformed_elon_musk_is_when_he_makes_a/) , 2024-12-18-0914
```
https://preview.redd.it/ekphsq71aa6e1.png?width=798&format=png&auto=webp&s=025884971824e9394aa64d398c70b1d44da62083


```
---

     
 
all -  [ Can research projects be replacement for internships in North America? ](https://www.reddit.com/r/csMajors/comments/1hc391a/can_research_projects_be_replacement_for/) , 2024-12-18-0914
```
I'm not a computer science major but do have extensive technical background (I'm in cognitive science with a computer sc
ience minor.) I've completed a research project for credit under a big name in cognitive science and it involved CNNs. I
 aim to refine the project so that I can submit it to a journal or a good conference (NeurIPS maybe!) I've also done thi
s unpaid internship thing from back home under a research organization my father knows very well and it too was technica
l in nature. I presented the research at a big psychology conference. I'm also volunteering at a computational psychiatr
y lab as a technical reviewer.

That being said, how much are research projects worth when finding full-time jobs, even 
the ones that are published or presented at conferences? I'm graduating next May. I haven't gotten any internships throu
ghout my undergrad, only got technical assessments without any interviews.

Edit: My GPA is bad because of long-term men
tal health issues
```
---

     
 
all -  [ [D] How to make friends and network at NeurIPS? ](https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/) , 2024-12-18-0914
```
I‚Äôm attending NeurIPS for the first time and it‚Äôs quite overwhelming seeing the amount of people and so many recruiters.
 I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is
 here.

I didn‚Äôt really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach
 others who are in big groups of people and (2) I‚Äôm feeling strong imposter syndrome and under-qualified for the jobs re
cruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other 
students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to
 potentially meet up and have a chat? I‚Äôm a 3rd year PhD student from the UK, but from Vancouver myself so know lots of 
stuff going on in the area. Cheers!
```
---

     
 
all -  [ NeurIPS 2024: What Matters When Building Vision Language Models ](https://www.reddit.com/r/computervision/comments/1hb2zk0/neurips_2024_what_matters_when_building_vision/) , 2024-12-18-0914
```
Check out [Harpreet Sahota‚Äôs](https://www.linkedin.com/in/harpreetsahota204/) conversation with [Hugo Lauren√ßon](https:/
/www.linkedin.com/in/hugo-lauren%C3%A7on/) of Sorbonne Universit√© and Hugging Face about his NeurIPS 2024 paper, ‚ÄúWhat M
atters When Building Vision Language Models.‚Äù

* [Complete paper presentation and Q&A on YouTube](https://www.youtube.co
m/watch?v=OfbsZeBBFrg)
* [Research paper on arXiv](https://arxiv.org/abs/2405.02246)

  
Preview video below:

https://r
eddit.com/link/1hb2zk0/video/9ebds5l7716e1/player


```
---

     
 
all -  [ NeurIPS is starting tomorrow but I‚Äôm too young to go thereüò≠üò≠üò≠ ](https://www.reddit.com/r/teenagers/comments/1haqs1i/neurips_is_starting_tomorrow_but_im_too_young_to/) , 2024-12-18-0914
```
It‚Äôs right where I live as well
```
---

     
 
all -  [ [R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effe ](https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/) , 2024-12-18-0914
```
We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**:¬†[https://arx
iv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**:¬†[https://github.com/trungtrinh44/DAMP](https://gi
thub.com/trungtrinh44/DAMP)  
**DAMP**¬†(Data augmentation via multiplicative perturbations) is a simple yet effective ap
proach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augme
ntation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness withou
t compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Found
ation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations
, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires onl
y random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while
 being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transforme
rs from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet wit
hout complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly imp
roves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corrupti
on error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58
.83% baseline)

**Why DAMP?**¬†Unlike traditional approaches that rely on complex data augmentation pipelines or computat
ionally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustnes
s. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing 
techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over sta
ndard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical disc
ussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond co
mputer vision!
```
---

     
 
all -  [ NeurIPS 2024 - Creating SPIQA: Addressing the Limitations of Existing Datasets for Scientific VQA ](https://www.reddit.com/r/computervision/comments/1ha9cup/neurips_2024_creating_spiqa_addressing_the/) , 2024-12-18-0914
```
Check out¬†[Harpreet Sahota](https://www.linkedin.com/in/harpreetsahota204/)‚Äôs conversation with¬†[Shraman Pramanick](http
s://www.linkedin.com/in/shramanpramanick/)¬†of Johns Hopkins University and Meta AI about his NeurIPS 2024 paper, ‚ÄúCreati
ng SPIQA: Addressing the Limitations of Existing Datasets for Scientific VQA.‚Äù

* [Complete paper presentation and Q&A o
n YouTube](https://youtu.be/p56kAbdYtUg)
* [Research paper on arXiv](https://arxiv.org/abs/2407.09413)

Preview video:


https://reddit.com/link/1ha9cup/video/z1vatdr5ot5e1/player

  

```
---

     
 
all -  [ NeurIPS 2024 - No ‚ÄúZero-Shot‚Äù Without Exponential Data: Pretraining Concept Frequency Determines Mul ](https://www.reddit.com/r/computervision/comments/1h9q0x1/neurips_2024_no_zeroshot_without_exponential_data/) , 2024-12-18-0914
```
Check out [Harpreet Sahota](https://www.linkedin.com/in/harpreetsahota204/)‚Äôs conversation with [Vishaal Udandarao](http
s://www.linkedin.com/in/vishaal-udandarao/) of the University of T√ºbingen and Cambridge about his NeurIPS 2024 paper, ‚ÄúN
o 'Zero-Shot' Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.‚Äù

* [Comp
lete paper presentation and Q&A on YouTube](https://youtu.be/_4cQJmz2u1c)
* [Research paper on arXiv](https://arxiv.org/
abs/2404.04125)

Preview video:

https://reddit.com/link/1h9q0x1/video/pcw40i25ao5e1/player
```
---

     
 
all -  [ NeurIPS 2024: A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis ](https://www.reddit.com/r/computervision/comments/1h82qz6/neurips_2024_a_textbook_remedy_for_domain_shifts/) , 2024-12-18-0914
```
Check out [Harpreet Sahota‚Äôs](https://www.linkedin.com/in/harpreetsahota204/) conversation with [Yue Yang](https://www.l
inkedin.com/in/yue-yang-4b2075174) of the University of Pennsylvania and AI2 about his NeurIPS 2024 paper, ‚ÄúA Textbook R
emedy for Domain Shifts: Knowledge Priors for Medical Image Analysis.‚Äù

* [Complete paper presentation and Q&A on YouTub
e](https://youtu.be/VjJWOQHgsmk)
* [Research paper on arXiv](https://arxiv.org/abs/2405.14839)

Video preview below:

ht
tps://reddit.com/link/1h82qz6/video/lintlyfuo85e1/player
```
---

     
 
MachineLearning -  [ [D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 p ](https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/) , 2024-12-18-0914
```
At NeurIPS 2024 I found a paper that got accepted that positions its main contribution in the form of ‚ÄúExisting algorith
ms for X ignore Y. We adapt algorithm Z for X to account for Y‚Äù.

On OpenReview I see that the reviewers in particular p
raised the novelty of the work, and recognised Y as an important aspect that had been ignored in the field of X.

Now th
e interesting bit: co-authors and I published a paper in Springer‚Äôs Machine Learning journal in 2023 that also proposes 
an algorithm for X that account for Y. We were also not the first to study the problem setting of X with Y: our paper‚Äôs 
related work section discusses 4 papers that have all proposed algorithms for X that account for Y. One is even from Neu
rIPS (2017), and the oldest one dates back to 2012 (an AAAI paper).

The authors of this 2024 NeurIPS paper completely m
issed all this prior literature and believed they were the first, and so did all the reviewers.

This week I e-mailed th
e authors of this NeurIPS 2024 paper and they acknowledged that these works (mine + the 4 others) indeed were all workin
g on the same problem setting, mentioned that they were unaware of all these works, and acknowledged that they can no lo
nger claim novelty of the problem setting.

NeurIPS allows updating the camera ready paper after the conference, and the
 authors promised to use this opportunity to incorporate those related works and modify their contribution statements to
 no longer claim novelty of a first solution of X with Y.

At the one hand, it makes me happy that our work will get cre
dited appropriately.

At the other hand I have my doubts about the ethics of severely modifying contribution statements 
post-review. The authors will no longer claim novelty, but the reviewers in particular praised this novelty, which makes
 me uncertain whether reviewers would have recommended acceptance had they known that this paper will ultimately no long
er be able to claim the novelty that it claimed to have in the reviewed version.

Moreover this makes me wonder about th
e experimental section. Almost surely, reviewers would have demanded comparison to those 5 prior works as baselines. Thi
s paper did not compare against baselines, which will have seemed reasonable to a reviewer who reviewed this work under 
the assumption that the problem setting was completely novel and no prior methods exist that could function as a baselin
e.

Asking the group here about any thoughts on how such cases should get resolved:
- should the paper be retracted?
- s
hould the area chair / program committee be informed? who may or may not take action
- should the paper just get updated
 by authors in the way that was promised, and that is it?
- something else?

I redacted X, Y and Z in order to not publi
cly shame the authors, as they have engaged with my e-mails and I am convinced that there is no foul play and they truly
 were unaware of those works.
```
---

     
 
MachineLearning -  [ [R] Agentic AI test suites. All the test environments used to benchmark BALROG. ](https://www.reddit.com/r/MachineLearning/comments/1gwqe0m/r_agentic_ai_test_suites_all_the_test/) , 2024-12-18-0914
```
# BabyAI

+ Purpose is to facilitate research on *grounded language learning.*  The current domain of BabyAI is a 2D gri
dworld in which synthetic natural-looking instructions (e.g. ‚Äúput the red ball next to the box on your left‚Äù) require th
e agent to navigate the world including unlocking doors) and move objects to specified locations.  

https://openreview.
net/forum?id=rJeXCo0cYX

----

# Crafter

+ Crafter features randomly generated 2D worlds where the player needs to fora
ge for food and water, find shelter to sleep, defend against monsters, collect materials, and build tools.

https://gith
ub.com/danijar/crafter?tab=readme-ov-file


----

# TextWorld

+ Microsoft TextWorld is an open-source, extensible engin
e that both generates and simulates text games. You can use it to train reinforcement learning (RL) agents to learn skil
ls such as language understanding and grounding, combined with sequential decision making.

https://www.microsoft.com/en
-us/research/project/textworld/

https://github.com/microsoft/TextWorld

https://arxiv.org/pdf/1806.11532

----

# Baba 
is AI 

+ Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine 
those rules and objectives.    We test three ***state-of-the-art multi-modal large language models (OpenAI GPT-4o, Googl
e Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically*** when generalization requires that the rul
es of the game must be manipulated and combined. 


https://github.com/nacloos/baba-is-ai

https://arxiv.org/abs/2407.13
729

----

# MiniHack

+ MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforc
ement Learning (RL).   The motivation behind MiniHack is to be able to perform RL experiments in a controlled setting wh
ile being able to increasingly scale the complexity of the tasks.

https://github.com/facebookresearch/minihack

https:/
/minihack.readthedocs.io/en/latest/


----

# NetHack

+ NetHack is an attractive research platform as it contains hundr
eds of enemy and object types, has complex and stochastic environment dynamics, and has a clearly defined goal (descend 
the dungeon, retrieve an amulet, and ascend) which can be achieved in a diverse set of ways. The game is considered one 
of the hardest in the world1, with winning episodes lasting 100,000s of steps, and a permadeath setting that starts agen
ts at the beginning in a whole new world if they die in the dungeon. NetHack is even difficult to master for human playe
rs who often rely on external knowledge.


https://proceedings.neurips.cc/paper_files/paper/2023/file/764ba7236fb6374301
4fafbd87dd4f0e-Paper-Conference.pdf

https://github.com/upiterbarg/hihack

https://arxiv.org/pdf/2203.11889

https://www
.youtube.com/watch?v=8L8LiQ-cIWA
```
---

     
 
MachineLearning -  [ [D] PhD in RL/ML Theory or LLM ](https://www.reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/) , 2024-12-18-0914
```
Hi guys,

I'm at a crossroads in my academic journey and would appreciate the community's insights. I'm trying to decide
 between pursuing a PhD focused on reinforcement learning/ML theory versus specializing in large language models with mo
re experimental/applied research (these are the only two offers I had).

# Key considerations are the following:

# Rese
arch Impact

* RL/ML Theory: Foundational work that could advance the field's mathematical understanding
* LLMs: Direct 
applications in today's most transformative AI systems

# Job Prospects

* Theory: Academia, research labs, potentially 
more limited industry roles
* LLMs: High industry demand, active research area in both academia and industry

# Long-ter
m Relevance

* Theory: Core principles likely to remain valuable regardless of specific technologies
* LLMs: Currently r
evolutionary but uncertain long-term trajectory

Personal background

* I'm an international student and about to finish
 my master program in US, so I no longer has enough time before making the final decision. I used to research in ml theo
ry, but did not end up with a real top conference publication in theory. I personally doubt if I have enough mathematica
l background to pursue a successful PhD in this area (e.g., at least publish 2 theory papers a year on ICML/NeurIPS/ICLR
/COLT/AISTATS). At the same time, I am personally doubting if theory works indeed advance the ML/AI community, as many p
apers are just proving vacuous bounds or propose some new algorithms that themselves cannot even implement or experiment
ally tested.
* I also used to research in more applied ml, with one aaai paper. My personal concerns is that I'm not fas
t at implementation and coding, the most strategic ability for a successful applied ML researcher. After we entered the 
LLM era, the pacing or applied ML research (especially in LLM and CV) becomes so fast. It's like competitive programming
 in research community (well, also the #GPUs competition).
```
---

     
 
MachineLearning -  [ [D] Expectation from Machine Learning Engineering jobs ](https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/) , 2024-12-18-0914
```
Hey everyone,

I‚Äôve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up
 a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems 
to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, whi
ch are apparently more in demand than research scientist positions.

I‚Äôm curious about the difference between these two 
roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master‚Äôs degree is often pr
eferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are you
r thoughts?
```
---

     
