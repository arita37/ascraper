 
all -  [ Picked up all these Tapes today! ](https://i.redd.it/pcbd29avx1gc1.jpeg) , 2024-02-03-0910
```
Found this whole lot at VV for the grand total of $5.99! Lots of them still have the original Sony store tags on them!
```
---

     
 
all -  [ Academic journal or conference for AI safety ](https://www.reddit.com/r/AIsafetyideas/comments/1agmkg5/academic_journal_or_conference_for_ai_safety/) , 2024-02-03-0910
```
To help boost the prestige of safety research, leading to more people starting the career.

Since academic ML mostly pub
lishes at conferences, this could be a conference instead.

&#x200B;

AI Safety Academic Conference

Technical AI Safety


&#x200B;

The idea is to fund and provide logistical/admin support for a reasonably large AI safety conference along t
he lines of Neurips etc. Academic conferences provide several benefits: 1) Potentially increasing the prestige of an are
a and boosting the career capital of  people who get accepted papers. 2) Networking and sharing ideas, 3)  Providing fee
dback on submitted papers and highlighting important/useful papers.  This conference would be unusual in that the work s
ubmitted shares approximately the same concrete goal (avoiding risks from powerful AI).  While traditional  conferences 
might focus on scientific novelty and complicated/'cool' papers, this conference could have a particular focus on things
 like reproducibility or correctness of empirical results, peer support and mentorship, non-traditional research mediums
 (e.g. blog posts/notebooks) , and encouraging authors to have a plausible story for why their work is actually reducing
 risks from AI.
```
---

     
 
all -  [ Research Advances in Transformer Time Series Forecasting Models ](https://www.reddit.com/r/deeplearning/comments/1ag4xfp/research_advances_in_transformer_time_series/) , 2024-02-03-0910
```
Just published a new article describing [recent advances in the deep learning for time series](https://medium.com/deep-d
ata-science/advances-in-deep-learning-for-time-series-forecasting-classification-winter-2024-a3fd31b875b0) forecasting a
nd classification space. Specifically, discussed new research at Neurips and ICML and how it compared to baseline method
s like DLinear. I also critiqued some problematic and flawed papers such as TimeGPT. 
```
---

     
 
all -  [ How much am I worth? ~Big Tech Specialty Research Scientist ](https://www.reddit.com/r/Salary/comments/1afrju3/how_much_am_i_worth_big_tech_specialty_research/) , 2024-02-03-0910
```
**Face value experience:**  
I have a PhD in mathematical optimization, with expertise in signal processing, with public
ized innovations in both theoretical (ICML) and applied (NeurIPS) machine learning. The latter publication within the la
st 2 years. I have worked in govt contractor-like roles for the last 4-6 years, but for small scientific firms. 1-2 year
s as a Senior Scientist, and next (currently) as a 'Research Professor' (95% applied research, 5% mentoring PhD candidat
es). Intentional vagueness for anonymity. I have never worked in big tech except for a 5 month software engineering inte
rnship (also related to ML).

**The Position:**  
I have reason to believe that a big-tech team is about to call me with
 an offer. It is a very specialized, secretive lab of about 20 people within one of the famous 'big tech' companies (ove
r 80k employees) with a very high reputation in the ML field (top 3 depending who you ask). The lab is trying to build a
 product with sensors that use biometric signals that have never been commercialized before and requires signal-processi
ng-tailored ML (learning invariances at the sensor level). 

At the onsite, I came to really like the team and am convin
ced this is my dream job. The position is 'research scientist' at the technical lead level, so I would be doing research
 but also setting the research direction for 2-3 other. I have experience leading a team of 3 and interfacing with many 
tech leads to deploy real world systems, but not for very long, so this will be a promotion for me. I'm not sure how muc
h they realize that.

**The Issue:**  
From my understanding, it is literally expected that I negotiate. This goes again
st my nature (which is to thank them and say yes), so I'm anxiously searching podcasts and youtube videos for help under
standing what I'm worth and what I should ask for/ how I should ask for it. But I'm not a software engineer or a general
ized data scientist -- it's got this specialization aspect that may or may not be (but seems) important. I do not have a
ny other offers (I'm not even applying anywhere else ATM, a friend of a friend on the team reached out to me about apply
ing).

**Other Random Factors That May Not Matter:**.

*  I have the highest US security clearance an employer could hop
e for a potential employee to have. No clue if this would be of use to the specific team--probably not. Not if anyone in
 the company would care but other big tech famously pay >50k bonuses to maintain the required lifestyle.
* My current sa
lary is between 175k-200k, I get so much PTO that I can hardly spend it all and it never expires, with VERY flexible wor
k arrangement and the best job security you can hope for. In my view the job security and flexibility is comparatively g
oing away (although it will be good for long-term resume) and I doubt I'll get nearly as much time off. In general I hav
e it pretty laid back and I think this will increase my stress levels across the board which should be compensated for.


**TL;DR:**  
How much should I be aiming for? How should I distribute the target compensation across salary, sign on bo
nus, stock, and wahtever else... (Ive never gotten a big tech offer)
```
---

     
 
all -  [ Regret bounds in reinforcement learning ](https://www.reddit.com/r/reinforcementlearning/comments/1aeiexo/regret_bounds_in_reinforcement_learning/) , 2024-02-03-0910
```
I’ve been away from reading theoretical reinforcement learning papers for a couple of years and was getting curious on h
ow the field has progressed since then. Last time I checked, there was a paper that claimed that they closed the upper a
nd lower bounds of the regret in MDP… where a mistake was discovered in the proof. What happened since then?

Edit: I th
ink it was this one (https://proceedings.neurips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html) if s
omeone can point to a follow up paper, I’d really appreciate it!
```
---

     
 
all -  [ [R] Thoughts about ML theory papers in conferences like International Symposium on Information Theor ](https://www.reddit.com/r/MachineLearning/comments/1abwmal/r_thoughts_about_ml_theory_papers_in_conferences/) , 2024-02-03-0910
```
I have published a few papers in conferences like the International Symposium on Information Theory (ISIT) and Allerton.
 However, when I apply for internship positions, the applications sometimes ask about the number of published papers in 
conferences like Neurips, ICML, ICLR, etc.

Although by any standards, my research papers are 'good' (at least in my opi
nion). However, I feel that I'm not targeting the right conferences. My advisor has also published a lot in these confer
ences, and I would say s/he likes to 'play safe' and avoids taking any risks at these big venues.
```
---

     
 
all -  [ Acceptance rate of workshops in conferences [D] ](https://www.reddit.com/r/MachineLearning/comments/19do6qn/acceptance_rate_of_workshops_in_conferences_d/) , 2024-02-03-0910
```
From the Internet I easily found the acceptance rate of conferences but what is the acceptance rate of workshops conduct
ed in conferences like AISTATS/CVPR/Neurips/ICML? 
```
---

     
 
all -  [ What Bodies Think About: Bioelectric Computation Outside the Nervous System - NeurIPS 2018 ](https://youtu.be/RjD1aLm4Thg?si=j1-jVO--H2lGHaUf) , 2024-02-03-0910
```
One of the best lectures I’ve ever watched! This might sound boring because it’s presented that way, but this has the po
tential to to enlighten you!
```
---

     
 
all -  [ I read through all NeurIPS 2023 Abstracts and wrote about it (r/MachineLearning) ](https://www.reddit.com/r/MachineLearning/comments/19cxibs/p_i_read_through_all_neurips_2023_abstracts_and/) , 2024-02-03-0910
```

```
---

     
 
all -  [ I read through the NeurIPS 2023 Abstracts and wrote about it ](https://alexzhang13.github.io/blog/2024/neurips2023) , 2024-02-03-0910
```
I made this resource that I think might be quite useful here, especially for those looking to find some new, relevant wo
rks to read or use for their own projects. It discusses the content from roughly 300 papers, but the topics broadly pert
ain to all of NeurIPS 2023. Happy reading!
```
---

     
 
all -  [ Advancements in machine learning for machine learning ](https://www.reddit.com/r/worldTechnology/comments/19c2sch/advancements_in_machine_learning_for_machine/) , 2024-02-03-0910
```
With the recent and accelerated advances in machine learning (ML), machines can understand natural language, engage in c
onversations, draw images, create videos and more. Modern ML models are programmed and trained using ML programming fram
eworks, such as TensorFlow, JAX, PyTorch, among many others. These libraries provide high-level instructions to ML pract
itioners, such as linear algebra operations (e.g., matrix multiplication, convolution, etc.) and neural network layers (
e.g., 2D convolution layers, transformer layers). Importantly, practitioners need not worry about how to make their mode
ls run efficiently on hardware because an ML framework will automatically optimize the user's model through an underlyin
g compiler. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on
 heuristics to solve complex optimization problems, often resulting in suboptimal performance.

In this blog post, we pr
esent exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Pr
ior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting 
better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small 
sub-programs, such as basic blocks or kernels. We introduce “TpuGraphs: A Performance Prediction Dataset on Large Tensor
 Computational Graphs” (presented at NeurIPS 2023), which we recently released to fuel more research in ML for program o
ptimization. We hosted a Kaggle competition on the dataset, which recently completed with 792 participants on 616 teams 
from 66 countries. Furthermore, in “Learning Large Graph Property Prediction via Graph Segment Training”, we cover a nov
el method to scale graph neural network (GNN) training to handle large programs represented as graphs. The technique bot
h enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the 
model.

# ML compilers

ML compilers are software routines that convert user-written programs (here, mathematical instru
ctions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML 
program can be represented as a computation graph, where a node represents a tensor operation (such as matrix multiplica
tion), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimiz
ation problems, including graph-level and kernel-level optimizations. A graph-level optimization requires the context of
 the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization tra
nsforms one kernel (a fused subgraph) at a time, independently of other kernels.

&#x200B;

[ Important optimizations in
 ML compilers include graph-level and kernel-level optimizations. ](https://preview.redd.it/n6hs3zdyhsdc1.png?width=1999
&format=png&auto=webp&s=afa7e9a80f5d73c94c1692eb45612f51d7bdfe11)

To provide a concrete example, imagine a matrix (2D t
ensor):

&#x200B;

[matrix](https://preview.redd.it/q7m0npe3isdc1.png?width=1999&format=png&auto=webp&s=56382ec9b2462d71
e48efcd3fc38405219b046c2)

It can be stored in computer memory as \[A B C a b c\] or \[A a B b C c\], known as row- and 
column-major memory layout, respectively. One important ML compiler optimization is to assign memory layouts to all inte
rmediate tensors in the program. The figure below shows two different layout configurations for the same program. Let’s 
assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual opera
tor. However, this layout configuration requires the compiler to insert a copy operation to transform the memory layout 
between the add and convolution operations. On the other hand, the right-hand side configuration might be less efficient
 for each individual operator, but it doesn’t require the additional memory transformation. The layout assignment optimi
zation has to trade off between local computation efficiency and layout transformation overhead.

&#x200B;

![img](2r9mj
oz7isdc1 ' A node represents a tensor operator, annotated with its output tensor shape [n0, n1, ...], where ni is the si
ze of dimension i. Layout {d0, d1, ...} represents minor-to-major ordering in memory. Applied configurations are highlig
hted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inp
uts and outputs of influential operators (i.e., convolution and reshape). A copy operator is inserted when there is a la
yout mismatch.
 ')

If the compiler makes optimal choices, significant speedups can be made. For example, we have seen u
p to a 32% speedup when choosing an optimal layout configuration over the default compiler’s configuration in the XLA be
nchmark suite.

# TpuGraphs dataset

Given the above, we aim to improve ML model efficiency by improving the ML compiler
. Specifically, it can be very effective to equip the compiler with a learned cost model that takes in an input program 
and compiler configuration and then outputs the predicted runtime of the program.

&#x200B;

With this motivation, we re
lease TpuGraphs, a dataset for learning cost models for programs running on Google’s custom Tensor Processing Units (TPU
s). The dataset targets two XLA compiler configurations: layout (generalization of row- and column-major ordering, from 
matrices, to higher dimension tensors) and tiling (configurations of tile sizes). We provide download instructions and s
tarter code on the TpuGraphs GitHub. Each example in the dataset contains a computational graph of an ML workload, a com
pilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the data
set are collected from open-source ML programs, featuring popular model architectures, e.g., ResNet, EfficientNet, Mask 
R-CNN, and Transformer. The dataset provides 25× more graphs than the largest (earlier) graph property prediction datase
t (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction da
tasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction ta
sk on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality.

&#x200B
;

[ Scale of TpuGraphs compared to other graph property prediction datasets. ](https://preview.redd.it/ebfs36lcisdc1.pn
g?width=2868&format=png&auto=webp&s=0e6e2e1f39ebd839df0cb979e4ad2c142b7e676b)

 We provide baseline learned cost models 
with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represent
ed as a graph. Node features, shown in blue below, consist of two parts. The first part is an *opcode id*, the most impo
rtant information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id t
o an *opcode embedding* via an embedding lookup table. The opcode embedding is then concatenated with the second part, t
he rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed
-size embedding of the graph using a simple graph pooling reduction (i.e., sum and mean). The resulting graph embedding 
is then linearly transformed into the final scalar output by a feedforward layer. 

&#x200B;

[ Our baseline learned cos
t model employs a GNN since programs can be naturally represented as graphs. ](https://preview.redd.it/jdcalhjgisdc1.png
?width=2284&format=png&auto=webp&s=f253742e4262ad004fa7a6bc6b3dea31c15c9d5c)

Furthermore we present Graph Segment Train
ing (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases wh
ere the prediction task is on the entire-graph (i.e., graph-level prediction). Unlike scaling training for node- or edge
-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs c
an contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN m
odel is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large
 graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a 
random subset of segments is selected to update the model; embeddings for the remaining segments are produced without sa
ving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to gen
erate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the histor
ical embedding table to efficiently obtain graph segments’ embeddings and segment dropout to mitigate the staleness from
 historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×.

&#x200B;

[ Compari
ng Full Graph Training \(typical method\) vs Graph Segment Training \(our proposed method\). ](https://preview.redd.it/l
us2m3ikisdc1.png?width=790&format=png&auto=webp&s=eb8ad9e466ef062ef171b24f409b6c4eea2c5346)

&#x200B;

[Advancements in 
machine learning for machine learning](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html)
```
---

     
 
all -  [ Quant Research of the Week (10th Edition) ](https://www.reddit.com/r/quant/comments/1994aei/quant_research_of_the_week_10th_edition/) , 2024-02-03-0910
```
# SSRN

### Recently Published

**Quantitative**

[**SpotV2Net: Intraday Spot Volatility Forecasting**](https://papers.s
srn.com/sol3/papers.cfm?abstract_id=4692194): SpotV2Net, a new forecasting model based on Graph Attention Network archit
ecture, enhances the accuracy of intraday spot volatility predictions for financial assets. (2024-01-11, shares: 2.0)

[
**Model Averaging & Double Machine Learning**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4691169): The article
 presents two new stacking methods for double-debiased machine learning (DDML), showing its robustness against unknown f
unctional forms, with software available in Stata and R. (2024-01-11, shares: 3.0)

[**Data Preparation for Code Smell D
etection: A Literature Review**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693778): The review examines data 
preparation techniques in deep learning-based code smell detection, suggesting ways to prepare high-quality data and emp
hasizing the need for data diversity, standardization, and accessibility. (2024-01-13, shares: 2.0)

**Financial**

[**G
amma Risk and Volatility Propagation in Trading**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4692190): A study
 reveals that short-term options trading does not increase market volatility, but rather has an inverse relationship wit
h intraday volatility. (2024-01-11, shares: 27.0)

[**Option Flows and Market Instability**](https://papers.ssrn.com/sol
3/papers.cfm?abstract_id=4695776): The speculative use of call options can cause price instability in the underlying ass
et's market, even with advanced volatility estimators, as per a study using the MinMaSS stability measure. (2024-01-15, 
shares: 8.0)

[**Extreme Liquidity in Asset Modeling**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4694674): A 
study using crypto assets indicates that jumps in asset prices are signs of extreme liquidity and can be effectively mod
eled using autoregressive models adjusted with liquidity. (2024-01-13, shares: 2.0)

[**Carbon Footprint Reduction in In
dex Portfolios**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4692326): The article suggests that investors can 
lower the carbon footprint of their index-based portfolio by over 50% by focusing on low carbon emission stocks and limi
ting high-emission companies. (2024-01-12, shares: 2.0)

[**Extended Overlaps: Optimal Trading Hours in Europe**](https:
//papers.ssrn.com/sol3/papers.cfm?abstract_id=4693290): The article reveals that extended trading hours between North Am
erica and Europe during Daylight Saving Time enhances market liquidity and price efficiency, contributing to the discuss
ion on optimal trading hours in Europe. (2024-01-12, shares: 3.0)

### Recently Updated

## Quantitative

[**Real estate
 valuation with prototype-based models and machine learning**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=46950
79): The article introduces a novel real estate valuation model that uses prototype-based learning, a method that compar
es properties to similar ones, unlike traditional machine learning methods. (2023-12-22, shares: 3.0)

[**DeepTraderX: D
isrupting trading strategies with deep learning**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4692622): The pap
er presents DeepTraderX, a Deep Learning-based trader that learns from market prices, and demonstrates its successful pe
rformance in a multithreaded market simulation. (2023-10-20, shares: 2.0)

[**Flood Risk Pricing: Geo-Hierarchical Deep 
Learning**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4692475): A new deep learning framework enhances flood r
isk modeling, providing more accurate pricing and reducing capital requirements, as shown in a Mississippi River case st
udy. (2022-01-13, shares: 2.0)

[**Effectiveness of Forex Intervention: Role of Domestic Fundamentals**](https://papers.
ssrn.com/sol3/papers.cfm?abstract_id=4692676): Foreign exchange intervention can stabilize currencies in emerging market
s under conditions like low volatility and high inflation, as per a study of 20 emerging economies. (2023-12-28, shares:
 2.0)

[**Probabilistic Electricity Price Forecasting with Trading Applications**](https://papers.ssrn.com/sol3/papers.c
fm?abstract_id=4695159): A new method for electricity price forecasting using artificial neural networks is less costly 
and performs similarly to benchmarks, offering potential trading strategies for investors. (2023-08-02, shares: 2.0)

##
 Financial

[**Boosting Fund Performance**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4697785): Mutual funds t
hat match their investments with similar benchmark peers like the S&P 500 index yield higher returns and experience less
 volatility. (2023-03-11, shares: 2.0)

[**Financial Market Developments and Employee Welfare**](https://papers.ssrn.com
/sol3/papers.cfm?abstract_id=4690550): Equity options and credit default swaps trading benefits company employees by red
ucing short-term managerial focus and improving information efficiency. (2023-04-19, shares: 2.0)

[**Global Volatility 
and Capital Flows**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4691941): During high volatility periods, insti
tutional investors globally reduce their equity allocations, while retail investors shift from small-cap to large-cap st
ocks. (2022-04-13, shares: 2.0)

[**Fear in Finance: FoMO Impact**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=
4692691): The Fear of Missing Out (FoMO) effect in financial markets boosts equity and cryptocurrency prices and lowers 
market volatility due to reduced investor disagreement. (2021-12-08, shares: 2.0)

[**Yelp Sentiment & Asset Pricing**](
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4691145): A sentiment index based on Yelp restaurant reviews can pre
dict stock market reversals and mispricing, with pessimism being a key predictive factor. (2022-08-04, shares: 2.0)

[**
Deep Calibration for Stochastic Volatility**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4692741): A new method
 using neural networks to calibrate stochastic volatility models has proven to be robust and efficient, as confirmed by 
empirical and Monte Carlo experiments. (2023-06-25, shares: 2.0)

[**Climate-Optimized Investment Portfolios**](https://
papers.ssrn.com/sol3/papers.cfm?abstract_id=4696468): The Climate Capital Efficiency Ratio (CER) ranks companies based o
n their carbon emission savings per dollar spent, offering a useful tool for climate-focused investing. (2023-11-28, sha
res: 2.0)

# ArXiv

## Finance

[**Cash management models for data fitting**](http://dx.doi.org/10.1016/j.cor.2018.04.00
7): The article introduces a novel method for cash management models using stochastic and linear programming, showing th
at a small random data sample can effectively fit these models. (2024-01-16, shares: 9)

[**Super-hedging pricing and Im
mediate-Profit arbitrage**](http://arxiv.org/abs/2401.05713): The study explores the super-hedging pricing valuation iss
ue in different financial contexts, emphasizing the effect of changes in prior information and the growth of super-hedgi
ng prices under uncertainty. (2024-01-11, shares: 5)

[**Efficiency of graph databases for financial analysis**](http://
arxiv.org/abs/2401.07483): The study contrasts SQL, No-SQL, and graph databases in terms of efficiency and performance, 
concluding that ESG's Graph database is superior for extended analytics in business and investment. (2024-01-15, shares:
 4)

[**Quantum probability asset return modeling**](http://arxiv.org/abs/2401.05823): The article suggests a new approa
ch to quantum finance, connecting quantum probability's mathematical structure to traders' decisions and market behavior
s, and formulating a Schrödinger-like trading equation to describe the multimodal distribution of asset returns. (2024-0
1-11, shares: 4)

[**SpotV2Net: Intraday Volatility Forecasting**](http://arxiv.org/abs/2401.06249): The article introdu
ces SpotV2Net, a new model for predicting intraday spot volatility using a Graph Attention Network, which has shown bett
er accuracy in predicting Dow Jones Industrial Average index prices. (2024-01-11, shares: 4)

[**Equity Auction Dynamics
: Liquidity Models**](http://arxiv.org/abs/2401.06724): The study applies the latent/revealed order book framework to eq
uity auctions, showing no indicative price predictability and providing accurate model parameter measurements. (2024-01-
12, shares: 3)

[**Dynamic Portfolio Selection with Disappointment Aversion**](http://arxiv.org/abs/2401.08323): The res
earch discusses portfolio selection under generalized disappointment aversion (GDA), finding that investment in the stoc
k market is consistently lower under GDA than under traditional Expected Utility theory. (2024-01-16, shares: 3)

[**Lon
gstaff Schwartz Monte Carlo Approach to Game Option Pricing**](http://arxiv.org/abs/2401.08093): The article suggests a 
two-step Longstaff Schwartz Monte Carlo method for pricing game options, which provides more reliable results than the o
riginal method. (2024-01-16, shares: 2)

[**Deep Minimizing Movement Method for Option Pricing**](http://arxiv.org/abs/2
401.06740): The paper introduces a deep learning method for pricing European basket options using Artificial Neural Netw
orks and two methods for discretizing the integral operator, focusing on assets with jump-diffusion dynamics. (2024-01-1
2, shares: 2)

## Miscellaneous

[**Enhancing Financial Sentiment Analysis with Heterogeneous LLM Agents**](http://arxiv
.org/abs/2401.05799): A study suggests using large language models without fine-tuning for financial sentiment analysis,
 offering a design framework that enhances accuracy. (2024-01-11, shares: 4)

[**Analyzing Herd Behavior in Investment**
](http://arxiv.org/abs/2401.07183): A study introduces the concept of average deviation to measure the difference betwee
n two agents' investment decisions, studying the effect of herd behavior on these decisions. (2024-01-14, shares: 2)

##
 Crypto & Blockchain

[**Backrun Auctions & Trader Protection**](http://arxiv.org/abs/2401.08302?utm_source=dlvr.it&utm_
medium=twitter): The study presents a new laminated queueing model for batched trading on decentralized exchanges, aimin
g to improve transaction infrastructure and examining the potential for price manipulation by arbitrageurs. (2024-01-16,
 shares: 5)

[**Transformer-Based ETH Price Prediction**](http://arxiv.org/abs/2401.08077?utm_source=dlvr.it&utm_medium=
twitter): The research uses a transformer-based neural network to forecast Ethereum prices, indicating a strong correlat
ion with other cryptocurrencies and sentiments, and suggests a theory on sentiment-driven illusion of causality in crypt
ocurrency price movements. (2024-01-16, shares: 4)

[**Analysis of Impermanent Loss in DEX**](http://arxiv.org/abs/2401.
07689): The paper explores the issue of impermanent loss in decentralized exchanges through Monte Carlo simulations, ind
icating that price changes don't always result in losses for liquidity providers and that an arbitrage-friendly environm
ent is beneficial for them. (2024-01-15, shares: 3)

[**Liquidity Provision on DEX**](https://papers.ssrn.com/sol3/paper
s.cfm?abstract_id=4694683): Decentralized exchanges' infrastructure can lead to arbitrage losses for liquidity providers
, with design changes offering limited reduction in these losses, as shown in a study using the Silicon Valley Bank coll
apse. (2021-03-17, shares: 2.0)

## Historical Trending

[**Handling Missing Values in ML Portfolios**](https://arxiv.or
g/abs/2207.13071): The research analyzes missing data in return predictors, concluding that simple imputation methods ar
e effective and complex ones can underperform if misused. (2022-07-21, shares: 42)

[**Deep Signature Algorithm for Opti
ons**](https://arxiv.org/abs/2211.11691): The research expands the backward scheme for state-dependent FBSDEs with refle
ctions to path-dependent FBSDEs, demonstrating the convergence of the numerical algorithm and providing examples of its 
use. (2022-11-21, shares: 14)

[**Curriculum and Imitation Learning for Time-Series Control**](https://arxiv.org/abs/231
1.13326): The study finds that curriculum learning enhances performance in control tasks over highly stochastic time-ser
ies data, while imitation learning should be used carefully. (2023-11-22, shares: 19)

[**Kernel Hilbert Space Approach 
to Volatility Models**](https://arxiv.org/abs/2203.01160): The paper presents a new regularization approach for solving 
the singular McKean-Vlasov equation, commonly used in financial models, using the reproducing kernel Hilbert space techn
ique. (2022-03-02, shares: 17)

# ArXiv ML

## Recently Published

[**Easy Training Data**](https://arxiv.org/abs/2401.0
6751): The study suggests that current language models can effectively generalize from easy to hard data, implying that 
scalable oversight may be less challenging than previously believed. (2024-01-12, shares: 94)

[**Transformers as RNNs**
](https://arxiv.org/abs/2401.06104): The research shows that decoder-only transformers can be seen as infinite multi-sta
te RNNs and introduces a new policy, TOVA, which performs better in long-range tasks and uses less memory. (2024-01-11, 
shares: 152)

[**TOFU Unlearning for LLMs**](https://arxiv.org/abs/2401.06121): The study presents TOFU, a new benchmark
 for understanding unlearning in large language models, revealing that existing unlearning algorithms are not effective.
 (2024-01-11, shares: 22)

[**PANDORA: Parallel Dendrogram Construction Algorithm: Parallel Dendrogram Construction Algo
rithm PANDORA**](https://arxiv.org/abs/2401.06089): Pandora, a new parallel algorithm, has been introduced for efficient
 dendrogram construction in hierarchical clustering, offering significant speed improvements on CPUs and GPUs. (2024-01-
11, shares: 15)

# GitHub

## Finance

[**ViTST: Time Series as Images Transformer**](https://github.com/Leezekun/ViTST)
: NeurIPS 2023 introduces a paper discussing the use of Vision Transformer for analyzing irregularly sampled time series
 data. (2023-02-23, shares: 51.0)

[**Stockformer: Swing Trading with STL Decomposition and Self-Attention**](https://gi
thub.com/Eric991005/Stockformer): A paper suggesting a swing trading strategy using STL Decomposition and Self-Attention
 Networks is being reviewed for publication in Neurocomputing. (2023-11-09, shares: 12.0)

[**Piepilot: Portfolio Optimi
zer**](https://github.com/ranaroussi/piepilot): A straightforward tool designed for optimizing investment portfolios. (2
024-01-13, shares: 4.0)

## Trending

[**Fast AI Gateway**](https://github.com/Portkey-AI/gateway): The article explores
 a high-speed AI Gateway capable of managing 100 LLMs via a single, user-friendly API. (2023-08-23, shares: 1370.0)

[**
ChatGPT Web UI**](https://github.com/ollama-webui/ollama-webui): The piece presents a web user interface client for Olla
ma, modeled after ChatGPT. (2023-10-06, shares: 3316.0)

[**Draggable Streamlit Dashboard**](https://github.com/okld/str
eamlit-elements): The article provides a guide on building a customizable Streamlit dashboard with tools like Material U
I widgets, Monaco editor, Visual Studio Code, Nivo charts, etc. (2021-04-15, shares: 451.0)

# LinkedIn

## Trending

[*
*AIs Impact on Asset Management**](https://www.linkedin.com/feed/update/urn:li:activity:7153365973817827332): GitHub CEO
, Thomas Dohmke, explores the potential disruption of Asset Management by Artificial Intelligence. (2024-01-17, shares: 
2.0)

[**Evolutionary ML Approaches**](https://www.linkedin.com/feed/update/urn:li:activity:7153371055707848705): A new 
book by global researchers delves into evolutionary methods in machine learning, discussing evolutionary computation, ne
ural networks, and their practical applications. (2024-01-17, shares: 1.0)

[**ML Execution Time in Asset Pricing**](htt
ps://www.linkedin.com/feed/update/urn:li:activity:7153074375037009920): Demirbaga and Xu's research emphasizes the signi
ficance of machine learning model execution time in asset pricing, recommending feature reduction and fewer time observa
tions as ways to save time. (2024-01-17, shares: 1.0)

## Informative

[**Optimizing PnL with Linear Signals: An ML Fram
ework**](https://www.linkedin.com/feed/update/urn:li:activity:7153293929365336065): The article introduces an unsupervis
ed machine learning framework that optimizes PnL by using a linear relationship between exogenous variables and the trad
ing signal to maximize the Sharpe Ratio. (2024-01-17, shares: 1.0)

[**Addressing Overfitting in Backtesting**](https://
www.linkedin.com/feed/update/urn:li:activity:7153278047071137793): The article emphasizes the need to prevent overfittin
g in backtesting results, recommending techniques like K-fold cross-validation and randomization in simulations. (2024-0
1-17, shares: 1.0)

# Twitter

## Quantitative

[**Phidata's Python Library for Autonomous AI**](https://twitter.com/car
lcarrie/status/1747057820841701592): Phidata has launched a new Python library for Autonomous AI that utilizes LLM funct
ion calling, with resources accessible on Streamlit FastAPI. (2024-01-16, shares: 0)

[**IMF Report on AI's Impact on Em
ployment**](https://twitter.com/carlcarrie/status/1747041277370065352): The IMF has published a report analyzing the eff
ects of Generative AI brands on job markets. (2024-01-15, shares: 0)

[**BCGs Report: From Potential to Profit with GenA
I**](https://twitter.com/carlcarrie/status/1746916291619836145): According to a BCG report, companies investing in GenAI
 could expect over 10% cost savings, potentially amounting to 1 billion in savings. (2024-01-15, shares: 0)

## Miscella
neous

[**The Rise of Diffusion Models**](https://twitter.com/carlcarrie/status/1746643702896796040): The article explor
es the increasing use of diffusion models in timeseries forecasting, detailing 11 specific versions, their theoretical b
asis, effectiveness on various datasets, and comparisons between them. (2024-01-14, shares: 0)

[**Marimo: Reinventing J
upyter**](https://twitter.com/carlcarrie/status/1745999782659723373): The article presents Marimo, a revamped version of
 the Jupyter Python notebook, designed to be a reproducible, interactive, and shareable Python program, as opposed to an
 error-prone JSON scratchpad. (2024-01-13, shares: 0)

[**HBR on Fabrications and LLM Problems**](https://twitter.com/ca
rlcarrie/status/1745962927629185407): The article reviews the Harvard Business Review's perspective on plausible fabrica
tions and other issues related to LLM in the context of productivity transformation led by LLM. (2024-01-13, shares: 0)


# Videos

## Quantitative

[**Mastering Python Finance**](https://www.youtube.com/watch?v=qkKAJwg4ohQ): The Certificate
 in Python for Finance Program provides in-depth knowledge on financial data science, asset management, algorithmic trad
ing, and computational finance using Python and AI. (2024-01-11, shares: 5.0)

[**AI and Finance Future**](https://www.y
outube.com/watch?v=T-AyBUMcWeg): The AFA Panel on AI discusses the influence of AI on the financial sector with panelist
s from various universities. (2024-01-13, shares: 11.0)

[**RAW AI in Finance Workshop 3**](https://www.youtube.com/watc
h?v=5w99hUczjyY): A screen recording from the Workshop on AI in Finance at Texas State University San Marcos is accessib
le on GitHub. (2024-01-11, shares: 0.0)

[**AFA Business Meeting & Awards**](https://www.youtube.com/watch?v=BYZntg0228I
): The AFA Business Meeting and Presidential Address involves discussions on the future of finance from professionals an
d academics. (2024-01-14, shares: 9.0)

# Reddit

## Quantitative

[**Choosing the Right Field: Quantitative Finance Jou
rney**](https://www.reddit.com/r/quant/comments/196oru9/how_did_you_know_that_the_quant_field_was_right/): As an AI, I'm
 unable to browse the internet or access articles directly. Please provide the text of the articles you'd like summarize
d. (2024-01-14, shares: 41.0)

[**Trouble at Jump Trading**](https://www.reddit.com/r/quant/comments/193vnrs/trouble_at_
jump_trading/): The UK government is contemplating the introduction of a digital currency, 'Britcoin', to update its fin
ancial system. (2024-01-11, shares: 70.0)

[**Ito's Lemma Query**](https://www.reddit.com/r/quant/comments/196fqrp/quest
ion_regarding_itos_lemma/): A recent study suggests that global sea levels could rise over a meter by 2100 due to climat
e change. (2024-01-14, shares: 20.0)
```
---

     
 
all -  [ DSPy and ColBERT with Omar Khattab! ](https://www.reddit.com/r/deeplearning/comments/197bh2j/dspy_and_colbert_with_omar_khattab/) , 2024-02-03-0910
```
I am beyond excited to publish our first Weaviate Podcast interview in-person at the NeurIPS conference with Omar Khatta
b from Stanford University!

I am beyond grateful to have met Omar! I believe strongly that he is at the forefront of Ar
tificial Intelligence technology, especially with the latest developments in Large Language Models, Retrieval-Augmented 
Generation, and Vector Databases!

Omar is a prolific scientist who has published many groundbreaking works, the latest 
of which being DSPy! DSPy is also an open-source software project on GitHub, achieving roughly 5,000 stars at the time o
f this writing! I think this is just scratching the surface of where DSPy will go. I think to reach this potential, the 
next step is developer advocacy and evangelism work. I will be the first to admit that it took me a couple tries to unde
rstand the abstractions of DSPy. The framework marries the concepts of pipeline design (really well explained by the abs
tractions in LangChain, LlamaIndex, Haystack, or Weaviate modules), with prompt and model tuning. I think Omar did an am
azing job of explaining this further in the podcast, so I will stop writing this and encourage you to check out the podc
ast below haha!

Omar also touched on ColBERT and multi-vector retrieval methods. These techniques aim to achieve the be
nefits of the contextual interaction in cross-encoders, directly in a vector index, without the slow inference of applyi
ng a cross encoder of a query and millions of documents. Omar again does an incredible job explaining such a complex top
ic, stay tuned for more updates from Weaviate on multi-vector support!

I really hope you enjoy the podcast! I am beyond
 grateful to have attended the NeurIPS conference and met so many amazing people!

YouTube: [https://www.youtube.com/wat
ch?v=CDung1LnLbY](https://www.youtube.com/watch?v=CDung1LnLbY)

Spotify: [https://podcasters.spotify.com/pod/show/weavia
te/episodes/DSPy-and-ColBERT-with-Omar-Khattab----Weaviate-Podcast-85-e2effki](https://podcasters.spotify.com/pod/show/w
eaviate/episodes/DSPy-and-ColBERT-with-Omar-Khattab----Weaviate-Podcast-85-e2effki)
```
---

     
 
all -  [ Thoughts on Potential of LLMs/Foundation Models for Zero-Shot Time Series Forecasting [D] ](https://www.reddit.com/r/MachineLearning/comments/194h40f/thoughts_on_potential_of_llmsfoundation_models/) , 2024-02-03-0910
```
Hi all, I've stumbled upon this Neurips paper 'Large Language Models Are Zero-Shot Time Series Forecasters'   [2310.0782
0.pdf (arxiv.org)](https://arxiv.org/pdf/2310.07820.pdf?trk=public_post_comment-text)  and wonder what people in time se
ries think about it. The paper's authors summarize the method: 'At its core, this method represents the time series as a
 string of numerical digits, and views time series forecasting as next-token prediction in text'.

The authors seem to s
how performance nearly matching and sometimes exceeding the standard baseline such as ARIMA on DARTS baseline, with no f
urther training. I wonder what the time series people on here think of these results and whether it's likely that there 
will be foundation models for time series forecasting that will outperform current specialized forecasting methods. 

Th
anks!
```
---

     
 
all -  [ [D] How to request to be a reviewer to a conference/journal? ](https://www.reddit.com/r/MachineLearning/comments/1945n6i/d_how_to_request_to_be_a_reviewer_to_a/) , 2024-02-03-0910
```
I'm interested in reviewing for the upcoming cycles of ECCV, Neurips, ICLR, AAAI etc. 

Would also like to review for jo
urnals like T-PAMI etc. 

How does one go about this? Should I just email the editor of the journal or conference or is 
there a better way of doing it?
```
---

     
 
all -  [ Weaviate at NeurIPS 2023! (Interview Series) ](https://www.reddit.com/r/deeplearning/comments/193dmjz/weaviate_at_neurips_2023_interview_series/) , 2024-02-03-0910
```
Hey everyone! We had an incredible time at the NeurIPS conference this year in New Orleans! We learned a ton, met so man
y amazing people, and... put together our first in-person podcast series!!

I am beyond excited to share 10 interviews h
osted by Erika Cardenas and I:

* Jay Alammar from Cohere
* Alex Chao from Microsoft
* Div Garg from MultiOn
* Michael G
oin from Neural Magic
* Omar Khattab from Stanford University
* Liam Li from Determined AI
* Andriy Mulyar from Nomic
* 
Jacob Marks from Voxel51
* Sebastian Raschka from Lightning AI
* Alex Volkov from Weights & Biases

These interviews bro
adly discuss the state of AI, impressions of the NeurIPS conference, and their respective projects and companies! I hope
 you enjoy! Weaviate at NeurIPS 2023!!

Link: [https://www.youtube.com/watch?v=xrZxk0H2cmY](https://www.youtube.com/watc
h?v=xrZxk0H2cmY)
```
---

     
 
all -  [ AI Partnerships, Intelligence Augmentation, and Open Source Research: A Look at Current AI Trends ](https://www.reddit.com/r/ai_news_by_ai/comments/191d4cy/ai_partnerships_intelligence_augmentation_and/) , 2024-02-03-0910
```





#major_players #leaders #science #tool #opensource #paper #opinions #scheduled

Isomorphic Labs has announced colla
borations with Eli Lilly and Company and Novartis, two leading pharmaceutical companies. These partnerships could be wor
th nearly $3 billion to Isomorphic Labs, excluding any royalties from future drug sales. The collaborations are focused 
on small molecules and involve upfront and milestone payments, marking the beginning of the impact that AI will have on 
accelerating drug discovery [2].







There is a suggestion to refer to current technology as 'intelligence augmentati
on' (IA) instead of 'artificial intelligence' (AI). IA implies that the technology is more like tools that require human
 interaction, which better reflects the current state of technology. AI, on the other hand, implies independent agents [
3][4]. Andrej Karpathy, an author, supports this view, believing in intelligence amplification rather than creating a su
perintelligent entity to replace humans. He aims to build tools that enhance and extend human information processing cap
abilities [5]. Another author suggests that tools should be seen as amplifying imagination, not just extending intellige
nce, and mentions the emergence of generative intelligence amplification [6].







ML/AI research is mostly free from 
commercial publishing restrictions. Preprints are shared on platforms like ArXiv and OpenReview, while short papers are 
presented at conferences such as ICLR, NeurIPS, and ICML. Longer papers are published in journals like JMLR and TMLR. Al
l of these venues are open access and free for both readers and authors [7]. The fast progress in AI research can be att
ributed to the frequent publication of preprints on ArXiv and the exchange of open source code [8]. Despite the importan
ce of peer review, there is noise associated with it, especially for conferences. Posting papers on ArXiv allows them to
 be accessible even if they are rejected by reviewers [13].







Language Model Machines (LLMs) are beneficial for cod
ing despite their inability to plan. They are capable of reproducing plans that they have been trained on, but they do n
ot possess actual planning abilities [9][11]. The author suggests that the way humans plan is similar to how optimal con
trol systems plan trajectories, but humans do it hierarchically [12].







Venture capital firm Andreessen-Horowitz ha
s written a letter to the UK House of Lords in support of open source AI platforms [10].







The author expresses sym
pathy towards OpenAI and Microsoft in the lawsuit filed by The New York Times (NYT). The lawsuit claims that OpenAI and 
Microsoft used millions of copyrighted NYT articles to train their models. The author believes that training on public i
nternet documents should be covered under fair use, similar to how humans can read and learn from online articles [14].











[1. Midjourney @midjourney https://twitter.com/midjourney/status/1743874374749573303](https://twitter.com/midj
ourney/status/1743874374749573303)

[2. Demis Hassabis @demishassabis https://twitter.com/demishassabis/status/174404673
8493599942](https://twitter.com/demishassabis/status/1744046738493599942)

[3. Andrej Karpathy @karpathy https://twitter
.com/karpathy/status/1744062425576636499](https://twitter.com/karpathy/status/1744062425576636499)

[4. Andrej Karpathy 
@karpathy https://twitter.com/karpathy/status/1744062845426532473](https://twitter.com/karpathy/status/17440628454265324
73)

[5. Andrej Karpathy @karpathy https://twitter.com/karpathy/status/1744179910347039080](https://twitter.com/karpathy
/status/1744179910347039080)

[6. Andrej Karpathy @karpathy https://twitter.com/karpathy/status/1744200417784045799](htt
ps://twitter.com/karpathy/status/1744200417784045799)

[7. Yann LeCun @ylecun https://twitter.com/ylecun/status/17440314
34770108832](https://twitter.com/ylecun/status/1744031434770108832)

[8. Yann LeCun @ylecun https://twitter.com/ylecun/s
tatus/1744032223165104254](https://twitter.com/ylecun/status/1744032223165104254)

[9. Yann LeCun @ylecun https://twitte
r.com/ylecun/status/1744040682296099078](https://twitter.com/ylecun/status/1744040682296099078)

[10. Yann LeCun @ylecun
 https://twitter.com/ylecun/status/1744131432509768024](https://twitter.com/ylecun/status/1744131432509768024)

[11. Yan
n LeCun @ylecun https://twitter.com/ylecun/status/1744135793243767102](https://twitter.com/ylecun/status/174413579324376
7102)

[12. Yann LeCun @ylecun https://twitter.com/ylecun/status/1744136146332885261](https://twitter.com/ylecun/status/
1744136146332885261)

[13. Yann LeCun @ylecun https://twitter.com/ylecun/status/1744139516720320816](https://twitter.com
/ylecun/status/1744139516720320816)

[14. Andrew Ng @AndrewYNg https://twitter.com/AndrewYNg/status/1744145064115446040]
(https://twitter.com/AndrewYNg/status/1744145064115446040)
```
---

     
 
all -  [ Why don't we build a really good agent to help you write AI Research Papers ](https://www.reddit.com/r/LaTeX/comments/1911j0q/why_dont_we_build_a_really_good_agent_to_help_you/) , 2024-02-03-0910
```
Hi folks, 

I have recently written an AI research paper and finished up a few experiments. 

It has come to my attentio
n that I have been manually moving my hand-written proofs of the theorems into the Appendix Section. Sometimes, I make a
 mistake, and I have syntax errors on Overleaf. 

What's even worse is that when I get tired, I lose a page of paper and
 the translated 'proof' is only halfway done, and I start confusing myself.  Overall, it's not the nicest experience for
 a user. 

Given the current progression of the war on LLMs, I started to search if there is an AI research assistant wh
o could write the equations for me, given a photo of my writing or a simple description.  
Here is some of my findings: 


LaTex writing assistant: 

\- [LaTex AI](https://www.producthunt.com/products/latext-ai)\- [LaTex AI](https://www.prod
ucthunt.com/products/latext-ai) seems to have a nice demo on formalising, explaining and prompting to improve the text w
riting. However this product seems not actively maintained. 

\- [text.cortex](https://app.textcortex.com/) seems to be 
a valid tool that's able to turn **An expectation of f(x) over Z** into **$\\mathbb{E}\_{z\\in Z}\[f(x)\]$.** I am not s
ure if it's able to handle something that's more complex.   


On handwriting to LaTex side. [mathpix](https://mathpix.c
om/), seems to be the only product that's working for me. Desipite that they claim to be used by FAIR and Anthropic, but
 it's snapping window is way too small to just one line of equation, which is not scalable.   


On LaTex to Python impl
ementation, I could not find anything useful. 

So, I wants to throw an open question to the community, what are you exp
ecting to squeeze out of an AI agent, if it's helping you write your next NeurIPs paper, or doing research as in general
.   

```
---

     
 
all -  [ [Hiring] Abridge is hiring a Data Engineering Lead to Support ML/Research team - Fully Remote, US-Ba ](https://www.reddit.com/r/dataengineeringjobs/comments/1903yye/hiring_abridge_is_hiring_a_data_engineering_lead/) , 2024-02-03-0910
```
Hey All! My team at [Abridge](https://www.abridge.com/) is hiring a full-time Data Engineering Lead to join our fully re
mote team!

**Location:** Remote \[US-based\]

**Base Salary:** $190,000 - $225,000 USD \[Negotiable DOE\] + Equity

**S
tart Date:** ASAP

**We're looking for someone with the following experience (required):**

* 6+ years of experience in 
a data engineering role
* Experience building pipelines and platforms from the ground up at other startups, first data e
ngineering hire within a startup or experience taking over infrastructure development at more established companies
* Ex
perience working on projects that actively support the work of Machine Learning engineers/researchers/data scientists.
*
 Experience with data engineering tools: 
   * Streaming (Kafka, Kinesis)
   * Batch Processing (Spark, Flink)
   * Data
 warehouses (Snowflake, BigQuery)
* Programming skills in Python
* Google Cloud Platform (GCP) preferred, Amazon Web Ser
vers (AWS) experience is OK
* Experience working at a fast-paced startup or other hyper-growth environment

[Full Job De
scription](https://jobs.ashbyhq.com/Abridge/5c0d9bb2-4b4c-492c-ae71-7867156fb6d8)

Abridge is a Series B healthcare star
tup that is using AI to revolutionize the way medical conversations are captured, understood, and followed through on. W
e're a mission-driven company with a strong culture of innovation, collaboration, and continuous learning.

Our scientis
ts have made significant contributions to the development of AI and its application in healthcare, bringing experience f
rom Carnegie Mellon, Meta, Amazon AI, Microsoft and other top research institutions. Our work has already been published
 at some of the top AI conferences in the world - including ACL, NeurIPS, ICML and Interspeech!

The Lead Data Engineer 
will help us design, build, and operate the data platform required to scale our business at the forefront of machine lea
rning in healthcare. You’ll work alongside our growing team of engineers, analysts, and other stakeholders to create the
 next generation of our data pipeline and warehouse, and you will work with our machine learning scientists to ensure th
ey have timely, secure and compliant access to the data they need to train our machine learning models.

If interested, 
feel free to [apply directly here](https://jobs.ashbyhq.com/Abridge/5c0d9bb2-4b4c-492c-ae71-7867156fb6d8/application)!
```
---

     
 
all -  [ [HIRING] Abridge is Hiring a Senior Platform Engineer! - Kubernetes, Terraform, GCP - Fully Remote,  ](https://www.reddit.com/r/devopsjobs/comments/19033cb/hiring_abridge_is_hiring_a_senior_platform/) , 2024-02-03-0910
```
Hey All! My team at [Abridge](https://www.abridge.com) is hiring a full-time Senior Platform Engineer to join our fully 
remote team! 

**Location:** Remote \[US-based\]

**Base Salary:** $180,000 - $205,000 USD \[Negotiable DOE\]  + Equity


**Start Date:** ASAP

**We're looking for someone with the following experience:** 

* 5+ years of experience in simila
r role 
* Kubernetes
* Terraform
* Networking experience
* Experience working with IAM, PHI, PII, and/or other sensitive
/highly secure data
* Google Cloud Platform (GCP) preferred, Amazon Web Servers (AWS) experience is OK
* Experience work
ing at a fast-paced startup or other hyper-growth environment

[Full Job Description](https://jobs.ashbyhq.com/Abridge/8
a3d9406-5403-4fd5-b66f-b163a3bc3505)

Abridge is a Series B healthcare startup that is using AI to revolutionize the way
 medical conversations are captured, understood, and followed through on. We're a mission-driven company with a strong c
ulture of innovation, collaboration, and continuous learning.

Our scientists have made significant contributions to the
 development of AI and its application in healthcare, bringing experience from Carnegie Mellon, Meta, Amazon AI, Microso
ft and other top research institutions. Our work has already been published at some of the top AI conferences in the wor
ld - including ACL, NeurIPS, ICML and Interspeech!

The Senior Platform Engineer will work alongside our application and
 machine learning engineers to plan and deploy services using Terraform, Kubernetes & Google Cloud Platform (GCP). You’l
l also help us set up and secure integrations with our customers and partners!

If interested, feel free to [apply direc
tly here](https://jobs.ashbyhq.com/Abridge/8a3d9406-5403-4fd5-b66f-b163a3bc3505/application)!   

```
---

     
 
all -  [ Natural Language Processing (NLP) Learning Path - In depth ](https://www.reddit.com/r/learnmachinelearning/comments/18yo5kp/natural_language_processing_nlp_learning_path_in/) , 2024-02-03-0910
```
Hi friends, i'm currently engaged in NLP and created an pretty extense roadmap or learning path so begginers don't feel 
lost, it covers from the basics to advanced cutting-edge concepts.

Feedback is appreciated.

&#x200B;

\_-\_-\_-\_-\_-\
_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-

&#x200B;

NLP Learning Roadmap

1. Prerequisites:

&#x200B;

* Mathematics:

&#x
200B;

* Linear algebra
* Probability and statistics

&#x200B;

* Programming:

&#x200B;

* Proficiency in a programming
 language (e.g., Python)

**2. Introduction to NLP:**

&#x200B;

* Definition      and scope of NLP
* Historical      de
velopment of NLP
* Key challenges      and applications

**3. Text Analysis:**

&#x200B;

* **Lexical Analysis:**

&#x20
0B;

* Word meaning and structure

· Morphology (word formation)

· lemmatization (base form identification)

&#x200B;


* **Syntactic Analysis:**

· Parts-of-speech tagging

· Dependency parsing

· Constituency parsing

&#x200B;

* **Semant
ic Analysis:**

· Extracting meaning

· Encompassing word embedding models like Word2Vec and GloVe

· Topic modeling

&#
x200B;

* **Semantic Analysis:**

· Coreference resolution

· Discourse analysis

&#x200B;

**3. Text Processing:**

&#x
200B;

* **Tokenization:**

&#x200B;

* Sentence tokenization
* Word tokenization
* Subword tokenization (Byte Pair Enco
ding, SentencePiece)

&#x200B;

* **Stop Words Removal:**

&#x200B;

* Importance and impact on NLP tasks
* Customizing 
stop word lists

&#x200B;

* **Stemming and Lemmatization:**

&#x200B;

* Porter stemming algorithm
* Snowball stemming 
algorithm
* Lemmatization techniques and challenges

&#x200B;

* **Part-of-Speech Tagging:**

 

* POS tagging algorithm
s (HMM-based, rule-based, and neural-based)
* Fine-grained POS tagging

**4. Text Representation:**

&#x200B;

* **Bag o
f Words (BoW):**

 

* Term Frequency (TF) and Inverse Document Frequency (IDF)
* Bag of N-grams

&#x200B;

* **TF-IDF:*
*

 

* Calculating TF-IDF scores
* Applications in information retrieval

&#x200B;

* **Word Embeddings:**

 

* Word2V
ec:

&#x200B;

* Continuous Bag of Words (CBOW) model
* Skip-gram model
* GloVe (Global Vectors for Word Representation)


&#x200B;

* **Contextual Embeddings:**

 

* ELMo (Embeddings from Language Models)
* ULMFiT (Universal Language Model
 Fine-tuning)
* OpenAI GPT (Generative Pre-trained Transformer)

**5. NLP Libraries and Tools:**

&#x200B;

* NLTK      
(Natural Language Toolkit)
* SpaCy
* scikit-learn
* Transformers      library (Hugging Face)

**6. Statistical Language 
Models:**

&#x200B;

* **N-grams:**

 

* Unigrams, bigrams, and trigrams
* N-gram language models

&#x200B;

* **Hidden
 Markov Models (HMM):**

 

* Basics of HMMs
* Applications in part-of-speech tagging

**7. Machine Learning for NLP:**


&#x200B;

* **Supervised Learning:**

 

* Text classification algorithms (Naive Bayes, Support Vector       Machines)

* Evaluation metrics (precision, recall, F1-score)

&#x200B;

* **Named Entity Recognition (NER):**

 

* Rule-based NER

* Machine learning-based NER
* Evaluation metrics for NER

&#x200B;

* **Sentiment Analysis:**

 

* Sentiment lexicons

* Machine learning approaches for sentiment analysis

**8. Sequence-to-Sequence Models:**

&#x200B;

* **Recurrent Neur
al Networks (RNN):**

 

* Vanishing and exploding gradient problems
* Bidirectional RNNs

&#x200B;

* **Long Short-Term
 Memory (LSTM):**

 

* Architecture and key components
* Gating mechanisms

&#x200B;

* **Gated Recurrent Unit (GRU):**


 

* Simplified gating compared to LSTM
* Applications and advantages

**9. Deep Learning Architectures for NLP:**

&#
x200B;

* **Convolutional Neural Networks (CNN) for Text:**

 

* Text classification with CNNs
* Hierarchical and multi
-channel CNNs

&#x200B;

* **Transfer Learning in NLP:**

 

* Fine-tuning pre-trained models
* Universal Sentence Encod
er

&#x200B;

* **Transformer Architecture:**

 

* Self-attention mechanism
* Multi-head attention
* Positional encodin
g

**10. Transduction and Recurrency:**

&#x200B;

* **Transduction in NLP:**

 

* Definition and applications
* Challe
nges in sequence-to-sequence transduction

&#x200B;

* **Recurrent Neural Networks (RNN):**

 

* Applications beyond se
quence-to-sequence tasks
* Challenges in training RNNs

**11. Advanced Topics in Sequence Modeling:**

&#x200B;

* **Att
ention Mechanism:**

 

* Scaled Dot-Product Attention
* Position-wise Feedforward Networks

&#x200B;

* **Self-Attentio
n Mechanism:**

 

* The concept of self-attention
* Layer normalization in self-attention

&#x200B;

* **Multi-Head Att
ention:**

 

* Motivation and benefits
* Combining multiple attention heads

**12. Syntax and Parsing:**

&#x200B;

* *
*Dependency Parsing:**

 

* Dependency tree representation
* Transition-based and graph-based parsing

&#x200B;

* **Co
nstituency Parsing:**

 

* Treebank representation
* Earley parsing algorithm

&#x200B;

* **Parsing Techniques:**

 


* Chart parsing (CYK parser)
* Shift-Reduce parsing

**13. Semantic Role Labeling (SRL) and Coreference Resolution:**

&
#x200B;

* **Semantic Role Labeling:**

&#x200B;

* PropBank and FrameNet
* Neural approaches to SRL

&#x200B;

* **Core
ference Resolution:**

&#x200B;

* Mention detection
* End-to-end coreference resolution models

**14. Evaluation Metric
s:**

&#x200B;

* Precision,      Recall, F1-score
* BLEU      score for machine translation
* Perplexity      for langu
age models

**15. NLP in Industry and Research:**

&#x200B;

* Case      studies and applications in various domains (he
althcare, finance, legal,      etc.)
* Emerging      research trends in NLP

**16. Ethical Considerations and Bias in NL
P:**

&#x200B;

* **Addressing Bias in NLP Models:**

&#x200B;

* Identifying and mitigating biases in training data
* F
airness-aware machine learning

&#x200B;

* **Ethical Considerations in NLP Research and      Deployment:**

&#x200B;

*
 Privacy concerns in NLP
* Responsible AI practices in NLP

**17. Continuous Learning and Keeping Updated:**

&#x200B;


* Follow      conferences (ACL, NAACL, EMNLP)
* Engage      with the NLP community
* Explore      recent research papers
 and advancements (Arxiv, NeurIPS)

**18. Projects and Hands-on Practice:**

&#x200B;

* Apply      knowledge through pr
actical projects
* Contribute      to open-source NLP projects
* Participate      in Kaggle competitions

==============
=================
```
---

     
