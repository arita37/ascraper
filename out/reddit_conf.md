 
all -  [ [D] Are people in ML Phds still happy? ](https://www.reddit.com/r/MachineLearning/comments/17fahm6/d_are_people_in_ml_phds_still_happy/) , 2023-10-25-0909
```
As an outsider who has many friends in ML Phds, this is my perspective of their lives:

1. long hours, working nights, w
eekends
2. no work-life balance, constant fear of being scooped and time pressure from deadlines
3. frustrating broken r
eview systems
4. many incremental, advertisement papers that produce very little actual contribution (which is justified
 by 2.)
5. 'engineering' and not 'science'
6. all this pressure amounts to severe imposter syndrome

Are people in the f
ield still happy? Where do people get their satisfaction? To me it looks like almost like a religion or a cult. The sele
ct few who say, get neurips outstanding paper are promoted to stardom - almost a celebrity status while everyone else su
ffers a punishing work cycle. Are the phd students all banking on AGI? What else motivates them?

Edit: the discussion i
s about whether 1-6 are worse in ML than other fields (or even the median experience). The reference for 'other field' i
s highly heterogenous. Experience obviously varies by lab, and then even by individuals within labs. 'It happens in othe
r fields too' is a trivial statement - of course some version of 1-6 affects somebody in another field.

Edit 2: small n
 but summarizing the comments - experience seems to differ based on geographic region, one's expectations for the phd, a
bility to exert work-life balance, and to some extent ignore the trends others are all following. Some people have reson
ated with problems 1-6, yet others have presented their own, anecdotal solutions. I recommend reading comments from thos
e who claim to have solutions.
```
---

     
 
all -  [ [R] Biologically plausible vision models for classification and grasping tasks ](https://www.reddit.com/r/MachineLearning/comments/17ea25h/r_biologically_plausible_vision_models_for/) , 2023-10-25-0909
```
Hey everyone! I am looking for papers that propose or explore biologically plausible vision models, primarily tasks like
 classification and grasping (predicting grasping bounding boxes) tasks. By biologically plausible, I mean papers that p
ropose models inspired by the human brain in some way or the other. I know convolution is loosely inspired by human cogn
ition, but everything I can find seems to suggest the opposite for ViT like models.

I have come across certain papers l
ike these:
- https://arxiv.org/abs/1901.00945
- https://proceedings.neurips.cc/paper/2020/hash/98b17f068d5d9b7668e19fb8a
e470841-Abstract.html

But I am still looking for more. Any suggestions?
```
---

     
 
all -  [ Inpaintint Not working ](https://www.reddit.com/gallery/17d832h) , 2023-10-25-0909
```
So I'm a begginer, I've been using SD for sometime. My inpaint Stopped working 1 or 2 months ago(i think).I usually upda
te to the latest version everytime.

I'm using A1111 direct ml (by Ishqqytiger) on Rx 590 8gb, chrome as browser, window
s 10 as os.

The generated image is either the same or has a blur on inapinted area. increasing mask blur also doesn't d
o anything it just makes the exact same face but with a weird sort of light blur on it.

I have tried changing every pos
sible setting to fix it such as clip skip, model, original mask mode, latent noise mask mode and the other 2 mask mode a
s well, changing cfg scale, denoising strength, sampler steps, samplers, inpaint area, resolution.

my command args is i
n the screen shot.

is this caused my some new update or some extension bug (I was not using any extension while generat
ing the images), or is it something just bad with my machine or is there a problem with SD directml version.
```
---

     
 
all -  [ [R] How to compare research results? ](https://www.reddit.com/r/computervision/comments/17cczj7/r_how_to_compare_research_results/) , 2023-10-25-0909
```
Hello all,

I am conducting research in the field of ViT. Research focuses on developing a method to improve ViT on a sm
all dataset from scratch and using ImageNet weights. In literature, I found similar work is already been proposed in the
 paper 'Efficient Training of Visual Transformers with Small Datasets' [https://proceedings.neurips.cc/paper/2021/file/c
81e155d85dae5430a8cee6f2242e82c-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e8
2c-Paper.pdf).

My question is with whom to compare my method? should I compare with **this paper** or should I compare 
my results with the **original** ViT-S/32, ViT-B/32, ViT-T/32, ViT-T/16, SWIN-T, CVT, T2T.

Further, should I use the sa
me dataset or can I replace some with other datasets?
```
---

     
 
all -  [ [R] How to compare research results? ](https://www.reddit.com/r/MachineLearning/comments/17ccypi/r_how_to_compare_research_results/) , 2023-10-25-0909
```
Hello all,

I am conducting research in the field of ViT. Research focuses on developing a method to improve ViT on a sm
all dataset from scratch and using ImageNet weights. In literature, I found similar work is already been proposed in the
 paper 'Efficient Training of Visual Transformers with Small Datasets' [https://proceedings.neurips.cc/paper/2021/file/c
81e155d85dae5430a8cee6f2242e82c-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e8
2c-Paper.pdf). 

My question is with whom to compare my method? should I compare with **this paper** or should I compare
 my results with the **original** ViT-S/32, ViT-B/32, ViT-T/32, ViT-T/16, SWIN-T, CVT, T2T.

Further, should I use the s
ame dataset or can I replace some with other datasets?
```
---

     
 
all -  [ MSCS Profile Evaluation - Fall 2024 ](https://www.reddit.com/r/gradadmissions/comments/17c2w15/mscs_profile_evaluation_fall_2024/) , 2023-10-25-0909
```
B.E. (ongoing) from a Tier 2 Institution in India  
CGPA: 9.46  
Research Papers: 4 (One accepted at NeurIPS workshop)  

Internships: 2 (one research-based at a US startup)  
LoRs: 2 from professors whom I have worked with (published resear
ch papers) and one from the startup (the founder is an MIT Postdoc)

This is where I am afraid:  
TOEFL: 100 (R:27, L:26
, S:23, W:24)  
This was my 2nd attempt to improve my speaking score but still couldn't do it.

Shortlisted Universities
:  


1. **Ambitious**  
MIT  
Stanford  
Princeton  
UCB  
UIUC  
UCSD  
UCLA  
UT Austin  
UMCP
2.  **Moderate**  
UMa
ss Amherst  
Purdue  
NEU  
ASU
```
---

     
 
all -  [ [R] Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data (NeurIPS ](https://www.reddit.com/r/MachineLearning/comments/17bzbaq/r_neural_relation_graph_a_unified_framework_for/) , 2023-10-25-0909
```
**paper**: [https://arxiv.org/abs/2301.12321](https://arxiv.org/abs/2301.12321)

**code**: [https://github.com/snu-mllab
/Neural-Relation-Graph](https://github.com/snu-mllab/Neural-Relation-Graph)

**TDLR**: We present a scalable and domain-
agnostic approach utilizing the relational structure of data for identifying label noise and outliers

https://preview.r
edd.it/o9k7kliqe9vb1.png?width=3108&format=png&auto=webp&s=b7c34bd7f4bc130915440986570104f9bebd4f07

>Diagnosing and cle
aning data is a crucial step for building robust machine learning systems. However, identifying problems within large-sc
ale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, un
der-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by u
tilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this 
end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational gr
aph structure of data. We further introduce a visualization tool that provides contextual information of a data point in
 the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error
 and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and lang
uage domain tasks, including ImageNet, ESC-50, and SST2. Our approach achieves state-of-the-art detection performance on
 all tasks considered and demonstrates its effectiveness in debugging large-scale real-world datasets across various dom
ains.

&#x200B;

[Detected samples with label error \(red colored\) from ImageNet \(top\) and SST2 \(bottom\).](https://
preview.redd.it/4x2244ure9vb1.png?width=2778&format=png&auto=webp&s=6c6cb9f8d1befce392d31546eab569ffa70d74cc)

&#x200B;


[Detected outlier samples from ImageNet \(top\) and SST2 \(bottom\) validation sets.](https://preview.redd.it/2kkfdspxe
9vb1.png?width=2720&format=png&auto=webp&s=312249638229b4cb5e815b76c1ef8309a829581b)
```
---

     
 
all -  [ [D] Has anybody heard back from NeurIPS financial aid yet? ](https://www.reddit.com/r/MachineLearning/comments/17bsyp6/d_has_anybody_heard_back_from_neurips_financial/) , 2023-10-25-0909
```
Was supposed to be Monday but instead it's rolling
```
---

     
 
all -  [ [R] Curve your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models ](https://www.reddit.com/r/MachineLearning/comments/17bfgsj/r_curve_your_enthusiasm_concurvity_regularization/) , 2023-10-25-0909
```
**Accepted at NeurIPS 2023**

*Link:* [https://arxiv.org/abs/2305.11475](https://arxiv.org/abs/2305.11475)

*Authors:* J
ulien Siems\*, Konstantin Ditschuneit\*, Winfried Ripken\*, Alma Lindborg\*, Maximilian Schambach, Johannes Otterbach, M
artin Genzel

\*equal contribution

*Abstract:* Generalized Additive Models (GAMs) have recently experienced a resurgenc
e in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear tran
sformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity - i.e., (possib
ly non-linear) dependencies between the features - has hitherto been largely overlooked. Here, we demonstrate how concur
vity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regulari
zer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicabl
e to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability b
y eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer 
in experiments on synthetic as well as real-world datasets for time-series and tabular data. Our experiments show that c
oncurvity in GAMs can be reduced without significantly compromising prediction quality, improving interpretability and r
educing variance in the feature importances.

*Keywords:* Interpretable Machine Learning, Generalized Additive Models, C
oncurvity, Multicollinearity, Regularization, Time-Series Forecasting, Interpretability

https://preview.redd.it/wtdkhb3
bsbvb1.png?width=1002&format=png&auto=webp&s=3067a2361f55603bf4b7769eaede32ca1f32496f
```
---

     
 
all -  [ prof eval for AI PhD, declined last cycle ](https://www.reddit.com/r/gradadmissions/comments/17b3kxe/prof_eval_for_ai_phd_declined_last_cycle/) , 2023-10-25-0909
```
Finished undergrad in math/cs in 2.5 years from a solid research uni in the US (top 30 overall, top 10 for cs). 3.7 GPA 
with a few grad courses

1 tiny publication in astrophysics (not related at all)
1 first/solo author publication at a po
pular workshop in my field. 
5 papers under submission to WACV, ICLR, NeurIPS workshops (will know 4/5 of them by the en
d of the month). Likely will see 2-3 acceptances out of the 4. 

Currently ai research intern after undergrad and had a 
prev ai research internship, so like 1-1.5 years total experience

signed up for GRE and took it pretty much immediately
, got 157 verbal 168 quant and not sure if I should retake it & do a full prep. 

Main priority:
UCSD DSC / CSE PhD

Oth
ers:
UCSD ECE PhD (ML track)
Harvard CS PhD (really good research match there)
UCLA CS PhD (good research match too)
MIT
 PhD

Harvard MIT are big reach, but how far off am I for UCSD? is it worth to take GRE again? Last year I didn’t really
 talk to a lot of professors or have a good research match at UCSD, so I got declined (also no papers).

Edit: graduated
 9 months ago, applied to similar schools last year with no papers (was working on them, but not finished), no GRE, and 
got declined.

My field of research is generally in deep learning architectures (NAS, Model Compression, Quantization, S
SL)
```
---

     
 
all -  [ How can I stay up-to-date with the latest advancements in machine learning after completing a course ](https://www.reddit.com/r/u_nearlearns/comments/1755o0j/how_can_i_stay_uptodate_with_the_latest/) , 2023-10-25-0909
```
 

In the fast-evolving realm of [machine learning](https://nearlearn.com/blog/top-10-machine-learning-training-institut
e-bangalore/), staying up-to-date with the latest advancements is crucial. As the technology landscape constantly evolve
s, it’s essential to keep pace with the newest trends, methodologies, and breakthroughs to remain relevant and competiti
ve. This article provides a comprehensive guide on how to stay informed and at the forefront of the ever-changing field 
of machine learning.

### Why Staying Updated is Essential

Before delving into strategies for staying current, let’s fi
rst understand why it’s imperative to do so. Machine learning is a dynamic and innovative domain, and advancements occur
 at a breakneck pace. Here are a few reasons why staying updated is crucial:

1. Competitive Edge: In a highly competiti
ve job market, professionals who are well-versed in the latest developments in machine learning have a significant advan
tage. Cutting-edge knowledge can make you stand out among your peers.
2. Relevance: Machine learning models, tools, and 
techniques quickly become outdated. By staying current, you ensure your skills and knowledge are relevant, avoiding obso
lescence.
3. Innovation: The latest advancements often lead to new applications and possibilities. Staying updated enabl
es you to be at the forefront of innovation, allowing you to create groundbreaking solutions.

### Leveraging Online Res
ources

### 1. Online Courses and Tutorials

To keep abreast of the latest in machine learning, consider enrolling in on
line courses and tutorials. Websites like Coursera, edX, and Udacity offer a plethora of courses taught by experts in th
e field.

### 2. Blogs and Forums

Following [machine learning](https://nearlearn.com/machine-learning-classroom-trainin
g-in-bangalore-india) blogs and participating in relevant forums is another effective way to stay updated. Websites like
 Towards Data Science and Kaggle provide valuable insights, discussions, and community support.

### 3. YouTube and Podc
asts

Visual and auditory learners can benefit from machine learning YouTube channels and podcasts. These platforms offe
r engaging content from industry experts, often in a digestible format.

### 4. Social Media

Platforms like Twitter and
 LinkedIn are excellent for following influential figures and organizations in the machine learning space. Regularly che
cking your social media feeds can keep you informed about the latest news, research, and trends.

&#x200B;

https://prev
iew.redd.it/z85dalvu4itb1.png?width=1200&format=png&auto=webp&s=21413a440d23fcce49c4b203445b0b8693169126

**Read More** 
: [Everything You Need To Know About Machine Learning In 2023](https://nearlearn.com/blog/everything-you-need-to-know-ab
out-machine-learning-in-2023/)

### Attending Conferences and Meetups

1. Machine Learning Conferences: Attending confer
ences like NeurIPS, ICML, and ICLR provides an opportunity to learn from thought leaders and connect with peers. These e
vents showcase the most recent research and developments in the field.
2. Local Meetups: Joining machine learning meetup
s in your area can help you stay updated on local developments and network with professionals who share your interests.


### Academic Journals and Publications

1. Research Papers: Regularly reading academic journals and research papers is 
essential for a deep understanding of the latest advancements. Websites like ArXiv and Google Scholar are valuable resou
rces.
2. Books: Explore textbooks and publications by experts in machine learning. These provide comprehensive insights 
and foundational knowledge.

### Hands-On Learning

1. Personal Projects: Applying your knowledge through personal proje
cts allows you to experiment with new concepts and techniques. It’s an excellent way to gain practical experience with t
he latest advancements.
2. Competitions: Participate in machine learning competitions on platforms like Kaggle. These co
mpetitions often involve cutting-edge challenges that push your skills to the limit.

### Networking and Collaboration


1. Join Professional Networks: Being part of professional organizations like the Association for Computing Machinery (AC
M) or the Institute of Electrical and Electronics Engineers (IEEE) can help you connect with experts in the field.
2. Co
llaborate with Peers: Collaborative projects with colleagues and peers can expose you to different perspectives and inno
vative ideas.

### Continuous Learning

The field of machine learning is characterized by constant change. To stay ahead
, it’s crucial to embrace a mindset of continuous learning. Regularly set aside time for self-improvement, be it through
 online courses, conferences, or personal projects.

### In Conclusion

Staying updated with the latest advancements in 
machine learning is vital for personal and professional growth. In this dynamic field, knowledge is power, and being wel
l-informed can open doors to exciting opportunities and innovations. By leveraging online resources, attending conferenc
es, reading academic publications, engaging in hands-on learning, and building a network of like-minded individuals, you
 can ensure that you’re always at the forefront of this ever-evolving field.
```
---

     
 
all -  [ Profile Review for my second attempts in AI PhD (mastered out my current) ](https://www.reddit.com/r/gradadmissions/comments/174nkew/profile_review_for_my_second_attempts_in_ai_phd/) , 2023-10-25-0909
```
**Area of Interest:** RL + NLP + Multi-agent

**Education:** BASc Chemical Engineering from UToronto, MSc Computer scien
ce from PKU, mainland China (CSRanking Asia #2 World #14). Transitioned from PhD due to unhappy experiences and career c
hoices. Undergrad cGPA 3.3/4.0, PhD cGPA 3.45/4.00

**GRE:** no plan on taking

**TOFEL**: Not required maybe given my B
ASc in Canada?

**Internship Experience**:

\- 1 year at Cenovus Energy Canada as DS intern (Co-op program during my bac
helor)

\- 1.2 year at Beijing Institute of General AI (similar but inferior to MSRA)

\- \~3 months at a Chinese LLM un
icorn startup

**Publications**:

\- 2 NeurIPS (1 co-first, 1 second), 1 ICLR (co-first), all posters no awards.

\- Thi
rd author and beyond in 2 other arXiv papers (both under review)

\- Given authorship in that startup's technical report
 of their open-sourced LLM models

\- Citation count is not good (\~20)

**LORs:**

**-** 1 from my supervisor who does 
nothing but a very nice guy (full professor)

\- 1 from my frequent collaborater who does the heavy lifting (assistant p
rofessor)

\- 1 from my technical manager at the start-up

I can't think of anyone else who could write me a very detail
ed letter, I don't know other professors really well. But these guys promised me good letters.

**Short-listed Uni so fa
r:**

*Ambitious:*

\- UWashington (dream school, very strong in NLP, one prof agree to host me for a potential visiting
)

\- UTAustin (very strong in NLP as well)

\- UIUC (strong in RL)

\- UCLA (costly)

\- UC San Diego (super costly)

\
- GeorgiaTech

*Less Ambitious:*

\- UToronto (strong in CS Theory, none of the profs aligns with my reserach direction)


\- UWaterloo (a couple of incoming profs have very interesting topics)

&#x200B;

*Safe*: people were saying safe scho
ol isn't a thing

*Nationality: Canadian (if this really matters)*

I don't have the balls to aim for the Top 4 (Stanfor
d, UCB, MIT, CMU). I assume you probably need like 10 CVPR papers to even be considered qualified.

Any suggestion/comme
nt/advice is greatly appreciated.

&#x200B;
```
---

     
 
all -  [ Final year PhD student aiming to transitioning from ML PhD to industry. ](https://www.reddit.com/r/resumes/comments/174fy9s/final_year_phd_student_aiming_to_transitioning/) , 2023-10-25-0909
```
I am a final year PhD student in France working on privacy and  fairness topics in deep neural networks. My publication 
record is decent but not top-tier (think EMNLP, TMLR rather than ICML/NeurIPS).  Recently, I've been applying to data sc
ience and machine learning  engineering roles. However, out of 50-75 applications over the past few  weeks, I have not r
eceived any callbacks.  
I'm wondering if there is something I should optimize on resume.  Any tips would be greatly app
reciated!  


P.S. - I also posted something similar to another sub, and the key advice I received was to add publicatio
ns on the second page.   
Resume - [https://i.imgur.com/x2IavdR.jpg](https://i.imgur.com/x2IavdR.jpg)
```
---

     
 
all -  [ Seeking advice on transitioning from ML PhD to industry ](https://www.reddit.com/r/cscareerquestionsEU/comments/173rtr4/seeking_advice_on_transitioning_from_ml_phd_to/) , 2023-10-25-0909
```
I am currently a final year PhD student in France working on privacy and fairness topics in deep neural networks. My pub
lication record is decent but not top-tier (think EMNLP, TMLR rather than ICML/NeurIPS). Recently, I've been applying to
 data science and machine learning engineering roles. However, out of 50-75 applications over the past few weeks, I have
 not received any callbacks.  
I'm wondering if anyone here has been in a similar situation or has advice on how to effe
ctively market myself in industry? I've attached my anonymized resume as well. Any tips would be greatly appreciated!  

Resume - https://i.imgur.com/x2IavdR.jpg
```
---

     
 
all -  [ How to write LOR ](https://www.reddit.com/r/MSCS/comments/171gsl6/how_to_write_lor/) , 2023-10-25-0909
```
Hey there!

I am a third year bachelor cse student and I want to apply to top unis in the UK, USA and Switzerland. I hav
e asked one of the profs for a recommendation letter (with whom I have done a research project which turned into a paper
, currently in submission at a workshop at NeurIPS). He agreed, and he allowed me to write a draft first. What are the u
nis looking for, exactly? What abilities should be emphasized in the letter? Is it just the usual hard-working student w
ho learns very fast and proved to have research skills etc?

Thank you very much!
```
---

     
 
all -  [ Profile evaluation for Master's (in CS mostly) applications in the US (Fall 24) ](https://www.reddit.com/r/gradadmissions/comments/16znemg/profile_evaluation_for_masters_in_cs_mostly/) , 2023-10-25-0909
```
Hi all,

I am looking for a realistic evaluation of my profile for MS CS applications in the US for Fall 2024. All help 
appreciated.

Undergrad college - Indian Institute of Technology (IIT), Delhi 

Undergrad Major - Mathematics and Comput
ing

Undergrad GPA - 9.53/10

Internships - 2 research internships (at Adobe Research & Max Planck Institute for Softwar
e Systems), 1 web development internship at a startup

Job experience - 1.5 years at a research lab in Adobe

Publicatio
ns - 2 first-author papers (at CVPR 2023 workshops and BMVC 2023). 2 papers currently under review at WACV and NeurIPS w
orkshops 

Patents - 3 filed (US PTO), 1 awaiting internal review

Field of interest: Computer Vision

GRE: 326/340 (Q:1
70, V:156, AWA:4.5/6)

TOEFL: TBD

Shortlisted universities/programs so far: MIT Media Lab, Stanford, CMU (MSR, MSML, MS
CV), UC Berkeley, UIUC, UT Austin, Georgia Tech, UCSD, Cornell.

What are my chances at these universities? Any specific
 things to consider while applying? and, any colleges/programs I should consider adding/removing from the list above? Al
so, is the GRE score too low for CMU?


Thank you in advance!
```
---

     
 
all -  [ Final Call for Papers: NeurIPS 2023 Workshop on Generalization in Planning ](https://groups.google.com/g/ml-news/c/ouC1kILPcS0) , 2023-10-25-0909
```

```
---

     
 
all -  [ Need help for University short listing | MS/PHD CS fall 2024 ](https://www.reddit.com/r/gradadmissions/comments/16viqze/need_help_for_university_short_listing_msphd_cs/) , 2023-10-25-0909
```
I am applying for fall 2024 for both MS and PhD in CS, with specialization in ML (NLP, Speech)

Nationality: Indian

Col
lege: top 10 in India 

CGPA: 9.8/10; CS Department rank 1

Research papers: Total 5. 2 in top speech conferences (like 
ICASSP; however speech conferencs are much less selective/reputable in comparison to say ICML, NeurIps), 1 in Comp biolo
gy journal (not sure of exact reputation but looked good to me), 2 in ML IEEE journals (international but pretty mediocr
e to the trained eye). 4 are 1st author, 1 is 3rd author.

LORs: 2 should be super strong; 1 should be strong

Work ex: 
1.5 years in ~FAANG.

Current shortlist:

PhD: MIT, UCB, UCL (London)

MS (research focused): MS CS Stanford, MLT CMU LT
I, MS CS UIUC, MS CS UT Austin, MS CS UW Madison.

Future goals: PhD if masters experience is good and I like living in 
academia in US, otherwise industry research position.

My current list is all quite ambitious and risky, but my philosop
hy is I should be happy/excited to get admission to the least favourite school in my list, otherwise no point.

Help nee
ded: Any suggestions for other excellent MS CS research focused masters (not professional masters) that I can add to abo
ve list? It's currently 8, I am planning to apply for a total of 10, so I need to decide 2 more. My current list of univ
ersities provide decent chance of landing funding in form of TA/RA, this is important to me and that's why I am avoiding
 UCSD, MS CS CMU, GATech, Columbia, NYU.
```
---

     
 
all -  [ Resume review for a software engineering internship (urgent!) ](https://www.reddit.com/r/Resume/comments/16tuunq/resume_review_for_a_software_engineering/) , 2023-10-25-0909
```
Hello. I am an international masters student in the US, applying for software engineering internships, but I haven't had
 any luck getting interviews so far. It turns out that I had been formatting my resume in a way that is not the norm her
e (but in a format which worked back in my home country). This is my first attempt at a resume since this realization; I
 more or less followed Jake's template. I would *really* appreciate any feedback on this resume, on the formatting as we
ll as the actual content!  


https://preview.redd.it/5thjr5viwuqb1.png?width=1344&format=png&auto=webp&s=0bab6f7296960f
0263caaccc47d635ffd88dba5f

[https://imgur.com/a/ASDLvV8](https://imgur.com/a/ASDLvV8)  


&#x200B;
```
---

     
 
all -  [ MS CS Profile Evaluation Fall 2024 ](https://www.reddit.com/r/gradadmissions/comments/16rwgd1/ms_cs_profile_evaluation_fall_2024/) , 2023-10-25-0909
```
 Academic Profile

· B.E. Electronics and Communication + Data Science Minor

· GPA - 9.29 from Tier 1 Indian university
 (Top 10 in department / Top 15 in uni)

· GRE: 169 Q 162V AWA 5

· TOEFL : Should get 110-115

========================
===============

Work & Research Experience

· \~1 year at Brown University

· 4 months at TU Dresden

· Upcoming thesis
 at Cambridge

· Two projects under university professors

=======================================

LORs

· PI from Brow
n University (Strong - Moderate)

· PI from TU Dresden (Strong - Moderate)

· BITS professor (Academic based rather than
 research based)

=======================================

Publication

· Co-Authorship in paper that got Oral Presentat
ion at ICML

· Submitting first authorship paper at NeurIPS workshop (relatively sure of its acceptance)

==============
=========================

Awards

· DAAD Schoalrship

· MITACS Scholarship

=======================================

Un
iversities and classification

Ambitious :

* ETH Zurich CS
* Cambridge MLMI
* Princeton CS
* UCSD CS
* UIUC

Moderate:


* EPFL DS
* GATech CS
* UC London ML
* UMaryland
* UT Austin

Safe(?) :

* Uni Edinburgh DS
* UCSD ECE MLDS
* NYU Coura
nt
* Brown

My main target is either a ML Scientist/Engineer role or a PhD in some focused topic in ML. So suggestions f
or universities according to the ML focus and research output along with my chances are greatly appreciated. I feel my p
rofile is quite good my main fear is that my degree is not CS (i have taken a few CS electives and DS minor to try and c
over up)
```
---

     
 
all -  [ Progressing from a small-ish ML startup to bigger name companies ](https://www.reddit.com/r/cscareerquestions/comments/16r1pym/progressing_from_a_smallish_ml_startup_to_bigger/) , 2023-10-25-0909
```
About to finish my PhD in a specific application of ML. The university is a pretty famous French institution, but withou
t much AI/ML prestige right now. I have a bunch of publications in industry specific conferences/journals, but no NeurIP
S/ICML/etc. After that, I'm going to work as an ML researcher for a startup which does some research and some product wo
rk.

In the long run, my ideal position would be something line a research scientist/engineer at an industry lab - I sti
ll want to do research, but definitely not in academia. DeepMind/Google, Meta, Anthropic and the likes would be perfect,
 with some preference for the first one.

What I'm looking for now is advice how to head towards that goal. As a PhD stu
dent I had - at least theoretically - a lot of flexibility in what I do, and if I got a few NeurIPS papers, I'd be golde
n. Now, I'll probably spend most of my time working on client products. This will certainly help me learn a lot and give
 many necessary skills, but that won't be as apparent on my resume.

So - how can I make sure that throughout working in
 a startup, I get stuff that will be appreciated by future, hopefully higher-profile employers, at least enough to get m
e through the resume screening stage?
```
---

     
 
all -  [ US B1 visa expedited appointment for NEURIPS! ](https://www.reddit.com/r/usvisascheduling/comments/16r1j5q/us_b1_visa_expedited_appointment_for_neurips/) , 2023-10-25-0909
```
I have an accepted paper in NEURIPS 2023, which will be held in Dec 2023.

I am planning to make an expedited appointmen
t for it, as mentioned on the website from Abu Dhabi, UAE (I am a resident here, but my passport is Indian)  
What are t
he chances/ wait times of expedited appointments?

Would you suggest any faster ways, given my passport (India) and coun
try of residence (UAE)?

I know about applying from other countries but they have huge denial rates is what I hear. Are 
any countries that allow non-residents to interview and have good chances of approval?

I am new to all this and would g
reatly appreciate help.  


  

```
---

     
