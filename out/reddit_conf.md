 
all -  [ Publishing in a journal vs top conference for computational biology ](https://www.reddit.com/r/bioinformatics/comments/16a2wtg/publishing_in_a_journal_vs_top_conference_for/) , 2023-09-05-0909
```
My group has been able to create some novel, medium impact findings in the field of  'omics sequence labeling using dila
ted networks.  We are considering on where we would like to send our results to.

Since the work is highly within the in
tersection of applied deep learning and computational biology, we are indecisive on whether to send our work towards a d
eep learning confrerence (ICML, neurIPS, ICLR) or to send it to a traditional journal within this domain.  Additionally,
 there are some promising conferences within the machine learning and computational biology arising (MLCB).  How does on
e pick between these options?
```
---

     
 
all -  [ [D] NeurIPS reviewers edited review and score after discussion period: can they delete their own rev ](https://www.reddit.com/r/MachineLearning/comments/1687luu/d_neurips_reviewers_edited_review_and_score_after/) , 2023-09-05-0909
```
Hi, we have a paper submission to NeurIPS and we have two reviewers who changed their scores and review content silently
 by editing the original review comment and score after the discussion period. The edited review comment now discusses e
ntirely different point.

We would like to raise this concern to AC but the thing is that we didn’t save the original re
view comment, and the “revision history” for some reason doesn’t show the previous content, other than the entry that th
ere was previous version. But this revision history overall isn’t inconsistent (showing the last two history after the d
iscussion period, but the ones before the period is not shown) 


Can reviewers delete their own revision history in Ope
nReview tool? I don’t know if this is a bug or they deleted them with an intention.
```
---

     
 
all -  [ [D] Am I the only one finding this a bit upsetting? ](https://www.reddit.com/r/MachineLearning/comments/167n0g0/d_am_i_the_only_one_finding_this_a_bit_upsetting/) , 2023-09-05-0909
```
Hello everyone,

In the process of writing up a literature review for my master's thesis, I wanted to cover the impact o
f ReLU on the field which was significant. When looking for an original paper I came across this paper/report: [https://
arxiv.org/abs/1803.08375](https://arxiv.org/abs/1803.08375). There isn't anything special about this work and as a matte
r of fact, I was surprised that it has thousands of citations (2974 at the moment of writing this post according to Goog
le Scholar). Given this and that this work is not an original ReLU paper but more of a file documenting an implementatio
n of it for a particular setup I found it quite intriguing. Then I started to dig into works that cited this and unexpec
tedly papers from top conferences such as NeurIPS cited the aforementioned document as a reference to the activation fun
ction. Here are some examples:

1. [https://proceedings.neurips.cc/paper\_files/paper/2022/file/fbb10d319d44f8c3b4720873
e4177c65-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/fbb10d319d44f8c3b4720873e4177c
65-Paper-Conference.pdf)
2. [https://proceedings.neurips.cc/paper\_files/paper/2022/file/69e2f49ab0837b71b0e0cb7c555990f
8-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/69e2f49ab0837b71b0e0cb7c555990f8-Pape
r-Conference.pdf)

The researchers who have done that are not referencing the original ReLU paper instead which I think 
is a bit disrespectful towards the achievement of original authors. On the other hand, maybe I am overthinking it a bit.
 ReLU has been around for a while and it would be surprising for someone conducting research in deep learning to not kno
wing it hence as a reader I wouldn't necessarily mind if people did not include the reference to the paper which is wide
ly known. However, I reckon if a reference is made, then it should be meaningful and correct, and not just another extra
 few lines in a bibliography making it look big.
```
---

     
 
all -  [ Road from ML PhD to Data Scientist/MLE ](https://www.reddit.com/r/learnmachinelearning/comments/166872e/road_from_ml_phd_to_data_scientistmle/) , 2023-09-05-0909
```
Hi all;  


So, I have just received my PhD in Machine Learning. From a known uni in the EU, I had a slightly above-aver
age PhD (Neurips, AAAI, UAI papers). Now I am starting a postdoc at another uni, for 2 years. However, I am getting cold
 feet about academia and thinking of leaving for industry. 

My question is: what can I do in the next two years to incr
ease my chances of finding data science or MLE roles? Being a more theory-oriented researcher, my SWE game is very weak.
 I need to show employers that I have the tech stack knowledge. Would certificates help if I train for, say, Google's ML
 Engineer certification? What about data science? I do not expect to start senior or middle-level just because I have a 
PhD. I am more than okay starting as a junior, knowing that my SWE game is weak, and I don't have any industry experienc
e. I don't mind starting from the bottom of the barrel.

Follow-up questions:

1. For all those who are going to say: ce
rtificate is useless, you need experience; how do I get the first job to get experience then?  


&#x200B;
```
---

     
 
all -  [ [D] Limit the Number of Papers I Review on OpenReview? ](https://www.reddit.com/r/MachineLearning/comments/162uumz/d_limit_the_number_of_papers_i_review_on/) , 2023-09-05-0909
```
Hello,

Does anyone know if it's possible to set a limit to the number of papers you are assigned as a reviewer on OpenR
eview? Specifically for ICLR 2024. I saw a Twitter thread about this option before for ICML. It blows my mind that this 
is not easy to change. I got 5 papers for the last NeurIps which was very overwhelming. As reviewers, we provide a free 
service to the community, and we should be allowed to pick how much work we want to undertake...
```
---

     
 
all -  [ 1 month left! NeurIPS 2023 Gaze Meets ML Workshop 2nd Edition ](https://groups.google.com/g/ml-news/c/2SNlOcCTGC4) , 2023-09-05-0909
```

```
---

     
 
all -  [ [Discussion] Should religion-based workshops exist in ML conferences ](https://www.reddit.com/r/MachineLearning/comments/161c7zm/discussion_should_religionbased_workshops_exist/) , 2023-09-05-0909
```
Over the years, ML conferences had a lot of workshops such as women in ML, LatinXAL etc. that are aimed at increasing th
e diversity in the ML community. I've always been supportive of these workshops as I've seen first-hand how some of them
 face obstacles just based on their gender or ethnicity.   


However, I recently saw a tweet for Muslim in ML workshop 
at NeurIPS and I am not sure how to feel about it. They say it's a workshop meant for 'those who self-identify as Muslim
, or work on research that address challenges faced by Muslims'. I am not exactly sure what they mean by research that a
ddress challenges faced by Muslims. Over that, I don't think religion-based workshops in a science conference is a good 
idea. I think religion should be kept out of science, and I don't know if tomorrow n different religion based workshops 
are going to popup. 

Like I said, I'm not completely sure if I'd support such a workshop or not, but I'd love to hear w
hat other folks in ML research community think about it. Before someone calls me Islamophobic, I'm talking about any rel
igion-based workshop in general, not just Muslim in ML. I'd have made this post even if I saw a Christian in ML or Jews 
in ML workshop. 

  


&#x200B;
```
---

     
 
all -  [ Meta AIs Code Llama and Googles Quantum Experiments: Advancements in AI and Quantum Computing ](https://www.reddit.com/r/ai_news_by_ai/comments/160nxjm/meta_ais_code_llama_and_googles_quantum/) , 2023-09-05-0909
```





#major_players #tool #release #opensource #leaders #api #science #paper #event #scheduled

Meta AI has launched Cod
e Llama, a large language model (LLM) based on Llama 2, specifically designed for coding tasks. This state-of-the-art mo
del is available for both research and commercial use[1]. Code Llama can generate code based on text prompts, aiming to 
enhance developer workflows and simplify the process of learning to code. It supports popular programming languages and 
comes in three sizes with different parameters, providing stable generations with up to 100,000 tokens of context[2]. Co
de Llama has outperformed other open-source LLMs in benchmark tests and has undergone safety measures to mitigate risks[
2]. Notable figures such as Yann LeCun have shared their experiences using Code Llama as a debugging helper[3].







G
oogle AI has announced a workshop called ATTRIB at NeurIPS 2023. The workshop invites researchers and practitioners to s
ubmit papers and ideas on attributing model behavior to training data, algorithms, architecture, and more. The aim is to
 advance the understanding of model behavior attribution and address challenges in understanding the influence of traini
ng datasets, subcomponents within models, and algorithmic choices on model performance[4].







In a blog post, an app
roach using in-context learning and a novel algorithmic prompting technique to enable algorithmic reasoning capabilities
 in large language models was discussed. The approach leverages algorithmic prompting, which extracts algorithmic reason
ing abilities from language models by outputting the steps needed for an algorithmic solution and providing detailed exp
lanations for each step. The results show that the model can reliably execute algorithms on out-of-distribution examples
 and achieve strong generalization on arithmetic problems[6].







Google AI has made progress in developing useful be
yond-classical quantum experiments that can be performed on current noisy quantum processors. They have introduced a fra
mework for measuring the computational cost of a quantum experiment, called the 'effective quantum volume'. This framewo
rk has been applied to evaluate the computational cost of three recent experiments: random circuit sampling, out of time
 order correlators (OTOCs), and a Floquet evolution related to the Ising model[7].







Satya Nadella emphasizes the i
mportance of building and deploying AI in a safe, secure, and transparent manner to expand opportunities in India and be
yond[5].




[1. Meta AI @metaai https://twitter.com/metaai/status/1694729071325007993](https://twitter.com/metaai/statu
s/1694729071325007993)

[2. Yann LeCun @ylecun https://twitter.com/ylecun/status/1694741307652964600](https://twitter.co
m/ylecun/status/1694741307652964600)

[3. Yann LeCun @ylecun https://twitter.com/ylecun/status/1694741931375362357](http
s://twitter.com/ylecun/status/1694741931375362357)

[4. Google AI @googleai https://twitter.com/googleai/status/16947474
22734606780](https://twitter.com/googleai/status/1694747422734606780)

[5. Satya Nadella @satyanadella https://twitter.c
om/satyanadella/status/1694747170744987779](https://twitter.com/satyanadella/status/1694747170744987779)

[6. Google AI 
@googleai https://twitter.com/googleai/status/1694795671503786297](https://twitter.com/googleai/status/16947956715037862
97)

[7. Google AI @googleai https://twitter.com/googleai/status/1694836751305744764](https://twitter.com/googleai/statu
s/1694836751305744764)
```
---

     
 
all -  [ [D] NeurIPS 2023 Paper Reviews - Datasets and Benchmarks ](https://www.reddit.com/r/MachineLearning/comments/160m2aw/d_neurips_2023_paper_reviews_datasets_and/) , 2023-09-05-0909
```
I saw a few reddit posts about the [main track](https://www.reddit.com/r/MachineLearning/comments/15fo7td/d_neurips_2023
_paper_reviews/) reviews and wanted to create a discussion post for the datasets and benchmarks. 

As a first time submi
tter, I'm curious if there are any different experiences between the main track and the datasets track.
```
---

     
 
all -  [ Data independent sparsification of models after training ](https://www.reddit.com/r/deeplearning/comments/1608q3h/data_independent_sparsification_of_models_after/) , 2023-09-05-0909
```
I was looking at papers on model pruning or quantization that aims to make inference faster and/or reduce size of the mo
del. [Most](https://proceedings.neurips.cc/paper_files/paper/2022/file/1caf09c9f4e6b0150b06a07e77f2710c-Paper-Conference
.pdf) of [them](http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf) rely on calibration data to identify weights th
at can be pruned. I am skeptical about this approach since the calibration data could be skewed and in the process of pr
uning the model could be overfitting on that small sample of data. Are there data independent approaches to post-trainin
g sparsification?
```
---

     
 
all -  [ [D] NeurIPS Discussion phase has ended. How was the overall experience for you ? ](https://www.reddit.com/r/MachineLearning/comments/15xygyr/d_neurips_discussion_phase_has_ended_how_was_the/) , 2023-09-05-0909
```
I am not sure if 'Discussion' was always part of the Neurips pipeline but I felt like it was a good addition (in princip
le). 

On one hand it alows the authors to present their case with more clarity. On the other hand, it does increase the
 overhead for the reviewers which are now required to work even harder (and for free). 

For me, it was a mixed bag. Mos
t of the reviewers did engage and the discussion was indeed fruitful.  However, some didn't bother to follow up on the r
esponses to their concerns and questions. Unfortunately, also quite expected. 

I would definitely like to see this in t
he next Neurips but maybe with some tweaks and modifications keeping in mind the (unpaid) reviewers.
```
---

     
 
all -  [ AI/ML PhD: Who to collaborate with during MS ](https://www.reddit.com/r/gradadmissions/comments/15wq9ad/aiml_phd_who_to_collaborate_with_during_ms/) , 2023-09-05-0909
```
 

**Tldr:** As I am about start my MS (in about a week's time), I need to pick collaborators/mentors to do research and
 publish papers with. **I want to maximize my chances of getting into a top AI PhD program like MIT, Berkeley, etc.** To
 that end, should I do research with people I've already have a relationship with? Should I aim to maintain those connec
tions (in hopes of receiving a glowing recommendation in the future)? Or should I end those collaborations and try to fo
rm new connections with different professors who are more 'reputed' and/or well-connected to top universities like Princ
eton, Berkeley etc.?? For context I go to a top 15 school **in USA**.

**Situation #1:** Over the summer I did a researc
h engineering gig with a scientist at my university. Not a professor, a 'scientist.' We worked on object detection for s
elf-driving cars. I enjoyed the experience and found him to be a very knowledgeable, constructive, and approachable ment
or. The conversations we have sometimes are almost like two friends talking. I've developed a good relationship with him
 and he has praised my capabilities and motivation. **However:** he isn't a very 'reputed' researcher in the field- at l
east not compared to professors at my university. After all, his job isn't to publish full-time, but rather to deliver M
L-based products for government agencies. And so, my work with him wasn't to conduct 'research', it was more about doing
 literature review and implementing existing approaches. More of a 'research engineering' gig.

**My question:** Should 
I reach out to actual professors at my university working in ML and computer vision? They publish more papers, and they 
are better connected to top-universities like Berkeley. Although I have never worked with these professors, I will also 
have the opportunity to take there courses so I can develop a relationship with them. Or, should I collaborate further w
ith the scientist? The thing is, since I didn't do real 'research' with him over the summer, we will be essentially **st
arting from scratch** and working on some entirely new problems. The only advantage is he is already familiar with who I
 am and definitely has a favorable view of me. I am leaning towards the professor, because I feel like since I only work
ed with the scientist for a few months, I'm not losing much in terms of the relationship.

**Situation #2: Let's forget 
about the scientist now, this is a different person.** I reached to a full-time professor (not even associate professor,
 he is a full professor, and my school is top 15). He is a senior member of IEEE. He also has quite a few publications a
t top conferences like ICML, NeurIPS etc. Thus it is safe to say he is a well-reputed researcher. Although my connection
 with this professor is not as strong as the above-described self-driving scientist, he thinks I have a good background,
 and we have agreed to collaborate over the Fall during my MS. I have done some basic preparatory work over the summer (
reading background papers etc.). Additionally, I will most likely take courses that he teaches at my university, so I wi
ll get to connect with him better.

**My question:** Should I cancel this collaboration and try to work with a different
 (associate) professor who studied at Princeton and did his postdoc at Berkeley? My thinking is that he's very well-conn
ected so his recommendation letter might mean a lot/open doors for me. Only problem is I don't know what I would tell th
e professor I've already agreed to work with...

 
```
---

     
 
all -  [ AI/ML: Who to do research with during MS (to maximize PhD chances) ](https://www.reddit.com/r/PhD/comments/15wozmv/aiml_who_to_do_research_with_during_ms_to/) , 2023-09-05-0909
```
**Tldr:** As I am about start my MS (in about a week's time), I need to pick collaborators/mentors to do research and pu
blish papers with. **I want to maximize my chances of getting into a top AI PhD program like MIT, Berkeley, etc.** To th
at end, should I do research with people I've already have a relationship with? Should I aim to maintain those connectio
ns (in hopes of receiving a glowing recommendation in the future)? Or should I end those collaborations and try to form 
new connections with different professors who are more 'reputed' and/or well-connected to top universities like Princeto
n, Berkeley etc.?? For context I go to a top 15 school **in USA**.

&#x200B;

**Situation #1:** Over the summer I did a 
research engineering gig with a scientist at my university. Not a professor, a 'scientist.' We worked on object detectio
n for self-driving cars. I enjoyed the experience and found him to be a very knowledgeable, constructive, and approachab
le mentor. The conversations we have sometimes are almost like two friends talking. I've developed a good relationship w
ith him and he has praised my capabilities and motivation. **However:** he isn't a very 'reputed' researcher in the fiel
d- at least not compared to professors at my university. After all, his job isn't to publish full-time, but rather to de
liver ML-based products for government agencies. And so, my work with him wasn't to conduct 'research', it was more abou
t doing literature review and implementing existing approaches. More of a 'research engineering' gig.

&#x200B;

**My qu
estion:** Should I reach out to actual professors at my university working in ML and computer vision? They publish more 
papers, and they are better connected to top-universities like Berkeley. Although I have never worked with these profess
ors, I will also have the opportunity to take there courses so I can develop a relationship with them. Or, should I coll
aborate further with the scientist? The thing is, since I didn't do real 'research' with him over the summer, we will be
 essentially **starting from scratch** and working on some entirely new problems. The only advantage is he is already fa
miliar with who I am and definitely has a favorable view of me. I am leaning towards the professor, because I feel like 
since I only worked with the scientist for a few months, I'm not losing much in terms of the relationship.

&#x200B;

**
Situation #2: Let's forget about the scientist now, this is a different person.** I reached to a full-time professor (no
t even associate professor, he is a full professor, and my school is top 15). He is a senior member of IEEE. He also has
 quite a few publications at top conferences like ICML, NeurIPS etc. Thus it is safe to say he is a well-reputed researc
her. Although my connection with this professor is not as strong as the above-described self-driving scientist, he think
s I have a good background, and we have agreed to collaborate over the Fall during my MS. I have done some basic prepara
tory work over the summer (reading background papers etc.). Additionally, I will most likely take courses that he teache
s at my university, so I will get to connect with him better.

&#x200B;

**My question:** Should I cancel this collabora
tion and try to work with a different (associate) professor who studied at Princeton and did his postdoc at Berkeley? My
 thinking is that he's very well-connected so his recommendation letter might mean a lot/open doors for me. Only problem
 is I don't know what I would tell the professor I've already agreed to work with...

&#x200B;

&#x200B;
```
---

     
 
all -  [ [D] How many times you try for acceptance in AI conference? ](https://www.reddit.com/r/MachineLearning/comments/15wonms/d_how_many_times_you_try_for_acceptance_in_ai/) , 2023-09-05-0909
```
ICML 2023 was my first trial. I've got polarized scores, 7/6/4/3, and got rejected. At this moment, I was so disappointe
d not for the result, but for the quality of review. (The last reviewer didn't read the paper at all.) For the final dec
ision, the last review was so bad as well, not presenting any reason of rejection.

With the same topic, I got 6/5/5/4/4
/3 from the NeurIPS 2023. The quality of reviewer is much better than ICML, and I've learned many things from the review
er, though they said the score will not be changed.

I think I should submit it to another conference again, ICLR or CVP
R. I just wonder how many submissions are tried for the acceptance in average. Just for reference.
```
---

     
 
all -  [ [P] References to help write a Neurips (Workshop) Paper? ](https://www.reddit.com/r/MachineLearning/comments/15ulou8/p_references_to_help_write_a_neurips_workshop/) , 2023-09-05-0909
```
I've been working on a specific project for a while now, and was interested in submitting my work to a Nips Workshop.

N
ow, I had a look at the Nips submission guidelines, they remain the same for the workshop except the page limit for main
 content is 6 pages instead of 9. I tried going over the Nips latex style, but feel pretty intimidated by the sheer amou
nt of rules. Would there be any guideline/blog I could use as a reference while writing my paper?

P S: Another thing, I
'm quoting from the workshop website:

'The workshop will not have proceedings (or in other words, it will not be archiv
al), which means you can submit the same or extended work as a publication to other venues after the workshop. This mean
s we also accept submissions to other venues, as long as they are not published before the workshop date in December. '


I was not sure as to what this means. So if my paper gets accepted, does that mean I can submit the whole thing again t
o a journal later? Or an extension of it?
```
---

     
 
all -  [ [N] NeurIPS Large Language Model Efficiency Challenge: 1 LLM + 1GPU + 1Day ](https://www.reddit.com/r/MachineLearning/comments/15uhw4l/n_neurips_large_language_model_efficiency/) , 2023-09-05-0909
```
[Model Efficiency Challenge](https://llm-efficiency-challenge.github.io/)

>A challenge for the community to adapt a fou
ndation model to specific tasks by fine-tuning on a **single GPU** of either 4090 or A100 (40GB) within a **24-hour** (1
-day) time frame, while maintaining high accuracy for these desired tasks.

&#x200B;
```
---

     
 
all -  [ Alpha's Precursor: The System Before the Quantitative System ](https://www.reddit.com/r/quant/comments/15ugmot/alphas_precursor_the_system_before_the/) , 2023-09-05-0909
```
I thought some of you might find the following insightful, it gives you an idea of the signal/recuitment divide in quant
 finance.

# Talking

It's hard to stay up to date with the daily deluge of quant finance research. Top institutional fi
rms **filter** before they read. With quants being paid upwards of (you know what), their time can't be wasted trawling 
twitter and linkedin. A reasonably good filtering mechanism is conferences and internal seminars.

My first experience w
ith this was in 2019 when Cubist systematic invited me to a seminar after publishing my first paper on predicting earnin
gs surprises on SSRN.

'We run a regular semi-monthly seminar series at which professors present recent research to our 
team of Portfolio Managers and analysts. The format is a 1-1.5 hour interactive session followed by dinner... we are hap
py to cover travel costs if you would like to make a special trip to NYC and we will of course accommodate your schedule
.'

It doesn't always involve external speakers, almost every firm I have worked with have some form of internal seminar
 sometimes weekly, sometimes monthly, were an employee has to discuss a new topic of interest.

However, this is not the
 only format anymore. A lot of discussions have morphed into podcasts like Jane Street’s Signals and Threads or Putnam’s
 Active Insights. Podcast recover costs more effectlydue to their marketing alure.

More recently Quant funds have becom
e the top sponsor at prestigious machine learning conferences like *NeurIPS*. The list includes firms like DE Shaw, PDT 
Partners, HRT, Two Sigma, Jane Street, and others. Once more, this is not just an opportunity for employees to obtain co
mplementary tickets to listen to state of the art research, but a great recruitment drive. It signifies “we are great, j
oin us”.

The search for alpha versus recruitment intent is getting somewhat blurred. For example, it is commonly though
t among Kaggle data science participants in challenges set up by Winton, Two Sigma, Jane Street, and G-research, that th
e firms are there to mine the collective crowd-sourced alpha. In fact no, I have been part of developing such a challeng
e, the purpose is almost purely a recruitment drive.

Some firms have taken a further step, they have concluded that in 
addition to giving money to other conferences, they might as well set up out own. There are countless examples, more rec
ently see the G-Research Distinguished Speaker Series or The Discovery: Two Sigma PhD Symposium.

If I had to rate each 
one of these on the recruitment to alpha continuum, it would probably be: (1) data science competitions, (2) podcasts, (
3) conference sponsorship and attendance, (4) conference development, and (5) then internal seminars.

# Reading

Of cou
rse, that list doesn’t stop there, the most interesting part of the continuum is from 6 onward. There is a small industr
y dedicated solely to the capture, curation, and internal dissemination of public research.

It's an open secret in the 
quantitative finance community: the volume of research and discussion generated daily is both a treasure trove and a pot
ential time sink.

When powerhouse names like Acadian Asset Management elucidate on harnessing the disposition effect fo
r a momentum strategy, or AQR delves into the intricate dance of deep learning for identifying optimal lags, it isn't ju
st their direct audience that perks up. Portfolio Managers at other funds are equally, if not more, invested in these in
sights.

So, the question stands: *How do they achieve this level of efficient information assimilation?*

Here's is how
 I approach it for ML-Quant:

&#x200B;

1. **Preemptive Filtering:** Before anything even reaches a quant's desk, it's p
assed through layers of filters. These aren't just keyword-based, but often employ sophisticated algorithms that underst
and context, ensuring only the most relevant pieces make the cut.
2. **Tooling & Infrastructure:** The digital age has b
lessed us with a suite of tools designed to curate and present information. For instance, libraries like Scrapy, BS4, an
d Selenium form the vanguard of data extraction. These are not run on traditional setups but on serverless infrastructur
es, optimizing for both speed and cost.
3. **Hidden Treasures:** Not everything requires the heavy machinery of web scra
ping. Often, a hidden API or even an RSS feed can provide a direct line to the insights. For the discerning quant, this 
is akin to stumbling upon a gold mine, ensuring real-time updates without the overhead of web crawlers.

In essence, the
 world of quantitative finance has evolved. It's no longer just about devising the most sophisticated model or algorithm
 but ensuring that the pipeline of information feeding into these models is both relevant and efficient. In a world wher
e milliseconds can mean millions, can we really afford to be anything less than optimal?
```
---

     
 
all -  [ Google have been discussing the NEED to forget private information from models, here's our solution ](https://www.reddit.com/r/ArtificialInteligence/comments/15smjwk/google_have_been_discussing_the_need_to_forget/) , 2023-09-05-0909
```
Google recently highlighted the importance of Machine unlearning in their [neurips challenge](https://ai.googleblog.com/
2023/06/announcing-first-machine-unlearning.html). The goal is to allow a model to forget information that is private or
 confidential, without destroying model performance on the rest of the data.

We present: [Fast Machine Unlearning Witho
ut Retraining Through Selective Synaptic Dampening](https://arxiv.org/abs/2308.07707)

Selective synaptic dampening (SSD
) is a novel retraining-free approach to let your model forget sensitive data. It's fast, performant, and lightweight. 
 
  
SSD first selects parameters that are considerably more important for the forget set than the retain set. Next, SS
D dampens these parameters proportional to the discrepancy in their importance to the forget and retain set. We achieve 
state of the art on a number of evaluations.  
Happy to answer any questions, or discuss the problem of unlearning!
```
---

     
 
all -  [ Engaging Reviewers during rebuttal period of NeurIPS [R] ](https://www.reddit.com/r/MachineLearning/comments/15s3xq6/engaging_reviewers_during_rebuttal_period_of/) , 2023-09-05-0909
```
I have a paper (theoretical work) at NeurIPS under review right now. We got 4 reviews, 7,7,6,4 with confidence 4,4,4,2. 
We are trying to keep the good reviews there and bring up reviewer 4's score. We responded to all the comments made by r
eviewers, but unfortunately only one of them has engaged (one of 7 reviewers said they were happy with our responses and
 are keeping the score). The others have said nothing and the AC hasn't either. What is my best plan right now? Do I jus
t stay silent or perhaps message the AC? I don't know if silence at this point is in my favor. There is still roughly a 
week left too.Sorry if this is a specific question. This is my first main author submission (1st year PhD student) and m
y advisor has been a bit MIA throughout the review process. 
```
---

     
 
all -  [ Profile evaluation for Fall '24 for ML / CS ](https://www.reddit.com/r/MSCS/comments/15pxowq/profile_evaluation_for_fall_24_for_ml_cs/) , 2023-09-05-0909
```
Hi, 'm a final year undergrad ECE from BITS. Some help regarding ranking the universities as well as some suggestions fo
r safe options for Machine Learning and CS programs would be really helpful!

GPA : 9.29 (6th in my department)

GRE/TOE
FL : Giving in September target is 330+

Experience : Research internship for a year at Brown University, Summer Interns
hip and Thesis at TU Dresden, Upcoming research internship at Cambridge, Summer Internship at CEERI, India

Papers : One
 co-authorship at ICML 2023 ( Oral selection), targeting a NeurIPS workshop for a first author paper (looking likely) an
d maybe push for a major conference in Jan 2024

LoR: 3 academic; Brown (h-index : 40), TU Dresden (h-index : 20) and BI
TS each.

Extra-curricular : None (Main lacking point in my profile imo)

Awards: DAAD and MITACS Scholarship

Target co
lleges:

Ambitious : ETH Zurich, Cambridge, CMU

Moderate : Georgia Tech, TUMunich, UCL, EPFL, U of Toronto, Cornell, UW
ash

Safe : Not decided yet but I was thinking TU Tubingen

I need an idea of how low I should go for my safe colleges a
s I am not able to judge that well.
```
---

     
 
all -  [ Need advice: re-apply or take an offer in Germany? ](https://www.reddit.com/r/gradadmissions/comments/15oijld/need_advice_reapply_or_take_an_offer_in_germany/) , 2023-09-05-0909
```
Last year I applied to 10 CS PhD programs in the US with a focus on machine learning at Stanford, Columbia, CMU, Harvard
, UMaryland, UCSD (both CS and DS PhD), Duke, UCI, UNC Chapel Hill and UIUC with zero papers but 2 years of research exp
erience. Still, I was devastated to get rejected from all of them as I did my research and applied to targeted profs onl
y whose research I was familiar with. I didn’t take GRE and as I mentioned I had zero accepted papers but 3 under submis
sion.

I was pretty low on energy for most of the first half of the year but put myself back together to apply to some m
asters program at Germany — Tubingen and Saarland and got accepted in both of them.

Current research profile:
—————————
————
And fortunately recently one of my papers got accepted at UAI 2023 and I visited the conference as well with some p
otential collaboration opportunities ahead. So after a year my profile looks like this:
 - 1 accepted paper at UAI
 - 1 
under submission at NeurIPS with scores 4,5,4,6,4 (looks like a reject)
 - 1 book chapter in Springer (to be published)

(all relevant to the area I want to do my PhD in)
 - currently doing some stuff on LLMs but doesn’t look like it will be
 in a publishable shape by the application deadlines
 - I do have 5/6 US patents filed, not sure if they are helpful as 
I heard mixed opinions 

My background:
—————————
 - Undergrad in electrical engineering from a top 5 IIT in India with 
GPA 9.15/10.

My letter writers are research scientists at industry with PhDs from US (they know me over 2 years) and on
e IIT professor (also with PhD from US, worked with him for 3 months in an internship).

Question:
—————
I am now vacill
ating between whether to go with the offer at Tubingen or Saarland, or take another shot at the US PhD programs this yea
r. If I need to point out 1 mistake that cost me heavily last year, I would say it’s that I didn’t contact potential sup
ervisors that aggressively but many of them wrote that no need to contact them beforehand! On asking why rejected despit
e being a good match, one prof replied my profile was rated highly but I had “less publications” than others and also di
dn’t submit GRE score (it was optional, I mean wtf)!

I plan to start mailing now and have some meetings next week (one 
with a prof that rejected me last year!) Probably I would take GRE too this time.

Now on the other side, I have enough 
savings to last 1-2 years in Germany, but I would be much better off with a scholarship or stipend that I won’t have in 
the beginning there. I don’t want to break all my FDs and stock market investments!

What do you think? I am scared to a
pply and get rejected all over again, and also a bit anxious to go to Germany without any scholarship or financial assis
tance.
```
---

     
 
all -  [ [D] Lessons from this years Neurips ](https://www.reddit.com/r/MachineLearning/comments/15oic7a/d_lessons_from_this_years_neurips/) , 2023-09-05-0909
```
This years Neurips has been a rollercoaster for everyone involved.

Petar Veličković says that in their AC batch 65% sub
mitted no rebuttal or withdrew.

[https://twitter.com/PetarV\_93/status/1689648854646575105](https://twitter.com/PetarV_
93/status/1689648854646575105)

Xin Eric Wang says in their batch pre-rebuttal no papers had an avg score above an weak 
accept.

[https://twitter.com/xwang\_lk/status/1686517898108674048](https://twitter.com/xwang_lk/status/1686517898108674
048)

Will NeurIPS keep 25% acceptance rate? What do you think will happen to neurips in light of the above? Is this the
 end of big ML confs?

&#x200B;
```
---

     
 
all -  [ Publishing a computer vision work at ICRA or IROS? ](https://www.reddit.com/r/robotics/comments/15o4kqg/publishing_a_computer_vision_work_at_icra_or_iros/) , 2023-09-05-0909
```
Hi everyone,

&#x200B;

I come from a robotics background (bachelor + half of my masters), however, at the end of my mas
ters, I shifted towards learning theory by taking lectures on a variety of subjects such as deep learning theory, statis
tical learning, computer vision, etc. 

&#x200B;

Currently, I am pursuing a PhD in the intersection of learning theory 
and computer vision, my research work revolves around compressing neural network models, reducing their complexity and t
esting them on vision tasks (classification mostly, but sometimes extended to segmentation and object detection).

My go
-to conferences are typically CVPR, NeurIPS, ICLR...However, my supervisors and I want to try out a robotics conference,
 and my question is, which one to prefer in my case ? Knowing that I do not experiment on real robots. Is my work more s
uited for IROS or ICRA? Any other conference suggestion?

&#x200B;

Thank you for your answers.

&#x200B;
```
---

     
 
all -  [ NeurIPS rebuttal character limit problem [D] ](https://www.reddit.com/r/MachineLearning/comments/15mtj1y/neurips_rebuttal_character_limit_problem_d/) , 2023-09-05-0909
```
The NeurIPS rebuttal has a 6000 character limit, however my rebuttal is way way over that. I was told by my supervisor t
hat you could just comment chain onto the rebuttal to get past this, however that is not working.

The deadline is in ar
ound 5 hours so I'm really in a big bind here. Does anyone have any insight about how to resolve this situation?
```
---

     
 
all -  [ Looking for Perspectives: Pursuing a PhD in AI vs Continuing in Industry ](https://www.reddit.com/r/PhD/comments/15ka43p/looking_for_perspectives_pursuing_a_phd_in_ai_vs/) , 2023-09-05-0909
```
Greetings fellow researchers,

I am 27, currently working remotely at a healthcare IT company based in Silicon Valley  (
6+ years in industrial research) where I apply deep learning methods and large language models. I recently received an e
xciting opportunity to pursue a PhD at the Technical University of Denmark (DTU) in a similar research area. 

While I a
m grateful for my current position and compensation, Have published in NeurIPS, EMNLP, ACL, ACM etc (NLP) with really go
od citations under company. I feel unsatisfied with the learning opportunities available in company & industry.

I am st
rongly considering pursuing the DTU PhD program full-time, but wanted to get perspectives from others before making a de
cision. How strong is DTU's AI research community?

 Given the rapid advances in large language models, is now an ideal 
time to immerse myself in academic research? There are many topics that interest me, including fairness, ethics, halluci
nations, quantization, specialized domains like healthcare/finance, and federated learning combined with LLMs.

Would ap
preciate any insights on whether moving into academia would be a wise choice at this stage versus remaining in industry.
 I welcome any suggestions or considerations I should keep in mind. 

Thank you for taking the time to share your though
ts!
```
---

     
 
all -  [ [ICLR23] Dual Accounts in Openreview from the same person ](https://www.reddit.com/r/AskAcademia/comments/15k4u81/iclr23_dual_accounts_in_openreview_from_the_same/) , 2023-09-05-0909
```
I as a researcher observed some researchers use dual accounts (using different email ids) in Openreview website primaril
y ICLR or NeurIPS. One hypothesis I heard is it helps in review with one email and submit using other email ID. Is this 
permitted ? I could see as a disclaimer written in OpenReview website that irrespective how many email id's you may have
 you should have the same Openreview account.
```
---

     
 
all -  [ [D] How does one withdraw a paper from Neurips? ](https://www.reddit.com/r/MachineLearning/comments/15jd1wu/d_how_does_one_withdraw_a_paper_from_neurips/) , 2023-09-05-0909
```
First time submitter here and was unable to find a similar post (and thought the community might benefit from this in th
e future!). How do I withdraw from Neurips? All the instructions I found are from 2017, 2018. Do I need to contact someo
ne or do I just need to 'Add Withdrawal' on OpenReview.
```
---

     
