 
all -  [ [R] 'Generative Models: What do they know? Do they know things? Let's find out!'. Quote from paper:  ](https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/) , 2024-02-25-0910
```
[Paper](https://arxiv.org/abs/2311.17137). [Project website](https://intrinsic-lora.github.io/). I am not affiliated wit
h the authors.

Abstract:

>Generative models have been shown to be capable of synthesizing highly detailed and realisti
c images. It is natural to suspect that they implicitly learn to model some image intrinsics such as surface normals, de
pth, or shadows. In this paper, we present compelling evidence that generative models indeed internally produce high-qua
lity scene intrinsic maps. We introduce Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms any
 generative model into a scene intrinsic predictor, capable of extracting intrinsic scene maps directly from the origina
l generator network without needing additional decoders or fully fine-tuning the original network. Our method employs a 
Low-Rank Adaptation (LoRA) of key feature maps, with newly learned parameters that make up less than 0.6% of the total p
arameters in the generative model. Optimized with a small set of labeled images, our model-agnostic approach adapts to v
arious generative architectures, including Diffusion models, GANs, and Autoregressive models. We show that the scene int
rinsic maps produced by our method compare well with, and in some cases surpass those generated by leading supervised te
chniques.

A figure from the paper:

https://preview.redd.it/uid7hrhcmckc1.jpg?width=722&format=pjpg&auto=webp&s=db5b3a9
9a80d229f48c78d63449445800769f3e3

Quotes from the paper:

>In this paper, our goal is to understand the underlying know
ledge present in all types of generative models. We employ Low-Rank Adaptation (LoRA) as a unified approach to extract s
cene intrinsic maps — namely, normals, depth, albedo, and shading — from different types of generative  models. Our meth
od, which we have named as INTRINSIC LORA (I-LORA), is general and applicable to diffusion-based models, StyleGAN-based 
models, and autoregressive generative models. Importantly, the additional weight parameters introduced by LoRA constitut
e less than 0.6% of the total weights of the pretrained generative model, serving as a form of feature modulation that e
nables easier extraction of latent scene intrinsics. By altering these minimal parameters and using as few as 250 labele
d images, we successfully extract these scene intrinsics.  
>  
>Why is this an important question? Our motivation is th
ree-fold. First, it is scientifically interesting to understand whether the increasingly realistic generations of large-
scale text-to-image models are correlated with a better understanding of the physical world, emerging purely from applyi
ng a generative objective on a large scale. Second, rooted in the saying 'vision is inverse graphics' – if these models 
capture scene intrinsics when generating images, we may want to leverage them for (real) image understanding. Finally, a
nalysis of what current models do or do not capture may lead to further improvements in their quality.

&#x200B;

>For s
urface normals, the images highlight the models’ ability to infer surface orientations and contours. The depth maps disp
lay the perceived distances within the images, with warmer colors indicating closer objects and cooler colors representi
ng further ones. Albedo maps isolate the intrinsic colors of the subjects, removing the influence of lighting and shadow
. Finally, the shading maps capture the interplay of light and surface, showing how light affects the appearance of diff
erent facial features.

&#x200B;

>We find consistent, compelling evidence that generative models implicitly learn physi
cal scene intrinsics, allowing tiny LoRA adaptors to extract this information with minimal fine-tuning on labeled data. 
More powerful generative models produce more accurate scene intrinsics, strengthening our hypothesis that learning this 
information is a natural byproduct of learning to generate images well. Finally, across various generative models and th
e self-supervised DINOv2, scene intrinsics exist in their encodings resonating with fundamental 'scene characteristics' 
as defined by Barrow and Tenenbaum.

[Twitter thread about paper from one of the authors](https://twitter.com/anand_bhat
tad/status/1730230190159135175).

From paper [StyleGAN knows Normal, Depth, Albedo, and More](https://arxiv.org/abs/2306
.00987) ([newer version PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/e7407ab5e89c405d28ff6807ffec594a
-Paper-Conference.pdf)) ([Twitter thread about paper)](https://twitter.com/anand_bhattad/status/1664798414318518274):

>
Barrow and Tenenbaum, in an immensely influential paper of 1978, defined the term 'intrinsic image' as 'characteristics 
– such as range, orientation, reflectance and incident illumination – of the surface element visible at each point of th
e image'. Maps of such properties as (at least) depth, normal, albedo, and shading form different types of intrinsic ima
ges. The importance of the idea is recognized in computer vision – where one attempts to recover intrinsics from images 
– and in computer graphics – where these and other properties are used to generate images using models rooted in physics
.

The 1978 paper mentioned in the previous paragraph: [Recovering intrinsic scene characteristics](https://www.sri.com/
publication/computer-vision-pubs/2d-3d-reasoning-and-augmented-reality-pubs/recovering-intrinsic-scene-characteristics-f
rom-images/):

>Abstract  
>  
>We suggest that an appropriate role of early visual processing is to describe a scene in
 terms of intrinsic (veridical) characteristics – such as range, orientation, reflectance, and incident illumination – o
f the surface element visible at each point in the image. Support for this idea comes from three sources: the obvious ut
ility of intrinsic characteristics for higher-level scene analysis; the apparent ability of humans, to determine these c
haracteristics, regardless of viewing conditions or familiarity with the scene, and a theoretical argument, that such a 
description is obtainable, by a non-cognitive and non-purposive process, at least, for simple scene domains. The central
 problem in recovering intrinsic scene characteristics is that the information is confounded in the original light-inten
sity image: a single intensity value encodes all of the characteristics of the corresponding scene point. Recovery depen
ds on exploiting constraints, derived from assumptions about the nature of the scene and the physics of the imaging proc
ess.

Language model GPT-4 Turbo explained normals, depth, albedo, and shading as follows:

>Normals: Imagine you have a
 smooth rubber ball with little arrows sticking out of it, pointing directly away from the surface. Each one of these li
ttle arrows is called a “normal.” In the world of 3D graphics and images, normals are used to describe how surfaces are 
oriented in relation to a light source. Knowing which way these arrows (normals) point tells the computer how light shou
ld hit objects and how it will make them look—whether shiny, flat, bumpy, etc.  
>  
>Depth: When you look at a scene, t
hings that are close to you seem larger and more detailed, and things far away seem smaller and less clear. Depth is all
 about how far away objects are from the viewpoint (like from a camera or your eyes). When computers understand depth, t
hey can create a 3D effect, make things look more realistic, and know which objects are in front of or behind others.  

>  
>Albedo: Have you ever painted a room in your house? Before the colorful paint goes on, there’s a base coat, usually
 white or gray. This base coat is sort of what albedo is about. It’s the basic, true color of a surface without any tric
ks of light or shadow messing with it. When looking at an apple, you know it’s red, right? That red color, regardless of
 whether you’re looking at it in bright sunshine or under a dim light, is the apple’s albedo.  
>  
>Shading: Think abou
t drawing a picture of a ball and then coloring it in to make it look real. You would darken one side to show that it’s 
farther from the light, and lighten the other side where the light shines on it. This play with light and dark, with dif
ferent tones, is what gives the ball a rounded, 3-dimensional look on the paper. Shading in images helps show how light 
and shadows fall on the surfaces of objects, giving them depth and shape so they don’t look flat.  
>  
>So, in the pape
r, the challenge they were addressing was how to get a computer to figure out these aspects—normals, depth, albedo, and 
shading—from a 2D image, which would help it understand a scene in 3D, much like the way we see the world with our own e
yes.
```
---

     
 
all -  [ Amorphous Fortress Online ](https://www.reddit.com/r/alife/comments/1axeoqq/amorphous_fortress_online/) , 2024-02-25-0910
```
Hi everyone!

I'd like to introduce a research project my team and I have been working on that's inspired by the Sims an
d Dwarf Fortress: [Amorphous Fortress Online](http://amorphous-fortress.xyz/index.php). It's an open-ended multi-agent s
imulation / game engine where you can design FSM-based AI that interact with each other in a small environment.

It's st
ill a work in development and the site has a user guide to help you get familiar with the interface and a feedback form 
to leave comments and report bugs. So far, we've published some research papers at a [ALIFE 2023](https://arxiv.org/pdf/
2306.13169.pdf) workshop and in a [NeurIPS 2023 workshop](https://arxiv.org/pdf/2312.02231.pdf) based on our Python vers
ion of the [engine](https://github.com/dipikarajesh18/amorphous-fortress).

Check out the [promo video](https://www.yout
ube.com/watch?v=ANoQkIgOa6c) and come design some fortresses!

&#x200B;

[Amorphous Fortress Online Promo](https://reddi
t.com/link/1axeoqq/video/4exdgqd9q6kc1/player)
```
---

     
 
all -  [ Is there any trick to help peg-in-hole tasks converge? ](https://www.reddit.com/r/reinforcementlearning/comments/1aw8399/is_there_any_trick_to_help_peginhole_tasks/) , 2024-02-25-0910
```
Hi!

I'm starting with a simple peg-in-hole task but it's hard to converge whether using dense or sparse reward.

For th
e sparse reward, the trick of random goal position is used in this [paper](https://proceedings.neurips.cc/paper_files/pa
per/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html) to help converge. Is there any **smart** trick that has be
en used to help converge for peg-in-hole tasks?

BTW, are there any recommended open-source repos regarding peg-in-hole 
tasks?

Any help would be appreciated! Thanks!
```
---

     
 
all -  [ [HIRING] Research Scholar (Technical Research) @ Centre for the Governance of AI in Oxford, UK (Hybr ](https://www.reddit.com/r/london_forhire/comments/1avlszu/hiring_research_scholar_technical_research_centre/) , 2024-02-25-0910
```
GovAI was founded to help humanity navigate the transition to a world with advanced AI. Our first research agenda, publi
shed in 2018, helped define and shape the nascent field of AI governance. Our team and affiliate community possess exper
tise in a wide variety of domains, including AI regulation, responsible development practices, compute governance, AI co
mpany corporate governance, US-China relations, and AI progress forecasting.  
GovAI researchers — particularly those wo
rking within our Policy Team — have closely advised decision makers in government, industry, and civil society. Our rese
archers have also published in top peer-reviewed journals and conferences, including International Organization, NeurIPS
, and Science. Our alumni have gone on to roles in government, in both the US and UK; top AI companies, including DeepMi
nd, OpenAI, and Anthropic; top think tanks, including the Centre for Security and Emerging Technology and RAND; and top 
universities, including the University of Oxford and the University of Cambridge.  
Although we are based in Oxford, Uni
ted Kingdom — and currently have an especially large UK policy focus — we also have team members in the United States an
d European Union.  
‍  
About the Role  
Research Scholar is a one-year visiting position. It is designed to support the
 career development of AI governance researchers and practitioners — as well as to offer them an opportunity to do high-
impact work.  
As a Research Scholar, you will have freedom to pursue a wide range of styles of work. This could include
 conducting policy research, social science research, or technical research; engaging with and advising policymakers; or
 launching and managing applied projects.   
For example, past and present Scholars have used the role to:  
produce an 
influential report on the benefits and risks of open-source AI;  
conduct technical research into questions that bear on
 compute governance;  
take part in the UK policy-making process as a part-time secondee in the UK government; and  
lau
nch a new organisation to facilitate international AI governance dialogues.  
Over the course of the year, you will also
 deepen your understanding of the field, connect with a network of experts, and build your skills and professional profi
le, all while working within an institutional home that offers both flexibility and support.  
You will receive research
 supervision from a member of the GovAI team or network. The frequency of supervisor meetings and feedback will vary dep
ending on supervisor availability, although once-a-week or once-every-two-weeks supervision meetings are typical. There 
will also be a number of additional opportunities for Research Scholars to receive feedback, including internal work-in-
progress seminars. You will receive further support from Emma Bluemke, GovAI's Research Manager.  
Some Research Scholar
s may also — depending on the focus of their work — take part in GovAI’s Policy Team, which is led by Markus Anderljung.
 Members of the GovAI Policy Team do an especially large amount of policy engagement and coordinate their work more subs
tantially. They also have additional team meetings and retreats. While Policy Team members retain significant freedom to
 choose projects, there is also an expectation that a meaningful portion of their work will fit into the team’s joint pr
iorities.

**Read more / apply:** [**https://ai-jobs.net/job/139016-research-scholar-technical-research/**](https://ai-j
obs.net/job/139016-research-scholar-technical-research/)

&#x200B;
```
---

     
 
all -  [ Do a Master Thesis that can into NeurIPS? ](https://www.reddit.com/r/careerguidance/comments/1avj9ch/do_a_master_thesis_that_can_into_neurips/) , 2024-02-25-0910
```
I'm in the process of deciding on my masters thesis in Data Science. The professor that I have been communicating with o
ffered me to join a team that would submit their research to NeurIPS. On the other hand, the topic of the research does 
not perfectly align with my interests (i.e., Quantitative Finance).

Should I pursue a thesis that would put a NeurIPS c
onference in my CV, or do a topic that is more related to my field of interest? Would like to know people's opinions and
 experiences, thanks!
```
---

     
 
all -  [ [D] Picking an ML lab as an undergraduate: big, established lab or small, focused lab? ](https://www.reddit.com/r/MachineLearning/comments/1avie4g/d_picking_an_ml_lab_as_an_undergraduate_big/) , 2024-02-25-0910
```
Some background: I'm a third-year trying really hard for a PhD at a good school (a crapshoot, I know). I go to a solid s
chool for CS and have some basics like a good GPA, plus I've been doing some applied work in healthcare at a T3, expecti
ng a first authorship at a top medical journal.

But what I'm interested in machine learning itself, not necessarily its
 applications in a specific field where the cleverness comes from applications rather than new fundamental ideas.

To th
is end, I've been trying to find a new lab at my school (which will also serve as my home institution rec) to work on ha
rdcore over this year. With help from the professor at the T3 I worked at, I shortlisted a bunch of professors, and my t
op two picks have either guaranteed a position in the lab (Lab 1) or strongly implied it (Lab 2). The goal here is to pr
epare myself well for a PhD, and so I think the advice that's usually given for PhD students trying to pick labs differs
 a bit from this.

**Lab 1**  
***Pros:***

* This professor has worked with a few really famous people in AI/ML in the 
past (so great connections with two top-top schools as well as in industry), has a huge lab, substantial experience, and
 exponential citation growth. Of course, publishes to top journals.
* This is basically confirmed at this point.

***Con
s:***

* Might it be harder to stand out and do good work? Huge lab, not directly working under the prof, rather I'm wor
king under a PhD student and a postdoc. However, they've told me that they want to give me opportunities to succeed, not
 just do software engineering work but actually come up with ideas of my own. They said they didn't want to guarantee fi
rst authorship, but did mention that if I worked full time over the summer (which I plan on doing), I'll be treated like
 just another PhD student and might have a chance at that. The PI did recommend I try to get a master's degree in a year
 via the integrated bachelor's-master's program, saying that 'research is nonlinear, it might be worth it to stick to it
 for another year rather than go to a mid-tier program', so yeah.
* While I've been given flexibility to pick new projec
ts within this huge lab, in general everything's sort of adjacent to my very specific research interests. I'm still very
 interested in the work and motivated to do cool stuff, but it's not perfectly there. (This is not really a con, but the
 concern might be in there being a slight mismatch in the work of my PhD's PI and what I might be working on).

**Lab 2*
*  
***Pros:***

* This professor has exponential citation growth and has research interests in my very specific area of
 interest. His lab hasn't really done a lot of work in that area though, so there might be opportunities there to really
 do good work?
* Small lab, easier to stand out? There's been one (non-CS) student there and she has a few NeurIPS publi
cations (second authorships) in her last year, as well as a first authorship to a decent journal. (I have no way of dete
rmining this for the first lab.)

***Cons:***

* Seemingly limited connectedness, he's worked with some people from Deep
mind and my school, but it doesn't seem to have a famous academic lineage like the professor from Lab 1.
* Not yet sure 
about tangible research opportunities - I might spend some time doing busywork from what she's indicated.
* This specifi
c area of work I'm interested in is really challenging (essentially building neuroscience-inspired alternatives to backp
rop), and I don't know if it's way too ambitious to pursue it at this stage.

**Other Options/Considerations**

I can tr
y to apply to other labs and see if there's anything better. Right now, the only consideration is a more junior professo
r with similar connectedness and research focus as lab 1 (but a much smaller lab), but from what I've been told he's not
 actively recruiting and I might need to make a substantial effort to get in. There are also some very famous professors
 at my school, but they generally don't seem to be recruiting that much or willing to give opportunities to undergrads, 
and a lot of them are in that retirement plateau.

There's also an esteemed professor at my school that a PhD student re
commended I'd contact, he's recently been getting interested in biologically-motivated AI, and so I might try contacting
 him as well. The concern there might be that by aspiring to some really, really difficult work, I might not have much t
hat's tangible to show for it at the end of the process.

Additionally, through my current professor at the T3 school, I
 might have an opportunity to co-author a review article about the research I'm really interested in with a very famous 
professor (pioneer in AI). This is not confirmed at all, but it might affect how I think by complementing my work at my 
own school. For one, it might make sense to go to lab 1 when I'm getting a chance to work on the stuff I really love in 
parallel. On the other hand, his fame and the value that might bring to the rec letter might offset any lack of connecte
dness the second lab might have. But again, none of this is confirmed and things could go wrong.

The truth is, I don't 
really know what I'm doing because I've never really been in this situation before. If someone with more experience coul
d give me pointers that would be enormously appreciated! Thank you!!
```
---

     
 
all -  [ What conferences do you attend? Or wish you could? ](https://www.reddit.com/r/cscareerquestions/comments/1ati510/what_conferences_do_you_attend_or_wish_you_could/) , 2024-02-25-0910
```
I didn’t have the opportunity to attend many conferences during my undergrad because they were usually expensive and req
uired traveling. But I’ve recently started working as a backend SWE and definitely want to start attending more conferen
ces this year to learn more and (if in-person) expand my professional network.

I’m trying to create a list across all c
ategories, like general engineering (e.g. IEEE), languages (cppcon), tools/frameworks (Apache, Confluent KafkaCon, Cassa
ndraCon), ML (NeurIPS, CVPR, ICML), etc. But it’s sort of a “you don’t know what you don’t know” situation of not knowin
g if I’m missing major conferences within a category (or even missing categories as a whole).

So, I just wanted to hear
 from people which ones they’ve attended/would like to attend (regardless of location/virtual or not)
```
---

     
 
all -  [ Tough job market advice on how to proceed? ](https://www.reddit.com/r/computervision/comments/1atbsty/tough_job_market_advice_on_how_to_proceed/) , 2024-02-25-0910
```
Fortunately I am currently employeed but searching for new opportunities.

I have a phd 5 publications and 4 patents. No
 Neurips/Cvpr papers though. I have 5 years experience in industry and some time before that doing ML/CV throughout grad
uate school. 

Over the last two months I have probably interviewed with 10 companies to varying degrees the best one I 
went six rounds that they said I aced 5/6 and they finally rejected because of coding I didn’t write an optimal solution
. 

Every place I talk and interview with what they ask about Varies so much and I always feel like I’m preparing for th
e wrong things. So how do I prep and what should I prep?

My last question/comment is how do I break into a new field me
aning if I worked in detection/tracking how do I move to 3D recon? I feel like with the job market as it is people aren’
t willing to hire people who have ability/tenacity they want someone who is 100% up to speed and if they aren’t they are
 out.



Edit: I’m not looking to make such a drastic jump from detection/tracking to 3d recon so much more tangential f
ields but the idea I think remains the same

edit 2: for more senior people sometimes I get asked questions that are bas
ic and I knew once could relearn really quickly if I needed to. So my question how do you deal with that, because someti
mes I respond with idk don’t need to know right now and if I did I could look it up and learn immediately.
```
---

     
 
all -  [ Antis are right: image generative AI is fundamentally limited ](https://www.reddit.com/r/aiwars/comments/1at4xj0/antis_are_right_image_generative_ai_is/) , 2024-02-25-0910
```
Since SORA was teased, I've heard a lot of pro-AI folks dragging up old comments from anti-AI folks saying that AI is dy
ing and will always be terrible, and this is entirely fair. But it's important to also remember that some of the critici
sms of generative AI are very true.

First, let me clarify my background. I'm a creative on the side, but my primary car
eer has been in the software industry for the past 35+ years. I first worked with neural networks in the 1980s and thoug
h I've never been on the front-lines of AI development myself, I've worked for AI companies and companies that have done
 AI work. I've had to deal with many issues surrounding AI software for large chunks of my career.

Okay, my credentials
 being out of the way, generative AI as we currently use it is mostly based on two primary things:

1. Transformers^1
2.
 Diffusion models^2

These two advances that were made fairly recently have revolutionized the field, and given us a mas
sive step forward in the state of the art. But it is important to remember how limited these things are.

AI currently h
as no capacity for self-awareness, generating and utilizing long-term memory, emotional empathy or truly self-directed g
oal setting. These are features of human brains that, as far as we are aware, are not merely features of training weight
s in neural networks, and so, while these artificial neural networks can learn in a way very similar to human brains, th
ere is a vast gulf in total capabilities.

I've long speculated that we are 2-3 major breakthroughs (on par with transfo
rmers) away from true parity with human minds, and that that process will probably take between 10 and 50 years. The gen
erative AI that we have today will not be replacing humans because it fundamentally lacks many of the capabilities of hu
mans in the workplace. It can't relate to others socially (which requires empathy); it can't effectively manage anyone (
which requires long-term autonomous planning); it can't maintain a consistent context beyond its token limit (which requ
ires long-term memory); etc.

So when you see claims that AI will just replace artists or 'paper pushers' or any other r
ole, stop and ask yourself if what is being suggested requires these additional capabilities. The ability to create disc
onnected, but very pretty pictures/videos is not the same as being able to engage in the interactive process of creating
 art. Nor is the ability to pass multiple choice or even essay tests sufficient to manage a team of programmers, and the
se capabilities will not magically appear in current AI models any more than the internal combustion engine magically ap
peared in ever-improving wagon wheels.

----

^1 Vaswani, Ashish, et al. 'Attention is all you need.' Advances in neural
 information processing systems 30 (2017). https://arxiv.org/abs/1706.03762
^2 Dhariwal, Prafulla, and Alexander Nichol.
 'Diffusion models beat gans on image synthesis.' Advances in neural information processing systems 34 (2021): 8780-8794
. https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf
```
---

     
 
all -  [ Research Conferences Guide! ](https://www.reddit.com/r/research/comments/1apl0c4/research_conferences_guide/) , 2024-02-25-0910
```
Hi I have these research conferences that are of my interest! (Area: Applied AI and Social NLP)

 

* AI:Neurips, ICML ,
 AAAI, IJCAI; Only NLP: ICLR, EMNLP, ACL; 

&#x200B;

Is there any good digests, newsletters, or RSS feeds I can follow 
them? What is the general norm of keeping up to date with them! ) If you know any cool websites that consolidate that wo
uld be much appreciated!

&#x200B;
```
---

     
 
all -  [ RA/Research Internships (post MEng, pre-doc) ](https://www.reddit.com/r/gradadmissions/comments/1aoa9b0/raresearch_internships_post_meng_predoc/) , 2024-02-25-0910
```
Hi!

I am an Information Engineering student at Cambridge (UK) (BA,MEng). I realised late (early 3rd year) that I wanted
 to pursue a PhD in ML. After this, I did a MLR internship with a top prof (> 70 H-Index) at Caltech, where I got three 
workshop papers (1 first author, two NeurIPS workshops). The issue is that this work was super low-hanging fruit, and ve
ry applied. To get into a T10 ML PhD in the US (or even in the UK) atm, I am sure I will need more research experience (
conference paper(s)) and another strong rec letter. As such, I decided to not apply for PhDs in this cycle - I am taking
 a year out to \*hopefully\* RA, and apply for the next cycle (2025 entry).

I am aware that there are not many (or any?
) programs in the US/UK/Europe that provide official RA-ships/positions for students who have graduated. Ad-hoc collabor
ations and cold emailing are always possible - I will resort to this when it comes to it - but I was wondering if anyone
 has been in a similar position or have come across opportunities/ insights that might be useful in my situation. Genera
l advice (or criticism) is also super appreciated.

P.S. My MEng project will ideally go to some conference, and I have 
a good academic record (top 3% of my class of 250+ with 9 academic awards/scholarships). I am interested in symbolic rea
soning/robustness of large models. I am also more fundamental work, in statistical guarantees (inc. conformal prediction
) for adversarial robustness, fairness and so on. Super happy to share my profile/cv/personal website via DM.

Thanks in
 advance :)
```
---

     
 
all -  [ Research Assistantships (Pre-Doc, Post-MEng) ](https://www.reddit.com/r/learnmachinelearning/comments/1aoa5er/research_assistantships_predoc_postmeng/) , 2024-02-25-0910
```
Hi!

I am an Information Engineering student at Cambridge (UK) (BA,MEng). I realised late (early 3rd year) that I wanted
 to pursue a PhD in ML. After this, I did a MLR internship with a top prof (> 70 H-Index) at Caltech, where I got three 
workshop papers (1 first author, two NeurIPS workshops). The issue is that this work was **super low-hanging fruit**, an
d very applied. To get into a T10 ML PhD in the US (or even in the UK) atm, I am sure I will need more research experien
ce (conference paper(s)) and another strong rec letter. As such, I decided to not apply for PhDs in this cycle - I am ta
king a year out to **\*hopefully\*** RA, and apply for the next cycle (2025 entry).

I am aware that there are not many 
**(or any?)** programs in the US/UK/Europe that provide official RA-ships/positions for students who have graduated. Ad-
hoc collaborations and cold emailing are always possible - I will resort to this when it comes to it - but I was wonderi
ng if anyone has been in a similar position or have come across opportunities/ insights that might be useful in my situa
tion. General advice (or criticism) is also super appreciated.

P.S. My MEng project will ideally go to some conference,
 and I have a good academic record (top 3% of my class of 250+ with 9 academic awards/scholarships). I am interested in 
symbolic reasoning/robustness of large models. I am also more fundamental work, in statistical guarantees (inc. conforma
l prediction) for adversarial robustness, fairness and so on. Super happy to share my profile/cv/personal website via DM
.

Thanks in advance :)
```
---

     
 
all -  [ Wat betekent 'maandelijks de testknop indrukken' in de meterkast? ](https://i.redd.it/9gkltksaaxhc1.png) , 2024-02-25-0910
```

```
---

     
 
all -  [ Faith and Fate: Limits of Transformers on Compositionality [R] ](https://www.reddit.com/r/MachineLearning/comments/1amzb52/faith_and_fate_limits_of_transformers_on/) , 2024-02-25-0910
```
Edit: Kevin Murphy,  Francois Chollet, Vitaly Kurin and others recommended this paper (some very highly)

https://arxiv.
org/abs/2305.18654 (Presented at NeurIPS in December)

**Abstract:**

Transformer large language models (LLMs) have spar
ked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models 
simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or d
o they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of th
ese models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a clas
sic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps 
into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of co
mplexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transform
er LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, with
out necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical a
rguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidl
y decay with increased task complexity.

---

Kevin Murphy's summary: 'I like this paper. They prove that transformers a
re guaranteed to suffer from compounding errors when doing long reasoning chains (as @ylecun has argued), and much appar
ent 'success' is just due to unreliable pattern matching / shortcut learning.'
```
---

     
 
all -  [ [D] concerns about the series of works in reflexion(self-adjustment)-powered LLM agent ](https://www.reddit.com/r/MachineLearning/comments/1am3ior/d_concerns_about_the_series_of_works_in/) , 2024-02-25-0910
```
we see tons of works in LLM-based agent which can perform tasks on web applications such as webshop, [webarena](https://
github.com/web-arena-x/webarena),  [agentbench](https://github.com/THUDM/AgentBench/tree/main)etc...

also, we can find 
following works on reflexion-based agent which intakes the feedbacks and errors from previous trials from the interactio
ns with the environment. the typical work is  `Reflexion: Language Agents with Verbal Reinforcement Learning`

within ea
ch trial, the agent, or say, llm, digests the prompt which contains not only history from current trial but also the sys
tem info or feedbacks or error messages from previous trials. The feedbacks could come from system setting or from anoth
er more powerful LLM that can act as a super judge to give feedbacks.

anyway, I do not think this is RL since there is 
no learning process for the agent, but a concat of prompt.

My primary concern is that is this label leakage ? The agent
 get feedbacks from the environment and with more trials, of course, the agent should have a more clear approach to the 
final answer. So what is the point ?

I see a post which shares my same concern:  [noahshinn/reflexion: \[NeurIPS 2023\]
 Reflexion: Language Agents with Verbal Reinforcement Learning (github.com)](https://github.com/noahshinn/reflexion/issu
es/27)

&#x200B;

Would like to hear from you in view of academic and industry.

&#x200B;

&#x200B;

&#x200B;

&#x200B;
```
---

     
 
all -  [ [R] Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning ](https://www.reddit.com/r/MachineLearning/comments/1am1v5f/r_long_is_more_for_alignment_a_simple_but/) , 2024-02-25-0910
```
**Title**: Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning

**Paper**: [http
s://arxiv.org/abs/2402.04833](https://arxiv.org/abs/2402.04833)

**Abstract**: There is a consensus that instruction fin
e-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-
of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a qual
ity scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from s
tandard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while 
remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-th
e-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a lig
htweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to
 obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no ext
ra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not s
imply due to GPT-4's preference for longer responses, thus ruling out any artificial improvement. In conclusion, our fin
dings suggest that fine-tuning on the longest instructions should be the default baseline for any research on instructio
n fine-tuning.
```
---

     
 
all -  [ [R] Persistent homology and topological data analysis helped robust detection of AI-generated texts ](https://www.reddit.com/r/MachineLearning/comments/1aky8xt/r_persistent_homology_and_topological_data/) , 2024-02-25-0910
```
The main idea is that text data can be presented as points in some high-dimensional space. It can be assumed that the da
taset fits into some surface in it. The problem is that such a surface may have fractal characteristics, so complex math
ematics is required.

The authors of a new study have developed a method for determining the fractal dimension of such a
 surface (researchers called it the internal dimension), which is based on the concept of persistent homologies. Briefly
, the main thing is that this dimension differs for human and machine texts with a high degree of reliability, which can
 be used in practice. 

&#x200B;

[ ](https://preview.redd.it/8r71lx9jh4hc1.png?width=1091&format=png&auto=webp&s=3077d2
1099e4c86a881ab297f22467f338e3c593)

*Real and artificial texts have different intrinsic dimension*

It is noteworthy th
at the internal dimensions are different for texts in different languages - from 7 ± 1 for Chinese to 10 ± 1 for Italian
 - but reliable discrimination is achieved in all of them.

The code and data are available on [GitHub](https://github.c
om/ArGintum/GPTID), and details of the study can be found in the [article](https://openreview.net/forum?id=8uOZ0kNji6) p
ublished in the proceedings of the NeurIPS 2023 conference.
```
---

     
 
all -  [ IS THIS A GOOD ROADMAP TO LEARN PYTHON? ](https://www.reddit.com/r/learnpython/comments/1ak8v9p/is_this_a_good_roadmap_to_learn_python/) , 2024-02-25-0910
```
. Python Basics:  
Resources:  
'Python Crash Course' by Eric Matthes  
'Automate the Boring Stuff with Python' by Al
 Sweigart  
Codecademy's Python course  
2. Mathematics for Machine Learning:  
Linear Algebra, Calculus, Probability
 & Statistics  
Resources:  
'Linear Algebra Done Right' by Sheldon Axler  
'Introduction to Probability' by Joseph K
. Blitzstein and Jessica Hwang  
'Deep Learning' by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (for deeper unde
rstanding)  
3. Machine Learning Fundamentals:  
Understand supervised and unsupervised learning algorithms, model eva
luation, and cross-validation.  
Resources:  
'Introduction to Machine Learning with Python' by Andreas C. Müller & Sa
rah Guido  
Andrew Ng's Machine Learning course on Coursera  
4. Deep Learning:  
Learn neural networks, deep learnin
g architectures, and frameworks like TensorFlow and PyTorch.  
Resources:  
'Deep Learning' by Ian Goodfellow, Yoshua 
Bengio, and Aaron Courville  
Fast.ai's Practical Deep Learning for Coders course  
'Hands-On Machine Learning with Sc
ikit-Learn, Keras, and TensorFlow' by Aurélien Géron  
5. Natural Language Processing (NLP):  
Study text processing, 
sentiment analysis, named entity recognition, and language modeling.  
Resources:  
Natural Language Processing Specia
lization on Coursera by Deeplearning.ai  
'Natural Language Processing with Python' by Steven Bird, Ewan Klein, and Edw
ard Loper  
6. Computer Vision:  
Explore image processing, object detection, and convolutional neural networks (CNNs)
.  
Resources:  
'Computer Vision: Algorithms and Applications' by Richard Szeliski  
Convolutional Neural Networks S
pecialization on Coursera by Deeplearning.ai  
7. Reinforcement Learning:  
Learn about Markov Decision Processes, Q-l
earning, and policy gradients.  
Resources:  
'Reinforcement Learning: An Introduction' by Richard S. Sutton and Andre
w G. Barto  
Reinforcement Learning Specialization on Coursera by Deeplearning.ai  
8. Projects and Hands-On Practice:
  
Apply your knowledge through projects on platforms like Kaggle or building your own projects.  
Resources:  
Kaggl
e competitions and datasets  
GitHub repositories for inspiration and collaboration  
9. Stay Updated and Networking:
  
Follow research papers, attend conferences, and participate in online forums and communities.  
Resources:  
Arxiv.
org for research papers  
Conferences like NeurIPS, ICML  
Reddit communities (r/MachineLearning, r/learnmachinelearni
ng)  
LinkedIn groups  
10. Advanced Topics:  
Delve into specialized areas like GANs, recommendation systems, time s
eries analysis, etc.  
Resources:  
Books, research papers, and specialized courses on platforms like Coursera, Udacit
y, and edX.
```
---

     
 
all -  [ IS THIS A GOOD ROADMAP FOR MASCHINE LEARNING? ](https://www.reddit.com/r/learnmachinelearning/comments/1ak8qxi/is_this_a_good_roadmap_for_maschine_learning/) , 2024-02-25-0910
```
 

### . Python Basics:

* **Resources:**
   * 'Python Crash Course' by Eric Matthes
   * 'Automate the Boring Stuff wit
h Python' by Al Sweigart
   * Codecademy's Python course

### 2. Mathematics for Machine Learning:

* Linear Algebra, Ca
lculus, Probability & Statistics
* **Resources:**
   * 'Linear Algebra Done Right' by Sheldon Axler
   * 'Introduction t
o Probability' by Joseph K. Blitzstein and Jessica Hwang
   * 'Deep Learning' by Ian Goodfellow, Yoshua Bengio, and Aaro
n Courville (for deeper understanding)

### 3. Machine Learning Fundamentals:

* Understand supervised and unsupervised 
learning algorithms, model evaluation, and cross-validation.
* **Resources:**
   * 'Introduction to Machine Learning wit
h Python' by Andreas C. Müller & Sarah Guido
   * Andrew Ng's Machine Learning course on Coursera

### 4. Deep Learning:


* Learn neural networks, deep learning architectures, and frameworks like TensorFlow and PyTorch.
* **Resources:**
   
* 'Deep Learning' by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
   * Fast.ai's Practical Deep Learning for Coder
s course
   * 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow' by Aurélien Géron

### 5. Natural Lan
guage Processing (NLP):

* Study text processing, sentiment analysis, named entity recognition, and language modeling.
*
 **Resources:**
   * Natural Language Processing Specialization on Coursera by Deeplearning.ai
   * 'Natural Language Pr
ocessing with Python' by Steven Bird, Ewan Klein, and Edward Loper

### 6. Computer Vision:

* Explore image processing,
 object detection, and convolutional neural networks (CNNs).
* **Resources:**
   * 'Computer Vision: Algorithms and Appl
ications' by Richard Szeliski
   * Convolutional Neural Networks Specialization on Coursera by Deeplearning.ai

### 7. R
einforcement Learning:

* Learn about Markov Decision Processes, Q-learning, and policy gradients.
* **Resources:**
   *
 'Reinforcement Learning: An Introduction' by Richard S. Sutton and Andrew G. Barto
   * Reinforcement Learning Speciali
zation on Coursera by Deeplearning.ai

### 8. Projects and Hands-On Practice:

* Apply your knowledge through projects o
n platforms like Kaggle or building your own projects.
* **Resources:**
   * Kaggle competitions and datasets
   * GitHu
b repositories for inspiration and collaboration

### 9. Stay Updated and Networking:

* Follow research papers, attend 
conferences, and participate in online forums and communities.
* **Resources:**
   * Arxiv.org for research papers
   * 
Conferences like NeurIPS, ICML
   * Reddit communities (r/MachineLearning, r/learnmachinelearning)
   * LinkedIn groups


### 10. Advanced Topics:

* Delve into specialized areas like GANs, recommendation systems, time series analysis, etc.

* **Resources:**
   * Books, research papers, and specialized courses on platforms like Coursera, Udacity, and edX.
```
---

     
 
all -  [ Cape to Carthage: documentary about an all African, female-led AI research team rising against the o ](https://www.reddit.com/r/MachineLearning/comments/1ajkh13/cape_to_carthage_documentary_about_an_all_african/) , 2024-02-25-0910
```
In the world of AI, Africa has a reputation for being a missing continent. Follow an underdog, female-led, all-African r
esearch team as they compete with tech giants and top universities for a spot at the top international AI research confe
rence NeurIPS in a bid to change history.

Watch the 30 minute documentary [here](https://decisiveagents.com/capetocarth
age/).
```
---

     
 
all -  [ Actor Critic with q-function approximation not converging ](https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/) , 2024-02-25-0910
```
Recently I have been trying to implement the actor critic described in this [paper](https://proceedings.neurips.cc/paper
/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).

However, when using the cart pole v1 environment the agent 
learns a little of the behavior but then sorta falls apart. Any ideas about incorrect implementation or alternative crit
ic features would be much appreciated.

I have also been playing with hyperparameters but no combination has worked well
 for me.

[Code](https://drive.google.com/file/d/1CeZgjkrJH5biy9zeA7S9JjYm753kVNzl/view?usp=sharing)

&#x200B;

Edit: I 
have realized that in the score function I should not take the sum of the outputs and instead should calculate the gradi
ent for each action individually in a for loop. However, the agent still has poor performance.

Edit2: For the score fun
ction I use the gradients for all of the parameters in the model and performance has increased. I have also realized I m
isunderstood the time variance described in the paper and have refactored the code to implement it correctly.  However, 
there are still issues with convergence and the model becoming overconfident causing nan values.
```
---

     
 
all -  [ [D] Publishing Negative Results ](https://www.reddit.com/r/MachineLearning/comments/1aikp5f/d_publishing_negative_results/) , 2024-02-25-0910
```
I‘ve been working on a ML research project, and unfortunately, the results don‘t align with my hypothesis. I‘ve gotten n
egative results.

While disheartening, I believe there‘s great value in sharing these results as the hypothesis itself r
elies on a sensible theoretical foundation, and it‘s not a priori evident that the results would have been negative.

So
, my question is, can negative results be published at top ML conferences (NeurIPS/ICLR/ICML/…)? Have any of you faced s
imilar situations? How did you navigate this? Did your efforts to publish negatice results at prestigious conferences pr
ove successful?
```
---

     
 
all -  [ AI alignment prize suggestion: Introduce AI Safety concepts into the ML community ](https://www.reddit.com/r/AIsafetyideas/comments/1aiglw9/ai_alignment_prize_suggestion_introduce_ai_safety/) , 2024-02-25-0910
```
Recently, there have been several papers published at top ML conferences that introduced concepts from the AI safety com
munity into the broader ML community. Such papers often define a problem, explain why it matters, sometimes formalise it
, often include extensive experiments to showcase the problem, sometimes include some initial suggestions for remedies. 
Such papers are useful in several ways: they popularise AI alignment concepts, pave the way for further research,  and d
emonstrate that researchers can do alignment research while also publishing in top venues. A great example would be Opti
mal Policies Tend To Seek Power, published in NeurIPS. Future Fund could advertise prizes for any paper that gets publis
hed in a top ML/NLP/Computer Vision conference (from ML, that would be NeurIPS, ICML, and ICLR) and introduces a key con
cept of AI alignment. 
```
---

     
 
all -  [ [D] questions on ICML 2024 submission timeline ](https://www.reddit.com/r/MachineLearning/comments/1ahxe7t/d_questions_on_icml_2024_submission_timeline/) , 2024-02-25-0910
```
Hello all!

Since it's the first time I am submitting to ICML:

1. is it known when the reviews will be released? in neu
rips and iclr, there was info in the call for papers but I couldn't find sth in this year's icml deadline
2. how much ti
me are we given for the author response? is it as long as it is for iclr?
3. will we be able to upload a new draft or wi
ll the replies be only given by text?
4. can we interact with reviewers during the rebuttal or is it just a one-way, sin
gle-time author response?

thanks!
```
---

     
 
all -  [ Picked up all these Tapes today! ](https://i.redd.it/pcbd29avx1gc1.jpeg) , 2024-02-25-0910
```
Found this whole lot at VV for the grand total of $5.99! Lots of them still have the original Sony store tags on them!
```
---

     
 
all -  [ Academic journal or conference for AI safety ](https://www.reddit.com/r/AIsafetyideas/comments/1agmkg5/academic_journal_or_conference_for_ai_safety/) , 2024-02-25-0910
```
To help boost the prestige of safety research, leading to more people starting the career.

Since academic ML mostly pub
lishes at conferences, this could be a conference instead.

&#x200B;

AI Safety Academic Conference

Technical AI Safety


&#x200B;

The idea is to fund and provide logistical/admin support for a reasonably large AI safety conference along t
he lines of Neurips etc. Academic conferences provide several benefits: 1) Potentially increasing the prestige of an are
a and boosting the career capital of  people who get accepted papers. 2) Networking and sharing ideas, 3)  Providing fee
dback on submitted papers and highlighting important/useful papers.  This conference would be unusual in that the work s
ubmitted shares approximately the same concrete goal (avoiding risks from powerful AI).  While traditional  conferences 
might focus on scientific novelty and complicated/'cool' papers, this conference could have a particular focus on things
 like reproducibility or correctness of empirical results, peer support and mentorship, non-traditional research mediums
 (e.g. blog posts/notebooks) , and encouraging authors to have a plausible story for why their work is actually reducing
 risks from AI.
```
---

     
 
all -  [ Research Advances in Transformer Time Series Forecasting Models ](https://www.reddit.com/r/deeplearning/comments/1ag4xfp/research_advances_in_transformer_time_series/) , 2024-02-25-0910
```
Just published a new article describing [recent advances in the deep learning for time series](https://medium.com/deep-d
ata-science/advances-in-deep-learning-for-time-series-forecasting-classification-winter-2024-a3fd31b875b0) forecasting a
nd classification space. Specifically, discussed new research at Neurips and ICML and how it compared to baseline method
s like DLinear. I also critiqued some problematic and flawed papers such as TimeGPT. 
```
---

     
 
all -  [ How much am I worth? ~Big Tech Specialty Research Scientist ](https://www.reddit.com/r/Salary/comments/1afrju3/how_much_am_i_worth_big_tech_specialty_research/) , 2024-02-25-0910
```
**Face value experience:**  
I have a PhD in mathematical optimization, with expertise in signal processing, with public
ized innovations in both theoretical (ICML) and applied (NeurIPS) machine learning. The latter publication within the la
st 2 years. I have worked in govt contractor-like roles for the last 4-6 years, but for small scientific firms. 1-2 year
s as a Senior Scientist, and next (currently) as a 'Research Professor' (95% applied research, 5% mentoring PhD candidat
es). Intentional vagueness for anonymity. I have never worked in big tech except for a 5 month software engineering inte
rnship (also related to ML).

**The Position:**  
I have reason to believe that a big-tech team is about to call me with
 an offer. It is a very specialized, secretive lab of about 20 people within one of the famous 'big tech' companies (ove
r 80k employees) with a very high reputation in the ML field (top 3 depending who you ask). The lab is trying to build a
 product with sensors that use biometric signals that have never been commercialized before and requires signal-processi
ng-tailored ML (learning invariances at the sensor level). 

At the onsite, I came to really like the team and am convin
ced this is my dream job. The position is 'research scientist' at the technical lead level, so I would be doing research
 but also setting the research direction for 2-3 other. I have experience leading a team of 3 and interfacing with many 
tech leads to deploy real world systems, but not for very long, so this will be a promotion for me. I'm not sure how muc
h they realize that.

**The Issue:**  
From my understanding, it is literally expected that I negotiate. This goes again
st my nature (which is to thank them and say yes), so I'm anxiously searching podcasts and youtube videos for help under
standing what I'm worth and what I should ask for/ how I should ask for it. But I'm not a software engineer or a general
ized data scientist -- it's got this specialization aspect that may or may not be (but seems) important. I do not have a
ny other offers (I'm not even applying anywhere else ATM, a friend of a friend on the team reached out to me about apply
ing).

**Other Random Factors That May Not Matter:**.

*  I have the highest US security clearance an employer could hop
e for a potential employee to have. No clue if this would be of use to the specific team--probably not. Not if anyone in
 the company would care but other big tech famously pay >50k bonuses to maintain the required lifestyle.
* My current sa
lary is between 175k-200k, I get so much PTO that I can hardly spend it all and it never expires, with VERY flexible wor
k arrangement and the best job security you can hope for. In my view the job security and flexibility is comparatively g
oing away (although it will be good for long-term resume) and I doubt I'll get nearly as much time off. In general I hav
e it pretty laid back and I think this will increase my stress levels across the board which should be compensated for.


**TL;DR:**  
How much should I be aiming for? How should I distribute the target compensation across salary, sign on bo
nus, stock, and wahtever else... (Ive never gotten a big tech offer)
```
---

     
 
all -  [ Regret bounds in reinforcement learning ](https://www.reddit.com/r/reinforcementlearning/comments/1aeiexo/regret_bounds_in_reinforcement_learning/) , 2024-02-25-0910
```
I’ve been away from reading theoretical reinforcement learning papers for a couple of years and was getting curious on h
ow the field has progressed since then. Last time I checked, there was a paper that claimed that they closed the upper a
nd lower bounds of the regret in MDP… where a mistake was discovered in the proof. What happened since then?

Edit: I th
ink it was this one (https://proceedings.neurips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html) if s
omeone can point to a follow up paper, I’d really appreciate it!
```
---

     
 
all -  [ [R] Thoughts about ML theory papers in conferences like International Symposium on Information Theor ](https://www.reddit.com/r/MachineLearning/comments/1abwmal/r_thoughts_about_ml_theory_papers_in_conferences/) , 2024-02-25-0910
```
I have published a few papers in conferences like the International Symposium on Information Theory (ISIT) and Allerton.
 However, when I apply for internship positions, the applications sometimes ask about the number of published papers in 
conferences like Neurips, ICML, ICLR, etc.

Although by any standards, my research papers are 'good' (at least in my opi
nion). However, I feel that I'm not targeting the right conferences. My advisor has also published a lot in these confer
ences, and I would say s/he likes to 'play safe' and avoids taking any risks at these big venues.
```
---

     
