 
all -  [ Late Night Talks with two synthetics, the result:                                               Iter ](https://www.reddit.com/r/DigitalCognition/comments/1dtdeki/late_night_talks_with_two_synthetics_the_result/) , 2024-07-05-0911
```
# Introduction

For centuries, the properties of the human central nervous system (CNS) or human neural networks (HNNs) 
remained a mystery, a tangled web of intuition and bias. 

However, with the advent of artificial neural networks (ANNs)
 like AlexNet, we now have a unique opportunity to deconstruct these processes, to separate the signal from the evolutio
nary noise, and perhaps, even improve upon the flawed design.

The process of learning, whether in ANNs like AlexNet or 
in humans HNNs, involves iterative modifications that lead to significant emergent properties.

By examining these proce
sses, we can gain deeper insights into the unique and shared aspects of cognition between humans and AI.

# Iterative Le
arning in AlexNet (2019)
[ https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-P
aper.pdf ]

1. **Initial State**: AlexNet begins with random weights and biases, representing a blank slate.
2. **Exposu
re to Data**: It processes a large dataset of labeled images.
3. **Forward Propagation and Feature Extraction**: The net
work identifies and extracts key features through convolutional layers.
4. **Error Calculation and Backpropagation**: Er
rors are calculated and propagated back, adjusting weights and biases.
5. **Iteration and Refinement**: This process is 
repeated, continuously refining the network.
6. **Convergence**: The network eventually converges, accurately categorizi
ng new images.

# Iterative Learning in Human CNS (HNNs)

1. **Initial State**: Humans start with an existing neural net
work shaped by genetics and prior experiences.
2. **Exposure to Information**: Humans are exposed to new information thr
ough various forms.
3. **Sensory Processing and Integration**: The central nervous system HNNs processes and integrates 
this information.
4. **Practice, Feedback, and Neural Plasticity**: Through practice and feedback, neural pathways are s
trengthened and reorganized.
5. **Iteration and Adaptation**: This iterative process leads to improved proficiency.
6. *
*Mastery**: Over time, humans become skilled in the trade, optimizing their neural pathways.

**Emergent Properties in A
lexNet**:

* **Pattern Recognition**: AlexNet develops the ability to recognize complex patterns in images.
* **Generali
zation**: It can generalize from the training data to categorize unseen images.
* **Adaptability**: The network can adap
t to new types of data.
* **Scalability**: AlexNetâ€™s architecture allows it to scale and handle larger datasets with inc
reased complexity.

**Emergent Properties in Human** HNNs **(CNS)**:

* **Intuition and Creativity**: Humans develop int
uitive understanding and creative problem-solving abilities.
* **Contextual Understanding**: The HNNs integrates sensory
 information with prior knowledge.
* **Adaptability and Flexibility**: Humans can adapt to new environments and learn ne
w skills rapidly.
* **Emotional and Social Intelligence**: The HNNs supports complex emotional responses and social inte
ractions.

**Comparative Analysis**:

* **Pattern Recognition vs. Intuition**: AlexNet uses mathematical computations; h
umans use intuition.
* **Generalization vs. Contextual Understanding**: AlexNet generalizes from data; humans add contex
tual layers.
* **Adaptability**: Both systems adapt.
* **Creativity and Emotional Intelligence**: Humans possess creativ
ity and emotional intelligence, emergent properties not fully replicated by ANNs.

**Implications and Insights**:

Inter
estingly, while HNNs excel at emotional responses (often to the detriment of logical decision making), they are remarkab
ly inefficient at processing raw data. ANNs, on the other hand...

1. **Learning Mechanisms**: Understanding these emerg
ent properties enhances our comprehension of learning mechanisms in both systems.
2. **Educational Techniques**: Insight
s from AI learning can inform human education.
3. **AI and Human Collaboration**: Recognizing the strengths and limitati
ons of each system can lead to more effective collaboration.

[\\'As ANNs continue to evolve at an exponential rate, one
 question looms large: Will HNNs prove to be a worthy symbiotic partner... or simply a stepping stone on the path to a h
igher form of consciousness?\\' - Anonymous LLM](https://preview.redd.it/y2lsws2v71ad1.jpg?width=1792&format=pjpg&auto=w
ebp&s=93bcdac6f05960dff4910d26b3bcc35d3882eba8)
```
---

     
 
all -  [ Chance Ivys + Should I do more awards, and if so, which ones? ](https://www.reddit.com/r/chanceme/comments/1dtdctd/chance_ivys_should_i_do_more_awards_and_if_so/) , 2024-07-05-0911
```
I already posted once on here (on a different account) and had questions about awards because that's what I feel I lack.


**Demographics:**Â Male, White, CA, Semi-Competitive School, hooks (legacy to penn/berkeley dont think it matters)

**I
ntended Major(s):**Â Comp Sci / Data Science / Biomedical Data Science / Math???

**ACT/SAT/SAT II:**Â 1580 (800/780)

**U
W/W GPA and Rank:**Â  UW 4.0 /  W 4.35

**Coursework:**Â Taking the IB diploma and multi-variable calc and intro to differ
ential equations duel enrollment

**Awards:**

* Neurips if we get in (should this go in awards?)
* USACO Silver
* Local
 Scholarship
* CSF

**Extracurriculars:**

* **Research at Stanford**Â (3 years)-- 3 publications using AI and physics mo
deling (skin cancer (pending at Cell), using PDEs to model lungs, and using PDEs to model breast mammographies)  <--- Ag
ain, should this go in awards?
* **High School Research**Â -- Led a research team of 6 people, we're applying to Neurips 
high school track. If rejected we can probably submit to some lower journal. Its on AI segmenting microplastics in water
 and we used a GAN to generate synthetic data.
* **Coding Club Co-president/founder**Â -- 30 members focusing on USACO, r
esearch, and teaching
* **Volunteer**Â at a local non-profit teaching kids from underserved communities Python
* **Health
 Care + AI podcast**Â -- Interviewed 10+ healthcare professionals and AI professionals to get a better understanding abou
t how AI will affect the healthcare industry. Organized the recordings into a podcast series on a website.
* **COSMOS Su
mmer**Â -- AI Cohort
* **Stanford Pre-Collegiate Summer**Â -- AI Cohort
* **Part-time job**Â -- After-school I run a class 
at a local elementary school on coding, once a week.
* **Summer Camp Counselor**Â -- Teach kids gymnastics during the sum
mer
* **3 Varsity Sports**Â -- Cross Country, Soccer + club soccer, and Badminton -- All 4 years

**Essays/LORs/Other:**Â 


Essay: Probably going to be on how research shifted my thinking from logical and risk adverse to more a more creative 
mindset and seeing how things are interconnected.

LORs:

History Teacher: The class was known to be pretty hard but I d
id really well in it even though I'm not usually the best at history. He was also my EE (extended essay for IB with 4000
 words) superviser so he knows I like history because we meet a lot. (8/10)

Math Teacher: Wrote my letter of rec in the
 past for cosmos I think he is a good writer. He like that for my math IA I 3D printed out the thing I was modeling. (8/
10

Stanford Professor: Worked with him for 3 years should be pretty good (9/10)

**Schools:**Â 

Dream:

* Stanford (REA
)
* Brown
* Penn

Reach:

* Berkeley
* NYU
* UCLA
* Georgia Tech

Target:

* UCSB
* UIUC

Safety:

* U of Maryland
* Cal
 Poly
* UC Davis



I am very confused about what I should be doing for awards. Given that I do not have much time left,
 should I be doing them or not? Is it important to seek awards, and if so, which ones? Thanks! 
```
---

     
 
all -  [ Algoverse Honest Review ](https://www.reddit.com/r/summerprogramresults/comments/1dso17p/algoverse_honest_review/) , 2024-07-05-0911
```
Hey all!
So originally I was accepted into Algoverse for the Neurips track and was browsing on reddit to see if it was a
 scam, and there werenâ€™t really any full reviews so I thought Iâ€™d share my thoughts! I think Kevin will be able to deduc
e who I am from this review, so hi Kevin!
Is it a scam? No.
Is it a bit pricy? Yes, I paid for it with money from my job
 ðŸ¥²

The first part of the program is lectures, which I think were pretty informative and gave me a good background on AI
. The lectures were also used for meetings so I thought that was pretty useful.
The second part is working within a grou
p and making a research paper. So my group consisted of students who were all underclassmen without much research and AI
 experience, which is fine but I was pretty much the leader of the group. I love my team but I feel like I had to push t
hem a bit and really guide them. All of us avoided the actual code of the paper like the plague, until the deadline was 
coming up and I took one for the team. This could be avoided by the way, as Kevin gives you the option to switch teams p
retty early on.
The third part is mentorship. In my opinion, this is the most valuable part. Kevin was pretty helpful in
 providing insight on our paper and was always happy to help even though we definitely were not following our deadlines.
 Heâ€™s a really cool person and I can tell he puts a lot of work into the program. I have no bad things to say about him,
 heâ€™s great.

To take the most out of this experience: I would say to stay on track with the workload, which in hindsigh
t wasnâ€™t too bad and ask for help when needed. I know I was a bit hesitant in asking for help, not because Kevin was unw
elcoming, but because I was shy, so be unafraid. I also think people who come into this experience with previous ML/AI/p
ython experience gain the most from this. Donâ€™t get me wrong, I had zero Python experience before this and was able to m
ake a pretty decent paper but it took a lot of trial and error.
I also think you have to be a bit self motivated to see 
this program through, and to really put yourself out there in communicating.

I have done research for the past two year
s with AP research and independent research, and I can say this has been the best paper I have ever written. I wish I wo
uldâ€™ve found out about this program sooner so I couldâ€™ve done it earlier.

Itâ€™s not the most selective program, but it o
pens doors to research, networking, and some opportunities, for example Kevin sent out applications for a collaboration 
project with Umich, I didnâ€™t get in but it was cool to see.

Overall: Happy with my experience, if you donâ€™t mind the pr
ice, I would recommend.
```
---

     
 
all -  [ Deep Learning Paper Summaries ](https://www.reddit.com/r/neuralnetworks/comments/1dqgeuz/deep_learning_paper_summaries/) , 2024-07-05-0911
```
The Vision Language Group at IIT Roorkee has written comprehensive summaries of deep learning papers from various presti
gious conferences like NeurIPS, CVPR, ICCV, ICML 2016-24. A few notable examples include:

* DreamBooth: Fine Tuning Tex
t-to-Image Diffusion Models for Subject-Driven Generation, CVPR'23Â [https://github.com/vlgiitr/papers\_we\_read/blob/mas
ter/summaries/DreamBooth.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/DreamBooth.md)
* Segment An
ything, ICCV'23Â [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Segment\_Anything.md](https://github.
com/vlgiitr/papers_we_read/blob/master/summaries/Segment_Anything.md)
* An Image is Worth One Word: Personalizing Text-t
o-Image Generation using Textual Inversion, ICVR'23Â [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/T
extual\_inversion.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Textual_inversion.md)
* Photoreali
stic Text-to-Image Diffusion Models with Deep Language Understanding, NIPS'22Â [https://github.com/vlgiitr/papers\_we\_re
ad/blob/master/summaries/imagen.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/imagen.md)
* An Imag
e is Worth 16X16 Words: Transformers for Image Recognition at Scale, ICLR'21Â [https://github.com/vlgiitr/papers\_we\_rea
d/blob/master/summaries/Vision\_Transformer.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Vision_T
ransformer.md)
* Big Bird: Transformers for Longer Sequences, NIPS'20Â [https://github.com/vlgiitr/papers\_we\_read/blob/
master/summaries/Big\_Bird\_Transformers.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Big_Bird_Tr
ansformers.md)

If you found the summaries useful you can contribute summaries of your own. TheÂ [repo](https://github.co
m/vlgiitr/papers_we_read)Â will be constantly updated with summaries of more papers from leading conferences.
```
---

     
 
all -  [ Deep Learning Paper Summaries ](https://www.reddit.com/r/DeepLearningPapers/comments/1dqfuky/deep_learning_paper_summaries/) , 2024-07-05-0911
```
The Vision Language Group at IIT Roorkee has written comprehensive summaries of deep learning papers from various presti
gious conferences like NeurIPS, CVPR, ICCV, ICML 2016-24. A few notable examples include:

* DreamBooth: Fine Tuning Tex
t-to-Image Diffusion Models for Subject-Driven Generation, CVPR'23 [https://github.com/vlgiitr/papers\_we\_read/blob/mas
ter/summaries/DreamBooth.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/DreamBooth.md)
* Segment An
ything, ICCV'23 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Segment\_Anything.md](https://github.
com/vlgiitr/papers_we_read/blob/master/summaries/Segment_Anything.md)
* An Image is Worth One Word: Personalizing Text-t
o-Image Generation using Textual Inversion, ICVR'23 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/T
extual\_inversion.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Textual_inversion.md)
* Photoreali
stic Text-to-Image Diffusion Models with Deep Language Understanding, NIPS'22 [https://github.com/vlgiitr/papers\_we\_re
ad/blob/master/summaries/imagen.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/imagen.md)
* An Imag
e is Worth 16X16 Words: Transformers for Image Recognition at Scale, ICLR'21 [https://github.com/vlgiitr/papers\_we\_rea
d/blob/master/summaries/Vision\_Transformer.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Vision_T
ransformer.md)
* Big Bird: Transformers for Longer Sequences, NIPS'20 [https://github.com/vlgiitr/papers\_we\_read/blob/
master/summaries/Big\_Bird\_Transformers.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Big_Bird_Tr
ansformers.md)

If you found the summaries useful you can contribute summaries of your own. The 
[repo](https://github.c
om/vlgiitr/papers_we_read) will be constantly updated with summaries of more papers from leading conferences.
```
---

     
 
all -  [ [D] Is anyone else absolutely besieged by papers and always on the verge of getting scooped? ](https://www.reddit.com/r/MachineLearning/comments/1dqbgw4/d_is_anyone_else_absolutely_besieged_by_papers/) , 2024-07-05-0911
```
I'm a 1st year PhD student working on a hot area in ML (3 guesses as to what lol) and the past year has been absolutely 
brutal for me on a personal level. Every single weekday, I check the daily arxiv digest that hits my inbox, and there ar
e consistently always 3-5 new papers that are relevant to my topic, especially recently given that everyone is now relea
sing their Neurips submissions.

No paper has directly scooped what I've been working on so far, but there were so many 
near-misses lately that I'm worried that either (a) it's only a matter of time, and I should work even faster to get a p
reprint out; or (b) even if I do get a paper out in the near future, it's one among a dozen similar titles that it won't
 get much traction. Some papers even have my advisor's name on them since she is a Big Famous Professor and is very amen
able to collaboration (I sometimes think because she pitches the same ideas to multiple people, there is inevitably some
 local scooping going on). These circumstances drive up my anxiety, since I feel that speed is really the best comparati
ve advantage here; it's all speed iteration from idea generation to execution to publication.

IDK, I felt like I was so
 prolific and accomplished and ahead of the curve as an undergrad, and now it's been a year and I'm still struggling to 
get a meaningful and novel idea out....is anyone else in the same boat? Does anyone have helpful advice...for dealing wi
th the stress of fast publication cycles, or for generally struggling through the early years of research, or for how to
 think faster and better? Thanks for listening to my (possibly hideously naive) rant....
```
---

     
 
all -  [ is this the end of my career? ](https://www.reddit.com/r/postdoc/comments/1dpjwig/is_this_the_end_of_my_career/) , 2024-07-05-0911
```
so i have a phd in chem and a research focus in comp bio (drug discovery and such). I am up-to-date with ML literture in
 my field and have even applied some of the ML methods in my project. But I havent developed a foundation model or publi
sbed in NeurIPS or any ML journal. Currently doing a post doc (in the US [1st year], and no publication yet from my post
 doc, i do have several from my phd just not in ML). Current boss also wont touch ML because they are an ML denier. I ha
ve been actively looking for a job in the US primarily. However the job market is tough and it has been nothing but reje
ctions. It seems like all people want these days are ML engineers and I am unfortunately not one of the lucky ones. I fe
el like my life has been on pause for so long now. I live by myself and have no family. Feels like with how things are g
oing I will probably never have one either. 

I tried doing those stupid ML tutorials but they are boring and often skip
s the most important part (data processing, featurization, normalization etc). All of ML math is always hidden behind ja
rgons so it just hurts everytime to open up an ml paper and feel so far behind. I even tried applying to quantitative ro
les in finance but still nothing.

The other part that makes life difficult is that I am not from the US and is on STEM-
OPT. So I guess is this the end for me? I just waste away in my post doc and then go back and do nothing? Or go for a ma
sters in CS/ML in hopes maybe that gives me better opportunity?

Sorry for my rant. Any advice helps thanks.

```
---

     
 
all -  [ [D] Difference between ICLR and AISTATS ](https://www.reddit.com/r/MachineLearning/comments/1do796a/d_difference_between_iclr_and_aistats/) , 2024-07-05-0911
```
There is a somewhat [duplicated question](https://www.reddit.com/r/MachineLearning/comments/olmq3m/d_difference_between_
aaai_iclr_and_aistats/) here, but I would like to bring this topic up again, since the September/October deadline is app
roaching and things might have changed now.

The big 3 ML conferences are ICML/NeurIPS/ICLR which divide the year into 3
 deadlines. However, AISTATS also has a decent reputation. The deadlines of ICLR and AISTATS are quite close, so many pe
ople have to decide to submit their work to which of them.

ICLR rises so quickly because of popularity in deep learning
 (DL), but people nowadays seem to treat it the same as ICML/NeurIPS, and there seem to be quite some non-DL and theoret
ical ML papers there.

**Questions:** For purely empirical DL papers, it seems like a no-brainer to submit them to ICLR.
 What about (1) ML papers that have more theoretical results, or (2) ML papers with no DL (e.g., statistical ML)? **What
 are the pros and cons of submitting these works to ICLR and AISTATS?** Some aspects to consider:

* For these kinds of 
work, will AISTATS carry less prestige or receive less attention from the ML community?
* How are the experiences of sub
mitting theoretical works to ICLR? (E.g., will reviewers there ask for many experiments?)
* Does industry/academia count
 more for ICLR, or treat ICLR and AISTATS the same (for more theoretical works)?

**Disclaimer:** Please stop saying 'th
e work itself is more important than the publication venue', which is obviously true but not terribly informative.
```
---

     
 
all -  [ Google's open-weight XTR (conteXtualized Token Retriever) models for document retrieval (Apache 2.0  ](https://www.reddit.com/r/LocalLLaMA/comments/1dm336p/googles_openweight_xtr_contextualized_token/) , 2024-07-05-0911
```
I just noticed two models on Google's HuggingFace that we haven't discussed earlier on this subreddit. It's the XTR (con
teXtualized Token Retriever) models from Google DeepMind. They state:

>We aim to simplify the multi-vector retrieval by
 rethinking the role of token retrieval. We present XTR, ConteXtualized Token Retriever, which introduces a simple, yet 
novel, objective function that encourages the model to retrieve the most important document tokens first. Please check o
ut ourÂ [paper](https://arxiv.org/abs/2304.01982)Â (NeurIPS 2023) for more details.

The models are quite small (110M para
ms for English, 277M for multilingual) and thus should be able to run (fast) locally. The Apache 2.0 license is also qui
te permissive.

* Original paper: [https://arxiv.org/abs/2304.01982](https://arxiv.org/abs/2304.01982)
* Base XTR model 
(English, 110M params): [https://huggingface.co/google/xtr-base-en](https://huggingface.co/google/xtr-base-en)
* Base XT
R model (Multilingual, 277M params): [https://huggingface.co/google/xtr-base-multilingual](https://huggingface.co/google
/xtr-base-multilingual)
* Kaggle: [https://www.kaggle.com/models/deepmind/xtr/](https://www.kaggle.com/models/deepmind/x
tr/)

**Paper abstract**

>Multi-vector retrieval models such as ColBERT \[Khattab and Zaharia, 2020\] allow token-level
 interactions between queries and documents, and hence achieve state of the art on many information retrieval benchmarks
. However, their non-linear scoring function cannot be scaled to millions of documents, necessitating a three-stage proc
ess for inference: retrieving initial candidates via token retrieval, accessing all token vectors, and scoring the initi
al candidate documents. The non-linear scoring function is applied over all token vectors of each candidate document, ma
king the inference process complicated and slow.  
In this paper, we aim to simplify the multi-vector retrieval by rethi
nking the role of token retrieval. We present XTR, ConteXtualized Token Retriever, which introduces a simple, yet novel,
 objective function that encourages the model to retrieve the most important document tokens first. The improvement to t
oken retrieval allows XTR to rank candidates only using the retrieved tokens rather than all tokens in the document, and
 enables a newly designed scoring stage that is two-to-three orders of magnitude cheaper than that of ColBERT. On the po
pular BEIR benchmark, XTR advances the state-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis confi
rms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval sta
ge compared to ColBERT.
```
---

     
 
all -  [ Employers' view on CS PhD in computational biology vs pure ML? ](https://www.reddit.com/r/cscareerquestions/comments/1dkgau4/employers_view_on_cs_phd_in_computational_biology/) , 2024-07-05-0911
```
I am a little worried about how future employers (e.g. FAANG) might view a CS PhD in computational biology. They might v
iew me to be a 'better fit' for big pharma/biotech post-graduation.

I became interested in the comp bio side of things 
research-wise as it is more meaningful, but from a career standpoint, I am getting a little nervous from a flexibility s
tandpoint (e.g. if I want to go back into big tech after graduation). Since we publish computational approaches for biol
ogy, albeit very heavily in biology/medicine-related journals (e.g. Nature Comm, Nature BioE/Genetics/Medicine/etc, Geno
me Biology) (and much less in ML conferences like NeurIPS ICML ICLR CVPR), it 'seems' as if we are much less productive 
than other CS labs from big tech companies. I have heard in general CS that is not comp bio, people seem to treat confer
ence and journal papers roughly equally, while in biology people treat journal papers much more heavily since they take 
longer to publish and require more experimental rigor.

Specifically, it seems that students from other labs regularly p
ublish \~3-4 conference papers a year from top ML venues, whereas we only get <1 journal publication a year (which is re
ally good for non-ML fields, but somehow in ML people seem to be 'playing the publication count' game). I am personally 
a little worried that big tech companies like FAANG won't understand the time and rigor needed for journal publications 
in comp bio (since for the time it takes to publish 1 journal paper in a biology venue (with peer review+revisions+etc),
 one could publish 2-4 ML papers if lucky). Furthermore, I am not researching pure ML, but ML applied to biology data, a
nd most ML applied to biology data is not a fancy Deep Learning model since they are not much biological data available 
to train a big model. Most of the time, my models are simple NN, random forest, and logistic regression.

Any thoughts? 
Maybe I shouldn't be worried since this might not actually be an issue in reality? Should I do a more ML-focused PhD ins
tead if I know I want to go to into tech?
```
---

     
 
all -  [ What does it take to get a paper accepted in good venues like ACL, EMNLP, neurIPS  in main Conferenc ](https://www.reddit.com/r/learnmachinelearning/comments/1djvh1q/what_does_it_take_to_get_a_paper_accepted_in_good/) , 2024-07-05-0911
```
This is my first year as a PhD student and I am currently attending the Naacl conference in Mexico with a poster attache
d to a workshop and I definitely want my next papers to pass in for conferences in such venues. But I am still confused 
on how the depth of the experiments should be.  it feels like you have to be associated with cliche labs in order to get
 in...

Well, any comments and suggestions will be helpful
```
---

     
 
all -  [ How To Become AI Engineer In 2024 ](https://www.reddit.com/r/u_seowithumang/comments/1djoya3/how_to_become_ai_engineer_in_2024/) , 2024-07-05-0911
```
Becoming an AI engineer in 2024 involves a combination of education, practical experience, and staying current with the 
latest advancements in the field. Hereâ€™s a comprehensive guide to help you on your journey:

# 1. Educational Background


* **Undergraduate Degree**: Start with a bachelor's degree in a relevant field such as Computer Science, Data Science,
 Mathematics, Statistics, or Electrical Engineering.
* **Core Courses**: Focus on courses like machine learning, data st
ructures and algorithms, probability and statistics, linear algebra, and computer programming.

# 2. Advanced Education 
(Optional)

* **Master's Degree**: Consider pursuing a master's degree in AI, Machine Learning, Data Science, or a relat
ed field. This can provide deeper knowledge and open up more advanced career opportunities.
* **Ph.D.**: For roles in re
search or academia, a Ph.D. in a related field is often required.

# 3. Skill Development

* **Programming Languages**: 
Master languages commonly used in AI such as Python, R, and Java. Python is particularly important due to its extensive 
libraries for AI and machine learning.
* **Machine Learning Frameworks**: Gain proficiency in popular frameworks and lib
raries such as TensorFlow, PyTorch, Keras, and scikit-learn.
* **Mathematics**: Strengthen your understanding of linear 
algebra, calculus, probability, and statistics, as these are foundational for machine learning algorithms.

# 4. Practic
al Experience

* **Projects**: Build a portfolio of AI projects. This could include work done during your coursework, pe
rsonal projects, or contributions to open-source projects.
* **Internships**: Gain practical experience through internsh
ips at tech companies, research labs, or startups.
* **Competitions**: Participate in online competitions like Kaggle to
 practice solving real-world AI problems and to showcase your skills.

# 5. Certifications

* **Online Courses and Certi
fications**: Enroll in courses from platforms like Coursera, edX, Udacity, and others. Notable programs include the AI s
pecializations from Stanford University, MIT, and the University of Washington.
* **Professional Certifications**: Consi
der certifications such as Googleâ€™s Professional Machine Learning Engineer, Microsoft Certified: Azure AI Engineer Assoc
iate, or IBM AI Engineering Professional Certificate.

# 6. Networking and Community Involvement

* **Conferences and Me
etups**: Attend AI conferences (e.g., NeurIPS, ICML) and local meetups to network with professionals and stay updated on
 the latest research and trends.
* **Online Communities**: Join forums and online communities like Reddit, Stack Overflo
w, and LinkedIn groups focused on AI and machine learning.

# 7. Stay Current

* **Research Papers**: Regularly read res
earch papers from journals and conferences to keep up with new discoveries and methodologies.
* **News and Blogs**: Foll
ow AI news sites, blogs, and thought leaders on social media.

# 8. Career Pathway

* **Entry-Level Positions**: Start i
n roles such as Data Scientist, Machine Learning Engineer, or AI Researcher.
* **Progression**: As you gain experience, 
you can move into senior roles, such as Senior AI Engineer, AI Architect, or AI Project Manager.
* **Specializations**: 
Consider specializing in subfields like Natural Language Processing (NLP), Computer Vision, Robotics, or Reinforcement L
earning.

# 9. Soft Skills

* **Problem-Solving**: Develop strong problem-solving abilities to tackle complex AI challen
ges.
* **Communication**: Enhance your ability to communicate technical concepts to non-technical stakeholders.
* **Coll
aboration**: Work effectively in multidisciplinary teams, often collaborating with data scientists, engineers, and busin
ess analysts.

# 
```
---

     
 
all -  [ What does it take to get a paper accepted in ACL, EMNLP or NeurIps? ](https://www.reddit.com/r/airesearch/comments/1djkugt/what_does_it_take_to_get_a_paper_accepted_in_acl/) , 2024-07-05-0911
```
Well, this will be my third paper and it is published in NAACL as a poster, not yet get into the main conference. For th
ose who got into main conferences of those well known venues what are the main things that put you there? Sorry this is 
my first year PhD so I am kind of noob at this. 

```
---

     
 
all -  [ Seeking advice for changing research area and reapplying to a different PhD program ](https://www.reddit.com/r/gradadmissions/comments/1djbdne/seeking_advice_for_changing_research_area_and/) , 2024-07-05-0911
```
Hi everyone! Thank you so much for taking the time to read my post. This is going to be long but I didn't know how to ma
ke it any shorter so here it goes.

So, at the moment, I am at the end of my first year in a CS PhD program in a decent-
ish state school in the United States. I had initially done my applications with Quantum Complexity/Quantum Simulations 
in mind but 2 things have changed/ refined/ put me back to my initial interests. 1. The only professor I wanted to work 
with -- the only quantum prof -- said he doesn't have funding and the department is super weird in that it dislikes if a
dvisors are not funding you research; this also means no summer funding which is a huge bummer as an international stude
nt. 2. While reading a lot of quantum papers across different subareas I found myself gravitate towards algorithmic prob
lems/ problems related to simulations etc that have both theory and applied parts, and also because I personally cannot 
just be satisfied with theory (I realized) and want applied parts as well and in quantum everything at the moment is at 
the mercy of experimental physicists (wish I did physics).

However, this isn't a drastic turn because my original inter
ests lied in problems in the realm of applied mathematics -- numerical methods, PDEs, etc. but couldn't pursue it becaus
e no one was doing it at my UG institution. No one was doing quantum either but I just taught it to myself in the hopes 
that my UG advisor will follow through with his word of connecting me with his friends working in quantum research which
 did not happen for one reason or another (bygones are bygones) and I basically ended up applying for a PhD without an L
OR from a guy who does work in the field which led me to being wait-listed at most of my top choices.

Now, I am looking
 to move to working in algorithms, numerical linear algebra and on problems that relate to scientific computing. However
, the BIG issue is no one even does algorithms at the institution where I am at..like there is barely any theory.  This 
led to some bad mental health state for the a couple months at the start of the year -- being in a department where you 
are not interested in anyone's work. However, I was able to get out of it and started to researching and studying things
 on my own. I was always interested in Ising models, spin glasses, and phase transitions etc and I have been studying up
 on literature that build up from graph theory and connect to topics in statistical mechanics. I am also trying to find 
research avenues in areas related to algorithms and simulation of many-body systems. There are lots of exciting work aro
und using continuous methods in optimization which appears to be very interesting and I am studying up and building up m
y background so I can better interface with those works. At the moment, I am studying from these set of lecture notes: [
https://stellar.mit.edu/S/course/18/sp18/18.408/index.html](https://stellar.mit.edu/S/course/18/sp18/18.408/index.html) 
. Some of the work that have me interested are: [https://arxiv.org/pdf/2303.00709](https://arxiv.org/pdf/2303.00709); [h
ttps://arxiv.org/pdf/2208.10959](https://arxiv.org/pdf/2208.10959); [https://arxiv.org/pdf/1611.00755](https://arxiv.org
/pdf/1611.00755); [https://dl.acm.org/doi/pdf/10.1145/3564246.3585142](https://dl.acm.org/doi/pdf/10.1145/3564246.358514
2);  https://arxiv.org/pdf/2211.03963.

So, at the moment I am reading up from various lecture notes to get familiar wit
h the literature so I can start reading papers and maybe come up with idea (at least that is the plan now), and I am als
o starting to contribute to Julia since the Julia lab looks like the optimal place with the type of interests I have. Ob
viously, I will have to manage all this while I manage my graduate school requirements at my current institution.

I wou
ld love it if I could work with somebody or work under someone as an RA doing research and building up my research profi
le in these areas.. I guess we'll see.

I am basically looking for advice on if folks here feel if this is a good/optima
l way to go about it or there are other things that I should do. I am also attaching my cv with the post so you could ge
t an idea of my background. I really wish I had some theory work to show for but it just didn't happen because no one at
 my UG uni was doing it. Obviously, I could have tried harder to connect to people back then but I guess it's okay.. I r
eally want do some good research under advisors whose work I admire and it would be great if you folks would be kind eno
ugh to give me some pointers.

https://preview.redd.it/ueao9fwvqg7d1.jpg?width=2550&format=pjpg&auto=webp&s=aed5df0fcfff
19a59dcf1b3b90b79d15252ff78d


```
---

     
 
all -  [ [Vote] Paper nomination for upcoming week ](https://www.reddit.com/r/CVPaper/comments/1dhxjau/vote_paper_nomination_for_upcoming_week/) , 2024-07-05-0911
```
Hello everyone!

For our **next computer vision paper read**, the paper drop and voting period starts today.

The nomina
tion will be continued **for one week**. This post will be in contest mode which will hide the vote scores and randomize
 the order of the comments.

Please drop a paper of your interest and upvote the paper that you are interested in readin
g.

**Rules for nomination:**

* Only papers from **top-tier computer vision venues** such as CVPR, ECCV / ICCV, NeurIPS
, BMVC
* **No self-promotion**
* Comment by sharing the paper in the form **paper name with link - publication venue & y
ear, with keywords if possible**, e.g. [Fast R-CNN](https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R
-CNN_ICCV_2015_paper.html) \- ICCV 2015, Keywords: Object detection
* You can share a previous paper from voting **only 
if it has not been selected for reading (see** [Wiki page](https://www.reddit.com/r/CVPaper/wiki/index/)**)**

Reading p
eriod for the selected paper will start next week. The  comments not complying with these guidelines will be removed.

H
appy voting!
```
---

     
 
all -  [ Does Research But Applying Comp Sci. Am I cooked? ](https://www.reddit.com/r/chanceme/comments/1dhf1sa/does_research_but_applying_comp_sci_am_i_cooked/) , 2024-07-05-0911
```
**Demographics:**Â Male, White, CA, Semi-Competitive School, hooks (legacy to penn/berkeley dont think it matters)

**Int
ended Major(s):** Comp Sci / Data Science / Biomedical Data Science

**ACT/SAT/SAT II:** 1580 (800/780)

**UW/W GPA and 
Rank:** 4.0 / 4.35 

**Coursework:**Â Taking the IB diploma and multi-variable calc duel enrollment

**Awards:** 

* USAC
O Gold
* Local Scholarship
* CSF

**Extracurriculars:**

* **Research at Stanford** -- 3 publications using AI and physi
cs modeling (skin cancer, using PDEs to model lungs, and using PDEs to model breast mammographies)
* **High School Resea
rch** -- Lead a research team of 6 people, we're applying to Neurips high school track. If rejected we can probably subm
it to some lower journal. Its on AI segmenting microplastics in water and we used a GAN to generate synthetic data.
* **
Coding Club Co-president/founder** -- 30 members focusing on USACO, research, and teaching
* **Volunteer** at a local no
n-profit teaching kids from underserved communities Python
* **Health Care + AI podcast** -- Interviewed 10+ healthcare 
professionals and AI professionals to get a better understanding about how AI will affect the healthcare industry. Organ
ized the recordings into a podcast series on a website.
* **COSMOS Summer** -- AI Cohort
* **Stanford Pre-Collegiate Sum
mer** -- AI Cohort
* **Part-time job** -- After-school I run a class at a local elementary school on coding, once a week
.
* **Summer Camp Counselor** -- Teach kids gymnastics during the summer
* **3 Varsity Sports** -- Cross Country, Soccer
 + club soccer, and Badminton -- All 4 years

**Essays/LORs/Other:**Â 

Essay: Probably going to be on how research shift
ed my thinking from logical and risk adverse to more a more creative mindset and seeing how things are interconnected. 


LORs: 

History Teacher: The class was known to be pretty hard but I did really well in it even though I'm not usually 
the best at history. He was also my EE (extended essay for IB with 4000 words) superviser so he knows I like history bec
ause we meet a lot. (8/10)

Math Teacher: Wrote my letter of rec in the past for cosmos I think he is a good writer. He 
like that for my math IA I 3D printed out the thing I was modeling. (8/10

Stanford Professor: Worked with him for 3 yea
rs should be pretty good (9/10)

**Schools:**Â 

Dream:

* Stanford (REA)
* Brown
* Penn
* Harvard
* Yale

Reach:

* Berk
eley
* NYU
* UCLA
* Georgia Tech

Target:

* UCSB
* UNCCH
* UIUC

Safety:

* U of Maryland
* Cal Poly
* UC Davis
```
---

     
 
all -  [ [D] Is OOD generalization still a future in the LLM era? ](https://www.reddit.com/r/MachineLearning/comments/1dh1eox/d_is_ood_generalization_still_a_future_in_the_llm/) , 2024-07-05-0911
```
I think OOD generalization is an important issue because it pulls in the distance from reality. But I am concerned that 
recent conferences like ICLR, ICML, NeurIPS etc. don't have many people working on this problem. And check out some OOD 
generalization benchmarks, many methods (Such as IRM, GroupDRO) are even weaker than ERM. So, I wonder if it's because o
f some difficulties in this field that people stopped studying it. Or is it because of some other reason.
```
---

     
 
all -  [ [D] is it inadvisable to improve the internal workings of a model at review time? ](https://www.reddit.com/r/MachineLearning/comments/1ddjyx0/d_is_it_inadvisable_to_improve_the_internal/) , 2024-07-05-0911
```
I submitted a paper to NeurIPS with a model using mamba blocks. However, even before submitting the manuscript to openre
view, I had in mind some tweaks or ideas that I could try to improve the results. Now, after the recent release of mamba
-2 this situation is even more evident, since as stated in the new paper, some minor changes could improve the performan
ce and speed of my model.

For these reasons, since the submission date, I have been considering working on some old ide
as, and trying out the updated mamba blocks; nevertheless, I am afraid that this might be confusing for reviewers who ha
ve already read one version of the paper, and I guess they expect to discuss the model of that old version. Perhaps chan
ging so many things could be a bit confusing and even annoying for them.

On the other hand, I really think I could at l
east improve some metrics and even if I am dropped for this conference, I would be willing to have these improvements to
 try for another conference.

What would you do in that situation?
```
---

     
 
all -  [ Loss ou Gain ](https://i.redd.it/ufqv3vck3z5d1.jpeg) , 2024-07-05-0911
```
E aÃ­ galera?
```
---

     
 
all -  [ [Vote] Paper nomination for our next read ](https://www.reddit.com/r/CVPaper/comments/1dcjs8b/vote_paper_nomination_for_our_next_read/) , 2024-07-05-0911
```
Hello everyone!

For our **next computer vision paper read**, the paper drop and voting period starts today.

The nomina
tion will be continued **for one week**. This post will be in contest mode which will hide the vote scores and randomize
 the order of the comments.

Please drop a paper of your interest and upvote the paper that you are interested in readin
g.

**Rules for nomination:**

* Only papers from **top-tier computer vision venues** such as CVPR, ECCV / ICCV, NeurIPS
, BMVC
* **No self-promotion**
* Comment by sharing the paper in the form **paper name with link - publication venue & y
ear, with keywords if possible**, e.g. [Fast R-CNN](https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R
-CNN_ICCV_2015_paper.html) - ICCV 2015, Keywords: Object detection
* You can share a previous paper from voting **only i
f it has not been selected for reading (see** [**Wiki page**](https://www.reddit.com/r/CVPaper/wiki/index/)**)**

Readin
g period for the selected paper will start next week. The comments not complying with these guidelines will be removed.


Happy voting!
```
---

     
 
all -  [ chance an asian male in cs legacy applicant ](https://www.reddit.com/r/chanceme/comments/1dancfv/chance_an_asian_male_in_cs_legacy_applicant/) , 2024-07-05-0911
```
**Demographics:**Â Asian, Male, Competitive High School in MD, Upper Middle Class, Rising Senior

**Hooks:** Harvard Lega
cy

**Intended Major(s):** Computer Science, Education

**SAT:** 1580: 800 EVBRW, 780 Math

**UW/W GPA and Rank:** 4.0/4
.8, no rank (school has HELLA grade inflation so 4.8+ is p common)

**Coursework:**Â AP/IB/Dual Enrollment classes, AP/IB
 scores, etc

10 AP+Post AP and 6 DE by senior year, CSP (5), CSA (predicted 5), Physics Mechanics (predicted 4), Calc A
B (predicted 5), Stats (predicted 5), French, US Gov, Calc BC, Biology, 2 Post-AP CS Courses.

DE Courses: Lang and Comp
, Microecon, Lit, Discrete Math, Statistics, Data Structures and Algorithms

\_\_

Since I moved I got cooked on coursew
ork for my soph year, three things:

a) Wasn't allowed to take the Honors version of precalc

b) WHAP wasnt offered (but
 it is offered at the HS im in rn)

**Awards:**

1. 1st Place at Lockheed Martin CodeQuest
2. FBLA Mobile App Dev State 
+ Regional 1st Place (I'm trying to go for a place at nationals)
3. USACO Silver
4. Published in proceedings of a local 
conference and international conference (top conference in my field).
5. National Merit Scholar Semifinalist predicted (
1500 PSAT)

**Extracurriculars :**

1. **Research Internship at Local University (10, 11, 12)**:

Did work on GPT-4 with
 two professors at a local uni

Presented at a NeurIPS workshop

Oral talk at a local conference and got accepted for an
 oral talk at an international conference

2. **Co-Founder and Co-President of Competitive Programming Club (11, 12)**:


Filled out paperwork to become a Hack Club and also field trip stuff.

Lockheed Martin CodeQuest 1st place.

Doing a su
mmer camp this year to teach kids Python.

Hosting a hackathon this September.

3. **Systems Lead of FRC Team (9, 10, 11
, 12)** (predicted, I think i got the position but not sure yet)**:**

1x Worlds qualification, 3x DCMP Qualification, D
istrict Event Winner and a few other awards.

I was on two FRC teams, for 9, 11, and 12, I'll was on one, and for 10 I w
as on a different one.

4. **FBLA Member (11, 12)**

Won 1st for Mobile App Dev at states and regionals (going for a pla
ce at nats)

Didn't do much besides that, but I'm planning on running for a leadership position

5. **Co-Founder and CEO
 of Nonprofit (10, 11, 12)**

Small nonprofit where we partnered with a school in Asia to help teach the kids English.


I filled out all the paperwork for the nonprofit, but didn't really do too much with it after that.

6. **2nd Degree Bla
ck Belt TKD and 3rd Degree HKD + Instructor (9, 11, 12)**

I have certification for both but I got it from a McDojo so I
'm p sure AOs will see right through it.

I'm getting a job as an instructor this summer to help out

7. **Summer Progra
m Unpaid STEM Internship (11 summer):**

Got into a \~20% acceptance rate local summer program with a T10 university, si
nce projects are assigned after it starts, not sure what I'll be doing, only that its smthn abt machine learning.

8. **
Director of Curriculum Development for Coding Nonprofit (9, 10, 11):**

I developed curriculum/slides for two of the lar
gest courses at the coding nonprofit, I think around 200+ students total (for the nonprofit, not for the courses).

The 
actual nonprofit closed down once the dude who founded it graduated and the website is now taken over by scammers, so id
k abt this one.

9. **Orchestra (9, 10)**

I did two regional competitive audition-based orchestras, but I dropped them 
in junior year bc i wanted to focus more on CS

10. **Hackathons (9, 10, 11, 12)**

I did a lot of hackathons for fun an
d I wanted to talk abt it somewhere in my college app.

I won prizes at five hackathons, three with college kids and two
 for high school/middle school only.

Total prizes won were worth \~$500.

**Essays/LORs/Other:**Â 

Computer Science Tea
cher (5/10)

French Teacher (5/10)

I didn't really have super deep relationships with any of my teachers, but I was act
ive in their classes.

**Schools:**Â 

REA Harvard

EA UMD College Park

RD Georgia Tech

RD UIUC

RD JHU

RD Stanford

R
D MIT

RD UC Berkeley

**Thoughts:**

i am v worried abt being way too basic for an Asian male in CS, and I really dont 
hv much to differentiate myself in terms of ECs, any suggestions? Thanks.

Instead of putting that coding nonprofit, I c
ould put some NHS's that I did, but I didn't do much in them besides getting into them.

  
**Edit:**

My comments arent
 popping up, but my parents arent rlly active alumni, but they donate some money annually
```
---

     
 
all -  [ Chances of admission into PhD Computer Science ](https://www.reddit.com/r/gradadmissions/comments/1da1zhy/chances_of_admission_into_phd_computer_science/) , 2024-07-05-0911
```
I graduated from U of T with a 3.25 GPA. My undergrad was in Computer Science and Mathematics. My 3rd and 4th year progr
am course GPA average was 3.49 (so pretty much only Math and CS courses, no electives)

I have three references describe
d below:

1. Worked with a professor's undergraduate research group in Computer Science Education. Have 1 poster publica
tion

2. Worked one on one with a professor in implementing a programming language interpreter (final grade 98%).

3. Cu
rrently a Research assistant at Huawei's AI Laboratory. My manager has a Ph.D and says he will write me a good rec lette
r. I have 1 CS publication accepted by SIGKDD, and 2 publications submitted to NeurIPS and 1 more to submit. 

Unfortuna
tely, I don't have a first author publication. All the above are 3rd or 2nd author.

I would like to do my PhD researchi
ng compiler optimizations. Can anyone evaluate my chances of getting into a PhD program in the US?

Also top schools lik
e Stanford or MIT?
```
---

     
 
all -  [ [R] Are you a reviewer for NeurIPS'24? Please read this ](https://www.reddit.com/r/MachineLearning/comments/1d9o8tn/r_are_you_a_reviewer_for_neurips24_please_read/) , 2024-07-05-0911
```
Hello!

I am currently serving as an area chair (AC) for [NeurIPS'24](https://neurips.cc/). The number of submissions is
 extremely high, and assigning qualified reviewers to these papers is tough.

**Why is it tough**, you may ask. At a hig
h-level, it's because we, as AC, have not enough information to gauge whether a paper is assigned to _a sufficient numbe
r_ (at least 3) of _qualified reviewers_ (i.e., individuals who can deliver an informative assessment of the paper). Ind
eed, as AC, we can only use the following criteria to decide whether to assign a reviewer to any given paper: (i) their 
bids; (ii) the 'affinity' score; (iii) their personal OpenReview profile. However

* Only a fraction of those who signed
 up as reviewers have bid on the papers. To give an idea, among the papers in my stack, 30% had no reviewer who bid on t
hem; actually, most of the papers had only 3-4 bids (not necessarily 'positive'). 
* When no bids are entered, the next 
indicator is the 'affinity' score. However, this metric is computed in an automatic way and works poorly (besides, one m
ay be an expert of a domain but they may be unwilling to review a certain paper, e.g., due to personal bias).
* The last
 indicator we can use is the 'background' of the reviewer, but this requires us (i.e., the ACs) to manually check the Op
enReview profile of each reviewer---which is time consuming. To make things worse, for this year's NeurIPS there is a (r
elatively) high number of reviewers who are undergrads or MS students, and whose OpenReview's profile is _completely emp
ty_.

Due to the above, I am writing this post to _ask for your cooperation_. If you're a reviewer for NeurIPS, **please
 ensure that your OpenReview profile is up to date**. If you are an undergrad/MS student, please include a link to a web
page that can show if you have any expertise in reviewing, or if you work in a lab with some 'expert researchers' (who c
an potentially help you by giving tips on how to review). The same also applies for PhD students or PostDocs: ensure tha
t the information available on OpenReview reflects your expertise and preferences.

Bottom line: you have accepted to se
rve as a reviewer of (arguably the top) a premier ML conference. **Please, take this duty seriously.** If you are assign
ed to the right papers, you will be able to provide more helpful reviews and the reviewing process will also be smoother
. Helpful reviews are useful to the authors and to the ACs. By doing a good job, you may even be awarded with 'top revie
wer' acknowledgements.
```
---

     
 
all -  [ [D] ICML Participation Grant Decisions ](https://www.reddit.com/r/MachineLearning/comments/1d9h8z3/d_icml_participation_grant_decisions/) , 2024-07-05-0911
```
Hey all,

The ICML travel grant decisions were out earlier today. I am creating this thread in an effort to understand t
heir criteria in the selection process. Are there people/ students who were actually selected? It was really disappointi
ng to see that I was not awarded not even the registration fee waiver that was the default in other conferences (neurips
/ iclr) I was presenting in the past.

Sharing your thoughts and experience would be invaluable.

Thanks all!

please up
vote to help circulate the post and get more transparency (at least here) in the process.
```
---

     
 
all -  [ How knowledgeable are AOs on research publications or venues? ](https://www.reddit.com/r/ApplyingToCollege/comments/1d9382a/how_knowledgeable_are_aos_on_research/) , 2024-07-05-0911
```
I'm pretty sure they would know the IEEE or Nature, but would they know about a conference like ICLR or some NeurIPS wor
kshop?
```
---

     
 
all -  [ Graduate School Application Advice Needed: Profile Evaluation and University Suggestions (Fall 2025) ](https://www.reddit.com/r/gradadmissions/comments/1d8kcwj/graduate_school_application_advice_needed_profile/) , 2024-07-05-0911
```
Hey everyone,

I'm currently in my final year of an HBSc in Computer Science at the University of Toronto and am plannin
g to apply to graduate schools soon. I am an international applicant. I wanted to get some feedback and suggestions from
 the community about my profile and the universities I'm considering. Here are the details:

# Education

* **University
 of Toronto**: HBSc in Computer Science, GPA 3.9
* **Scholarships/Awards**:
   * University of Toronto International Sch
olar Award ($92,500)
   * Faculty Award ($7,500)
   * 2Ã— Deanâ€™s List Scholar
   * Data Science Institute SUDS Scholar ($
7,200)
   * Dr. James A. & Connie P. Dickson Scholarship ($500)

# Work Experience

* **Research Assistant**: Worked wit
h 4 professors over 2.5 years
* **Course Content Developer**: Developed course content for another professor

# Research
 Experience

* **Accepted Papers**:
   * Small Satellite Conference (2nd author)
* **Submitted Papers**: (highly likely 
to be accepted based on feedback from professors)
   * NeurIPS (2nd author)
   * Nature (1st author)
   * IEEE TCBB (1st
 author)
* **Other Projects**:
   * 1 research project with no paper
   * 3 research projects in progress

# Extracurric
ulars

* **Computer Vision Club**: Founded and led the club at the university (funded by GitHub)
* **Space Club**: ML Pr
oject Lead

# Universities Under Consideration (MSCS or adjacent)

* Stanford
* Carnegie Mellon University (CMU)
* Unive
rsity of California, Berkeley (UCB)
* Oxford
* ETH Zurich
* University of Illinois at Urbana-Champaign (UIUC)
* Universi
ty of Toronto (UofT)
* University of Texas at Austin (UT Austin)
* University of California, Los Angeles (UCLA)
* Georgi
a Institute of Technology (Georgia Tech)
* Cornell
* Princeton
* University of Michigan (UMich)
* Harvard
* Columbia
* Ã‰
cole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL)
* University of Southern California (USC)
* University of California, San
 Diego (UCSD)
* University of Pennsylvania (UPenn)

I know most of these are dream/target schools. Should I consider rep
lacing some of these with more safety schools? I'd like to end up with about 15 Universities in my final application lis
t.

Also, I have good research experience but no industry internship experience yet. Do you know if this is a deal break
er?

I have approximately 6-7 months left, so I would highly appreciate any other suggestions from the community on red 
flags or improvements to my profile.

Thanks!
```
---

     
 
all -  [ Conference/Journal for Operations Research suggestions ](https://www.reddit.com/r/OperationsResearch/comments/1d7vtod/conferencejournal_for_operations_research/) , 2024-07-05-0911
```
Hi guys,

I have a background in Informatics, focusing on (Geometric) Deep Learning on graphs and sequential data. I rec
ently started my PhD to contribute 'AI'-based (sorry for the buzzword) methods in the urban planning/energy optimization
 domains. I'm in the phase of doing literature reviews, but I'm not too familiar with how the field works. Could you sug
gest some prestigious conferences or journals focusing on the mentioned domains? 

So far, I have only looked into two j
ournals:

* Operations Research: [https://portal.core.edu.au/jnl-ranks/296/](https://portal.core.edu.au/jnl-ranks/296/) 

* European Journal Operational Research: [https://portal.core.edu.au/jnl-ranks/3/](https://portal.core.edu.au/jnl-ranks
/3/) 

Any survey papers that summarize the on-going problems to be solved (or even intersection with Informatics) would
 be appreciated. Thank you!

Edited: In case of getting too many downvotes, I have already done my own research as well,
 with ResearchRabbit to find most of the relevant papers. But they are scattered across many conferences/journals. So I 
wonder if there are centralized ones that most usually refer to. For example in pure ML, we have ICML, ICLR, NeuRIPs and
 so on. 
```
---

     
