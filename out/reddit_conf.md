 
all -  [ Visa rejected for No Proper reason ](https://www.reddit.com/r/CanadaVisitorVisa/comments/1ghdzz6/visa_rejected_for_no_proper_reason/) , 2024-11-03-0914
```
I was supposed to go for NeurIPS conference from India and I had Invitation letter as well as cover letter from my compa
ny. Yet they have rejected with absolutely no logical reasons. How to appeal this? 
```
---

     
 
all -  [ [R] QTIP: Quantization with Trellises and Incoherence Processing ](https://www.reddit.com/r/MachineLearning/comments/1ggyj3l/r_qtip_quantization_with_trellises_and/) , 2024-11-03-0914
```
We're pleased to introduce QTIP, a new LLM quantization algorithm that uses trellis coded quantization and incoherence p
rocessing to achieve a state of the art combination of speed and quantization quality.

Paper (NeurIPS 2024 Spotlight): 
[https://arxiv.org/pdf/2406.11235](https://arxiv.org/pdf/2406.11235)

Codebase + inference kernels: [https://github.com/
Cornell-RelaxML/qtip](https://github.com/Cornell-RelaxML/qtip)

Prequantized models (including 2 Bit 405B Instruct): [ht
tps://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803](https://huggingface.co/collecti
ons/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803)

QTIP has significantly better quality over QuIP# while bein
g just as fast. QTIP is also on par with or better than PV-Tuning while being much faster (\~2-3x).


```
---

     
 
all -  [ New Quantization Method -- QTIP: Quantization with Trellises and Incoherence Processing ](https://www.reddit.com/r/LocalLLaMA/comments/1ggwrx6/new_quantization_method_qtip_quantization_with/) , 2024-11-03-0914
```
We're pleased to introduce QTIP, a new LLM quantization algorithm that uses trellis coded quantization and incoherence p
rocessing to achieve a state of the art combination of speed and quantization quality.

Paper (NeurIPS 2024 Spotlight): 
[https://arxiv.org/pdf/2406.11235](https://arxiv.org/pdf/2406.11235)

Codebase + inference kernels: [https://github.com/
Cornell-RelaxML/qtip](https://github.com/Cornell-RelaxML/qtip)

Prequantized models (including 2 Bit 405B Instruct): [ht
tps://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803](https://huggingface.co/collecti
ons/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803)

QTIP has significantly better quality over QuIP# while bein
g just as fast. QTIP is also on par with or better than PV-Tuning while being much faster (\~2-3x).

[2 Bit 405B Instruc
t running pipelined on 2 GPUs. The inference backend uses torch.compile and HF so this should be much faster on somethin
g like llama.cpp.](https://reddit.com/link/1ggwrx6/video/rz8ghv5fc8yd1/player)
```
---

     
 
all -  [ Chance a Chess Fiend With Below Average Grades but Good EC's and SAT ](https://www.reddit.com/r/chanceme/comments/1ggukfh/chance_a_chess_fiend_with_below_average_grades/) , 2024-11-03-0914
```
**Demographics**:

* mid to low comp HS, 250 ish grad class
* Asian
* I play chess?
* CS BA or BS

**GPA**: 3.7 W(good r
easons why so low just trust)

No rank

7 APs, 8 Tests

SAT - 1510 composite

**Awards**:

* AP scholar with honors
* ho
nor roll
* Top 100 nationally ranked chess players In age groups for the past 4 years
* USCF candidate master
* Won an i
nternational/national tournament + state champs in chess

\*\*ECs(\*\*nốt **ordered properly):**

* High School Chess le
ague president - 20+ schools, 100+ participants, $1k+ raised
* 1st author to Novel Ai paper - published and submitted to
 conferences like Neurips + COLING, available on arXiv
* Chess Club president - 3 peat champion in regional league, top 
5 teams in State
* DECA - 3x state qualifier
* Motorola Solutions Intern - made a REST API for one of their apps in prod

* Paid Chess coach - Apart of non-profit group for underprivileged youth in chicago(not from there did remote)
* Volunt
eer Chess Coach - volunteered apart of local chess academy, 200 ish hours over the 4 years
* Wrestling - Varsity
* SASA(
south asian student association) treasurer - raised 10k from sponsors and events, provided scholarships for the first ye
ar to south asian students
* Inspirit AI scholars program(free) - Made Chess bot with GPT 4o capable of playing at an ex
pert level

**Personal Statement:** 7.5/10 (not insanely good, but everyone who reads it likes it, and reviewers can't f
ind problems with it, so conservative 7.5)

**Colleges**:  
Umass, UMD, UW Madison, BU, NEU, Ohio State, Penn State, Pur
due, IU, Vtech, UPitt, RIT, Bentley, Rutgers
```
---

     
 
all -  [ [D] Is TMLR good enough to consider as an alternative to A* conferences? ](https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/) , 2024-11-03-0914
```
Hi there, I am a current PhD student in Artificial Intelligence working on Multi-Armed Bandits. More recently, I have co
mpleted one of my works on the intersection of Bandits and LLMs and was wondering for a suitable venue for publication.


The closest conference I see is ICML having deadline of 31st January which is about two months from now, therefore was 
wondering about a suitable alternate venue. While previous reddit threads (a year back) claim that TMLR is better than A
AAI, IJCAI and similar conferences but falls way short compared to ICML, NeurIPS, ICLR, etc, I was wondering if it's sti
ll true. 

Does the ML community still considers TMLR to be a potential place to submit it, given that the deadline for 
the closest conference is too far?
```
---

     
 
all -  [ Neural network recognizer for hand-written zip code digits (1988): 'with a high-performance preproce ](https://www.reddit.com/r/mlscaling/comments/1ggr0j4/neural_network_recognizer_for_handwritten_zip/) , 2024-11-03-0914
```
This paper was published just before LeNet-1. Notable features:

* 18 hand-designed kernels (??).
* An early bitter less
on? 'In the early phases of the project, we found that neural network methods gave rather mediocre results. Later, with 
a high-performance preprocessor, plus a large training database, we found that a layered network gave the best results, 
surpassing even Parzen Windows.'
   * 'Several different classifiers were tried, including Parzen Windows, K nearest nei
ghbors, highly customized layered networks, expert systems, matrix associators, fea ture spins, and adaptive resonance. 
We performed preliminary studies to identify the most promising methods. We determined that the top three methods in thi
s list were significantly better suited to our task than the others, and we performed systematic comparisons only among 
those three \[Parzen Windows, KNN, neural networks\].'
* Nevermind, seems they didn't take the bitter lesson. 'Our metho
ds include low-precision and analog processing, massively parallel computation, extraction of biologically-motivated fea
tures, and learning from examples. We feel that this is, therefore, a fine example of a Neural Information Processing Sy
stem. We emphasize that old-fashioned engineering, classical pattern recognition, and the latest learning-from-examples 
methods were all absolutely necessary. Without the careful engineering, a direct adaptive network attack would not succe
ed, but by the same token, without learning from a very large database, it would have been excruciating to engineer a su
fficiently accurate representation of the probability space.'

Denker, John, et al. '[Neural network recognizer for hand
-written zip code digits](https://proceedings.neurips.cc/paper/1988/hash/a97da629b098b75c294dffdc3e463904-Abstract.html)
.' *Advances in neural information processing systems* 1 (1988).
```
---

     
 
all -  [ Florence-2-as-a-Judge ](https://www.reddit.com/r/LocalLLaMA/comments/1gfv0me/florence2asajudge/) , 2024-11-03-0914
```
I learned about Judge Distillation from slide 14 in [this deck](https://nips.cc/media/neurips-2023/Slides/83968_5GxuY2z.
pdf) describing how Phi-2 researchers scaled their data quality filter to a large synthetic dataset.

I'm planning to sc
ale up data synthesis for the [OpenSpaces dataset](https://huggingface.co/datasets/remyxai/OpenSpaces) and have found I 
can use [SpaceLLaVA](https://huggingface.co/remyxai/SpaceLLaVA) in VLM-as-a-Judge with [prometheus-vision](https://githu
b.com/prometheus-eval/prometheus-vision). Check out the [SpaceJudge Dataset](https://huggingface.co/datasets/salma-remyx
/SpaceJudgeDataset) to see the an assessment of a small split.

Now, I'm fine-tuning Florence-2 on this dataset, introdu
cing the new <JUDGE> task to help filter out low-quality synthetic samples. Here's the [experiment collection](https://h
uggingface.co/collections/salma-remyx/vlm-judge-distillation-671fc8fe1925c49630307a82).

Will discuss some of this at OD
SC West tomorrow, let's connect!


```
---

     
 
all -  [ [0 YOE, Recent Grad, Machine learning engineer, United States] ](https://i.redd.it/yc7sqkex7txd1.png) , 2024-11-03-0914
```

```
---

     
 
all -  [ [D]ended up with a poster in NuerIPS-24 ](https://www.reddit.com/r/MachineLearning/comments/1gdxef5/dended_up_with_a_poster_in_nuerips24/) , 2024-11-03-0914
```
I have a poster in NuerIPS this year through the  journal track(MLRC) along with the main conference papers.I didnt expe
ct this to happen so i hadnt planned/researched about the expenses/funding prior.I already had my visa and conference re
gistration arranged but have no clue about further proceedings of Nuerips and how to fund it(i am an UG junior).If you h
ave already attended NeurIPS before please pour your ideas and experiences.
```
---

     
 
all -  [ Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CVPR,  ](https://www.reddit.com/r/computervision/comments/1gd6812/looking_for_collaborations_on_ongoing/) , 2024-11-03-0914
```
Hey everyone,

Our group, **Vision and Language Group, IIT Roorkee,** recently got three workshop papers accepted at Neu
rIPS workshops! 🚀 We’ve also set up a website 👉 [VLG](https://vlgiitr.github.io/), featuring other publications we’ve wo
rked on, so our group is steadily building a portfolio in ML and AI research. Right now, we’re collaborating on several 
work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have even
 more ideas we’re excited about. Still, a few of our main limitations have been access to proper guidance and funding fo
r GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interested i
n working together, we’d love to explore intersections in our fields of interest and any new ideas you might bring to th
e table!

If you have resources available or are interested in discussing potential collaborations, please feel free to 
reach out! Looking forward to connecting and building something impactful together! Here is the link for our Open Slack 
👉 [Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)
```
---

     
 
all -  [ Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CVPR,  ](https://www.reddit.com/r/neuralnetworks/comments/1gd64zq/looking_for_collaborations_on_ongoing/) , 2024-11-03-0914
```
# Hey everyone,

Our group, **Vision and Language Group, IIT Roorkee,** recently got three workshop papers accepted at N
eurIPS workshops! 🚀 We’ve also set up a website 👉 [VLG](https://vlgiitr.github.io/), featuring other publications we’ve 
worked on, so our group is steadily building a portfolio in ML and AI research. Right now, we’re collaborating on severa
l work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have ev
en more ideas we’re excited about. Still, a few of our main limitations have been access to proper guidance and funding 
for GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interested
 in working together, we’d love to explore intersections in our fields of interest and any new ideas you might bring to 
the table!

If you have resources available or are interested in discussing potential collaborations, please feel free t
o reach out! Looking forward to connecting and building something impactful together! Here is the link for our Open Slac
k 👉 [Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)
```
---

     
 
all -  [ Some General Rules for Uni Selection Fall'25 ](https://www.reddit.com/r/MSCS/comments/1gcs53j/some_general_rules_for_uni_selection_fall25/) , 2024-11-03-0914
```
1. If you dont have 9+ GPA from tier 3 / 8.7 GPA from NIT / 8.5 GPA from IIT, do not apply to any of UC's MSCS (except U
C Riverside and UC Santa Cruz) .
2. Do not apply to Ivy Leagues MSCS if you don't have 9.5+ GPA from tier 3/ 9+ GPA from
 Tier 2.
3. If you are not from IIT NIT, do not apply to TAMU unless you have 9.5+ GPA.
4. The earlier you apply to NEU 
with a 8+ GPA/100+ TOEFL, the higher your chances of getting admit  for MSCS (Mostly full by Nov end). After that you wi
ll be offered alternate campuses.
5. Dont apply to Stony Brook MSCS if you dont have 8.5+ GPA, 315+ GRE, 167+ Quants. Yo
u can try MSDS though.
6. Dont apply to UMASS if you dont have 315+ GRE.
7. If you have 2+ years of work ex, try MCS in 
UC's, especially UCI, and TAMU.
8. Dont apply to USC if you cant afford extreme tuition fees and 1000$ deposit.
9. Use [
admits.fyi](http://admits.fyi), [https://gpa.eng.uci.edu/](https://gpa.eng.uci.edu/) to get estimates.
10. Conferences d
ont matter, no matter how many, unless they are NeurIPS, ICCV, CVPR etc. Especially conferences and journals like IJET, 
IJSER, IJRASET etc. can have negative impact on your profile. IEEE Access is another common Journal, its good, but wont 
create much of impact on your profile.
11. MSDS is much more gettable than MSCS.
12. You dont need to send you GRE and T
OEFL scores before application in most cases. You can upload unofficial copy, and send official scores after you get an 
admit. Dont waste money.
13. Work ex wont create much of a difference for MSCS, but def creates a great diff when it com
es to getting a job.
14. Prefer East coast for finance, West Coast for tech jobs ( Location matters more than rankings).
 For pure research, prefer reputation over anything else.
15. SJSU MSCS is best uni for getting a job in tech for those 
who have low gpa, low scores and financial issues. Be aware you need CS undergrad degree background to get MSCS at SJSU.

16. Purdue took very few people last year as they had funding issues. Purdue was harder than UCSD to get in.
17. GPA ma
tters more than anything else. 9 GPA tier 3 = 8.7 GPA tier 2 = 8.5 GPA tier 1.
18. [https://github.com/SimplifyJobs/Summ
er2025-Internships](https://github.com/SimplifyJobs/Summer2025-Internships),  [https://github.com/cvrve/Summer2025-Inter
nships](https://github.com/cvrve/Summer2025-Internships) . Get an Idea of what things are currently in demand in the US,
 dont just go for any branch or any specialization because you have interest in it.
19. Rankings along with minimum idea
l gpa according to tier 3 MIT, Stanford (9.9 GPA) > CMU, UC Berkeley, UIUC (9.8) > GATECH, UT Austin, UWash (9.6)> UCSD,
 Columbia, CalTech, UCLA, UPenn (9.5)> UWM, UMCP, Purdue (9.4)> UMass, TAMU, UCI, UCSB (9.3)> UCD, USC, NYU Courant (9.1
)> Stony Brook, Penn State, Virginia Tech (8.7\~9) > Rutgers, NEU, NYU Tandon (8.5) >  NCSU, BU (8.5)
20. Dont go for Co
nsultations, waste of money.

Anymore questions, feel free to ask.
```
---

     
 
all -  [ 'Foundations for Machine Learning' ](https://www.reddit.com/r/learnmachinelearning/comments/1gcc2fc/foundations_for_machine_learning/) , 2024-11-03-0914
```


https://preview.redd.it/pea5gbvrr0xd1.png?width=1280&format=png&auto=webp&s=452854ecdc3e378b5b4a5b57dbbf145cf093b551


In 2022, I graduated with a PhD in Mechanical Engineering from MIT. 



Although a big component of my research was pure
ly hands-on experiments, my exposure to foundational graduate-level ML courses at MIT, research courses, and Scientific 
Machine Learning via Julia gave me the confidence of a Machine Learning researcher. 



I incorporated ML into my resear
ch, and it solved a problem that is otherwise difficult to solve theoretically or experimentally. Now I have co-authored
 multiple AI-ML research papers and two of them are accepted to the upcoming NeurIPS workshop. 



Behind all of this ef
fort, there is the confidence that stems from knowing what happens underneath the ML algorithms.



Most of the online c
ourses have little emphasis on fundamentals. People are so used to spending time on toy Kaggle projects. Very few people
 I know can build a neural network from scratch or explain what happens behind them.



For the last 4 months, I have be
en working to launch a new course titled on 'Foundations for Machine Learning.' This will be a 45-hour course with \~65 
lectures. I will be hosting all lectures on this playlist: [https://www.youtube.com/playlist?list=PLPTV0NXA\_ZSiLI0ZfZYb
HM2FPHKIuMW6K](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiLI0ZfZYbHM2FPHKIuMW6K)



My singular goal with this c
ourse is to teach you the entire foundations required to learn ML from scratch.



There are no prerequisites. If you ha
ve basic logical thinking capability and a willingness to dedicate time, consistently, you can follow this course.



I 
have split the course into 5 modules.



1) First I will cover the 4 mathematical pillars of ML: Linear Algebra, Probabi
lity, Statistics, and Calculus.



2) In the second module I cover the basic programming fundamentals for a complete beg
inner. I will teach you Python from scratch and it's some of the most important packages for ML including NumPy and PyTo
rch.



3) In the 3rd module we will learn about optimization and gradient descent. I wanted to dedicate an entire modul
e to optimization because when you actually build ML models, you will be spending a lot of time on optimization.



4) I
n the 4th module, I will give you an overview of the AI landscape. What happened from 2010-2020 and what does it look li
ke from 2020-2030? This overview will help you understand overall where ML, DL, NLP, CV, and GenAI are heading.



5) In
 the final module, I will cover the most important 2 steps you will have to master as a Data Scientist or ML engineer: p
rocessing data and communication via storytelling. I will teach you some of the most powerful preprocessing and visualiz
ation techniques.



I have already published the first lecture. Check out here. I am sure you will enjoy and learn a lo
t: [https://youtu.be/C8hEa2qb46k?si=7dRHM6EZwlUBDC5C](https://youtu.be/C8hEa2qb46k?si=7dRHM6EZwlUBDC5C)
```
---

     
 
all -  [ [R] Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CV ](https://www.reddit.com/r/u_vlg_iitr/comments/1gc2yzd/r_looking_for_collaborations_on_ongoing/) , 2024-11-03-0914
```
Hey everyone,

Our group, **Vision and Language Group, IIT Roorkee,** recently got three workshop papers accepted at Neu
rIPS workshops! 🚀 We’ve also set up a website 👉 [VLG](https://vlgiitr.github.io/), featuring other publications we’ve wo
rked on, so our group is steadily building a portfolio in ML and AI research. Right now, we’re collaborating on several 
work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.

That said, we have even
 more ideas we’re excited about. Still, few of our main limitations have been access to proper guidance along with fundi
ng for GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interes
ted in working together, we’d love to explore intersections in our fields of interest and any new ideas you might bring 
to the table! 

If you have resources available or are interested in discussing potential collaborations, please feel fr
ee to reach out! Looking forward to connecting and building something impactful together! Here, is the link for our Open
 Slack 👉 [Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)
```
---

     
 
all -  [ Paper summaries for some of our papers that recently got accepted in NeurIPS ](https://www.reddit.com/r/learnmachinelearning/comments/1gb78i9/paper_summaries_for_some_of_our_papers_that/) , 2024-11-03-0914
```
Hey everyone, here is the list of papers by our groups that got accepted recently in NeurIPS 2024; It is a proud moment 
for us as an all-UG group; all the papers were published without any external support from the academia; here is a summa
ry of our papers. We hope this inspires others to pursue AI and look into research as a perspective where we can work to
gether, and all you require is the right guidance (not even necessarily a PhD or a professor). If you find these papers 
useful and want to working/collabrating with us, feel free to connect with us!

* Give me a hint: Can LLMs take a hint t
o solve math problems? 👉 [Arxiv link](https://arxiv.org/abs/2410.05915)
   * We propose improving LLM performance on adv
anced math problems using 'hints,' inspired by human pedagogy. We also test the model's robustness to incorrect hints. O
ur approach is evaluated on various LLMs using diverse problems from the MATH dataset, comparing it with one-shot, few-s
hot, and chain of thought prompting.
* Attention Shift: Steering AI Away from Unsafe Content 👉 [Arxiv link](https://arxi
v.org/abs/2410.04447)
   * This study explores methods to restrict unsafe content in generative models. We propose a nov
el training-free approach using attention reweighing to remove unsafe concepts during inference. Our method is compared 
to existing techniques, evaluated on direct and adversarial jailbreak prompts. We also discuss potential causes, limitat
ions, and broader implications.
* Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright P
rotection in Images 👉 [Arxiv link](https://arxiv.org/abs/2406.12592v1)
   * This paper extends the study of concept abla
tion in pre-trained models, as introduced by Kumari et al. (2022). We reproduce results from various concept ablation te
chniques and propose a novel variant, 'trademark ablation,' to address branded elements in model outputs. We also analyz
e the model's limitations, behavior under ablation leakage prompts, and performance degradation on unrelated concepts.


**The Vision Language Group at IIT Roorkee** has compiled an excellent repository of **comprehensive summaries** for dee
p learning papers from top conferences like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down ke
y papers in computer vision, NLP, and machine learning—perfect if you want to stay updated without diving deep into the 
full papers.
```
---

     
 
all -  [ Paper summaries for some of our papers that recently got accepted in NeurIPS ](https://www.reddit.com/r/u_vlg_iitr/comments/1gb6csa/paper_summaries_for_some_of_our_papers_that/) , 2024-11-03-0914
```
Hey everyone, here is the list of papers by our groups that got accepted recently in NeurIPS 2024; It is a proud moment 
for us as an all-UG group; all the papers were published without any external support from the academia; here is a summa
ry of our papers. We hope this inspires others to pursue AI and look into research as a perspective where we can work to
gether, and all you require is the right guidance (not even necessarily a PhD or a professor). If you find these papers 
useful and want to working/collabrating with us, feel free to connect with us! 

* Give me a hint: Can LLMs take a hint 
to solve math problems? 👉 [Arxiv link](https://arxiv.org/abs/2410.05915)
   * We propose improving LLM performance on ad
vanced math problems using 'hints,' inspired by human pedagogy. We also test the model's robustness to incorrect hints. 
Our approach is evaluated on various LLMs using diverse problems from the MATH dataset, comparing it with one-shot, few-
shot, and chain of thought prompting.
* Attention Shift: Steering AI Away from Unsafe Content 👉 [Arxiv link](https://arx
iv.org/abs/2410.04447)
   * This study explores methods to restrict unsafe content in generative models. We propose a no
vel training-free approach using attention reweighing to remove unsafe concepts during inference. Our method is compared
 to existing techniques, evaluated on direct and adversarial jailbreak prompts. We also discuss potential causes, limita
tions, and broader implications.
* Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright 
Protection in Images 👉 [Arxiv link](https://arxiv.org/abs/2406.12592v1)
   * This paper extends the study of concept abl
ation in pre-trained models, as introduced by Kumari et al. (2022). We reproduce results from various concept ablation t
echniques and propose a novel variant, 'trademark ablation,' to address branded elements in model outputs. We also analy
ze the model's limitations, behavior under ablation leakage prompts, and performance degradation on unrelated concepts.


**The Vision Language Group at IIT Roorkee** has compiled an excellent repository of **comprehensive summaries** for de
ep learning papers from top conferences like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down k
ey papers in computer vision, NLP, and machine learning—perfect if you want to stay updated without diving deep into the
 full papers.
```
---

     
 
all -  [ New to Research - Need Info on Publications [R][D] ](https://www.reddit.com/r/MachineLearning/comments/1gaa3o8/new_to_research_need_info_on_publications_rd/) , 2024-11-03-0914
```
I have been writing and publishing a few papers/journals in the field of AI for the last two years now, but I am really 
not sure what the best journals and conferences are. In my case, I usually write a paper, and my professor, based on the
 content of the paper, submits it to that conference/journal.

So would like to understand some info from this sub.

\->
 What are some really good journals/conferences where you can publish a paper? (How are the journals /conferences ranked
, is there a way to check? I heard ICML, NeurIPS are the top conferences in this field)

\-> What are the best publisher
s?

\-> What are sci Q1, Q2 journals and A\*  journals?

I have a paper that I am writing now which is in the field of m
edicine (In the speech domain), can anyone suggest to me, what the best Journals/Conferences in this field are?

Sorry, 
if these are some basic questions, (I only know about the publishers: IEEEXplore, Springer, Elseveir and used to think i
f it's Scopus-indexed, it is a good conference/journal).
```
---

     
 
all -  [ [D] Responses to false accusations of plagiarism for Gaunt Tensor Product paper ](https://www.reddit.com/r/MachineLearning/comments/1ga12d8/d_responses_to_false_accusations_of_plagiarism/) , 2024-11-03-0914
```
I’m posting this on behalf of the authors of the paper. The first author tried to make a post about this, but the post g
ot removed for some reason. The author reached out to me because I was one of the people defending them, so see below fo
r the author writeup about the accusations.

**TL;DR**: We're the authors of the Gaunt Tensor Product paper, and we want
 to directly address the false plagiarism accusations against our work. Our main contribution, a new perspective on tens
or products of irreducible representations (irreps) in machine learning and equivariant neural networks, is novel and or
iginal. The claimed 'similarity' are actually algorithms from elementary math and CS courses, and are not the main contr
ibution of our work: our independent implementation is clear if you look at our code, which is quite different because w
e had a completely different application area in mind. On the other hand, our core contributions, including establishing
 the connection between tensor products of irreps and integrals of products of spherical harmonics and various design pa
radigms of equivariant operations, are completely omitted. There is an oversight of citation due to the gap between fiel
ds (machine learning vs. graphics), but this is not plagiarism, and now that we know about this, we are updating the pap
er with the citation and discussion accordingly. This is similar situation to areas such as neural ODEs, where the origi
nal ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until mu
ch later. The anonymous accuser is selectively replying, omitting key details, and controlling the narrative.

**More de
tails below**:

We are the authors of \['Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor
 Products'\](https://openreview.net/forum?id=mhyQXJ6JsK) . We are creating a new post to clearly outline our responses t
o the false accusations of plagiarism that we've received for the Gaunt Tensor Product paper in another thread. While we
 have replied on that thread, the anonymous OP of that thread is selectively replying and omitting a lot of information 
from our responses, and we don't think it is fair that they single-handedly control the narrative. Note that we never go
t any emails or posts on OpenReview from the author, who has instead decided to anonymously post on here.

Firstly, we w
ould like to comprehensively respond to the false accusations again:

\- **The contributions of our work**: as emphasize
d in our paper, our main contribution in this work is the new perspective on tensor products of irreps, which is novel a
nd original to the machine learning community (Equations 3 and 4). The whole Section 3.1 elaborates on how to establish 
the connection between tensor products of irreps and integrals of products of spherical harmonics. Although the OP claim
s 'However, it is important to note that this derivation accounts for less than one page of the nine-page paper.', the f
act is that our establishment and derivation are based on a series of rigorous deductions with many efforts on building 
a solid mathematical foundation including group theory and quantum mechanics (please refer to Appendix A.1-A.7, page 16-
28), which is not straightforward and trivial to obtain. Without these efforts, we cannot establish such connections, le
t alone the efficient algorithm. In the context of equivariant machine learning, this derivation presents significance t
o refresh the understanding of basic equivariant operations, which cannot be omitted.

\- **The similarities of the effi
cient algorithms between our work and FSHP work**: Firstly, we would like to apologize that we did not cite the FSHP wor
k in our submission, which is unintentional and due to the gap between these two communities (we are from the ML communi
ty, and they are from graphics, and the paper was not known to us until recently). We will update the arXiv version of o
ur paper asap by adding a discussion paragraph to carefully discuss the FSHP work and our work. **On the other hand, we 
also would like to clarify that there does not exist any plagiarism behavior of the FFT algorithm**: after we figure out
 the relation between the tensor product of irreps and integrals of the product of three spherical harmonics, it is rath
er natural to connect it with products of spherical functions. Moreover, there exist classical results for efficient com
putation of products of spherical functions, i.e., Convolution Theorem and FFT, which involve elementary knowledge that 
can be learned in several undergraduate classes: (1) change of basis, which can be learned in linear algebra and signal 
processing and is used in both paper to connect spherical harmonics and Fourier basis; (2) FFT, which is commonly taught
 in signal processing and numerical computation classes and is used for acceleration. Due to the basicness of these math
ematical tools, both works follow the standard way to formalize and present, which leads to similarity. As we said, this
 cannot be misrepresented as plagiarism because we independently worked on this, and did not know about the other work u
ntil later because of the different communities. This is similar to work in areas such as neural ODEs, where the origina
l ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until much
 later.

\- **The differences in implementation**: it is noteworthy that, as a work for the equivariant machine learning
 community, it is not enough to simply propose an approach just for the tensor product operation. What we really care ab
out is the various design paradigms of equivariant operations, which are built upon tensor products. In Section 3.3, we 
categorize these paradigms into three classes in terms of their different characteristics and applied range. For each cl
ass of equivariant operations, we carefully specialize our approach by combining their properties and considering the re
strictions. For example, for the equivariant convolution, we figure out that we can further leverage eSCN/EquiformerV2's
 findings to achieve further acceleration; for the equivariant many-body interactions, a divide-and-conquer approach is 
natural, which is also generally taught in various CS courses and projects. There also exist different instantiation str
ategies in modern equivariant networks when applying these classes of operations, please refer to the Discussion paragra
ph in Section 3.3 and Appendix C. Simply proposing the efficient approach for tensor products is not feasible to these m
entioned points. Without these additional efforts and contributions, the efficient algorithm is not practical to be used
 for the equivariant machine learning community,  which cannot be omitted.

\- There is quite a lot of literature in the
 last few decades in the graphics community on this, and this is another general point is that work on the graphics comm
unity on efficient algorithms is not heard of and/or undercited in the rotationally equivariant neural networks communit
y, when these algorithms pop up in a lot of equivariant NN work. Additionally, this graphics paper is not in the field o
f ML, and this algorithm is being applied to a completely different area, which is why we did not see it originally and 
had an independent formalization. Perhaps an analogy here is that there are papers applying Transformers to different ar
eas like vision instead of language, but this shouldn't be 'plagiarism' at all. Likewise, neural ODEs shouldn't be consi
dered plagiarism of traditional ODE solvers simply because they are using the same method (and indeed, some of the origi
nal ideas of neural ODEs were in engineering papers from the '90s, and not discovered/cited in ML papers until later bec
ause of the different communities). One user on this thread also put it well that the concepts here like FFT are quite w
ell-known: 'After skimming, my impression is that those are well known results from textbooks and signal processing cour
ses that nobody bother to cite anymore. I could be wrong.'

\- The implementation in the GTP paper is fairly different f
rom the FSHP paper and was implemented independently because we derived our implementation based on being motivated by o
ur specific application area of ML for molecular modeling: their code is in C++, doesn't support efficient computations 
for lower rotation orders (L), and is not made for use with irreducible representations. This should be clear when you s
ee the code.

\- The main purpose of the Equiformerv2 experiment with the self-mix layer was a proof-of-concept to show 
that such a self-mix layer can be implemented because of the Gaunt Tensor Product formulation. Without this formulation 
(and using the more standard Clebsch-Gordan Tensor Product), it would have been very slow to add this layer (and not gre
at from a memory usage perspective). This can be made more clear in the arXiv version.

Secondly, we would like to point
 out that the anonymous OP of that thread is selectively replying to posts, and omitting a lot of information (including
 in how they are updating their own thread, they do not include all of the details of our responses). To us, the posts a
lso seem LLM generated but you should draw your own conclusions. We also posted this new topic because the authors respo
nses on the original thread are all folded, which cannot be directly seen by new readers.

Finally, we appreciate that m
any people have been commenting on the thread to defend us. These types of anonymous, sensational claims can have seriou
s implications and to post anonymously on Reddit before emailing us or posting on OpenReview is really problematic. We h
ope that you all read these threads carefully before jumping to conclusions.
```
---

     
 
all -  [ facechain open source TopoFR face embedding model ! ](https://www.reddit.com/r/StableDiffusion/comments/1g98si0/facechain_open_source_topofr_face_embedding_model/) , 2024-11-03-0914
```
Our work \[TopoFR\](https://github.com/modelscope/facechain/tree/main/face\_module/TopoFR) got accepted to NeurIPS 2024,
 welcome to try it out !
```
---

     
 
all -  [ Confused about where to submit the paper for a conference ](https://www.reddit.com/r/PhD/comments/1g8d9st/confused_about_where_to_submit_the_paper_for_a/) , 2024-11-03-0914
```
I started working on a specific category of power balance problem using reinforcement learning. RL in power quality is n
ot new but I’m working on community of microgrids using RL which can help supply electricity to remote areas in developi
ng countries so novelty is in how we formulate the optimization problem. My professor thinks it could be submitted to so
me ASME conferences but I want to try my luck at ICML or NeurIPS. Do you think this paper is worthy of those conferences
?
```
---

     
 
all -  [ How to finish PhD within 3 years ? ](https://www.reddit.com/r/PhD/comments/1g7w95o/how_to_finish_phd_within_3_years/) , 2024-11-03-0914
```
I am about to start a PhD in mechanistic interpretability (subfield of explainable AI) in Germany. I worked in industry 
before and am already 30 yo. I want to finish within 3 years. My graduation requirements are three journal papers. High 
prestige conference papers also count (Neurips, ICLR..) .

For those that finished early:

* What was your experience ?

* What was your strategy ?

My thoughts are:

* Pick up specific niche subject and do variations of it.
* Publish one pa
pers in first year and network as much as possible in conference to find collaborators
* Pick up work from lab mates and
 collaborate.
```
---

     
 
all -  [ [R] Molecular Topological Profile (MOLTOP) - Simple and Strong Baseline for Molecular Graph Classifi ](https://www.reddit.com/r/MachineLearning/comments/1g7gj4m/r_molecular_topological_profile_moltop_simple_and/) , 2024-11-03-0914
```
Accepted at ECAI 2024 conference, ArXiv: [https://arxiv.org/abs/2407.12136](https://arxiv.org/abs/2407.12136)

Some high
lights:

- simple feature engineering on graphs

- it outperforms GROVER (NeurIPS 2020) and GraphMVP (ICLR 2024) on Mole
culeNet, and all models on peptide function prediction on LRGB (e.g. SAN+RWSE graph transformer)

- no hyperparameters t
o tune, takes seconds on most datasets

- surprisingly powerful in distinguishing non-isomorphic graphs

In short, if yo
u need a good and simple baseline for molecular graph classification, MOLTOP may be a good choice. We designed it not to
 be the best, but to be fast, easy-to-use, and also give strong results on average.

Abstract:

>We revisit the effectiv
eness of topological descriptors for molecular graph classification and design a simple, yet strong baseline. We demonst
rate that a simple approach to feature engineering - employing histogram aggregation of edge descriptors and one-hot enc
oding for atomic numbers and bond types - when combined with a Random Forest classifier, can establish a strong baseline
 for Graph Neural Networks (GNNs). The novel algorithm, Molecular Topological Profile (MOLTOP), integrates Edge Betweenn
ess Centrality, Adjusted Rand Index and SCAN Structural Similarity score. This approach proves to be remarkably competit
ive when compared to modern GNNs, while also being simple, fast, low-variance and hyperparameter-free. Our approach is r
igorously tested on MoleculeNet datasets using fair evaluation protocol provided by Open Graph Benchmark. We additionall
y show out-of-domain generation capabilities on peptide classification task from Long Range Graph Benchmark. The evaluat
ions across eleven benchmark datasets reveal MOLTOP's strong discriminative capabilities, surpassing the 1-WL test and e
ven 3We revisit the effectiveness of topological descriptors for molecular graph classification and design a simple, yet
 strong baseline. We demonstrate that a simple approach to feature engineering - employing histogram aggregation of edge
 descriptors and one-hot encoding for atomic numbers and bond types - when combined with a Random Forest classifier, can
 establish a strong baseline for Graph Neural Networks (GNNs). The novel algorithm, Molecular Topological Profile (MOLTO
P), integrates Edge Betweenness Centrality, Adjusted Rand Index and SCAN Structural Similarity score. This approach prov
es to be remarkably competitive when compared to modern GNNs, while also being simple, fast, low-variance and hyperparam
eter-free. Our approach is rigorously tested on MoleculeNet datasets using fair evaluation protocol provided by Open Gra
ph Benchmark. We additionally show out-of-domain generation capabilities on peptide classification task from Long Range 
Graph Benchmark. The evaluations across eleven benchmark datasets reveal MOLTOP's strong discriminative capabilities, su
rpassing the 1-WL test and even 3-WL test for some classes of graphs. Our conclusion is that descriptor-based baselines,
 such as the one we propose, are still crucial for accurately assessing advancements in the GNN domain.

Happy to discus
s / answer any questions in the comments.
```
---

     
 
all -  [ [D] Why do PhD Students in the US seem like overpowered final bosses  ](https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/) , 2024-11-03-0914
```
Hello,

I'm a PhD student in a European university, working on AI/ML/CV ..etc. my PhD is 4 years. The first year I liter
ally just spent learning how to actually do research, teaching one course to learn how things work...etc. Second year, I
 published my first publication as a co-author in CVPR. By third year, I can manage research projects, I understand how 
to do grants applications, how funding works, the politics of it all ...etc. I added to my CV, 2 publications, one journ
al and another conference as first author. I'm very involved in industry and I also write a lot of production grade code
 in regard to AI, systems architecture, backend, cloud, deployment, etc for companies that have contracts with my lab.


The issue is when I see PhD students similar to me in the US, they be having 10 publications, 5 of them 1st author, all 
of them are either CVPR, ICML, ICLR, NeurIPS ...etc. I don't understand, do these people not sleep ? How are they able t
o achieve this crazy amount of work and still have 3 publications every year in A\* journals ?

I don't think these peop
le are smarter than I, usually I get ideas and I look up if something exists, and I can see that something was just publ
ished by some PhD student in Stanford or DeepMind ..etc like 1 month ago, So I can see that my reasoning isn't late in r
egard to SOTA. but the concepts that you would need to grasp to just have one of those publications + the effort and the
 time you need to invest and the resources to get everything done, wouldn't be possible for 2\~3 months project. How is 
it possible for these people to do this ?

Thank you !
```
---

     
 
all -  [ [5 YOE] Physics postdoc trying switch. ~100 applications, handful of interviews, no offers. ](https://www.reddit.com/r/EngineeringResumes/comments/1g60iw4/5_yoe_physics_postdoc_trying_switch_100/) , 2024-11-03-0914
```
Hi! My postdoc is expiring, and I've been looking for ML Engineer jobs. I'm also looking for ML Researcher roles. I've a
pplied to 40+ places, with only a handful of interviews. Am I making any dumb errors as I'm trying to transition? I know
 it'll be a long process, but any advice is appreciated!

I'm also not terribly sure how many years of experience to put
. I'm counting the last years of grad school, as I was working pretty much unsupervised.

https://preview.redd.it/4kglup
tqldvd1.png?width=5949&format=png&auto=webp&s=d36454ef7e65fae545a5bf7b8cf6baf745b68613


```
---

     
 
all -  [ How I Started Learning Machine Learning ](https://www.reddit.com/r/learnmachinelearning/comments/1g4x299/how_i_started_learning_machine_learning/) , 2024-11-03-0914
```
Hello, everyone. As promised, I'll write a longer post about how I entered the world of ML, hoping it will help someone 
shape their path. I'll include links to all the useful materials I used alongside the story, which you can use for learn
ing.

I like to call myself an AI Research Scientist who enjoys exploring new AI trends, delving deeper into understandi
ng their background, and applying them to real products. This way, I try to connect science and entrepreneurship because
 I believe everything that starts as scientific research ends up 'on the shelves' as a product that solves a specific us
er problem.

I began my journey in ML in 2016 when it wasn't such a popular field. Everyone had heard of it, but few wer
e applying it. I have several years of development experience and want to try my hand at ML. The first problem I encount
ered was where to start - whether to learn mathematics, statistics, or something else. That's when I came across a name 
and a course that completely changed my career.

# Let's start

You guessed it. It was Professor Andrew Ng and his globa
lly popular Machine Learning course available on Coursera (I still have the certificate, hehe). This was also my first o
fficial online course ever. Since that course no longer exists as it's been replaced by a new one, I recommend you check
 out:

1. [Machine Learning (Stanford CS229)](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

2. [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)

These two 
courses start from the basics of ML and all the necessary calculus you need to know. Many always ask questions like whet
her to learn linear algebra, statistics, or probability, but you don't need to know everything in depth. This knowledge 
helps if you're a scientist developing a new architecture, but as an engineer, not really. You need to know some basics 
to understand, such as how the backpropagation algorithm works.

I know that Machine Learning (Stanford CS229) is a very
 long and arduous course, but it's the right start if you want to be really good at ML. In my time, I filled two thick n
otebooks by hand while taking the course mentioned above.

# TensorFlow and Keras

After the course, I didn't know how t
o apply my knowledge because I hadn't learned specifically how to code things. Then, I was looking for ways to learn how
 to code it. That's when I came across a popular framework called Keras, now part of TensorFlow. I started with a new co
urse and acquiring practical knowledge:

1. [Deep Learning Specialization](https://www.coursera.org/specializations/deep
-learning)
2. [Deep Learning by Ian Goodfellow](https://www.deeplearningbook.org/)
3. [Machine Learning Yearning by Andr
ew Ng](https://info.deeplearning.ai/machine-learning-yearning-book)

These resources above were my next step. I must adm
it that I learned the most from that course and from the book Deep Learning by Ian Goodfellow because I like reading boo
ks (although this one is quite difficult to read).

# Learn by coding

To avoid just learning, I went through various Gi
tHub repositories that I manually retyped and learned that way. It may be an old-fashioned technique, but it helped me a
 lot. Now, most of those repositories don't exist, so I'll share some that I found to be good:

1. [Really good Jupyter 
notebooks that can teach you the basics of TensorFlow](https://github.com/mrdbourke/tensorflow-deep-learning)
2. [Anothe
r good repo for learning TF and Keras](https://github.com/codebasics/deep-learning-keras-tf-tutorial)

# Master the chal
lenge

After mastering the basics in terms of programming in TF/Keras, I wanted to try solving some real problems. There
's no better place for that challenge than Kaggle and the popular Titanic dataset. Here, you can really find a bunch of 
materials and simple examples of ML applications. Here are some of my favorites:

1. [Titanic - Machine Learning from Di
saster](https://www.kaggle.com/c/titanic/overview)
2. [Home Credit Default Risk](https://www.kaggle.com/competitions/hom
e-credit-default-risk/overview)
3. [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/h
ouse-prices-advanced-regression-techniques)
4. [Two Sigma: Using News to Predict Stock Movements](https://www.kaggle.com
/competitions/two-sigma-financial-news)

I then decided to further develop my career in the direction of applying ML to 
the stock market, first using predictions on time series and then using natural language processing. I've remained in th
is field until today and will defend my doctoral dissertation soon.

# How to deploy models

To continue, before I move 
on to the topic of specialization, we need to address the topic of deployment. Now that we've learned how to make some b
asic models in Keras and how to use them, there are many ways and services, but I'll only mention what I use today. For 
all my ML models, whether simple regression models or complex GPT models, I use FastAPI. It's a straightforward framewor
k, and you can quickly create API endpoints. I'll share a few older and useful tutorials for beginners:

1. [AI as an AP
I tutorial series](https://www.youtube.com/watch?v=56qQNcHJxyQ)
2. [A step-by-step guide](https://medium.com/@ganiyuabdu
lwajeed2002/a-step-by-step-approach-to-building-a-fast-api-for-deep-learning-classification-projects-cbd2ea6bc2f2)
3. [P
roductizing an ML Model with FastAPI and Cloud Run](https://medium.com/semantixbr/deploy-an-ml-model-with-fastapi-and-cl
oud-run-part-1-1a0b0f3b3a5d)

Personally, I've deployed on various cloud providers, of which I would highlight GCP and A
WS because they have everything needed for model deployment, and if you know how to use them, they can be quite cheap.


# Chose your specialization

The next step in developing my career, besides choosing finance as the primary area, was my
 specialization in the field of NLP. This happened in early 2020 when I started working with models based on the Transfo
rmer architecture. The first model I worked with was BERT, and the first tasks were related to classifications. My recom
mendations are to master the Transformer architecture well because 99% of today's LLM models are based on it. Here are s
ome resources:

1. [The legendary paper 'Attention Is All You Need'](https://proceedings.neurips.cc/paper_files/paper/20
17/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
2. [Hugging Face Course on Transformers](https://huggingface.co/lear
n/nlp-course/chapter1/1)
3. [Illustrated Guide to Transformers - Step by Step Explanation](https://towardsdatascience.co
m/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)
4. [Good repository](https://github.com/Niels
Rogge/Transformers-Tutorials)
5. [How large language models work, a visual intro to transformers](https://www.youtube.co
m/watch?v=wjZofJX0v4M)

After spending years using encoder-based Transformer models, I started learning GPT models. Good
 open-source models like Llama 2 then appear. Then, I started fine-tuning these models using the excellent Unsloth libra
ry:

1. [How to Finetune Llama-3 and Export to Ollama](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-exp
ort-to-ollama)
2. [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3)


After that, I focused on studying various RAG techniques and developing Agent AI systems. This is now called AI enginee
ring, and, as far as I can see, it has become quite popular. So I'll write more about that in another post, but here I'l
l leave what I consider to be the three most famous representatives, i.e., their tutorials:

1. [LangChain tutorial](htt
ps://python.langchain.com/docs/tutorials/)
2. [LangGraph tutorial](https://langchain-ai.github.io/langgraph/tutorials/)

3. [CrewAI examples](https://github.com/crewAIInc/crewAI-examples)

# Here I am today

Thanks to the knowledge I've gene
rated over all these years in the field of ML, I've developed and worked on numerous projects. The most significant publ
icly available project is developing an agent AI system for well-being support, which I turned into a [mobile applicatio
n](https://sintelly.com/download/). Also, my entire doctoral dissertation is related to applying ML to the stock market 
in combination with the development of GPT models and reinforcement learning (more on that in a separate post). After lo
ng 6 years, I've completed my dissertation, and now I'm just waiting for its defense. I'll share everything I'm working 
on for the dissertation publicly [on the project](https://primoinvesting.com/), and in tutorials I'm preparing to write.


If you're interested in these topics, I announce that I'll soon start with activities of publishing content on Medium 
and a blog, but I'll share all of that here on Reddit as well. Now that I've gathered years of experience and knowledge 
in this field, I'd like to share it with others and help as much as possible.

If you have any questions, feel free to a
sk them, and I'll try to answer all of them.

Thank you for reading.
```
---

     
 
all -  [ Does Grokking show that Scale will be enough to get LLMs to AGI? ](https://www.reddit.com/r/ChatGPT/comments/1g3eati/does_grokking_show_that_scale_will_be_enough_to/) , 2024-11-03-0914
```
Currently, there has been a lot of debate about whether LLMs truly reason or just memorize their training data (see this
 recent paper from Apple [\[2410.05229\] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large 
Language Models (arxiv.org)](https://arxiv.org/abs/2410.05229)).

On the other hand, there have been numerous papers sho
wing that models can generalize if trained beyond the point where they overfit, known as '**grokking**' ([Towards Unders
tanding Grokking: An Effective Theory of Representation Learning (neurips.cc)](https://proceedings.neurips.cc/paper_file
s/paper/2022/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)).

Based on grokking, we could argue that i
f we just train current LLMs enough, they will always converge to generalization. Seemingly, **memorization is just a lo
cal minimum** in which it can get stuck, and the true **global minimum is generalization**.

How is this possible if mem
orization is already giving near-perfect performance on the dataset for a **specific task**? Well, by looking at overall
 performance as opposed to task-specific performance, you can imagine how generalizing helps the model increase its over
all performance:

1. Generalizations use less parameter space than memorization, which the model then can use for other 
tasks, increasing its overall performance (reduction in effective dimension by generalization [\[2205.10343\] Towards Un
derstanding Grokking: An Effective Theory of Representation Learning (arxiv.org)](https://arxiv.org/abs/2205.10343))
2. 
Generalizations from one task can increase the performance on another unrelated task, increasing its overall performance
 (a recent paper shows that GPT models get better at chess and reasoning by looking at the emergent behavior of cellular
 automata: [Intelligence at the Edge of Chaos (arxiv.org)](https://arxiv.org/html/2410.02536)).

But then what happens i
f we grok the model not on a specific task, but on **all its data**? We can imagine that it would just memorize the whol
e dataset, without being incentivized to generalize since it now has near-perfect performance on the whole dataset. In t
his case, where the **global minimum is memorization**, the model can still reach generalization by changing the loss la
ndscape using **weight-decay / regularization**. Regularization punishes big weights, forcing the model to prefer simple
r solutions, reducing the minima around memorization, while leaving the minima around generalization intact. This will g
eneralize the new global minima.

Considering this convergence towards generalization over training time, for both task-
specific and overall performance, could we assume that scaling will logically make models generalize over time? In other
 words, is scale really all we need to AGI? Or is there a flaw in my reasoning, grokking is not the end-all-be-all and w
e will need new breakthroughs to get to AGI.
```
---

     
 
all -  [ Will Scale be enough to get LLMs to Reason through Grokking? ](https://www.reddit.com/r/ArtificialInteligence/comments/1g3d6kf/will_scale_be_enough_to_get_llms_to_reason/) , 2024-11-03-0914
```
Currently there has been a lot of debate whether LLMs truly reason or just memorize their training data (see this recent
 paper from Apple [\[2410.05229\] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Languag
e Models (arxiv.org)](https://arxiv.org/abs/2410.05229)).

On the other hand, there has been numerous papers showing tha
t models can generalize, if trained beyond the point where they overfit, known as 'grokking' ([Towards Understanding Gro
kking: An Effective Theory of Representation Learning (neurips.cc)](https://proceedings.neurips.cc/paper_files/paper/202
2/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)).

Based on grokking, we could argue that if we just t
rain current LLMs enough, they will always converge to generalization. Seemingly, memorization is just a local minima in
 which it can get stuck, and the true global minima is generalization.

How is this possible if memorization is already 
giving near perfect performance on the dataset for a specific task? Well, by looking at overall performance opposed to t
ask-specific performance, you can imagine how generalizing helps the model increase its overall performance:

1. General
izations use less parameter space than memorization, which the model then can use for other tasks, increasing its overal
l performance (reduction in effective dimension by generalization [\[2205.10343\] Towards Understanding Grokking: An Eff
ective Theory of Representation Learning (arxiv.org)](https://arxiv.org/abs/2205.10343))
2. Generalizations from one tas
k can increase the performance on another unrelated task, increasing its overall performance (recent paper shows that GP
T models get better at chess and reasoning by looking at the emergent behaviour of cellular automata: [Intelligence at t
he Edge of Chaos (arxiv.org)](https://arxiv.org/html/2410.02536)).

But then what happens if we grok the model not on a 
specific task, but on all its data? We can imagine that it would just memorize the whole dataset, without being incentiv
ised to make generalization since it now has near perfect performance on the whole dataset. In this case, where the glob
al minima is memorization, the model can still reach generalization by changing the loss landscape using weight-decay / 
regularization. Regularization punishes big weights, forcing the model to prefer simpler solutions, reducing the minima 
around memorization, while leaving the minima around generalization in tact. This will make generalization the new globa
l minima.

Considering this convergence towards generalization over training time, for both task-specific as overall per
formance, could we assume that scaling will logically make models generalize over time? In other words, is scale really 
all we need to AGI? 
```
---

     
 
all -  [ [D] Will Scale be enough to get LLMs to Reason through Grokking? ](https://www.reddit.com/r/MachineLearning/comments/1g3cumr/d_will_scale_be_enough_to_get_llms_to_reason/) , 2024-11-03-0914
```
Currently there has been a lot of debate whether LLMs truly reason or just memorize their training data (see this recent
 paper from Apple [\[2410.05229\] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Languag
e Models (arxiv.org)](https://arxiv.org/abs/2410.05229)).

On the other hand, there has been numerous papers showing tha
t models can generalize, if trained beyond the point where they overfit, known as '**grokking**' ([Towards Understanding
 Grokking: An Effective Theory of Representation Learning (neurips.cc)](https://proceedings.neurips.cc/paper_files/paper
/2022/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html)).

Based on grokking, we could argue that if we ju
st train current LLMs enough, they will always converge to generalization. Seemingly, **memorization is just a local min
ima** in which it can get stuck, and the true **global minima is generalization**.

How is this possible if memorization
 is already giving near perfect performance on the dataset for a **specific task**? Well, by looking at overall performa
nce opposed to task-specific performance, you can imagine how generalizing helps the model increase its overall performa
nce:

1. Generalizations use less parameter space than memorization, which the model then can use for other tasks, incre
asing its overall performance (reduction in effective dimension by generalization [\[2205.10343\] Towards Understanding 
Grokking: An Effective Theory of Representation Learning (arxiv.org)](https://arxiv.org/abs/2205.10343))
2. Generalizati
ons from one task can increase the performance on another unrelated task, increasing its overall performance (recent pap
er shows that GPT models get better at chess and reasoning by looking at the emergent behaviour of cellular automata: [I
ntelligence at the Edge of Chaos (arxiv.org)](https://arxiv.org/html/2410.02536)).

But then what happens if we grok the
 model not on a specific task, but on **all its data**? We can imagine that it would just memorize the whole dataset, wi
thout being incentivised to make generalization since it now has near perfect performance on the whole dataset. In this 
case, where the **global minima is memorization**, the model can still reach generalization by changing the loss landsca
pe using **weight-decay / regularization**. Regularization punishes big weights, forcing the model to prefer simpler sol
utions, reducing the minima around memorization, while leaving the minima around generalization in tact. This will make 
generalization the new global minima.

Considering this convergence towards generalization over training time, for both 
task-specific as overall performance, could we assume that scaling will logically make models generalize over time? In o
ther words, is scale really all we need to AGI? Or is there a flaw in my reasoning, grokking is not the end-all-be-all a
nd we will need new breakthroughs to get to AGI?
```
---

     
 
all -  [ University Recommendations for AI/ML Specialization in Europe ](https://www.reddit.com/r/learnmachinelearning/comments/1g39xgf/university_recommendations_for_aiml/) , 2024-11-03-0914
```
Hi everyone,

I’m a 25-year-old ML Engineer from India with a Bachelor's in CS from a tier-3 college (8.56 CGPA). I have
 4 years of experience working in a French telecom organization, focusing on pure research projects. My work has led to 
2 research papers (one at a main conference and one workshop paper) in IEEE NOMS, as well as a tutorial I presented at I
CIN, Paris. Most of my research revolves around Reinforcement Learning (RL).

# Career Goals:

I aim to pursue a career 
in intensive research roles in AI/ML. My long-term goal is to get a PhD in RL and control theory. However, I want to fir
st build a solid foundation, especially in the underlying mathematics, by completing a master’s program in AI/ML.

# Why
 Europe?

I’m leaning toward Europe over the US for a few reasons:

* I plan to eventually live and work in Europe.
* Pr
oximity to family in India.
* The lower cost of master’s programs.

# Excluding the UK for Master's:

I am not consideri
ng the UK due to the shorter 1-year Master's program duration, which I feel may be insufficient, and the higher costs. I
 may go to the UK if it is for a PhD.

# What I’m Looking For:

I'm seeking universities in Europe with strong internati
onal reputations and rankings that can provide a good pathway to a PhD. So far, I’m considering:

* University of Amster
dam
* ETH Zurich
* EPFL

I’ve been basing my choices on QS rankings and the number of publications these universities ha
ve in key AI/ML conferences (e.g., NeurIPS, ICML).

I am targeting Autumn 2025 admission. Considering the low acceptance
 rates of ETH and EPFL and approaching deadlines for them. I’d love to hear any recommendations for other universities I
 should consider.

P.S. Can someone throw the light on whether I go for PhD directly or should pursue a master's first? 
I feel if I go for PhD directly, I will be left with some knowledge gap.
```
---

     
 
all -  [ Chance an underperforming Asian ](https://www.reddit.com/r/chanceme/comments/1g35v1n/chance_an_underperforming_asian/) , 2024-11-03-0914
```
**Demographics**:

* mid to low comp HS, 250 ish grad class
* Asian
* I play chess?
* Econ or CS

**GPA**: 3.7 W(Struggl
ed with Depression and certain tendencies, bounced around therapist offices freshman sophomore year. Locked in and made 
drastic improvements in gpa)

No rank

7 APs, 8 Tests

SAT - 1510 composite

**Awards**:

* AP scholar with honors
* hon
or roll
* Top 100 nationally ranked chess players In age groups for the past 4 years
* USCF candidate master
* top 1k [c
hess.com](http://chess.com) rapid global?(idk if I want to add this)

**ECs(nốt ordered yet):**

* High School Chess lea
gue president - 20+ schools, 100+ participants,  $1k+ raised
* 1st author to Novel Ai paper - published and submitted to
 conferences like Neurips + COLING
* Chess Club president - 3 peat champion in regional league, top 5 teams in State
* D
ECA - 3x state qualifier
* Motorola Solutions Intern - made a REST API for one of their apps in prod
* Paid Chess coach 
- Apart of non-profit group for underprivileged youth in chicago(not from there did remote)
* Volunteer Chess Coach - vo
lunteered apart of local chess academy, 200 ish hours over the 4 years
* Wrestling - Varsity
* SASA(south asian student 
association) treasurer - raised 10k from sponsors and events, provided scholarships for the first year to south asian st
udents
* Inspirit AI scholars program(free) - Made Chess bot with GPT 4o capable of playing at an expert level

**Colleg
es**:  
Udubs, Umass, Umich, UMD, UW Madison, BC, BU, NEU, Ohio State, Penn State, Purdue, IU, Vtech, UPitt, RIT, Bentle
y

Majors are varied between Econ or CS with focus in AI
```
---

     
 
all -  [ Low Undergrad Gpa decent exp? concerned about my chances? Should I do Gre? ](https://www.reddit.com/r/gradadmissions/comments/1fzj4wb/low_undergrad_gpa_decent_exp_concerned_about_my/) , 2024-11-03-0914
```
Hey guys  
I am an undergrad in ECE at a top 5 school with a 3.37 GPA and internship experience at a FAANG company. I am
 also waiting on a decision on a paper that might be accepted at a NeurIPS workshop. The GPA has been bothering me a bit
 as to getting auto-rejected from master's programs. So far, I have narrowed down

UIUC MCS

Georgia Tech MS CS

Univers
ity of Michigan MS CS

UC Berkeley MEng

Carnegie Mellon MEng

Brown University MEng

University of Washington MS CS

Co
rnell MEng

Columbia MEng

NYU Tandon MS CS

UCSD MS CS

USC MS CS

Would love to hear any feedback on my school list or
 advice for someone with my GPA and background!
```
---

     
 
all -  [ [Article] HiCoM: Hierarchical Coherent Motion for Dynamic Streamable Scenes with 3D Gaussian Splatti ](https://www.reddit.com/r/Scholar/comments/1fzgn8o/article_hicom_hierarchical_coherent_motion_for/) , 2024-11-03-0914
```
NeurIPS [https://nips.cc/virtual/2024/poster/96081](https://nips.cc/virtual/2024/poster/96081)
```
---

     
 
all -  [ Some Research Papers We Read recently ](https://www.reddit.com/r/deeplearning/comments/1fy6enm/some_research_papers_we_read_recently/) , 2024-11-03-0914
```
Hey everyone, here is the list of papers we discussed and their summaries this week. If you find these summaries useful,
 feel free to contribute your own! The repo is constantly updated with new papers from major conferences, so it's a grea
t way to keep up with the latest AI and deep learning.

* Image Hijacks: Adversarial Images Can Control Generative Model
s at Runtime 👉 [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Image_Hijacks.md)
* AI Control:
 Improving Safety Despite Intentional Subversion 👉 [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summa
ries/AI_Control.md)
* Evaluating Text-to-Visual Generation with Image-to-Text Generation 👉 [Summary](https://github.com/
vlgiitr/papers_we_read/blob/master/summaries/VQAscore.md)
* WARM: On the Benefits of Weight Averaged Rewarded Model 👉 [S
ummary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/WARM.md)

**The Vision Language Group at IIT Roo
rkee** has put together an excellent repository of **comprehensive summaries** for deep learning papers from top confere
nces like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down key papers in computer vision, NLP, 
and machine learning—perfect if you want to stay updated without diving deep into the full papers.

📂 **Check out the fu
ll repo and contribute he**re  
[Vision Language Group Paper Summaries](https://github.com/vlgiitr/papers_we_read)

Happ
y reading! 🎉
```
---

     
 
all -  [ Some Research Papers We Read recently ](https://www.reddit.com/r/u_vlg_iitr/comments/1fy6a5e/some_research_papers_we_read_recently/) , 2024-11-03-0914
```
Hey everyone, here is the list of papers we discussed and their summaries this week. If you find these summaries useful,
 feel free to contribute your own! The repo is constantly updated with new papers from major conferences, so it's a grea
t way to keep up with the latest AI and deep learning.

* Image Hijacks: Adversarial Images Can Control Generative Model
s at Runtime 👉 [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Image_Hijacks.md)
* AI Control:
 Improving Safety Despite Intentional Subversion 👉 [Summary](https://github.com/vlgiitr/papers_we_read/blob/master/summa
ries/AI_Control.md)
* Evaluating Text-to-Visual Generation with Image-to-Text Generation 👉 [Summary](https://github.com/
vlgiitr/papers_we_read/blob/master/summaries/VQAscore.md)
* WARM: On the Benefits of Weight Averaged Rewarded Model 👉 [S
ummary](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/WARM.md)

**The Vision Language Group at IIT Roo
rkee** has put together an excellent repository of **comprehensive summaries** for deep learning papers from top confere
nces like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down key papers in computer vision, NLP, 
and machine learning—perfect if you want to stay updated without diving deep into the full papers.

  
📂 **Check out the
 full repo and contribute her**e  
[Vision Language Group Paper Summaries](https://github.com/vlgiitr/papers_we_read)

H
appy reading! 🎉
```
---

     
 
all -  [ [0 YoE, Student, ML Research/SWE Internship, USA] ](https://www.reddit.com/r/resumes/comments/1fy0sj4/0_yoe_student_ml_researchswe_internship_usa/) , 2024-11-03-0914
```
Hi! I've never made a resume before, so this one isn't ATS optimized yet, way too long, and imo very uninteresting for a
n interviewer or recruiter to look at. I'm in my junior year, so it's the time to fix all that for internships and resea
rch for the coming summer. Specifically, I'm looking to apply to both corporate SWE/ML-related internships, as well as a
ny research opportunities that comes up as I'm not fully decided on grad school vs job post-university. Even though the 
resume is really unpolished right now, I appreciate the help a ton and please be really honest on it so i can make it as
 good as possible. Thanks!

https://preview.redd.it/4rh7hwqhx9td1.jpg?width=827&format=pjpg&auto=webp&s=c061fe2d0687f870
ba5f0d7efeeb8b3e7d55fe46

https://preview.redd.it/619j8nqhx9td1.jpg?width=827&format=pjpg&auto=webp&s=77fe2509aa62505277
4ae5dd61dc537568ae7184

P.S. couple of specific things I'm a bit questionable on. Namely, the job titles - they feel lik
e a bit inflated, but like for the Expii one, I was literally the first SWE person there as it was a startup, and I hand
led the hiring and training and lead the rest of the team myself. So when i asked my old boss, he just told me to write 
principal swe and call it a day. Same with being a 'researcher', i'm first author and heading the lab group myself with 
a couple advisors. But im concerned that people will see i'm a student and and writing these titles and take it that I'm
 overstating my roles, so idk if i should just bump them down to 'student researcher' or like 'SWE intern' or something.
 Also the skills section is gross, but idk what to do with it. Thanks again!!!  

```
---

     
 
all -  [ Are IEEE/CVF the top conferences for CV/Image Processing? ](https://www.reddit.com/r/computervision/comments/1fxfwpo/are_ieeecvf_the_top_conferences_for_cvimage/) , 2024-11-03-0914
```
As the title say, are IEEE/CVF to CV what ICLR, ICML, NeurIPS are to AI?
```
---

     
 
all -  [ approved I797-A but expired visa stamp; travel to Canada for 6 days? ](https://www.reddit.com/r/h1b/comments/1fx81is/approved_i797a_but_expired_visa_stamp_travel_to/) , 2024-11-03-0914
```
Hi, my H1-B visa stamp on my passport expired on Aug 6, 2023.

I have an approved I797-A (H1-B) valid from 05/11/2023 to
 04/19/2026.

Prior to the my latest (and current) I797-A, I also have an approved I-140 (notice date 2022). I was on H1
-B from 2017-2020 and then 2020-2023 before. Before my H1-B I was on F-1 status.

Can I travel to Canada to present at N
eurIPS 2024 conference on Dec 9, 2024 – Dec 15, 2024, and will I be allowed to enter USA with my approved I797-A without
 the visa stamp.

I have seen other posts on automatic visa revalidation and also saw the page [https://travel.state.gov
/content/travel/en/us-visas/visa-information-resources/visa-expiration-date/auto-revalidate.html](https://travel.state.g
ov/content/travel/en/us-visas/visa-information-resources/visa-expiration-date/auto-revalidate.html) but don't understand
 everything on the page clearly.

I am scared because if I can't enter back in USA, then I would have to go back to Indi
a, wait for appointment to get my visa stamped and my wife and 15-months daughter would be alone here in USA.

Travel wi
ll be via flight. Please advise.
```
---

     
 
all -  [ Consent in Crisis (NeurIPS 2024) Paper Summary via DeepDive ](https://www.reddit.com/r/learnmachinelearning/comments/1fwpt7r/consent_in_crisis_neurips_2024_paper_summary_via/) , 2024-11-03-0914
```
[https://www.youtube.com/watch?v=1I-ABssKrps](https://www.youtube.com/watch?v=1I-ABssKrps)
```
---

     
 
MachineLearning -  [ [D] Option to make NeurIPS rejected paper reviews public? ](https://www.reddit.com/r/MachineLearning/comments/1fvy0n4/d_option_to_make_neurips_rejected_paper_reviews/) , 2024-11-03-0914
```
The decision notification e-mail from NeurIPS mentioned that we would be offered the option to opt in to publicly releas
ing reviews for a rejected paper and that instructions would follow in a few days.

It's been over a week and we have no
t yet received any e-mail nor is there any author task to opt in. Since last year this e-mail came only 3 days after the
 notification I'm wondering if there was some issue and if no1 has received the e-mail yet?


```
---

     
 
MachineLearning -  [ [R] Announcing the first series of Liquid Foundation Models (LFMs) – a new generation of generative  ](https://www.reddit.com/r/MachineLearning/comments/1fvgo7o/r_announcing_the_first_series_of_liquid/) , 2024-11-03-0914
```
https://www.liquid.ai/liquid-foundation-models

https://www.liquid.ai/blog/liquid-neural-networks-research

https://x.co
m/LiquidAI_/status/1840768716784697688

https://x.com/teortaxesTex/status/1840897331773755476

'We announce the first se
ries of Liquid Foundation Models (LFMs), a new generation of generative AI models built from first principles.

Our 1B, 
3B, and 40B LFMs achieve state-of-the-art performance in terms of quality at each scale, while maintaining a smaller mem
ory footprint and more efficient inference.'

'LFM-1B performs well on public benchmarks in the 1B category, making it t
he new state-of-the-art model at this size. This is the first time a non-GPT architecture significantly outperforms tran
sformer-based models.

LFM-3B delivers incredible performance for its size. It positions itself as first place among 3B 
parameter transformers, hybrids, and RNN models, but also outperforms the previous generation of 7B and 13B models. It i
s also on par with Phi-3.5-mini on multiple benchmarks, while being 18.4% smaller. LFM-3B is the ideal choice for mobile
 and other edge text-based applications.

LFM-40B offers a new balance between model size and output quality. It leverag
es 12B activated parameters at use. Its performance is comparable to models larger than itself, while its MoE architectu
re enables higher throughput and deployment on more cost-effective hardware.

LFMs are large neural networks built with 
computational units deeply rooted in the theory of dynamical systems, signal processing, and numerical linear algebra.


LFMs are Memory efficient LFMs have a reduced memory footprint compared to transformer architectures. This is particular
ly true for long inputs, where the KV cache in transformer-based LLMs grows linearly with sequence length.

LFMs truly e
xploit their context length: In this preview release, we have optimized our models to deliver a best-in-class 32k token 
context length, pushing the boundaries of efficiency for our size. This was confirmed by the RULER benchmark.

LFMs adva
nce the Pareto frontier of large AI models via new algorithmic advances we designed at Liquid: 
 
Algorithms to enhance 
knowledge capacity, multi-step reasoning, and long-context recall in models + algorithms for efficient training and infe
rence.

We built the foundations of a new design space for computational units, enabling customization to different moda
lities and hardware requirements.

What Language LFMs are good at today:
General and expert knowledge,
Mathematics and l
ogical reasoning,
Efficient and effective long-context tasks,
A primary language of English, with secondary multilingual
 capabilities in Spanish, French, German, Chinese, Arabic, Japanese, and Korean.

What Language LFMs are not good at tod
ay:
Zero-shot code tasks,
Precise numerical calculations,
Time-sensitive information,
Counting r’s in the word “Strawber
ry”!,
Human preference optimization techniques have not yet been applied to our models, extensively.'

'We invented liqu
id neural networks, a class of brain-inspired systems that can stay adaptable and robust to changes even after training 
[R. Hasani, PhD Thesis] [Lechner et al. Nature MI, 2020] [pdf] (2016-2020). We then analytically and experimentally show
ed they are universal approximators [Hasani et al. AAAI, 2021], expressive continuous-time machine learning systems for 
sequential data [Hasani et al. AAAI, 2021] [Hasani et al. Nature MI, 2022], parameter efficient in learning new skills [
Lechner et al. Nature MI, 2020] [pdf], causal and interpretable [Vorbach et al. NeurIPS, 2021] [Chahine et al. Science R
obotics 2023] [pdf], and when linearized they can efficiently model very long-term dependencies in sequential data [Hasa
ni et al. ICLR 2023].

In addition, we developed classes of nonlinear neural differential equation sequence models [Mass
aroli et al. NeurIPS 2021] and generalized them to graphs [Poli et al. DLGMA 2020]. We scaled and optimized continuous-t
ime models using hybrid numerical methods [Poli et al. NeurIPS 2020], parallel-in-time schemes [Massaroli et al. NeurIPS
 2020], and achieved state-of-the-art in control and forecasting tasks [Massaroli et al. SIAM Journal] [Poli et al. Neur
IPS 2021][Massaroli et al. IEEE Control Systems Letters]. The team released one of the most comprehensive open-source li
braries for neural differential equations [Poli et al. 2021 TorchDyn], used today in various applications for generative
 modeling with diffusion, and prediction.

We proposed the first efficient parallel scan-based linear state space archit
ecture [Smith et al. ICLR 2023], and state-of-the-art time series state-space models based on rational functions [Parnic
hkun et al. ICML 2024]. We also introduced the first-time generative state space architectures for time series [Zhou et 
al. ICML 2023], and state space architectures for videos [Smith et al. NeurIPS 2024]

We proposed a new framework for ne
ural operators [Poli et al. NeurIPS 2022], outperforming approaches such as Fourier Neural Operators in solving differen
tial equations and prediction tasks.

Our team has co-invented deep signal processing architectures such as Hyena [Poli 
et al. ICML 2023] [Massaroli et al. NeurIPS 2023], HyenaDNA [Nguyen et al. NeurIPS 2023], and StripedHyena that efficien
tly scale to long context. Evo [Nguyen et al. 2024], based on StripedHyena, is a DNA foundation model that generalizes a
cross DNA, RNA, and proteins and is capable of generative design of new CRISPR systems.

We were the first to scale lang
uage models based on both deep signal processing and state space layers [link], and have performed the most extensive sc
aling laws analysis on beyond-transformer architectures to date [Poli et al. ICML 2024], with new model variants that ou
tperform existing open-source alternatives. 

The team is behind many of the best open-source LLM finetunes, and merges 
[Maxime Lebonne, link].

Last but not least, our team’s research has contributed to pioneering work in graph neural netw
orks and geometric deep learning-based models [Lim et al. ICLR 2024], defining new measures for interpretability in neur
al networks [Wang et al. CoRL 2023], and the state-of-the-art dataset distillation algorithms [Loo et al. ICML 2023].'
```
---

     
