 
all -  [ [R] Up to 17% of Recent AI Conference Peer Reviews Written by ChatGPT ](https://www.reddit.com/r/machinelearningnews/comments/1bnsuxg/r_up_to_17_of_recent_ai_conference_peer_reviews/) , 2024-03-27-0909
```
A new study has uncovered that a significant fraction of peer reviews for top AI conferences in 2023-2024 likely include
d substantial AI-generated content from models like ChatGPT.

Using a novel statistical technique, researchers estimated
 the percentage of text generated by AI in large collections of documents. Analyzing peer reviews, they found:

* 10.6% 
of ICLR 2024 reviews had significant AI content
* 9.1% for NeurIPS 2023
* 6.5% for CoRL 2023
* 16.9% for EMNLP 2023

In 
contrast, only 1-2% of pre-ChatGPT reviews from 2022 and earlier were flagged as having substantial AI contribution.

So
me key findings:

1. AI-heavy reviews tended to come in close to the deadline
2. Fewer scholarly citations in AI-flavore
d reviews
3. Reviewers with AI-tinged reviews engaged less in author discussion
4. AI content made reviews more semantic
ally homogeneous
5. Lower reviewer confidence correlated with higher AI estimates

The study, I think, raises some quest
ions for proactive policy development in academia around responsible AI use in research. AI may be eroding the quality a
nd integrity of peer review through these 'shadow' influences. Open questions include:

* Should AI assistance in peer r
eview be disclosed?
* How should we incentivize good practices despite AI temptations?
* Can we preserve intellectual di
versity under AI homogenization?
* Should we rethink credit for hybrid human/AI knowledge work?

Overall, an interesting
 empirical glimpse into AI's rapidly growing tendrils in the foundations of scientific quality control! I thought the ap
proach of measuring the frequency of certain AI wording 'ticks' made a lot of sense (some of the adjectives GPT4 uses, f
or example, are clear tells).

I'm curious to read the comments on this one! I have a [much more detailed summary availa
ble here](https://aimodels.substack.com/p/new-study-finds-up-to-17-of-ai-conference) as well if you're interested, and t
he original paper is [here](https://arxiv.org/pdf/2403.07183.pdf).
```
---

     
 
all -  [ [R] Up to 17% of Recent AI Conference Peer Reviews Written by ChatGPT ](https://www.reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/) , 2024-03-27-0909
```
A new study has uncovered that a significant fraction of peer reviews for top AI conferences in 2023-2024 likely include
d substantial AI-generated content from models like ChatGPT.

Using a novel statistical technique, researchers estimated
 the percentage of text generated by AI in large collections of documents. Analyzing peer reviews, they found:

* 10.6% 
of ICLR 2024 reviews had significant AI content
* 9.1% for NeurIPS 2023
* 6.5% for CoRL 2023
* 16.9% for EMNLP 2023

In 
contrast, only 1-2% of pre-ChatGPT reviews from 2022 and earlier were flagged as having substantial AI contribution.

So
me key findings:

1. AI-heavy reviews tended to come in close to the deadline
2. Fewer scholarly citations in AI-flavore
d reviews
3. Reviewers with AI-tinged reviews engaged less in author discussion
4. AI content made reviews more semantic
ally homogeneous
5. Lower reviewer confidence correlated with higher AI estimates

The study, I think, raises some quest
ions for proactive policy development in academia around responsible AI use in research. AI may be eroding the quality a
nd integrity of peer review through these 'shadow' influences. Open questions include:

* Should AI assistance in peer r
eview be disclosed?
* How should we incentivize good practices despite AI temptations?
* Can we preserve intellectual di
versity under AI homogenization?
* Should we rethink credit for hybrid human/AI knowledge work?

Overall, an interesting
 empirical glimpse into AI's rapidly growing tendrils in the foundations of scientific quality control! I thought the ap
proach of measuring the frequency of certain AI wording 'ticks' made a lot of sense (some of the adjectives GPT4 uses, f
or example, are clear tells). 

I'm curious to read the comments on this one! I have a [much more detailed summary avail
able here](https://aimodels.substack.com/p/new-study-finds-up-to-17-of-ai-conference) as well if you're interested, and 
the original paper is [here](https://arxiv.org/pdf/2403.07183.pdf).
```
---

     
 
all -  [ I am not aware that we have so many outstanding PhDs on the job market. ](https://i.redd.it/egc3me1vkjpc1.png) , 2024-03-27-0909
```

```
---

     
 
all -  [ [D] simple typos or errors in my understanding?! ](https://www.reddit.com/r/MachineLearning/comments/1bicphu/d_simple_typos_or_errors_in_my_understanding/) , 2024-03-27-0909
```
Though I mailed to all authors, I couldn't receive any reply...

&#x200B;

I really enjoyed reading paper, “Compositiona
l Visual Generation with Energy Based Models” (NeurIPS\`20; [https://arxiv.org/abs/2004.06030](https://arxiv.org/abs/200
4.06030)).

While I am following the idea in the paper, I am facing a problem in understanding the equation (8) in the p
aper.

According to the paper's notation, the EBM is defined as (1):

[Eq. \(1\)](https://preview.redd.it/lnerbdlm58pc1.
png?width=202&format=png&auto=webp&s=e7ce768304cafa8e0fb88202466ad7cf40d9d3e8)

i.e.,, the *energy function*, E\_\\theta
(x), is defined as the negative term on top of the exponent.

&#x200B;

In eq. (3) below, authors defines SGLD step for 
acquiring samples from any energy based model (EBM).

Next, according to 'concept disjunction' proposed by authors, the 
union-ized probability densities are defined in (7), in the form of the exponential of log-sum-exp of different EBMs' *e
nergy function*s.

https://preview.redd.it/sas3f4yg58pc1.png?width=1005&format=png&auto=webp&s=2bb0aa67cf2767ede091102ce
22965e8c8bdbe7c

From (7), I understood that the unionized energy function is accordingly -logsumexp(-E(x|c1), ...),  
t
hus corresponding SGLD should have plus sign.  
However, in the paper, authors stated the SGLD with ***minus sign***, an
d I've lost...

After looking through the official code implementation,   
([https://github.com/yilundu/ebm\_composition
ality/blob/c7ac54366d2d5a15f71871448bd720bf5b3eb82d/celeba\_combine.py#L150C9-L150C32](https://github.com/yilundu/ebm_co
mpositionality/blob/c7ac54366d2d5a15f71871448bd720bf5b3eb82d/celeba_combine.py#L150C9-L150C32))  
I found that the combi
ned energy function derivation of mine ( E(x|\\cup\_i c\_i) ) seems to be correct,  
according to the assignment of the 
\`e\_pos\` variable in the code.

&#x200B;

Still, I'd like to acquire some clarification on this flow.  
Can anybody sk
im through this post and check my thought?

Thank you in advance!
```
---

     
 
all -  [ Stanford MS/CS 2024 - REJECTED ](https://www.reddit.com/r/gradadmissions/comments/1bhwguc/stanford_mscs_2024_rejected/) , 2024-03-27-0909
```
I unfortunately got denied from Online Stanford MS in CS Today. I was fairly confident in my application. I had 4.00/4.0
0 GPA (ranked 1st in my graduation class from decent US college), ML Engineer at FAANG, published a paper in NeurIPS. My
 references include Dean of Engineering, My college Prof (Oxford grad) and my manager from work - MIT (PhD). I had alrea
dy got rejected 2 years ago, since then I made a publication switched to an ML oriented role and tailored my referrals a
ccordingly. It makes sense that a lot of people flooded in to AI track this year but I really want to get a sense of oth
er profiles who got admitted this year.

Maybe next year I will try again :/
```
---

     
 
all -  [ [D] papers that only evaluate on cifar10 ](https://www.reddit.com/r/MachineLearning/comments/1bhp54a/d_papers_that_only_evaluate_on_cifar10/) , 2024-03-27-0909
```
Hi everyone!

While reviewing for NeurIPS 2024, one of the things I keep noticing is that a lot of papers only evaluate 
on datasets of very small size, like Cifar-10. his feels weird to me: I consider Cifar10 to be a toy-dataset and testbed
 for my methods, not something I'd use to show that my method actually works/is relevant in practice. So my first intuit
ion is always 'this approach probably does not scale to larger datasets'. I mean, ImageNet is 12 years old now, and I've
 personally been giving results on imagenet for my papers since ~8 years. most computer vision applications I know requi
re larger resolutions than just 32x32. it's also my impression that almost all of the 'good' papers I read have results 
on larger scale data. But given how often I encounter this situation, I have to wonder: am  I just working in a very pri
vileged environment, or are the manuscript-authors just lazy? How much faith do you have in papers that only evaluate on
 MNIST and CIFAR10?
```
---

     
 
all -  [ 2 yoe in research labs only in computer vision. What should be my next steps? ](https://www.reddit.com/r/cscareerquestionsEU/comments/1bdslud/2_yoe_in_research_labs_only_in_computer_vision/) , 2024-03-27-0909
```
Hi,
Based in France. I have a masters where I did 2 internships in a good research lab and then started working as resea
rch engineer in a public lab for ~2 years. They only offer temporary contracts and there is a limit to how long you can 
work with these temporary contracts in France (you can try for the few research scientists positions but jts if you have
 phd/post doc plus good publiching track and experience, so i need to move on).
I didn't go for a phd and don't want to 
now since I felt I didn't have the right motivation and didn't want to do it for the sake of it. Didn't see myself as a 
researcher either publishing all my life. Research Engineer is a sweet spot.

From internships/job, I've so far co-autho
red 4 papers in computer vision conferences (neurips, iccv, cvpr, miccai) in the topics of 3d reconstruction and human m
odeling. But that's pretty much the achievement which is probably meaningless for jobs in the industry. And thing is, I 
feel my skill stack being also limited to research workflow might be little interesting to the industry (practical exper
ience with only python vs c++ for example). 

And i feel i should switch to industry (instead of another contract in a r
esearch lab) since honestly, even though these papers might seem nothing, they take a lot of effort and work and the who
le publishing cycle drains you mentally. And the compensation you get is really low I feel in these public labs (32k bru
t).

When I look around for jobs, I see mostly poorly defined descriptions but titled data scientist or mainly MLops. No
thing specifically related to what ive done being needed. I have 7 months left on my contract. What should I start learn
ing/doing or what could be an ideal focus for job search with such profile? Id like to stay in France and even in the sa
me city near lyon since im a foreigner and i feel more settled due to having made friends and understanding the language
. Your insights would be helpful. I feel very lost.
```
---

     
 
all -  [ ML Internships aren't supposed to be this difficult to get - Rant ](https://www.reddit.com/r/cscareerquestions/comments/1bd940o/ml_internships_arent_supposed_to_be_this/) , 2024-03-27-0909
```
As an international master's student, I'm on the verge of quitting my search as my internship hunt for the past 8 months
 has practically given 0 offers. I've applied to over 600 postings (every single ML/DS posting that has come). I did get
 3-4 callbacks but they never converted to offers despite the interviews going well. All I can say is that the competiti
on is unbelievably high, and the companies seem to be taking just 1 intern per team or none. Every position I am interes
ted in requires a Ph.D. candidate or papers in conferences like ICML, NeurIPS, or CVPR on the exact niche area the team 
works on. When there's a lack of jobs and an abundant supply of candidates, they can get choosy. But these are internshi
ps we are talking about, things meant to be entry-level positions.

Is it actually supposed to be this difficult? The wo
rst part is that all the callbacks I got so far are through direct contact with the hiring manager. None of the regular 
applications(even with referrals) gave me anything. As a final plea, if any of my fellow Redditors here have leads I'd r
eally appreciate some help.
```
---

     
 
all -  [ People with no top-tier ML papers, where are you working at? ](https://www.reddit.com/r/reinforcementlearning/comments/1b1suv1/people_with_no_toptier_ml_papers_where_are_you/) , 2024-03-27-0909
```
I am graduating soon, and my Ph.D. research is about RL algorithms and their applications.  
However, I failed to publis
h papers in top-tier ML conferences (NeurIPS, ICLR, ICML).   
But with several papers in my domain, how can I get hired 
for an RL-related job?  
I have interviewed a handful of mobile and e-commerce (RecSys) companies, all failed.  


I don
't want to do a postdoc and I am not interested in anything related to academia.   


Please let me know if there are an
y opportunities in startups, or other positions I have not explored yet.
```
---

     
 
all -  [ Postdoc requirements ](https://www.reddit.com/r/PhD/comments/1b1i5vr/postdoc_requirements/) , 2024-03-27-0909
```
Hi colleagues,

In some of the postdoc or industrial research scientist positions i see requirements like this:  


* St
rong publications record at top tier venues (CVPR, ICCV, ECCV, Siggraph, etc.)
* We are looking for candidates with a st
rong track record at top-tier computer vision and machine learning venues, such as CVPR, ICCV, ECCV, and NeurIPS.
* Firs
t-authored publications at peer-reviewed conferences (e.g. CVPR. ECCV, ICCV, NeurIPS, and SIGGRAPH).

In case one joins 
robotics related PhD on vision and perception and would focus on publications on IROS, ICRA(these 2 are top robotics con
ference, which have vision section), will these publications be considered for above mentioned positions? Or only the me
ntioned top-tier computer vision only conferences are accepted?  
I do understand that robotics conferences are a little
 less competitive, but still they are top-tier and peer-reviewed. 
```
---

     
