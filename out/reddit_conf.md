 
all -  [ Advice Needed: To pursue PhD with a focus in Computer Vision/Deep Learning. Has limited Research Bac ](https://www.reddit.com/r/PhD/comments/1hrf5os/advice_needed_to_pursue_phd_with_a_focus_in/) , 2025-01-03-0913
```
Hello everyone!

Iâ€™m a master's graduate currently on F1 STEM OPT in the U.S. and Iâ€™d love your input regarding my plans
 to pursue a PhD in Computer Vision/Deep Learning. Hereâ€™s a snapshot of my journey and where Iâ€™m at:

**Background & Pro
jects:**

* Bachelors in Mechatronics Engineering (built a hexapod robot, worked on object detection for fish tracking, 
dabbled in PLCs, CAD, and embedded systems).
* 2 years of industry experience at TCS as a Data engineer (not directly re
levant to AI research, but it gave me decent programming and data skills).
* Masterâ€™s in Robotics & Autonomous Systems f
rom ASU: did course projects in pose estimation, object detection, one-shot imitation learning, image classification, pl
us a Raspberry Pi-based IoT security project.
* After graduation, I volunteered on stereo vision and depth estimation pr
oject, and also on drone path planning/gesture recognition.

**Current Situation:**

* I have no formal research publica
tions or major conference presentations.
* My student loans are significant, so Iâ€™m cautious about finances.
* Iâ€™m deepl
y interested in modern CV trends (Nerfs and 3D Reconstruction, ViTs, VLMs, Generative AI for images and videos, Percepti
on in Autonomous vehicles etc.,).
* Iâ€™m on STEM OPT until mid-2026, so Iâ€™m working full-time to stay afloat while trying
 to map out a PhD plan.

**Main Concerns:**

* **Lack of Publications:** Many people I see in top PhD programs have publ
ished at workshops or conferences like CVPR/ICCV/NeurIPS. Has anyone started a PhD with no prior publications and succes
sfully ramped up?
* **Timeline:** Also, Iâ€™m debating whether to target Spring 2026 or wait until Fall 2026 to give mysel
f more time for research experience and financial prep. Is postponing to Fall 2026 a good move? Iâ€™d love any tips on vol
unteering for remote research while working full-time, especially on how to reach out to professors or labs if youâ€™ve do
ne something similar!
* **Research Portfolio Building:** Iâ€™m willing to work on open-source projects, replicate state-of
-the-art papers, or collaborate remotely with labs. Iâ€™d appreciate any advice on how to make a strong case for admission
s without an official publication record.

**Questions for You:**

* Has anyone been in a similar boatâ€”entering a PhD wi
thout prior publicationsâ€”and successfully built a solid research portfolio along the way?
* Any tips on balancing full-t
ime employment with research activities, especially to get a letter of recommendation that speaks to my research potenti
al?
* How did you manage finances, especially if you had existing student debt or limited savings, during the first year
 or two of your PhD?
* Any personal experiences navigating U.S. visa transitions from OPT back to F-1 for a PhD program?


Iâ€™d really value input from anyone who faced similar hurdles or from current PhD students who can share what ultimatel
y worked for them. Thanks in advance for reading, and I look forward to any advice or shared experiences!
```
---

     
 
all -  [ [Profile Review] MSCS / DS Fall'25 ](https://www.reddit.com/r/MSCS/comments/1hp3hbr/profile_review_mscs_ds_fall25/) , 2025-01-03-0913
```
Hi, this is my profile with colleges I've applied to, I need strong feedback because I'm kinda concerned now xD.

* GRE:
 323 (168Q + 155V)
* TOEFL: 114 (R:29, L:30, S:29, W:26)
* Undergraduate: B.Tech. CS at BITS Goa (2025), idk if it's Tie
r 1 / 2
* GPA: 9.22 (I have some Bs on my CS courses)
* Papers: 1 published at a NeurIPS workshop (3rd author), rest are
 under review, I have 4 preprints (2 at B confs and one at CVPR (A\*) and one at an AAAI (A\*) workshop, I am first the 
author on all of these).
* Research Exp 1: I was a DAAD WISE scholar in my 3rd year summer (quite selective scholarship)
 did my summer research in Germany, very productive work, this was submitted to CVPR, I got a very strong LoR from the P
I (h-index \~20).
* Research Exp 2: I worked with the AI lab at BITS for over a year-and-a-half now, got 3 preprints fro
m there (as I detailed), I topped courses (top-5 / 1st rank) these profs took + TA'd them (quite involved in my TA work)
. Got 2 equally strong LoRs from here (one at h-index \~40 and one at h-index \~15).
* Research Exp 3: I interned at a s
tartup for about 5 months working on AI for materials science, decent work, I have some open-source contribs here, and b
efore interning I worked informally which led to that NeurIPS workshop acceptance. Moderate LoR from here (the CEO has h
-index \~25 and he gave an LoR).
* Extra: I am the president of the AI club here (was the Gen Sec before), have organize
d symposiums, made assignments, gave talks, etc. I also started a blog-posts initiative, our first blog-post was accepte
d at an ICML (A\*) workshop.

Finally, Colleges: UPenn MS CIS, UCSD MS DS + CSE, CMU MS ML, UCLA MS CS, UT Austin MS CS,
 UIUC MS CS, UW Madison MS CS, Princeton MSE CS, Purdue MS CS, Yale MS CS (2 year), UMich MS CSE, Stanford MS DS + Stats
.

I have a Full-time job offer in hand, which is why I was going all ambitious, but now I'm kinda conflicted if I reall
y want the job. Let me know my realistic chances (I'll go anywhere I am accepted from these), you can be upfront xD. And
 feel free to suggest any more if their deadline hasn't already passed.

Thanks!
```
---

     
 
all -  [ There is no model that is phd level until it can write a paper that is accepted to Neurips on its ow ](https://www.reddit.com/r/ChatGPT/comments/1hopxko/there_is_no_model_that_is_phd_level_until_it_can/) , 2025-01-03-0913
```
Pretty much the title. But ultimately phd level means you can write a paper that gets accepted to a top conference. Unti
l it can come up with a new and novel idea to get submitted to a conference, I donâ€™t think any model can claim phd level

```
---

     
 
all -  [ DeepSeek-v3 | Best open-source model on ProLLM ](https://www.reddit.com/r/LocalLLaMA/comments/1ho5ave/deepseekv3_best_opensource_model_on_prollm/) , 2025-01-03-0913
```
Hey everyone!

Just wanted to share some quick news -- the hype is real! DeepSeek-v3 is now the best open source model o
n our benchmark: [check it here](https://prollm.ai/leaderboard/stack-unseen). It's also the cheapest model in the top-10
 and shows a 20% improvement across our benchmarks compared to the previous best DeepSeek model.

If you're curious abou
t how we do our benchmarking, we published a [paper at NeurIPS](https://arxiv.org/abs/2412.05288) about our methodology.
 We share how we curated our datasets and conducted a thorough ablation on using LLMs for natural-language code evaluati
on. Some key takeaways:

* Without a reference answer, CoT leads to overthinking in LLM judges.
* LLM-as-a-Judge does no
t exhibit a self-preference bias in the coding domain.

We've also made some small updates to our leaderboard since our 
last post:

* Added new benchmarks (OpenBook-Q&A and Transcription)
* Added 15-20 new models across multiple of our benc
hmarks

Let me know if you have any questions or thoughts!

Leaderboard: [https://prollm.ai/leaderboard/stack-unseen](ht
tps://prollm.ai/leaderboard/stack-unseen)  
NeurIPS paper:Â [https://arxiv.org/abs/2412.05288](https://arxiv.org/abs/2412
.05288)

https://preview.redd.it/ibvp9yjk8l9e1.png?width=1060&format=png&auto=webp&s=1f638d3970baf912ae03b5f0073595ff033
be4ab
```
---

     
 
all -  [ Wyzwanie: sztuczna inteligencja! - z BarbarÄ… RychalskÄ… rozmawia DobrosÅ‚awa GogÅ‚oza ](https://www.reddit.com/r/libek/comments/1ho3uoy/wyzwanie_sztuczna_inteligencja_z_barbarÄ…/) , 2025-01-03-0913
```
[Wyzwanie: sztuczna inteligencja! - z BarbarÄ… RychalskÄ… rozmawia DobrosÅ‚awa GogÅ‚oza - LibertÃ©!](https://liberte.pl/wyzwa
nie-sztuczna-inteligencja-z-barbara-rychalska-rozmawia-dobroslawa-gogloza/)

**DobrosÅ‚awa GogÅ‚oza: ChciaÅ‚abym zaczÄ…Ä‡ od 
zadania sobie pytania o to, jak znaleÅºliÅ›my siÄ™ w tym momencie w historii â€“ jakie byÅ‚y najwiÄ™ksze przeÅ‚omy w dziaÅ‚aniu s
ztucznej inteligencji w ostatnich latach?**

Barbara Rychalska: JeÅ›li chodzi o jÄ™zykowe modele GenAI, czyli popularne LL
M-y (*large language models*, pol. wielkie modele jÄ™zykowe), przeÅ‚omem byÅ‚o opracowanie nowego modelu nazwanego transfor
merem. DokonaÅ‚ tego zespÃ³Å‚ Googleâ€™a â€“ co ciekawe, z udziaÅ‚em polskiego naukowca, Åukasza Kaisera. Przed opracowaniem tra
nsformera istniaÅ‚y juÅ¼ rÃ³Å¼ne, zaawansowane modele jÄ™zyka naturalnego, oparte na sieciach neuronowych. CzytaÅ‚y one jednak
 zdania token po tokenie (w przybliÅ¼eniu â€“ sÅ‚owo po sÅ‚owie), co powodowaÅ‚o stopniowe zanikanie informacji. W transformer
ze natomiast zastosowano nowatorski mechanizm atencji. UmoÅ¼liwia on wczytywanie wielu tokenÃ³w naraz, w ramach jednej â€po
rcjiâ€. PoniewaÅ¼ transformer analizuje relacjÄ™ kaÅ¼dego tokenu wobec kaÅ¼dego innego tokenu w przetwarzanym kontekÅ›cie, two
rzy bardzo szczegÃ³Å‚owy â€rozkÅ‚ad uwagiâ€, ktÃ³ry wskazuje, ktÃ³re tokeny sÄ… bardziej istotne dla danego kontekstu. W szczegÃ³
lnoÅ›ci pozwala to wykryÄ‡ zwiÄ…zki znaczeniowe pomiÄ™dzy sÅ‚owami znajdujÄ…cymi siÄ™ daleko od siebie. Ma to rÃ³wnieÅ¼ skutki wy
dajnoÅ›ciowe â€“ operacjÄ™ jednoczesnej analizy par tokenÃ³w Å‚atwiej jest zrÃ³wnolegliÄ‡ niÅ¼ sekwencyjne czytanie tekstu. Oprac
owanie transformera wywoÅ‚aÅ‚o lawinÄ™ â€“ powstaÅ‚y oparte na tej architekturze modele takie jak BERT, RoBERTa, XLNet oraz wi
ele innych, ktÃ³re osiÄ…gnÄ™Å‚y znacznie lepsze wyniki niÅ¼ klasyczne modele sekwencyjne, praktycznie w kaÅ¼dym zadaniu dotycz
Ä…cym jÄ™zyka naturalnego, takim jak wykrywanie emocji w tekÅ›cie, podsumowywanie go, odpowiadanie na pytania czy tÅ‚umaczen
ia. RÃ³Å¼nica w jakoÅ›ci ich dziaÅ‚ania w porÃ³wnaniu do starszych metod byÅ‚a piorunujÄ…ca â€“ byÅ‚o widaÄ‡, Å¼e to juÅ¼ nie inkreme
ntalne ulepszenie, a zupeÅ‚nie nowa jakoÅ›Ä‡.

NastÄ™pnie okazaÅ‚o siÄ™, Å¼e w przeciwieÅ„stwie do wielu innych typÃ³w modeli, zw
iÄ™kszanie iloÅ›ci danych treningowych oraz rozmiaru modelu transformerowego nie powoduje szybkiego wypÅ‚aszczenia przyrost
Ã³w jakoÅ›ci. Wprost przeciwnie, umiejÄ™tnoÅ›ci i jakoÅ›Ä‡ modelu szybko rosÅ‚y wraz z rozmiarem danych i liczbÄ… parametrÃ³w, je
Å›li pomiÄ™dzy tymi wartoÅ›ciami zachowana byÅ‚a odpowiednia rÃ³wnowaga (tzw. â€LLM scaling lawsâ€). Na tym zjawisku opiera siÄ™
 sukces firmy OpenAI i modeli z rodziny GPT, a nastÄ™pnie kolejnych, o ktÃ³rych sÅ‚yszymy na co dzieÅ„ â€“ modeli Mistral, Cla
ude, Llama i innych.

**Czy moÅ¼esz wyjaÅ›niÄ‡, jak dziaÅ‚ajÄ… duÅ¼e modele jÄ™zykowe (LLM) i jakie sÄ… ich obecne ograniczenia?
**

Nowoczesne modele LLM wciÄ…Å¼ opierajÄ… siÄ™ na architekturze transformera. ZostaÅ‚y w nich dodatkowo zastosowane liczne 
nowe techniki, takie jakÂ *instruction tuning*Â czy RLHF (*reinforcement learning from human feedback*) â€“ uczenie modelu r
ozpoznawania intencji pytaÅ„ i prÃ³Å›b, jakie kieruje do niego uÅ¼ytkownik oraz wykrywania, ktÃ³re odpowiedzi bÄ™dÄ… bardziej p
oÅ¼Ä…dane. Pierwsze modele transformerowe uczone byÅ‚y przewidywania nastÄ™pnego tokenu (moÅ¼na sobie to wyobraziÄ‡ w ten spos
Ã³b, Å¼e pokazujemy modelowi niedokoÅ„czony tekst i prosimy go o uzupeÅ‚nienie, tak aby caÅ‚oÅ›Ä‡ tekstu byÅ‚a logiczna). Modele
 byÅ‚y wiÄ™c uczone okreÅ›lania, jakie sÅ‚owa sÄ… najbardziej prawdopodobne w danym kontekÅ›cie. Jak siÄ™ okazaÅ‚o w praktyce, z
adanie to Å›ciÅ›le wiÄ…Å¼e siÄ™ z rozumieniem logiki jÄ™zyka, znaczenia sÅ‚Ã³w oraz generowaniem pÅ‚ynnego i poprawnego tekstu. D
ziÄ™ki metodom takim jakÂ *instruction tuning*, modele sÄ… w stanie pÃ³jÅ›Ä‡ jeszcze krok dalej â€“ odczytywaÄ‡ intencjÄ™ pytaÅ„ uÅ¼
ytkownika i speÅ‚niaÄ‡ jego cele.Â 

Nieustannie powstajÄ…ce nowe techniki trenowania pozwalajÄ… na wzmocnienie rÃ³Å¼nych umiej
Ä™tnoÅ›ci LLM-Ã³w. Na przykÅ‚ad wypuszczony niedawno model OpenAI â€o1â€ wykazuje daleko wyÅ¼sze niÅ¼ wczeÅ›niej zdolnoÅ›ci do log
icznego rozumowania â€“ dziÄ™ki zastosowaniu metodyÂ *chain-of-thought*. Polega ona na rozbijaniu skomplikowanych zadaÅ„ na s
ekwencjÄ™ niezbÄ™dnych krokÃ³w rozumowania, ktÃ³re prowadzÄ… do rozwiÄ…zania. DziÄ™ki nauce dzielenia problemu na podproblemy, 
model jest w stanie zastosowaÄ‡ zdobyte umiejÄ™tnoÅ›ci do wielu nowych zadaÅ„. OczywiÅ›cie daleko tu do medialnego stwierdzen
ia, Å¼e â€modele rozumujÄ… na poziomie dorosÅ‚ego czÅ‚owieka/studenta/doktoranta/itp.â€. Takie wyraÅ¼enie jest problematyczne n
a wielu poziomach, nawet tym najbardziej podstawowym, poniewaÅ¼ nie wiemy dokÅ‚adnie, w jaki sposÃ³b rozumuje punkt odniesi
enia, czyli czÅ‚owiek.

Nie wszystko jednak zaleÅ¼y od konkretnych celÃ³w trenowania. LLM-y majÄ… ciekawÄ… wÅ‚asnoÅ›Ä‡ znanÄ… jak
o â€emergenceâ€, czyli spontaniczne nabywanie nieoczekiwanych umiejÄ™tnoÅ›ci, do ktÃ³rych nie byÅ‚y bezpoÅ›rednio trenowane.Â *E
mergence*Â wystÄ™puje przy zastosowaniu odpowiednio duÅ¼ych danych i przy odpowiednio duÅ¼ym rozmiarze modelu. PrzykÅ‚adem ta
kiej emergentnej umiejÄ™tnoÅ›ci jest dokonywanie tÅ‚umaczeÅ„ czy wykonywanie prostych obliczeÅ„. Przypuszcza siÄ™, Å¼e LLM-y ta
kie jak GPT nie byÅ‚y trenowane specjalnie do tych zadaÅ„, jednak ich ekspozycja na ogromne, wielojÄ™zyczne dane spowodowaÅ‚
a, Å¼e sÄ… w stanie stworzyÄ‡ ekwiwalentny znaczeniowo tekst w innym jÄ™zyku oraz rozumieÄ‡ podstawowe znaczenie liczb. Nie w
iemy, jakie emergentne umiejÄ™tnoÅ›ci modeli pojawiÄ… siÄ™ w przyszÅ‚oÅ›ci.

Å¹rÃ³dÅ‚em fascynacji LLM-ami sÄ… ich ogÃ³lnie bardzo 
wysokie zdolnoÅ›ci jÄ™zykowe (pÅ‚ynnoÅ›Ä‡, poprawnoÅ›Ä‡ gramatyczna wypowiedzi), duÅ¼a zasobnoÅ›Ä‡ pamiÄ™ci (sÄ… w stanie odpowiedzi
eÄ‡ na wiele pytaÅ„ z najrÃ³Å¼niejszych dziedzin, takich jak historia, prawo, chemia, botanika, tak naprawdÄ™ dowolne tematy)
, oraz dobre zrozumienie intencji pytaÅ„.

Natomiast braki i zagroÅ¼enia zwiÄ…zane z LLM-ami to przede wszystkim halucynacj
e (wypowiedzi niepoprawne, zawierajÄ…ce bÅ‚Ä™dy merytoryczne, jednak artykuÅ‚owane z duÅ¼Ä… pewnoÅ›ciÄ… siebie). IstniejÄ… na to 
pewne Å›rodki zaradcze, jednak nie sÄ… w 100% skuteczne. Na przykÅ‚ad, LLM-y czÄ™sto mogÄ… poprawiÄ‡ siÄ™ i zadenuncjowaÄ‡ swojÄ…
 wÅ‚asnÄ… halucynacjÄ™, jeÅ›li dopytamy je, czy sÄ… pewne swojej odpowiedzi lub dopytamy je o fragment, ktÃ³ry wydaje nam siÄ™ 
zaskakujÄ…cy lub dziwny. Warto zatem podwaÅ¼aÄ‡ odpowiedzi LLM-Ã³w i zadawaÄ‡ pytania typu â€Czy jesteÅ› pewien, Å¼eâ€¦?â€ Niestety
, im bardziej specjalistyczne pytanie, tym wiÄ™ksza podatnoÅ›Ä‡ na bÅ‚Ä™dy. Z moich obserwacji: zadajÄ…c LLM-om pytania z pozy
cji laika, np. z dziedziny farmacji, moÅ¼emy byÄ‡ pozytywnie zaskoczeni (dlatego, Å¼e nasze pytania bÄ™dÄ… dosyÄ‡ proste meryt
orycznie). LLM ogÃ³lnego przeznaczenia, taki jak ChatGPT nie poradzi sobie jednak ze specjalistycznymi pytaniami, ktÃ³re z
adaÅ‚by farmaceuta w zwiÄ…zku ze swojÄ… pracÄ… codziennÄ…. Do takich zastosowaÅ„ warto wykorzystywaÄ‡ usÅ‚ugi wyspecjalizowane w
 odpowiadaniu na pytania, a najlepiej jeszcze podajÄ…ce ÅºrÃ³dÅ‚a odpowiedzi (np. darmowe Perplexity) lub modele dziedzinowe
.

NiektÃ³rzy pokÅ‚adajÄ… teÅ¼ zbyt duÅ¼o zaufania w kreatywnoÅ›Ä‡ modeli, przypisujÄ…c im na przykÅ‚ad zdolnoÅ›Ä‡ tworzenia strate
gii rozwoju firm, umiejÄ™tnoÅ›Ä‡ tworzenia zaskakujÄ…cych kampanii marketingowych itp. LLM-y mogÄ… na pewno podpowiedzieÄ‡ wie
le interesujÄ…cych pomysÅ‚Ã³w w tych tematach, jednak bÄ™dÄ… to bardziej propozycje â€zdroworozsÄ…dkoweâ€ i powtarzalne niÅ¼ rewo
lucyjne. LLM-y nie sÄ… wystarczajÄ…co twÃ³rcze, poruszajÄ… siÄ™ raczej w obrÄ™bie pewnych uÅ›rednionych konceptÃ³w, ktÃ³re zaobse
rwowaÅ‚y w swoich danych treningowych. Nie mogÄ… wiÄ™c pÃ³ki co przewyÅ¼szyÄ‡ czÅ‚owieka. Inaczej mÃ³wiÄ…c â€“ biznesowa porada od 
LLM bÄ™dzie brzmiaÅ‚a bardziej jak raport McKinseyâ€™a niÅ¼ strategia, ktÃ³rÄ… sporzÄ…dziÅ‚by osobiÅ›cie geniusz biznesu, planujÄ…c
 swÃ³j nastÄ™pny ruch.

**Jakie potencjalne korzyÅ›ci dla spoÅ‚eczeÅ„stwa widzisz w dalszym rozwoju AI? A jakie zagroÅ¼enia?**


KorzyÅ›ci widzÄ™ na co dzieÅ„ w swoim wÅ‚asnym Å¼yciu, zaczynajÄ… siÄ™ teÅ¼ pojawiaÄ‡ badania dokumentujÄ…ce przydatnoÅ›Ä‡ GenAI. 
Nie jest siÄ™ jednak Å‚atwo przebiÄ‡ przez szum informacyjny. Na poczÄ…tku fali zachwytu pierwszymi modelami GPT utrzymywaÅ‚a
m daleko idÄ…cÄ… ostroÅ¼noÅ›Ä‡. ZwÅ‚aszcza w obliczu pojawiajÄ…cych siÄ™ jak grzyby po deszczu influencerÃ³w, obiecujÄ…cych cuda. 
NastÄ™pnie, w odpowiedzi na wszechobecny entuzjazm, pojawiÅ‚y siÄ™ grupy â€hejterÃ³wâ€, twierdzÄ…cych, Å¼e GenAI jest trendem, k
tÃ³ry szybko przeminie.

Tymczasem, w badaniu Harvard Business School pt. â€Navigating the Jagged Technological Frontierâ€ 
stwierdzono, Å¼e pracownicy sÄ… w stanie wykonaÄ‡ zadania szybciej i lepiej (z wiÄ™kszÄ… poprawnoÅ›ciÄ…) uÅ¼ywajÄ…c AI, jeÅ›li spe
Å‚niony jest warunek odpowiedniego z niej korzystania. Co to znaczy? OtÃ³Å¼ zadania stawiane przed uczestnikami zostaÅ‚y spe
cjalnie stworzone w taki sposÃ³b, aby czÄ™Å›Ä‡ z nich byÅ‚a trudna do wykonania lub â€niekompatybilnaâ€ z AI (zadania â€outside 
the frontierâ€), zaÅ› czÄ™Å›Ä‡ znajdowaÅ‚a siÄ™ w obrÄ™bie moÅ¼liwoÅ›ci modelu AI (zadania â€inside the frontierâ€). Grupa pracownik
Ã³w, ktÃ³ra potrafiÅ‚a krytycznie oceniaÄ‡ odpowiedzi modelu i z zaangaÅ¼owaniem wchodziÄ‡ w dyskusje z modelem, kwestionujÄ…c 
jego oceny, dobrze radziÅ‚a sobie z zdaniami â€outside the frontierâ€. Pracownicy, ktÃ³rzy przyjmowali bezkrytycznie odpowie
dzi AI, wykonywali te zadania gorzej. JeÅ›li chodzi o zadania w obrÄ™bie moÅ¼liwoÅ›ci AI, obserwowano wzrost metryk jakoÅ›ci 
pracy. Optymistyczne doniesienia pojawiajÄ… siÄ™ rÃ³wnieÅ¼ na temat wpÅ‚ywu GenAI na pracÄ™ programistÃ³w â€“ zresztÄ… wiÄ™kszoÅ›Ä‡ m
i znanych korzysta juÅ¼ z asysty LLM-Ã³w do rozwiÄ…zywania albo szczegÃ³lnie powtarzalnych, bardzo prostych, ale czasochÅ‚onn
ych zadaÅ„, albo tych zwiÄ…zanych z nowymi frameworkami czy narzÄ™dziami, ktÃ³re muszÄ… szybko zastosowaÄ‡. W takich sytuacjac
h przed erÄ… GenAI musieli przejÅ›Ä‡ dÅ‚ugÄ… Å›cieÅ¼kÄ™ prÃ³b i bÅ‚Ä™dÃ³w.Â 

JednoczeÅ›nie faktem jest to, co juÅ¼ wczeÅ›niej wspomniaÅ‚
am â€“ wiele specjalistycznych branÅ¼, takich jak medycyna, rÃ³Å¼ne gaÅ‚Ä™zie inÅ¼ynierii, farmacja czy prawo, najpewniej nie sk
orzysta z LLM-Ã³w ogÃ³lnego przeznaczenia, bo sÄ… one po prostu zbyt sÅ‚abe do tak specyficznych zastosowaÅ„. BranÅ¼e te potrz
ebujÄ… dokÅ‚adniejszych, dedykowanych im narzÄ™dzi.

Na tak fundamentalne przemiany naleÅ¼y jednak patrzeÄ‡ szerzej niÅ¼ tylko
 nasza satysfakcja (lub jej brak) ze sprawniejszego wykonywania zadaÅ„. GenAI moÅ¼e pomÃ³c rodzicom stworzyÄ‡ bajkowÄ… kontyn
uacjÄ™ przygÃ³d ulubionych bohaterÃ³w swoich dzieci. Ale tak samo moÅ¼e posÅ‚uÅ¼yÄ‡ do szybkiego tworzenia tysiÄ™cy tekstÃ³w, wyg
lÄ…dajÄ…cych na tweety pisane przez prawdziwych ludzi, ktÃ³re sÄ… gotowe do rozsyÅ‚ania przez media spoÅ‚ecznoÅ›ciowe w przeciÄ…
gu sekund. Niezwykle Å‚atwo jest tworzyÄ‡ nawet te najniebezpieczniejsze, jak agitujÄ…ce politycznie i szerzÄ…ce nienawiÅ›Ä‡ w
pisy na forach lub nawet faÅ‚szywe zdjÄ™cia i filmy â€“ tzw.Â *deepfakes*. To juÅ¼ siÄ™ dzieje â€“ na przykÅ‚ad w Korei PoÅ‚udniowe
j mamy w tym momencie do czynienia z masowym tworzeniem pornograficznychÂ *deep fakeâ€™Ã³w*Â z uÅ¼yciem wizerunkÃ³w istniejÄ…cyc
h osÃ³b, czÄ™sto nieletnich. Ich twarze sÄ… nakÅ‚adane na wygenerowane sylwetki. Sprawcami sÄ… czÄ™sto szkolni koledzy przeÅ›la
dowanych dziewczynek. Wykrywane przez wÅ‚adze kanaÅ‚y na Telegramie, ktÃ³re rozpowszechniajÄ… te materiaÅ‚y, miewajÄ… po kilka
set tysiÄ™cy czÅ‚onkÃ³w. Niestety, wydaje siÄ™, Å¼e w dobie generatywnej sztucznej inteligencji dobrze zaprojektowane regulac
je sÄ… niezbÄ™dne, aby zabezpieczyÄ‡ nasze dane, prywatnoÅ›Ä‡ oraz uczÄ™szczane przez nas przestrzenie internetowe przed zalan
iem przez wygenerowane treÅ›ci.

CzÄ™sto podnoszonym argumentem przeciwko AI jest teÅ¼ zanikanie miejsc pracy. RzeczywiÅ›cie
, moÅ¼e dojÅ›Ä‡ do sytuacji, w ktÃ³rej trudniej bÄ™dzie znaleÅºÄ‡ pracÄ™ osobom dopiero zaczynajÄ…cym w danej branÅ¼y, poniewaÅ¼ pr
ostsze zadania bÄ™dÄ… rozwiÄ…zywane przez AI na zadowalajÄ…cym poziomie. UwaÅ¼am jednak, Å¼e ostatecznie nie da siÄ™ polegaÄ‡ wy
Å‚Ä…cznie na AI â€“ mimo Å¼e nasza praca moÅ¼e byÄ‡ zÅ‚oÅ¼ona ze wzglÄ™dnie prostych podzadaÅ„, to kumulacja choÄ‡by maÅ‚ych bÅ‚Ä™dÃ³w i
 przeoczeÅ„ spowoduje ostatecznie duÅ¼e problemy. KtoÅ› musi tworzyÄ‡ prompty i aktywnie pracowaÄ‡ z AI, aby dojÅ›Ä‡ do poÅ¼Ä…dan
ego rezultatu, a nastÄ™pnie weryfikowaÄ‡ i ewentualnie poprawiaÄ‡ efekty. RÃ³wnieÅ¼ pracodawcy muszÄ… zdaÄ‡ sobie sprawÄ™, Å¼e br
ak edukacji mÅ‚odszych pracownikÃ³w spowoduje, Å¼e juÅ¼ za 3-5 lat zabraknie specjalistÃ³w.

Innym zagadnieniem jest zdolnoÅ›Ä‡
 AI do speÅ‚niania zÅ‚oÅ¼onych kryteriÃ³w wykonania danego zadania. Na przykÅ‚ad, AI potrafi generowaÄ‡ robiÄ…ce wraÅ¼enie obraz
y, jednak w pracy grafika waÅ¼ne sÄ… konkretne wymagania stawiane przez zleceniodawcÄ™ (np. na temat skojarzeÅ„, ktÃ³re majÄ… 
wywoÅ‚ywaÄ‡ poszczegÃ³lne elementy obrazu, liczby osÃ³b i przedmiotÃ³w na obrazie, itp.). Nie wystarczy po prostu wygenerowan
ie â€Å‚adnejâ€ grafiki, a modele majÄ… problemy z rygorystycznym speÅ‚nianiem zÅ‚oÅ¼onych wymagaÅ„. Nie bez znaczenia jest teÅ¼ k
westia prawnej odpowiedzialnoÅ›ci za poprawnoÅ›Ä‡ wynikÃ³w naszej pracy. UwaÅ¼am, Å¼e ostatecznie dojdziemy do paradygmatu pra
cownika szeroko wspieranego przez AI, jednak wciÄ…Å¼ niezbÄ™dnego w swoim miejscu pracy.

**Jak oceniasz obecny stan badaÅ„ 
nad interpretowalnoÅ›ciÄ… i wyjaÅ›nialnoÅ›ciÄ… AI? Czy zbliÅ¼amy siÄ™ do â€otworzenia czarnej skrzynkiâ€ systemÃ³w AI?**

Modele L
LM naleÅ¼Ä… do grupy modeli trudno wyjaÅ›nialnych, co zwiÄ…zane jest chociaÅ¼by z ich rozmiarem â€“ ciÄ™Å¼ko jest zrozumieÄ‡ znacz
enie kaÅ¼dego parametru lub ich kombinacji, gdy mamy do czynienia z miliardami parametrÃ³w. Znacznie wiÄ™cej wysiÅ‚kÃ³w jest 
obecnie kierowanych w stronÄ™ osiÄ…gania coraz bardziej imponujÄ…cych zdolnoÅ›ci modeli niÅ¼ prÃ³b zrozumienia tego, co dokÅ‚ad
nie dzieje siÄ™ w ich â€gÅ‚owieâ€. JednakÅ¼e niektÃ³re badania nad interpretowalnoÅ›ciÄ… modeli dostarczyÅ‚y ciekawych wnioskÃ³w n
a temat samego ich dziaÅ‚ania â€“ na przykÅ‚ad udowodniÅ‚y, Å¼e duÅ¼a czÄ™Å›Ä‡ parametrÃ³w modelu w rzeczywistoÅ›ci nie sÅ‚uÅ¼y do nic
zego waÅ¼nego i moÅ¼e zostaÄ‡ usuniÄ™ta.Â 

CzÄ™sto wspominamy o wyjaÅ›nialnoÅ›ci w kontekÅ›cie weryfikacji luk i brakÃ³w w modela
ch. Czyli tak naprawdÄ™ interesuje nas kwestia wyciekÃ³w danych osobowych, biasÃ³w czy dyskryminacji dokonywanej przez mode
le. W celu walki z tymi problemami pojawiajÄ… siÄ™ obecnie inne podejÅ›cia niÅ¼ czysta wyjaÅ›nialnoÅ›Ä‡ â€“ na przykÅ‚ad, szybko r
ozwijajÄ…ca siÄ™ dziedzina LLM Red Teaming pozwala na identyfikacjÄ™ luk bezpieczeÅ„stwa w modelach, za pomocÄ… metod przypom
inajÄ…cych dziaÅ‚ania etycznych hakerÃ³w. W ramach Red Teamingu projektuje siÄ™ specjalne prompty, majÄ…ce na celu sprowokowa
nie modelu do zrobienia czegoÅ› â€zÅ‚egoâ€. DziÄ™ki temu co prawda nie zrozumiemy dokÅ‚adnego dziaÅ‚ania modelu, ale za to mamy
 szansÄ™ wykryÄ‡ konkretne zagroÅ¼enia i luki.

JednakÅ¼e myÅ›lÄ™, Å¼e problem braku wyjaÅ›nialnoÅ›ci boli wielu twÃ³rcÃ³w AI. Ilya
 Sutskever, jeden z zaÅ‚oÅ¼ycieli OpenAI, ogÅ‚osiÅ‚ ostatnio uruchomienie swojego startupu, ktÃ³rego celem jest stworzenie â€s
afe superintelligenceâ€. MoÅ¼na mieÄ‡ nadziejÄ™, Å¼e przyczyni siÄ™ do rozwoju metod wyjaÅ›nialnoÅ›ci.

**Jakie kompetencje powi
nni rozwijaÄ‡ Polacy, aby byÄ‡ przygotowanymi na erÄ™ AI?**

Mamy tutaj co najmniej 2 perspektywy: osoby tworzÄ…cej lub wdra
Å¼ajÄ…cej AI i osoby â€nietechnicznejâ€, ktÃ³ra chcÄ…c nie chcÄ…c, jest juÅ¼ wystawiona na dziaÅ‚ania AI kaÅ¼dego dnia.

JeÅ›li cho
dzi o praktykÃ³w AI, to podstawÄ… ich pracy sÄ… dobre umiejÄ™tnoÅ›ci programistyczne. UmiejÄ™tnoÅ›Ä‡ sprawnego tworzenia dobrego
, skalowalnego kodu pozwala szybko przeprowadzaÄ‡ eksperymenty i oszczÄ™dzaÄ‡ zasoby obliczeniowe, ktÃ³re sÄ… drogie i potrze
ba ich coraz wiÄ™cej. WedÅ‚ug mnie Å‚atwiej jest zdobyÄ‡ umiejÄ™tnoÅ›ci AI bÄ™dÄ…c dobrym programistÄ… niÅ¼ zdobyÄ‡ umiejÄ™tnoÅ›ci pr
ogramistyczne bÄ™dÄ…c niekodujÄ…cym lub sÅ‚abo kodujÄ…cym praktykiem AI. NastÄ™pnie naleÅ¼y siÄ™ skupiÄ‡ na umiejÄ™tnoÅ›ciach zwiÄ…z
anych z samym uczeniem maszynowym. Wszystkie potrzebne materiaÅ‚y i kursy sÄ… dostÄ™pne w Internecie: na przykÅ‚ad, kursy de
eplearning.ai czy polski AI Devs, otwarte wykÅ‚ady uczelni amerykaÅ„skich, jak na przykÅ‚ad MIT, rzesza kanaÅ‚Ã³w na YouTube 
tÅ‚umaczÄ…cych intuicyjnie zagadnienia matematyczne lub stricte dotyczÄ…ce AI, na przykÅ‚ad 3Blue1Brown czy kanaÅ‚ Yannica Ki
lchera, repozytoria publikacji â€“Â [arxiv.com](http://arxiv.com/), otwarte zasoby publikacji z konferencji AI â€“ ICLR, KDD,
 NeurIPS, ICML, SIGIR i wiele innych. Zwykle pojawia siÄ™ jednak problem z dostÄ™pem do zasobÃ³w obliczeniowych. Dlatego ob
ecnie ciÄ™Å¼ko jest niestety rozwijaÄ‡ AI w przysÅ‚owiowym garaÅ¼u, wskazane jest dziaÅ‚anie w ramach jednostki badawczej lub 
firmy, ktÃ³ra posiada wymagane zasoby lub korzystanie z grantÃ³w przyznawanych przez operatorÃ³w chmur komercyjnych.

BÄ™dÄ…c
 osobÄ… niezwiÄ…zanÄ… profesjonalnie z AI, rÃ³wnieÅ¼ musimy wÅ‚oÅ¼yÄ‡ pewien wysiÅ‚ek w edukacjÄ™, aby zapewniÄ‡ sobie z jednej str
ony komfort uÅ¼ycia narzÄ™dzi AI, a z drugiej strony bezpieczeÅ„stwo. Wskazane jest poznanie technik promptowania, na przyk
Å‚ad z pomocÄ… poradnika i biblioteki promptÃ³w firmy Anthropic. PojawiÅ‚o siÄ™ niedawno czasopismo poÅ›wiÄ™cone AI â€“ hAI Magaz
ine, z artykuÅ‚ami na rÃ³Å¼nych poziomach trudnoÅ›ci â€“ od podstawowych do bardziej zaawansowanych. Wielka szkoda, Å¼e nie ma,
 pÃ³ki co, oficjalnego szkolenia tworzonego przez instytucje paÅ„stwowe, otwartego dla wszystkich, przedstawiajÄ…cego korzy
Å›ci i ryzyka. IstniejÄ… poradniki dla konkretnych grup, np. administracji, ale to nie wystarczy.

Nawet jeÅ›li zupeÅ‚nie ni
e chcemy korzystaÄ‡ z AI, to treÅ›ci przez niÄ… wygenerowane znajdÄ… nas wczeÅ›niej lub pÃ³Åºniej. NiezaleÅ¼nie od wszystkiego, 
polecam zapoznaÄ‡ siÄ™ z dziaÅ‚alnoÅ›ciÄ… Instytutu NASK (Naukowa i Akademicka SieÄ‡ Komputerowa), ktÃ³ry zajmuje siÄ™ zagadnien
iami cyberbezpieczeÅ„stwa, w tym AI. NASK jest obecna w mediach spoÅ‚ecznoÅ›ciowych i publikuje ciekawe zasoby edukacyjne, 
jak np. raport â€CyberbezpieczeÅ„stwo AI. AI w cyberbezpieczeÅ„stwieâ€.

**AI Act â€“ co wiemy juÅ¼ teraz i czego moÅ¼emy siÄ™ sp
odziewaÄ‡? Czy istnieje niebezpieczeÅ„stwo, Å¼e takie regulacje wpÅ‚ynÄ… na innowacyjnoÅ›Ä‡ i konkurencyjnoÅ›Ä‡ polskich firm tec
hnologicznych?**

WspomniaÅ‚yÅ›my juÅ¼ o zagroÅ¼eniach pÅ‚ynÄ…cych z masowego generowania nieprawdziwych treÅ›ci. Czy AI Act na
s przed tym ochroni? Na razie wiemy na pewno, Å¼e sama treÅ›Ä‡ AI Actu zawiera niestety wiele niejasnoÅ›ci i sÄ… one na tyle 
znaczÄ…ce, Å¼e nie wiadomo, jaka bÄ™dzie praktyka jego stosowania.

AI Act okreÅ›la wiele dziedzin zastosowania AI jako syst
emy wysokiego ryzyka (*high risk*) â€“ np. AI stosowane w edukacji, zatrudnieniu, zarzÄ…dzaniu migracjÄ…, infrastrukturÄ… kry
tycznÄ… miast i paÅ„stw i wielu innych dziedzin Å¼ycia. Warunki nadzoru tych modeli sÄ… opisane bardzo ogÃ³lnie. BÄ™dÄ… musiaÅ‚y
 zostaÄ‡ stworzone dla nich osobne instytucje w krajach czÅ‚onkowskich. Nie wiadomo, w jaki sposÃ³b bÄ™dÄ… one dziaÅ‚aÄ‡ i jak 
bardzo kosztowny i czasochÅ‚onny bÄ™dzie proces nadzoru. MoÅ¼na to sobie jednak wyobraziÄ‡, obserwujÄ…c sytuacjÄ™ bieÅ¼Ä…cÄ…. W t
ym momencie wymÃ³g certyfikacji dotyczy modeli AI dokonujÄ…cych diagnoz medycznych, ktÃ³re traktowane sÄ… jak wyrÃ³b medyczny
. Niedawno zrezygnowaliÅ›my z wykonania projektu z uÅ¼yciem takiego modelu, poniewaÅ¼ brakuje jednostek certyfikujÄ…cych i n
a samo rozpoczÄ™cie procesu musielibyÅ›my czekaÄ‡ ponad rok. WyobraÅºmy sobie, co siÄ™ stanie, jeÅ›li wiÄ™kszoÅ›Ä‡ dostawcÃ³w syst
emÃ³w AI bÄ™dzie musiaÅ‚a certyfikowaÄ‡ swoje systemy. Rok w dziedzinie AI jest epokÄ…. JeÅ¼eli dojdzie do takiej blokady, caÅ‚
y Å›wiat nam ucieknie.

AI Act nakÅ‚ada teÅ¼ dodatkowe obowiÄ…zki regulacyjne na tzw. â€modele ogÃ³lnego przeznaczenia z ryzyk
iem systemowymâ€, do ktÃ³rych zaliczajÄ… siÄ™ wielkie modele jÄ™zykowe (LLM), ktÃ³re do trenowania potrzebujÄ… wystarczajÄ…co du
Å¼ej liczby operacji zmiennoprzecinkowych (FLOP). Definicja FLOP zawarta w AI Act moim zdaniem stwarza jednak ryzyko nadu
Å¼yÄ‡. Nie wiadomo zatem, ktÃ³re modele realnie bÄ™dÄ… wpadaÄ‡ w tÄ™ kategoriÄ™, biorÄ…c pod uwagÄ™ fakt, Å¼e twÃ³rcy bÄ™dÄ… mieli Å¼yw
otny interes w wykorzystaniu kaÅ¼dego bÅ‚Ä™du w definicji do obniÅ¼enia raportowanej przez siebie liczby FLOP.

AI Act poroz
umiewa siÄ™ z czytelnikiem kryteriami tak ogÃ³lnymi i wieloznacznymi, Å¼e nie da siÄ™ udowodniÄ‡ ich speÅ‚nienia, np.: â€(â€¦) po
dmiot ten zapewnia adekwatnoÅ›Ä‡ i wystarczajÄ…cÄ… reprezentatywnoÅ›Ä‡ danych wejÅ›ciowych (â€¦)â€. Czym jest â€adekwatnoÅ›Ä‡â€ i kto 
jest ostatecznÄ… instancjÄ… do jej oceny? Kary za niedopasowanie siÄ™ do regulacji sÄ… juÅ¼ jednak niezwykle konkretne i bard
zo wysokie â€“ np. 35 mln euro lub 7% rocznego obrotu przedsiÄ™biorstwa.

Opisane przeze mnie problemy to tylko wierzchoÅ‚ek
 gÃ³ry lodowej. Czy w obliczu takich niejasnoÅ›ci moÅ¼na powiedzieÄ‡, Å¼e AI Act w tym momencie zwiÄ™ksza nasz komfort, poczuc
ie bezpieczeÅ„stwa lub daje nadziejÄ™ na bardziej etyczny i zrÃ³wnowaÅ¼ony rozwÃ³j? Niestety, nie. NiepewnoÅ›Ä‡ przynosi juÅ¼ pi
erwsze owoce â€“ na przykÅ‚ad, Meta nie udostÄ™pni w UE swojego modelu Multimodal Llama (jest to model unikalny, poniewaÅ¼ pr
zetwarza dane wideo, audio, tekstowe i obrazowe). Mimo Å¼e model jest udostÄ™pniany w formie otwartego oprogramowania na l
icencji niekomercyjnej, ryzyko regulacyjne jest zbyt duÅ¼e. Modele Llama, dziÄ™ki swojej otwartej licencji, sÄ… Å›wietnym na
rzÄ™dziem do badaÅ„ i wdroÅ¼eÅ„.

**Czy Polska ma szansÄ™ staÄ‡ siÄ™ europejskim hubem AI? Jakie warunki musiaÅ‚yby zostaÄ‡ speÅ‚n
ione? Czy istniejÄ… jakieÅ› unikalne cechy polskiego ekosystemu AI, ktÃ³re mogÄ… byÄ‡ konkurencyjne na arenie miÄ™dzynarodowej
?**

Polska ma potencjaÅ‚, aby takim hubem siÄ™ staÄ‡ â€“ w ciÄ…gu ostatnich lat branÅ¼a IT staÅ‚a siÄ™ jednym z koni pociÄ…gowych
 polskiej gospodarki. W zwiÄ…zku z tym zostaÅ‚o wyksztaÅ‚cone (lub wyksztaÅ‚ciÅ‚o samo siebie) szerokie grono specjalistÃ³w. C
hÄ™tnych do rozwoju w branÅ¼y nie brakuje, kariera w IT jest nieustannie postrzegana jako poÅ¼Ä…dana. Polscy informatycy dal
i siÄ™ poznaÄ‡ jako wiarygodni specjaliÅ›ci o wysokich umiejÄ™tnoÅ›ciach, chÄ™tnie doksztaÅ‚cajÄ…cy siÄ™ i dostarczajÄ…cy kod wyso
kiej jakoÅ›ci. Mamy wiÄ™c solidne podstawy. Co do samego AI, uwaÅ¼am, Å¼e nie mamy jeszcze wystarczajÄ…co dobrej oferty uczel
ni wyÅ¼szych. Nie jest to zarzut do wszystkich uczelni, poniewaÅ¼ czÄ™Å›Ä‡ z nich oferuje Å›wietny poziom. CzÄ™sto jednak nawet
 na solidnych uczelniach zajÄ™cia sÄ… na podstawowym poziomie, brakuje omawiania najnowszych trendÃ³w badawczych i projektÃ³
w studenckich skupionych na problemach z bieÅ¼Ä…cych publikacji. OmawiajÄ…c metody sprzed 15 lat moÅ¼emy poÅ‚oÅ¼yÄ‡ fundament d
la zrozumienia obecnych technologii, ale zbyt czÄ™sto te tradycyjne metody grajÄ… centralnÄ… rolÄ™.Â 

JeÅ›li chodzi o badania
 â€“ tworzone sÄ… obecnie polskie modele jÄ™zykowe, ktÃ³re bÄ™dÄ… lub sÄ… otwarte do uÅ¼ytku dla wszystkich: Bielik, stworzony pr
zez SpeakLeash, PLLuM, za ktÃ³ry odpowiada konsorcjum instytucji badawczych pod przewodnictwem Politechniki WrocÅ‚awskiej,
 czy Qra z Politechniki GdaÅ„skiej. Modele takie najczÄ™Å›ciej tworzone sÄ… na bazie istniejÄ…cych modeliÂ *open source*, jak 
Llama czy Mistral. Jednostki badawcze, takie jak IDEAS NCBR, prowadzÄ… badania fundamentalne nad AI, przyciÄ…gajÄ… Å›wiatowe
 talenty, a ich prace sÄ… prezentowane na najlepszych konferencjach.

Mimo tego wszystkiego, brak jest AI w polskiej stra
tegii rozwoju. Ministerstwo Cyfryzacji rozpoczÄ™Å‚o pewne inicjatywy zwiÄ…zane z AI, takie jak powoÅ‚anie zespoÅ‚u doradczego
 PL/AI czy ogÅ‚oszenie Funduszu AI. WyglÄ…da jednak na to, Å¼e wysiÅ‚ki kierowane sÄ… w stronÄ™ wspierania wdroÅ¼eÅ„ AI, a nie t
worzenia nowych technologii. Jest to zasadnicza rÃ³Å¼nica. WdraÅ¼ajÄ…c AI, jesteÅ›my w stanie wesprzeÄ‡ konkretne obszary paÅ„s
twa, usprawniÄ‡ ich dziaÅ‚anie, jednak narzÄ™dzia ktÃ³rych uÅ¼yjemy, siÅ‚Ä… rzeczy bÄ™dÄ… kupowane od ich zagranicznych twÃ³rcÃ³w. 
WdroÅ¼enia jako takie nie wspierajÄ… rozwoju ÅºrÃ³dÅ‚owej technologii w Polsce. Inaczej mÃ³wiÄ…c, nie zbudujemy w ten sposÃ³b na
stÄ™pcy dla dzisiejszych LLM-Ã³w, tylko bÄ™dziemy opakowywaÄ‡ i sprzedawaÄ‡ w Polsce technologie wymyÅ›lone przez innych.Â 

Na
 razie nie ma Å›rodkÃ³w dedykowanych temu, abyÅ›my mieli choÄ‡ szansÄ™ postawiÄ‡ krok przed OpenAI i podobnymi, nie ma teÅ¼ pla
nÃ³w lepszego rozliczania naukowcÃ³w i wspierania doskonaÅ‚oÅ›ci naukowej (bo jestem zdania, Å¼e samo dosypanie pieniÄ™dzy do 
wadliwego systemu nie pomoÅ¼e).

**Jakie sÄ… twoim zdaniem najwaÅ¼niejsze globalne trendy w rozwoju AI, ktÃ³re bÄ™dÄ… ksztaÅ‚to
waÄ‡ tÄ™ dziedzinÄ™ i wpÅ‚ywaÄ‡ na polskÄ… gospodarkÄ™ w najbliÅ¼szych latach?**

NajwaÅ¼niejsze trendy to moim zdaniem:

* syste
my wieloagentowe â€“ zÅ‚oÅ¼one z wielu LLM-Ã³w, ktÃ³re posiadajÄ… zwiÄ™kszone zdolnoÅ›ci do samodzielnej korekcji bÅ‚Ä™dÃ³w i popraw
iania jakoÅ›ci swojego dziaÅ‚ania. Badania pokazujÄ…, Å¼e grupy nawet bardzo prostych agentÃ³w pracujÄ…cych razem w ramach wiÄ™
kszego systemu osiÄ…gajÄ… jakoÅ›Ä‡ znacznie lepszÄ… niÅ¼ modele skÅ‚adowe. ByÄ‡ moÅ¼e bÄ™dÄ… mogÅ‚y byÄ‡ uÅ¼ywane do (prawie) autonomi
cznego wykonywania zÅ‚oÅ¼onych czynnoÅ›ci, np. uÅ¼ywania wielu programÃ³w w sekwencji lub nawet poprawy struktury wÅ‚asnego sy
stemu;
* systemy trenowane w wiÄ™kszym kontakcie ze Å›wiatem fizycznym, w schemacie takim jak np. proponowany przez Yanna 
LeCuna â€World Modelâ€ â€“ nauka przewidywania nastÄ™pnego stanu Å›rodowiska na podstawie obecnego stanu. Obecne duÅ¼e modele n
ie sÄ… jeszcze do koÅ„ca multimodalne (nie przetwarzajÄ… danych pochodzÄ…cych ze wszystkich zmysÅ‚Ã³w) i trenujÄ… raczej na dan
ych statycznych;
* mniejsze modele o zdolnoÅ›ciach porÃ³wnywanych z dzisiejszymi LLM-ami â€“ trend minimalizacji jest wyraÅºn
y i bardzo potrzebny, gdyÅ¼ fundusze niezbÄ™dne do trenowania wielkich modeli sÄ… ogromne. Podnoszone sÄ… nawet koszty Å›rodo
wiskowe trenowania LLM-Ã³w. Historia minimalizacji modeli jest optymistyczna â€“ np. model DistilBERT, zminimalizowana wers
ja transformerowego modelu BERT, zachowaÅ‚a 97% wydajnoÅ›ci BERT-a przy redukcji rozmiaru o 60%. WÅ›rÃ³d LLM-Ã³w rÃ³wnieÅ¼ widz
imy juÅ¼ pewne zwiastuny sukcesu â€“ np. bardzo dobre dziaÅ‚anie modelu GPT-4o mini. Niestety nie wiemy, o ile GPT-4o mini j
est mniejszy od GPT-4, ale z pewnoÅ›ciÄ… jest duÅ¼o mniejszy, co widaÄ‡ choÄ‡by po cenie (GPT-4o mini jest ok. 30 razy taÅ„szy
, jeÅ›li chodzi o cenÄ™ za tokeny wejÅ›ciowe).

MyÅ›lÄ™, Å¼e bÄ™dziemy obserwowaÄ‡ rÃ³wnieÅ¼ trendy regionalne, zwiÄ…zane np. z reg
ulacjami â€“ czyli w kontekÅ›cie europejskim zobaczymy jakÄ…Å› odpowiedÅº twÃ³rcÃ³w modeli na AI Act, np. dÄ…Å¼enie do utrzymania 
siÄ™ pod progami wyznaczajÄ…cymi funkcjonalnoÅ›Ä‡ lub wielkoÅ›Ä‡ modeli.

\[1\]Â [https://www.hbs.edu/ris/Publication%20Files/2
4-013\_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf](https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a
1c6-c72fb70c7282.pdf)

\[2\]Â [https://link.springer.com/chapter/10.1007/978-3-031-64881-6\_21](https://link.springer.com
/chapter/10.1007/978-3-031-64881-6_21)

\[3\]Â [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/p
rompt-generator](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator)

\[4\]Â [https
://haimagazine.com](https://haimagazine.com/)
```
---

     
 
all -  [ AMA: PhD Researcher in Computer Vision/Machine Learning ](https://www.reddit.com/r/Btechtards/comments/1hnvw3s/ama_phd_researcher_in_computer_visionmachine/) , 2025-01-03-0913
```
Hello! I am a doctoral researcher working at the intersection of computer vision and machine learning at UCF, one of the
 top vision research institutes in the US. I have four years of research experience in computer vision before joining UC
F.

Feel free to comment on this post if you seek career guidance in Vision/ML or relevant fields. Post your questions a
s comments to this thread, and I'll try to respond to everyone. This thread is aimed to guide students/aspirants, partic
ularly those pursuing/completing undergrad degrees and who want to get into Vision/ML research.

Note: Please don't use 
'Sir/Ma'am/XYZ' in your comments. Just use '**OP**.'

**Edit:** It is late night here in my timezone, and morning in Ind
ia. Sorry that, it had to be this way. So, I'll respond to every question I get in 24 hours.

# Resources/Roadmap to ML/
Vision:

**Prerequisites:**

* Linear Algebra: Use Dr. Gilbert Strang's book and lectures on YouTube.
* Calculus: Brush 
up your school-level calculus, and that would do for starters.
* Probability: I have used [probabilitycourse.com](https:
//www.probabilitycourse.com) and [statlect.com](https://www.statlect.com) but feel free to use any good resource you fin
d. MIT OCW lectures are good resources.
* Follow 3Blue1Brown for a lot of concepts.
* You may also want to learn the bas
ics of Information Theory or Coding Theory. Use MIT OCW lectures for that.

**Basic Machine Learning:**

* Start with th
e ML for Everyone course by Dr. Andrew Ng in Coursera if you are an absolute beginner, and if you're learning the prereq
uisites on the side. This used to be the absolute best (and probably the only good enough resource) back when I started.
 All the videos are on YouTube. I am not sure how good their new ML Specialization is, but I am assuming it would be pre
tty good.
* Your goal will be to go towards CS229 Stanford. Use their lecture notes. It is a very good resource.
* Refer
ence Books: Machine Learning and Pattern Recognition by Bishop & Machine Learning Trilogy by Kevin P. Murphy. All of the
se books are available in PDF copies on the internet.

**Deep Learning:**

* You may start with CS230 Stanford. It's a g
ood resource.
* You can try the Deep Learning Specialization in Coursera. It is decent enough to go through. Again, back
 in the day, when I started in 2017, it was one of the best.
* For generative models, you can start with the GAN Special
ization of Coursera. It teaches you GANs. Work your way towards VAEs and Diffusion models through papers and blogs.
* Tr
ansformers, you can start learning from Dr. Andrej Karpathy's blog and YouTube [channel](https://www.youtube.com/playlis
t?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ).
* Reference Books: Deep Learning by Ian Goodfellow. If you want to go toward
s the more obscure statistical part of it for deeper theoretical understanding, use Elements of Statistical Learning and
 Introduction to Statistical Learning by Tibshirani.
* HuggingFace [blog](https://huggingface.co/blog) is a very good pl
ace to learn. Particularly works by the Diffusers team.
* Another decent blog is [Lil'log](https://lilianweng.github.io/
) by Lilian Weng. She is very good at this.
* You can find more on Analytics Vidhya, Medium, and Towrads Data Science.


**Computer Vision:**

* Fundamental computer vision is very different from these. Use Tubingen [lectures](https://youtub
e.com/playlist?list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_) from YouTube. Their other lectures are very good as well.
* Refe
rence Books: Foundations of Computer Vision by Torralba, Isola, Freeman

**Programming Languages:**

* Python is absolut
ely necessary. Learn Numpy and Pandas well. Correy Schafer YouTube channel has a good Pandas [series](https://youtube.co
m/playlist?list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS). Scikit-Learn will satisfy the majority of the classical ML problems
 you'll approach
* Tensorflow is kind of outdated and hence, so is Keras. Learn PyTorch properly. And not just the [Fast
.ai](http://Fast.ai) API. HuggingFace API is good for engineers.
* Learn C++ if you are going towards GPU programming wi
th CUDA. A lot of theoretical ML researchers use it and it s needed for a lot of custom and efficient implementations in
 real-world applications. Triton is a new alternative, but it is to be seen how good it is as an alternative.

**Researc
h:**

The only thing you can do is read papers and blogs. Particularly, read top-venue A\*s (Vision - CVPR, ICCV, ECCV, 
TPAMI, IJCV; ML - ICLR, ICML, NeurIPS, TMLR, PMLR; Theoretical AI - AAAI, IJCAI, TAI; NLP - ACL, NAACL, EMNLP) Try to st
ick to paper, but if you are stuck somewhere find blogs that explain it. Not necessarily you'll find blogs always. Searc
h through arXiv and Google Scholar. Keep following people who work in this domain. Yannic Kilcher's [podcast](https://ww
w.youtube.com/@YannicKilcher), [Machine Learning Street Talk](https://www.youtube.com/@MachineLearningStreetTalk) are go
od YouTube channels to stay updated as well.

**Implementations:**

There are a few implementations that are easier to u
nderstand or use.

* For any paper or implementation, if you are seeing Meta's repository, it's the best thing out there
.
* Lucidrains has a good [profile](https://github.com/lucidrains) with repositories and lots of implementations.
* [Seq
2Seq](https://github.com/bentrevett/pytorch-seq2seq) is good for RNN to transformer explanations with codes.
* Find impl
ementations of any paper in [Papers with Code](https://paperswithcode.com/).
```
---

     
 
all -  [ # of papers vs. citations ](https://www.reddit.com/r/eb_1a/comments/1hm9j9b/of_papers_vs_citations/) , 2025-01-03-0913
```
How are the two compared?

My lawyers are aiming for EB1B for me but Iâ€™m worried that I might not have enough papers.

I
 have 4 publications, 1 thesis, 3 preprints (one of which was also presented as an industrial demo at CVPR). My works ar
e mostly focused in vision / nlp (CVPR, WACV, AACL). All first author except 1 publication and 1 preprint.

Now, I got l
ike 450 citations, and am getting cited rapidly - decent chance to be at 500 by the time the doc is ready. Have served a
s reviewer for all top tier conferences in ML/CV (NeurIPS, CVPR, ICLR, ICML, ICCV, etc.) for something like 50+ papers.


My big worry is having issues stemming from only 4 of my works having been actually published (although the non-publish
ed ones are also getting cited). What do you guys think?
```
---

     
 
all -  [ i sensed anxiety and frustration at NeurIPS 24 ](https://kyunghyuncho.me/i-sensed-anxiety-and-frustration-at-neurips24/) , 2025-01-03-0913
```

```
---

     
 
all -  [ [D] What would you like in a ML/ML-related course in university? ](https://www.reddit.com/r/MachineLearning/comments/1hhdch4/d_what_would_you_like_in_a_mlmlrelated_course_in/) , 2025-01-03-0913
```
Hi!

I'm invited to give a course in university (not really a university, it's a different educational system, they call
 it engineering school but it's equivalent) in ML or ML-related.

The course is 22 hours in total. Which is short. The c
ourse is divided in both theoretical classes and practices classes. But I can change the proportion of hours. When I say
 practice it's more like a project they can do and then I grade it.

It's not the only ML course the students have, I wa
s told the students already have a machine learning course where they cover all the basics in Machine Learning and some 
statistical models (the usual ones like random forests, SVMs etc.), and they also have an in-depth NLP course, so I don'
t think I'm going with that.

What bothers me is, how to balance the theory with practice. I don't want to cover some to
pic superficially but at the same time I don't know if it's worth it for the students to cover a specific topic too deep
ly.

I don't know if it's a good idea to do something like two topics, 11 hours each with like 5 hours of theory and 6 h
ours of practice. Or do I go with just one topic.

I was suggested to show them about MLOps and tooling like Git, Docker
, Mlflow, basically just a bit of Mlops, monitoring models, how to productionize them etc. But I don't know if it's wort
h it, I feel like it's superficial to teach them how to use these tools, and there are a lot of resources online anyways
 and I guess recruiters won't expect them to know that or have experience with for junior positions.

I was also suggest
ed time series as a course, but I don't know if going in-depth in them would be interesting to the students ğŸ˜… there's a 
lot of math, and though professors assured me that they have a good level in math, I don't know if they'll be interested
 in that.

Another drawback is that I don't have access to computational resources for this course so I'm a bit limited.
 I think if I were at their place I'd have loved a course in low-level stuff like how flash attention works, some distri
buted training mechanisms, cuda etc. But I don't have means to ensure that for them :(

Another thing I'd love to do is 
to take some of the best awards papers of this year or something and help them gain the knowledge and understanding nece
ssary to understand the paper and the topics around it. Or maybe have different sessions with different topics like, one
 about diffusion models, one about multi-modal models etc., like 'let's understand how they came about qwen2-vl', 'let's
 understand what's the main contribution and novelty of the best paper in neurips main track about var' etc.

So I'm a b
it lost and I'd love to have your ideas and suggestions. What I care about is giving the students enough knowledge about
 some topic(s) so they don't only have a high-level idea (I've had interns to which I asked what is a transformer and th
ey went 'we import a transformer from hugging face') but at the same time equip them with skills or knowledge that can h
elp them get recruited for junior positions

Thank you!

```
---

     
 
all -  [ Winning edge models from Neurips 2024 competition  ](https://www.reddit.com/r/LocalLLaMA/comments/1hhbl6a/winning_edge_models_from_neurips_2024_competition/) , 2025-01-03-0913
```
I have been following up the neurips edge llm competition for a while and recently they announced the winners. The compe
tition had two tracks. One was compression challenge and another was training from scratch. Though the models and associ
ated compression techniques are not yet made public, it is interesting to see the edge llm space getting more traction 


https://edge-llms-challenge.github.io/edge-llm-challenge.github.io/leaderboard
```
---

     
 
all -  [ NeurIPS 2024: Capital One showcases leading AI research ](https://www.reddit.com/r/u_CapitalOne/comments/1hh7gli/neurips_2024_capital_one_showcases_leading_ai/) , 2025-01-03-0913
```
# This past week, many Capital One associates were active participants at the r/NeurIPS conference. Explore our contribu
tions to the world's premier AI research conference, from papers and workshops to expert presentations.

The 38th Annual
 Conference on Neural Information Processing Systems ([NeurIPS](https://neurips.cc/)), returned this December, and Capit
al One is excited to be a part of it! As a company committed to [responsible and innovative AI](https://www.capitalone.c
om/tech/machine-learning/applied-ai-research/), we're eager to share our latest research, connect with fellow researcher
s and engage in the vibrant exchange of ideas that defines this event.

# Capital One's impact at NeurIPS 2024

At Capit
al One, we're [leveraging AI to unlock new possibilities in financial services](https://www.capitalone.com/tech/machine-
learning/machine-learning-research-roundup/) and deliver exceptional customer experiences. NeurIPS provides a crucial pl
atform to engage and exchange ideas with some of the best minds in AI and science. We're eager to engage with leading ac
ademics, researchers and industry experts to discuss the latest advancements and challenges in AI. Our research efforts 
are focused on applying AI/ML techniques to address real-world challenges in the financial domain, such as improving the
 efficiency and interpretability of models, developing advanced techniques for analyzing financial time series data and 
building transparent and understandable AI systems. This work is critical to developing trustworthy AI solutions, and we
're particularly excited to share our progress in areas like deep learning, sequence modeling, explainable AI and genera
tive AI.

At [NeurIPS 2023](https://www.capitalone.com/tech/machine-learning/neurips-applied-research/), our Applied Res
earch team had several works accepted. This year, weâ€™re continuing to showcase our research, foster collaboration and su
pport the next generation of AI talent

# Advancing AI research: Main conference papers

We have contributed to three pa
pers accepted to the main conference track:

* [**Distributional Preference Alignment of LLMs via Optimal Transport**](h
ttps://neurips.cc/virtual/2024/poster/96822): Distinguished Applied Researcher, [Igor Melnyk](https://scholar.google.com
/citations?user=4vDRTWwAAAAJ&hl=en), takes the lead as first-author exploring novel methods for aligning large language 
models (LLMs) with user preferences using optimal transport theory. This work enhances LLMs' ability to generate content
 that aligns with specific needs and values.Â 
* [**Searching for Efficient Linear Layers over a Continuous Space of Stru
ctured Matrices**](https://neurips.cc/virtual/2024/poster/94195): VP of AI Research [Bayan Bruss](https://scholar.google
.com/citations?user=ClqvGRQAAAAJ&hl=en) and Senior Machine Learning Engineer [Christopher Ferri](https://scholar.google.
co.uk/citations?hl=en&user=rGuDTOIAAAAJ) contribute their expertise to this collaborative research with NYU, which focus
es on optimizing the efficiency of neural networks by exploring a continuous space of structured matrices for linear lay
ers. This has the potential to lead to faster and more efficient models.
* [**Tiny Time Mixers (TTMs): Fast Pre-trained 
Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series**](https://neurips.cc/virtual/2024/poster/9674
8): In partnership with IBM Corp, Senior Distinguished Applied Researcher [Nam Nguyen](https://scholar.google.com/citati
ons?user=zzBcUpEAAAAJ&hl=en) helps to introduce innovative techniques for time series forecasting with Tiny Time Mixers 
(TTMs). These pre-trained models are optimized for fast and accurate predictions, even with limited fine-tuning data.

[
Dr. Furong Huang](https://scholar.google.com/citations?user=13yyuCcAAAAJ&hl=en), our inaugural Visiting Scholar and an A
ssociate Professor in Computer Science at the University of Maryland, also contributed to [six additional NeurIPS papers
](https://nips.cc/virtual/2024/papers.html?filter=authors&search=Furong+Huang). Dr. Huangâ€™s expertise in trustworthy mac
hine learning strengthens our research collaborations and reflects our commitment to bridging academia and industry.Â 

#
 Fostering the next generation of AI talent: Workshop papers

Capital One's presence at NeurIPS extends beyond the main 
conference with seven accepted workshop papers, further demonstrating our commitment to advancing AI research and foster
ing the next generation of AI talent through our internship programs.Â 

# Applied Research Internship Program (ARIP)

Fi
ve of the accepted papers are authored by talented individuals from our 2024 [Applied Research Internship Program](https
://www.capitalonecareers.com/upgrade-ai-work-with-the-applied-research-internship-students-tech) (ARIP), a program desig
ned to provide PhD students with hands-on experience tackling real-world AI challenges in the financial sector.Â 

* [**R
efusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models**](https://scholar.google.com/citations?view_
op=view_citation&hl=en&user=nSn7jtIAAAAJ&citation_for_view=nSn7jtIAAAAJ:4DMP91E08xMC): This paper explores a novel metho
d for controlling the refusal behavior of large language models by introducing 'refusal tokens' during training. This te
chnique allows for fine-grained control over the model's tendency to refuse certain prompts or questions, enhancing safe
ty and reliability.
* [**Dense Backpropagation Improves Routing for Sparsely-Gated Mixture of Experts**](https://openrev
iew.net/pdf?id=huy8g3iKy0): This paper investigates the use of dense backpropagation in Mixture of Experts (MoE) models,
 demonstrating its effectiveness in improving routing decisions and overall model performance. This work contributes to 
the advancement of MoE models, which are known for their efficiency and scalability.
* [**Language Model Scaling Laws an
d Zero-sum Learning**](https://openreview.net/forum?id=yBq2g832Go&referrer=%5Bthe%20profile%20of%20Irina%20Rish%5D(%2Fpr
ofile%3Fid%3D~Irina_Rish1))**:** This research delves into the relationship between language model scaling laws and zero
-sum learning, exploring how the competitive dynamics of zero-sum games can influence the scaling behavior and performan
ce of large language models.
* [**StructMoE: Augmenting MoEs with Hierarchically Routed Low Rank Experts**](https://open
review.net/pdf?id=v71Nsh6R7m): This paper proposes StructMoE, a novel architecture that enhances Mixture of Experts (MoE
) models by incorporating hierarchically routed low-rank experts. This approach aims to improve the efficiency and expre
ssiveness of MoE models, further advancing their capabilities in various applications.

# Data Science Internship Progra
m (DSIP)

We are also proud to highlight a workshop paper authored by a former intern from our [Data Science Internship 
Program](https://campus.capitalone.com/employment/new-york-data-science-jobs/1786/8161296/6252001-5128638-5128581/4) (DS
IP), which offers aspiring data scientists the opportunity to contribute to cutting-edge research and development.

* [*
*Enhancing Table Representations with LLM-powered Synthetic Data Generation**](https://openreview.net/forum?id=h9465s0xu
7#discussion): This paper explores the use of large language models (LLMs) to generate synthetic tabular data for improv
ing table representations. This approach aims to enhance the performance of similar table recommendation systems, which 
are crucial for efficient data management and analysis in data-driven enterprises. The research introduces a novel synth
etic data generation pipeline that leverages LLMs to create a large-scale dataset tailored for table-level representatio
n learning, leading to improved accuracy in recommending similar tables.

# Collaborative research

Finally, we have wor
kshop papers co-authored by Senior Distinguished Applied Researcher [Nam Nguyen](https://scholar.google.com/citations?us
er=zzBcUpEAAAAJ&hl=en), and Distinguished Applied Researcher [Supriyo Chakraborty](https://scholar.google.com/citations?
user=UIM7nGwAAAAJ&hl=en) further demonstrating our commitment to collaborative research and knowledge sharing within the
 AI community.

* [**Scaling-laws for Large Time-series Models**](https://arxiv.org/pdf/2405.13867): This research, cond
ucted in collaboration with Johns Hopkins University, explores the scaling laws that govern the performance of large tim
e-series models. By examining the relationships among model size, data volume, and computational resources, this study o
ffers valuable insights into the efficient training and deployment of these models for diverse time-series forecasting t
asks in finance and other domains.
* [**MyCroft: Towards Effective and Efficient External Data Augmentation**](https://s
cholar.google.com/citations?view_op=view_citation&hl=en&user=QOsVyPMAAAAJ&citation_for_view=QOsVyPMAAAAJ:LkGwnXOMwfcC): 
This research introduces MyCroft, a new data-efficient framework implementing techniques to evaluate relative utility of
Â  relevant external data sources that can augment internal data to improve model performance. These techniques leverage 
feature space distances and gradient matching to identify small but informative data subsets to maximize performance wit
h minimal data [exposure.Capital](http://exposure.Capital) One's presence at NeurIPS extends beyond the main conference 
with seven accepted workshop papers, further demonstrating our commitment to advancing AI research and fostering the nex
t generation of AI talent through our internship programs.Â 

# Our two engaging expo talks

# Sequence Modeling in Finan
cial Services

Led by Senior Distinguished Applied Researcher [Nam Nguyen](https://scholar.google.com/citations?user=zzB
cUpEAAAAJ&hl=en), this talk delves into the intricacies of applying sequence modeling to financial data. Learn about cut
ting-edge research, including novel approaches for leveraging powerful transformer models for enhanced insights and pred
ictions.

* **Date/Time:** Tuesday, December 10th, 4:00 PM local time
* **Location:** West Ballroom B

# Deep Tabular Da
ta

Led by Distinguished Machine Learning Engineer [Doron Bergman](https://scholar.google.com/citations?user=FeCagRUAAAA
J&hl=en), this talk explores the challenges and opportunities of deep learning for tabular data in finance. Discover how
 deep learning can surpass traditional methods and unlock new possibilities for financial modeling.

* **Date/Time:** We
dnesday, December 11th, 1:00 PM local time
* **Location:** West Meeting Room 109/110

# Connect with Capital One at Neur
IPS 2024!

We're excited to connect with you at NeurIPS 2024! Come visit us at booth #315 where you can:

* **Explore ou
r research**: Dive deep into our latest [advancements in AI](https://www.capitalone.com/tech/ai/) and machine learning.

* **Discover career opportunities**: Learn about exciting [applied research career paths](https://www.capitalonecareers.
com/search-jobs/applied%20research/234/1) at Capital One for researchers and engineers passionate about AI and join our 
world-class team.
* **Engage with our team**: Meet our researchers and AI experts, ask questions and discuss the [future
 of AI in finance](https://www.capitalone.com/tech/ai-research/).

This Reddit post is aÂ [\~repurposed Tech blog\~](http
s://www.capitalone.com/tech/ai-research/?utm_campaign=always-on&utm_source=reddit&utm_medium=organic-social&utm_content=
neur-ips). For more learning opportunities check out theÂ [\~Capital One Tech blog\~](https://www.capitalone.com/tech/blo
g/)Â and keep the progress going!Â 
```
---

     
 
all -  [ Can o1-preview find major mistakes amongst 59 NeurIPS '24 MLSB papers? ](https://www.reddit.com/r/slatestarcodex/comments/1hh25xz/can_o1preview_find_major_mistakes_amongst_59/) , 2025-01-03-0913
```
[Link to the essay](https://www.owlposting.com/p/can-o1-preview-find-major-mistakes)

  
Summary: I saw this Twitter thr
ead recently about how o1 [was able to find a major error in a scientific paper. ](https://x.com/emollick/status/1868329
599438037491)I wondered: could it do something similar in my own field of biology x ML? I downloaded 59 papers from [Neu
rIPS '24 MLSB](https://www.mlsb.io/), a structural biology + chemistry + AI workshop that happened just last week, pushe
d them through o1 to ask if there are any errors, and interpreted its response. Of the 59, o1 said 3 have major errors. 
Upon reviewing the 3, none of the complaints seem well-founded. But all were intelligent and fun to grapple with! But fo
r at least one of the papers, it took quite a bit of effort (contacting the authors) to disprove. All this to say, o1 is
n't a drop in replacement for an academic reviewer, but its critiques are still often interesting and useful. 
```
---

     
 
all -  [ Influential creators at tech conference: 'Don't say AI democratizes art-making. Should we democratiz ](https://www.reddit.com/r/aiwars/comments/1hgq91u/influential_creators_at_tech_conference_dont_say/) , 2025-01-03-0913
```
Despite being mostly public figures, names still censored as per rules.

https://preview.redd.it/x3x3wzuubi7e1.png?width
=721&format=png&auto=webp&s=b61a67d70cba854f9aa57743566211748ae073f9

Here's the issue.

You can't democratize marathon 
running with mopeds, because then it would no longer be running. Marathon running is a specific activity performed for a
 specific purpose.

What you *can* democratize -- and what we have successfully democratized to everyone's benefit -- is
 getting from place-to-place quickly. For this you can give people mopeds, electric wheelchairs, cars, planes, whatever.
 Because the goal of the activity is not to use your legs to run, but simply to get from one place to another. There was
 a time when those who couldn't walk were mostly out of luck. There were times when there weren't ramps for wheelchairs 
to get into most buildings. But we've taken steps to make it easier for everyone to get around; we've democratized trave
l.

  
To take all this back to AI, AI doesn't democratize drawing or painting, because those are specific activities wh
ich AI is not. But it does democratize art-making. If you don't like calling it art, fine, it democratizes the ability t
o get ideas from your head into imagery which can be enjoyed or shared. If you'd say it didn't need to be democratized b
ecause anyone can draw, well sure, and anyone can walk. But we have technology like bicycles and mopeds to get to places
 faster...or Photoshop with all its conveniences like layering and undo...and as long as you're not in a context where t
hose tools don't match the activity, there's nothing wrong with that.
```
---

     
 
all -  [ Am I Making the Right Choice? Masters in ML, Research Lab Experience, and Building Things That Matte ](https://www.reddit.com/r/learnmachinelearning/comments/1hgng0o/am_i_making_the_right_choice_masters_in_ml/) , 2025-01-03-0913
```
Iâ€™m currently pursuing my masterâ€™s in machine learning, and I love building things â€” thatâ€™s how I understand concepts be
st. But my first semester hit me with a tough realization: I joined a research lab way too early, and it just wasnâ€™t the
 right fit for me.

The labâ€™s environment felt off. The code was sloppy, results were rushed, and even the smallest net-
positive outcome led to a question: â€œWhich journal should we target?â€ Maybe this is how it works in many research labs â€”
 I donâ€™t know, this was my first experience. But the emphasis on quick publication, without deeper exploration or clean 
fundamentals, didnâ€™t sit well with me.

For context, I was working with LLMs. What surprised me is how many papers get p
ublished even when theyâ€™re essentially hacks â€” a lot of prompt engineering, and observations that openly admit â€œWe donâ€™t
 know why this works, but it does.â€ I respect research, but it started to feelâ€¦ unfulfilling. I wasnâ€™t enjoying it, and 
I had no time to work on projects of my own.

Now Iâ€™m planning to quit the lab. But hereâ€™s where Iâ€™m conflicted: it seem
s like most ML jobs require a strong research profile â€” X papers in NeurIPS, ICML, etc. Part of me wonders if I should s
tick with it, keep my head down, and publish papers just to â€œcheck the box.â€ But then I remember why Iâ€™m here in the fir
st place: I genuinely enjoy ML, especially when Iâ€™m building things that matter, not just chasing publications.

To thos
e whoâ€™ve been down this road: Am I sabotaging my career prospects by walking away from research so early? Is it better t
o focus on building meaningful projects, even if they donâ€™t come with a DOI? Or am I missing something about the value o
f sticking it out in the lab?
```
---

     
 
all -  [ Legal Techâ€™s Data Dilemma: Trust, Betrayal, and Competition. ](https://www.reddit.com/r/legaltech/comments/1hgmxc4/legal_techs_data_dilemma_trust_betrayal_and/) , 2025-01-03-0913
```
Ilya Sutskever, co-founder of OpenAI, recently highlighted a critical issue at the NeurIPS 2024 conference: the AI indus
try is facing a data scarcity problem, often referred to as 'peak data.' Despite advancements in computing power, the av
ailability of high-quality training data is becoming a bottleneck for AI development. Sutskever emphasized that syntheti
c data, while a potential solution, does not fully address this challenge.

In this landscape, companies promising not t
o mine your data face immense pressure to break that pledge. The competitive advantage of leveraging vast, real-world da
tasets is simply too great to ignore. Discarding millions of dollarsâ€™ worth of high-quality dataâ€”data that could refine 
models, boost performance, and outpace competitorsâ€”is a hard sell for any profit-driven firm.

And here lies the uncomfo
rtable truth: no amount of compliance paperwork, signed audits, or certifications can fully guarantee your dataâ€™s safety
. Unless you examine production code directly, thereâ€™s no way to ensure that your data isnâ€™t being anonymized and quietl
y used to train systems. Unlike static cloud storage, generative AI operates on a completely different scale. Its rapid 
feedback loops and massive bandwidth allow companies to quickly organize and refine reinforcement-learning-grade dataset
sâ€”even with anonymized or de-identified data.

Weâ€™re decisively moving from the compute era to the data era of AI, where
 success is no longer about the size of your GPU cluster but the quality of your post-training data. In this new paradig
m, aligning models with the correct data is essentialâ€”placing tools for data curation, human supervision, and evaluation
 at the heart of AI development.

The legal tech industry must take heed: make sure you own your AI. AI in the cloud is 
not aligned with youâ€”itâ€™s aligned with the company that owns it. To protect sensitive data and retain control, on-premis
e solutions and transparent practices are no longer optionalâ€”they are imperative.

[NeurIPS 2024 conference](https://pre
view.redd.it/k32axcw1lh7e1.jpg?width=2048&format=pjpg&auto=webp&s=3887497b862f3190153705674c943c94697a7bd3)


```
---

     
 
all -  [ ChanceMe : Asian Male CS ğŸ™ğŸ™ğŸ™ ](https://www.reddit.com/r/chanceme/comments/1hgkgzi/chanceme_asian_male_cs/) , 2025-01-03-0913
```
I think its a strong application but holy shit my GPA is eating at my confidence rn, please chanceme would be much appre
ciated

Currently a junior - any senior related stuff is likely predictions - chanceme as such

**Demographics:**Â   
Mal
e, Indian, Southern USA, Looking at top CS programs, potentially recruited for d2 level swim, upper middle class income


**Intended major(s):**

CS w a focus on AI/ML, EECS

**Academics:**

* **SAT:**Â 1600 superscore
* **Class rank:**Â top 2
0% atleast, most likely top 15%
* **UW/W GPA: 3.9/6.05 on a 6.7 scale**
   * My school grades by semester, so i have 63 
total semester grades of which 6 are B's.
* **Coursework:**Â 
   * AP Human geo, Discrete Math
   * AP physics, AP CSA, A
P Precalc, AP World history, Linear Algebra
   * AP Lang, AP Physics C:Mech, AP Calc AB, AP Stats, APUSH, AP CSP
   * AP
 Lit, AP Physics C: EM, AP Calc BC, Multivariable Calc & Differential EQ, AP Macro/Micro, AP Gov
      * 5's on all exce
pt a 4 on AP human geo
* **Awards:**
   * USACO Plat
   * USAJMO Qual
   * ISEF grand award
   * USAPhO Medallist (Not g
old)
   * Published research in a major CS conference (Neurips level but not neurips)
   * Deciding between All state ja
zz saxophone or a business related award,

**Extracurriculars:**

* AI/ML startup - 5 figure revenue, interviewed by Y c
ombinator, abt 15k users
* Software/CSE Intern at a major company working in lawtech
* Nonprofit that provides olympiad 
tutoring and classes for free, about 500 hours taught total, 15 volunteers, personally taught about 100 hours early on
*
 d2/3 swim, some minor schools reaching out, 25 second 50 free
* Math competition club president - major club with about
 100 members, schools ranked pretty high nationally in this one competition
* Speech and debate congressional debate com
petitor, finaled some pretty big tournaments and have accumulated 15 bids to TOC
* AI ML publication - if not in awards 
ill mention here, did prompting research and technical research with a PhD at berkeley - through cold emailing
* Patent 
for a novel bio-based material, considering commercializing but probably not
* Jazz saxophone player for abt 8 years, pl
ay a lot for fun and am decent at it, considering putting it on like a portfolio
* Youtube channel with 1.1 million life
time views and about 5k subscribers, where I post music and education content

**Schools:**

* Basically the top 20 CS s
chools, preference on MIT/CMU/Stanford

LOR

10/10 - research teacher

10/10 - Math teacher

7/10 - Saxophone tutor

Ess
ays  
Common app - 8/10 - not an amazing writer but fairly good feeling about it  
Supps - didnt write yet - re: current
 junior

**H**ad a shitty GPA because close family member died first semester of sophomore year, so I got some B's, and 
got into some disciplinary trouble around that time with the school, nothing major, no suspension etc. all the classes w
ith B's were A+ next semester.

I feel decently confident, but im fairly worried about my GPA and how it measures up her
e, I go to a rich private school full of tryhards :()

Would really appreciate yall's thoughts on this :D
```
---

     
 
all -  [ Valence & Recursion Sweep Awards at Foundation Models for Science Workshop at NeurIPS ](https://www.reddit.com/r/RecursionPharma/comments/1hfo8d6/valence_recursion_sweep_awards_at_foundation/) , 2025-01-03-0913
```
https://preview.redd.it/0jg8zf1sv87e1.png?width=1200&format=png&auto=webp&s=951c8af14784f575ef01a613f15a8c80e2f73d17

Th
is past weekend at NeurIPS, Valence Labs and Recursion won first, second and third place awards at the Foundation Models
 for Science workshop. These foundation models offer breakthroughs in leveraging machine learning to better model the in
tersection of biology and chemistry necessary to improve and scale AI drug discovery. 

**First place** was awarded to a
 paper on MolPhenix, a foundation model that can predict the effect of any given molecule and concentration pair on phen
otypic cell assays and cell morphology by integrating phenomics data with chemistry data. Over the past decade, Recursio
n has generated billions of phenomics images through automated, high-throughput experiments. Paired with new phenomics f
oundation models like Phenom-1, Recursion can extract meaningful representations from these high-dimensional images to b
uild Maps of Biology, allowing them to navigate which molecules and genes map to the same space of morphological changes
. MolPhenix mines that rich data and delivers 10X improvement over previous methods â€“ from 7.9% to 77.3% on the Top 1% r
ecall of active molecules. **Paper:** [https://www.arxiv.org/abs/2409.08302](https://www.arxiv.org/abs/2409.08302?fbclid
=IwZXh0bgNhZW0CMTAAAR1DNiyBVjVqVLuNtS9J5NV1MFqG6gxWIEMc8VxgmY_h1F61pVC1QMYBx5E_aem_eMEc0w9oQCwp4X5KhTwjSA)

**Second pla
ce** was awarded to a paper on Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as
 building blocks to explore drug-like chemical space more comprehensively. The model uses an unsupervised pre-training a
pproach using offline drug-like molecule datasets, conditioning A-GFNs on inexpensive yet informative molecular descript
ors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve 
as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. **
Paper:** [https://arxiv.org/abs/2409.09702](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F2409.09702%3F
fbclid%3DIwZXh0bgNhZW0CMTAAAR1TFhx9hYXN7h0fUw0yz-jj1XbKnG5VVmeGbhBjXq3D_mI-0EaoAFSuPdM_aem_DSvWO2X4lUCyArxe25tAxg&h=AT3p
MSPJmPbXaJMGgs_or31Oprx-0d6DEJfLPfDpG7eaT2mbKba97Q38wnlIRWtiiybJk6-ebgL6mKUxVGSjRbxBmELjxYpjXNjdHgL29VeAnS-hGjDCqAKKMQMj
w7aG1g&__tn__=-UK-R&c[0]=AT0iM8paHD3SnWn3-5OATLDMYPKlw0l87qqsKblkdsjHHaCw_MGxpfhRaSEvxsy0AUorpVBjv6c2ceWmePe-Jl-BcrAc3Ma
QXhUG9VtgoQ9q8Jm8kZAX4gjgN8BWRSb9QeIYE2XwWCLl8bqvpU0AKz5RAGcDYz5s2BukDaagpBsjOI07tAnlVJ-4_HtQZ-b4sEweKQ) 

**Third place
** was awarded to a paper on the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter 
ViT-G/8 MAE trained on over 8 billion microscopy images that achieves a 60% improvement in linear separability of geneti
c perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate co
nsistency benchmarks. **Paper:** [https://arxiv.org/abs/2411.02572](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.o
rg%2Fabs%2F2411.02572%3Ffbclid%3DIwZXh0bgNhZW0CMTAAAR1TFhx9hYXN7h0fUw0yz-jj1XbKnG5VVmeGbhBjXq3D_mI-0EaoAFSuPdM_aem_DSvWO
2X4lUCyArxe25tAxg&h=AT2mulqFETtdCih5m7gG70wHri8uodEz6hXHak0aAKc6GwUbm4BJnRDlDjN6L8dYBc_aXrweThNM8xEMPZ5GqovHoSWE3w2YETVv
k99BY8TWSX6EyOpJw5-OvDXcHdj2fw&__tn__=-UK-R&c[0]=AT0iM8paHD3SnWn3-5OATLDMYPKlw0l87qqsKblkdsjHHaCw_MGxpfhRaSEvxsy0AUorpVB
jv6c2ceWmePe-Jl-BcrAc3MaQXhUG9VtgoQ9q8Jm8kZAX4gjgN8BWRSb9QeIYE2XwWCLl8bqvpU0AKz5RAGcDYz5s2BukDaagpBsjOI07tAnlVJ-4_HtQZ-b
4sEweKQ)

\#NeurIPS #NeurIPS2024 #ML
```
---

     
 
all -  [ NeurIPS conference in Vancouver draws 16,000 AI researchers $META ](https://www.reddit.com/r/alertscreener/comments/1hfitr5/neurips_conference_in_vancouver_draws_16000_ai/) , 2025-01-03-0913
```
With major companies like Meta $META, Alphabet $GOOGL, and Microsoft $MSFT showcasing their latest AI advancements and p
roducts.

### Follow [@alertscreener](https://x.com/intent/user?screen_name=alertscreener) for more
```
---

     
 
all -  [ Last Evening in Vancouver: Must-See Experiences Before Heading Back? ](https://www.reddit.com/r/askvan/comments/1hf3fah/last_evening_in_vancouver_mustsee_experiences/) , 2025-01-03-0913
```
I've been here for a week attending NeurIPS 2024 and will be heading back Toronto tomorrow. I only have this evening lef
tâ€”what's the must-see or unique experience I shouldnâ€™t miss to avoid regretting it later
```
---

     
 
all -  [ Last Evening in Vancouver: Must-See Experiences Before Heading Back? ](https://www.reddit.com/r/canadatravel/comments/1hf37do/last_evening_in_vancouver_mustsee_experiences/) , 2025-01-03-0913
```
I've been here for a week attending NeurIPS 2024 and will be heading back Toronto tomorrow. I only have this evening lef
tâ€”what's the must-see or unique experience I shouldnâ€™t miss to avoid regretting it later


```
---

     
 
all -  [ [D] Are We Okay With This? Questionable Poster Behavior at NeurIPS ](https://www.reddit.com/r/MachineLearning/comments/1heo36q/d_are_we_okay_with_this_questionable_poster/) , 2025-01-03-0913
```
This was my first year at NeurIPS. Itâ€™s inspiring to see so much cutting-edge research being presented, but something tr
oubling caught my attention during the poster sessions that I feel compelled to share, especially given [the recent inci
dent with Rosalind Picard](https://www.reddit.com/r/MachineLearning/comments/1hdxbru/d_what_happened_at_neurips/).

Gett
ing a paper accepted at NeurIPS is a huge achievement. Each poster spot represents so much hard work and is highly covet
ed.

I saw two posters that *shouldnâ€™t* have been there, and it has left me wondering about the exploitation of these sp
aces.

**Illegal Poster #1:** [Generative Boba](https://x.com/BoyuanChen0/status/1778565953627775453). This was a â€œcute,
 look at meâ€ poster, but it also featured a QR code linking to the creatorâ€™s X/Twitter. While the poster itself was plac
ed on a side wall in the exhibition hall and not in an official poster spot (when I saw it anyway), it still felt odd. W
hy did they make this poster? Was this about sparking joy, or gaining attention and followers?

[Illegal Poster #1: Gene
rative Boba.](https://preview.redd.it/u3vfvszkoy6e1.jpg?width=3363&format=pjpg&auto=webp&s=8c09ddda45e0ac002223dadf0eac4
165bfdc0433)

**Illegal Poster #2:** [Benchmarkthing](https://x.com/xdotli/status/1867823150068535797)**.** This was far
 more concerning. It blatantly promoted a new AI startup, mentioning funding by a prominent figure in our field, Jeff De
an. Unlike the boba poster, this could visually pass as a real NeurIPS poster. Probably most passersby didnâ€™t give it a 
second thought, but the poster's presenter (who is also the companyâ€™s founder) was essentially promoting his new startup
, sometimes to a significant audience size AND across *multiple* poster sessions. This feels deceptive and exploitative 
â€” gaming the trust of the community to cheatingly gain visibility in a sacred academic space.

[Illegal Poster #2: Bench
markthing.](https://preview.redd.it/qn7vpos4py6e1.jpg?width=2646&format=pjpg&auto=webp&s=4cfd1aa535bdf74cdb57ba8e44f1fa8
13b9d28a7)

A different type of gaming involves authors putting up their poster at unused spots while leaving a sign in 
their formally assigned location that says â€œSee poster at #{better spot}â€. If the authors for the unused spot arrived, t
heyâ€™d just move their poster back â€” but if not, they would presumably revel in the extra attention from being located, f
or example, closer to the hallâ€™s entrance with more foot traffic.

Relocating posters still seems problematic, but at le
ast the posters *belong* at the conference. On the other hand, I feel much more strongly that unauthorized posters for p
ersonal or commercial promotion hurts the integrity of the space, disrespects the presenters whose posters truly belong 
there, and undermines the conference overall.

Questions for the community:

1. Should there be stricter policies or bet
ter enforcement for poster sessions?
2. How do we differentiate between minor gaming (e.g. relocating posters) and outri
ght exploitation (e.g. unauthorized posters)?
3. Is it fair to tolerate some flexibility as long as the intentions are l
ighthearted or still academic?Â 
4. How do we address these behaviors moving forward? Should there be consequences?
```
---

     
 
all -  [ A little bit of drama: Pre-training is only over if you have no imagination - Logan Kilpatrick ](https://www.reddit.com/r/singularity/comments/1he9tsn/a_little_bit_of_drama_pretraining_is_only_over_if/) , 2025-01-03-0913
```
https://x.com/OfficialLoganK/status/1868002617311596552?t=uNazJ-3HPuWlBrXGagkAag&s=19

It's a slow saturday so why not s
hitpost a little.
This is in response to Ilya Sutskever's talk during NeurIPS 2024.
```
---

     
 
all -  [ A Perfect Storm for AI Inference TPU will be new king  ](https://www.reddit.com/r/Bard/comments/1he4e14/a_perfect_storm_for_ai_inference_tpu_will_be_new/) , 2025-01-03-0913
```
Ilya Sutskever's recent bombshell at NeurIPS â€“ [https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-op
enai-model-data-training](https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-traini
ng) that we've reached 'peak data' and the era of pre-training as we know it is ending â€“ has sent ripples through the AI
 world. His vision of a future dominated by 'agentic,' reasoning AI, capable of learning from limited data, sets the sta
ge for a fundamental shift in how we develop and deploy artificial intelligence. This shift, it turns out, might just be
 the perfect storm for the rise of the TPU and Broadcom's latest chip announcement could be the catalyst.Â [https://www.b
roadcom.com/company/news/product-releases/62691](https://www.broadcom.com/company/news/product-releases/62691)

**Why TP
Us are Poised to Shine in a Post-'Peak Data' World:**

1. **Inference, Inference, Inference:**Â Sutskever's emphasis on a
 future where AI is smarter, not just bigger, puts the spotlight squarely onÂ **inference**. This is where AI applies its
 learned knowledge to make predictions and decisions in real-world scenarios. And this is precisely where TPUs have a di
stinct advantage.
2. **Enter Broadcom: The 3.5D XDSiP and the TPU Advantage:**Â Broadcom's new 3.5D chip isn't just anoth
er incremental improvement; it's a potential game-changer, especially for TPUs. Its innovative design, featuring vertica
l die stacking and face-to-face interconnects, directly addresses the key challenges of inference:
   * **Latency Killer
:**Â By drastically reducing the distance data needs to travel, Broadcom's chip minimizes latency, enabling the rapid-fir
e calculations that TPUs are built for. This is crucial for real-time inference applications.
   * **Power Saver:**Â The 
3.5D architecture slashes power consumption, a critical factor for deploying TPUs in data centers and edge devices where
 energy efficiency is paramount.
   * **Density Champion:**Â The compact form factor allows for denser packing of TPUs, p
aving the way for more powerful and efficient inference systems.

The potential rise of TPUs, spurred by the need for mo
re efficient inference and enabled by innovations like Broadcom's, could trigger a paradigm shift, compelling all major 
players in the AI field to develop their own specialized chips and hardware solutions to remain competitive in this rapi
dly evolving landscape. This may be the dawn of the age of custom AI silicon, and potentially the beginning of the TPU e
ra.
```
---

     
 
all -  [ Ilya Sutskever, cofondateur et ancien directeur scientifique d'OpenAI, a fait une rare apparition pu ](https://www.reddit.com/r/actutech/comments/1hdxdjs/ilya_sutskever_cofondateur_et_ancien_directeur/) , 2025-01-03-0913
```
Il a notamment affirmÃ© que le prÃ©-entraÃ®nement des modÃ¨les tel que nous le connaissons va inÃ©vitablement prendre fin, co
mparant les donnÃ©es Ã  un 'combustible fossile' limitÃ©. Selon lui, nous avons atteint un pic des donnÃ©es disponibles, car
 il n'existe qu'un seul internet.  
  
Pour l'avenir, il prÃ©dit que les prochaines gÃ©nÃ©rations d'IA seront plus 'agentiq
ues' et capables de raisonner vÃ©ritablement, contrairement aux systÃ¨mes actuels qui se contentent principalement de reco
nnaÃ®tre des motifs. Ces systÃ¨mes deviendront plus imprÃ©visibles Ã  mesure qu'ils dÃ©velopperont leur capacitÃ©.  
  
Il a Ã©
galement Ã©tabli un parallÃ¨le intÃ©ressant entre l'Ã©volution de l'IA et la biologie Ã©volutive, suggÃ©rant que l'IA pourrait
 dÃ©couvrir de nouvelles approches de mise Ã  l'Ã©chelle, tout comme l'Ã©volution a trouvÃ© un nouveau modÃ¨le pour le cerveau
 des hominidÃ©s.

https://preview.redd.it/2ly7b8fejr6e1.jpg?width=960&format=pjpg&auto=webp&s=ae6cfe8241a9fc37bf648d189d8
2908a8624094c


```
---

     
 
all -  [ [D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams ](https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/) , 2025-01-03-0913
```
Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the
 other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his col
leagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.git
hub.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is
 asserted and if this is true this would be nice to confirm.
```
---

     
 
all -  [ Feels good to see Mr.X getting noted ](https://i.redd.it/oexht6c8rg6e1.jpeg) , 2025-01-03-0913
```
Link: https://x.com/elonmusk/status/1866797259968614885?s=46
```
---

     
 
MachineLearning -  [ [D] How to make friends and network at NeurIPS? ](https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/) , 2025-01-03-0913
```
Iâ€™m attending NeurIPS for the first time and itâ€™s quite overwhelming seeing the amount of people and so many recruiters.
 I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is
 here.

I didnâ€™t really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach
 others who are in big groups of people and (2) Iâ€™m feeling strong imposter syndrome and under-qualified for the jobs re
cruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other 
students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to
 potentially meet up and have a chat? Iâ€™m a 3rd year PhD student from the UK, but from Vancouver myself so know lots of 
stuff going on in the area. Cheers!
```
---

     
 
MachineLearning -  [ [R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effe ](https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/) , 2025-01-03-0913
```
We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**:Â [https://arx
iv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**:Â [https://github.com/trungtrinh44/DAMP](https://gi
thub.com/trungtrinh44/DAMP)  
**DAMP**Â (Data augmentation via multiplicative perturbations) is a simple yet effective ap
proach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augme
ntation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness withou
t compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Found
ation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations
, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires onl
y random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while
 being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transforme
rs from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet wit
hout complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly imp
roves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corrupti
on error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58
.83% baseline)

**Why DAMP?**Â Unlike traditional approaches that rely on complex data augmentation pipelines or computat
ionally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustnes
s. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing 
techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over sta
ndard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical disc
ussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond co
mputer vision!
```
---

     
