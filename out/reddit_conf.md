 
all -  [ Am I cooked? Pursuing a PhD in Statistics ](https://www.reddit.com/r/gradadmissions/comments/1ht0lug/am_i_cooked_pursuing_a_phd_in_statistics/) , 2025-01-04-0912
```
Hey everyone, I'm looking for advice on pursuing a PhD in Statistics. Wondering if I'm cooked or not. 

**Background**


* **BS in Statistics (state school, 3.3 GPA)** with some undergrad ML research
* **Online MS in CS (3.3 GPA)**, purely c
oursework, finishing soon
* **2+ years of research** at a national lab, with an accepted NeurIPS workshop paper (in appl
ied probabilistic ML)

I'd like to stay at the national lab or do similar work, but a PhD is required for both. I'm worr
ied my grades might keep me out of stats programs and limit my research career.

I plan to apply this fall and want to f
igure out how best to address my lower GPAs. Are my options limited to getting another MS in Stats at a lower-ranked sch
ool to improve my GPA, or should I pivot to do PhD in CS? I'd really appreciate any insight or advice from anyone who’s 
been in a similar situation.
```
---

     
 
all -  [ Deurbel gong ](https://www.reddit.com/gallery/1hsp7w7) , 2025-01-04-0912
```
Ik heb deze deurbel gong gekocht. Een Byron 776. Ik heb hem aangesloten op 0 & 2 zoals beschreven in de handleiding. Dan
 werkt de bel niet als ik op de knop druk.

Per ongeluk heb ik ontdekt dat wanneer ik de draden aansluit op 0 & 3 de bel
 wel afgaat. Wat gaat er mis?

Belangrijk om te weten: de bovenste twee draden komen van mijn deurbeltrafo. De onderste 
twee zijn rechtstreeks geschakeld op 230v.

De deurbel wordt geschakeld door een relais dat ik heb geprogrammeerd met ee
n esp32. In mijn oude setup werkte dat perfect.

Wat gaat er mis? 

Linkje naar handleiding: https://www.smartwares.eu/d
e-de/product/attachment?productId=DBW-23081&attachmentName=b2f69f3b-f7da-40a4-9fa9-f33d7353edc5__00.640.82_IM.pdf 
```
---

     
 
all -  [ How Can I Start Publishing Research Papers in AI/ML as a Third-Year Student? ](https://www.reddit.com/r/AICareer/comments/1hsmajj/how_can_i_start_publishing_research_papers_in/) , 2025-01-04-0912
```
**Hi everyone,**

I’m a third-year Computer Science and Engineering student specializing in Artificial Intelligence and 
Machine Learning (AI/ML). I’ve recently developed an interest in publishing research papers but don’t know where or how 
to start.

While I understand what a research paper is, I’m unfamiliar with the detailed process of creating and publish
ing one. I’m eager to contribute to the field of AI/ML but currently don’t have a specific topic or idea in mind.

Here 
are some areas where I’d appreciate guidance:

1. **Choosing a Research Topic**:
   * How can I identify a research-wort
hy problem in AI/ML?
   * What are some impactful areas in AI/ML that are suitable for someone starting out in research?

2. **Getting Started**:
   * How should I approach my first research paper?
   * What steps should I take to ensure my 
work is structured and valuable?
3. **Learning Resources**:
   * Are there any books, courses, or tools you recommend fo
r learning about the research and publication process?
   * How can I effectively read and analyze existing research pap
ers to deepen my understanding?
4. **Collaboration and Mentorship**:
   * Should I seek out a mentor or collaborator? If
 so, what’s the best way to approach professors or peers for guidance?
5. **The Publishing Process**:
   * What are the 
steps involved in publishing a research paper in AI/ML?
   * How do I select the right platform or conference for my pap
er (e.g., NeurIPS, arXiv)?
   * What strategies can improve the likelihood of my paper being accepted?
6. **Tools and Me
thodologies**:
   * Which tools, libraries, or frameworks in AI/ML are essential for conducting research?
   * How do I 
organize and write my paper effectively?
7. **Mistakes to Avoid**:
   * What are some common pitfalls to watch out for w
hen writing or publishing research papers?

I’m excited to embark on this journey and would be grateful for any advice, 
resources, or personal experiences you can share. Thank you!
```
---

     
 
all -  [ 【扯淡】讨论政治得去技术（含金融技术）论坛。雄性这玩意儿极易走偏，但是如果当工具用那还是可以用。Reddit 上就好好讨论工作生活细节琴棋书画，建政去技术论坛有个现实底子托着不至于走火入魔。 ](https://www.reddit.com/r/wenjin/comments/1hscikt/扯淡讨论政治得去技术含金融技术论坛雄性这玩意儿极易走偏但是如果当工具用那还是可以用reddit/) , 2025-01-04-0912
```
本意是回顾下技术八卦：字节跳动（中国部分）某雄性实习生，被曝出暗中破坏其他组大模型训练。可能是蓄意使绊子以保证自己领先，可能不满 GPU 资源分配不足？也有各种其他说法，这不重要，流言就是流言，处于真实和虚构重叠态。有个现实是，[此实习生后
来获比较重要奖项](https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/)。
结果相关帖子里，看到有华人雄性给美国雄性科普，居然还有点常识，当然也不能深究。真浪费人生啊……   

P.S.  
看了下此实习生的获奖论文，内容充实，是挺好的小改进，但颁奖方某会议绝不像 36Kr 那篇文章吹嘘，“人工智能界的诺贝尔奖”
。事实上，该会议常年**充斥各种灌水论文（一年大几千篇），喜欢跟风发奖，[对真正的基础性成果论文反倒辨识不足](https://www.sohu.com/a/689374605_701814)**（比如，这波大模型风口的基础 “Transf
ormer 这篇论文虽然现在影响力很大，但在当年的全球顶级 AI 会议 NeurIPS 2017 上，连个 Oral 都没拿到，更不用说拿到奖项了”）。该会议更像是医疗药物器械、房地产、传销行业大会——这些大会也没啥不好啊，行业交流，吃吃喝
喝，促进感情。从架构创新上来说，70 年代 vanilla 神经网络、80-00 年的 CNN 卷积网络、2014-2018 年的 transformer 算是三大步，其他都是小改进。     

&nbsp;   
&nbsp;  
  

---       
 
markus_zhang 73 days ago | parent | prev | next [–]

China does have a tight information control but it ma
y not be what you think it is.
All communication software (QQ/Wechat are the two most used) have sort of backend scanner
 that detects topics that are in the 'in-season' blacklist and ban accounts accordingly. No one knows what the list is s
o people could get banned for random reasons, but in general bashing current policies or calling out names of the standi
ng members of Politburo is the quickest way to get banned -- and in many instances also got the Wechat group banned.

On
 the other side, surprisingly, there are many contents that are apparently inappropriate floating on the social media wi
thout getting banned. This also throws people off feet.

What I gathered is:

- Don't shit on current party leaders. Act
ually don't discuss current politics at all. AIs don't always recognize contents correctly so you could be banned for su
pporting one side or desisting it at the same time.

- Don't ever try to call up other people to join whatever an unoffi
cial cause, whatever it is. Like, even if it's purely patriotic, just don't do it. You do it and you might go to prison 
very quickly -- at least someone is going to call you to STFU. Grassroot movements is the No.1 enemy of the government a
nd they don't like it. You have to go through official channels for those.

This leads to the following conclusion:

Ess
entially, the government wants as much control as possible. You want to be patriotic? Sure, but it has to be controlled 
patriotic. You can attend party gathering to show your patriotism, but creating your own, unofficial gathering is a big 
No. They probably won't put you into a prison if the cause is legit, but police are going to bug you from time to time -
>

IMO this is how the CCP succeed. It has successfully switched from an ideologic party to an 'All-people' party. **It 
doesn't really care about ideology**. But it wants to assimilate everyone who potentially can be out of control. If you 
are a successful businessman, it will invite you to participate in political life. If you are an activist who can call u
p thousands of people, it wants you in. It is essentially, a cauldron of elitists. **It has nothing to do with 'Communis
m'. It is essentially, GOP + DEM in the US**.

```
---

     
 
all -  [ Advice Needed: To pursue PhD with a focus in Computer Vision/Deep Learning. Has limited Research Bac ](https://www.reddit.com/r/PhD/comments/1hrf5os/advice_needed_to_pursue_phd_with_a_focus_in/) , 2025-01-04-0912
```
Hello everyone!

I’m a master's graduate currently on F1 STEM OPT in the U.S. and I’d love your input regarding my plans
 to pursue a PhD in Computer Vision/Deep Learning. Here’s a snapshot of my journey and where I’m at:

**Background & Pro
jects:**

* Bachelors in Mechatronics Engineering (built a hexapod robot, worked on object detection for fish tracking, 
dabbled in PLCs, CAD, and embedded systems).
* 2 years of industry experience at TCS as a Data engineer (not directly re
levant to AI research, but it gave me decent programming and data skills).
* Master’s in Robotics & Autonomous Systems f
rom ASU: did course projects in pose estimation, object detection, one-shot imitation learning, image classification, pl
us a Raspberry Pi-based IoT security project.
* After graduation, I volunteered on stereo vision and depth estimation pr
oject, and also on drone path planning/gesture recognition.

**Current Situation:**

* I have no formal research publica
tions or major conference presentations.
* My student loans are significant, so I’m cautious about finances.
* I’m deepl
y interested in modern CV trends (Nerfs and 3D Reconstruction, ViTs, VLMs, Generative AI for images and videos, Percepti
on in Autonomous vehicles etc.,).
* I’m on STEM OPT until mid-2026, so I’m working full-time to stay afloat while trying
 to map out a PhD plan.

**Main Concerns:**

* **Lack of Publications:** Many people I see in top PhD programs have publ
ished at workshops or conferences like CVPR/ICCV/NeurIPS. Has anyone started a PhD with no prior publications and succes
sfully ramped up?
* **Timeline:** Also, I’m debating whether to target Spring 2026 or wait until Fall 2026 to give mysel
f more time for research experience and financial prep. Is postponing to Fall 2026 a good move? I’d love any tips on vol
unteering for remote research while working full-time, especially on how to reach out to professors or labs if you’ve do
ne something similar!
* **Research Portfolio Building:** I’m willing to work on open-source projects, replicate state-of
-the-art papers, or collaborate remotely with labs. I’d appreciate any advice on how to make a strong case for admission
s without an official publication record.

**Questions for You:**

* Has anyone been in a similar boat—entering a PhD wi
thout prior publications—and successfully built a solid research portfolio along the way?
* Any tips on balancing full-t
ime employment with research activities, especially to get a letter of recommendation that speaks to my research potenti
al?
* How did you manage finances, especially if you had existing student debt or limited savings, during the first year
 or two of your PhD?
* Any personal experiences navigating U.S. visa transitions from OPT back to F-1 for a PhD program?


I’d really value input from anyone who faced similar hurdles or from current PhD students who can share what ultimatel
y worked for them. Thanks in advance for reading, and I look forward to any advice or shared experiences!
```
---

     
 
all -  [ [Profile Review] MSCS / DS Fall'25 ](https://www.reddit.com/r/MSCS/comments/1hp3hbr/profile_review_mscs_ds_fall25/) , 2025-01-04-0912
```
Hi, this is my profile with colleges I've applied to, I need strong feedback because I'm kinda concerned now xD.

* GRE:
 323 (168Q + 155V)
* TOEFL: 114 (R:29, L:30, S:29, W:26)
* Undergraduate: B.Tech. CS at BITS Goa (2025), idk if it's Tie
r 1 / 2
* GPA: 9.22 (I have some Bs on my CS courses)
* Papers: 1 published at a NeurIPS workshop (3rd author), rest are
 under review, I have 4 preprints (2 at B confs and one at CVPR (A\*) and one at an AAAI (A\*) workshop, I am first the 
author on all of these).
* Research Exp 1: I was a DAAD WISE scholar in my 3rd year summer (quite selective scholarship)
 did my summer research in Germany, very productive work, this was submitted to CVPR, I got a very strong LoR from the P
I (h-index \~20).
* Research Exp 2: I worked with the AI lab at BITS for over a year-and-a-half now, got 3 preprints fro
m there (as I detailed), I topped courses (top-5 / 1st rank) these profs took + TA'd them (quite involved in my TA work)
. Got 2 equally strong LoRs from here (one at h-index \~40 and one at h-index \~15).
* Research Exp 3: I interned at a s
tartup for about 5 months working on AI for materials science, decent work, I have some open-source contribs here, and b
efore interning I worked informally which led to that NeurIPS workshop acceptance. Moderate LoR from here (the CEO has h
-index \~25 and he gave an LoR).
* Extra: I am the president of the AI club here (was the Gen Sec before), have organize
d symposiums, made assignments, gave talks, etc. I also started a blog-posts initiative, our first blog-post was accepte
d at an ICML (A\*) workshop.

Finally, Colleges: UPenn MS CIS, UCSD MS DS + CSE, CMU MS ML, UCLA MS CS, UT Austin MS CS,
 UIUC MS CS, UW Madison MS CS, Princeton MSE CS, Purdue MS CS, Yale MS CS (2 year), UMich MS CSE, Stanford MS DS + Stats
.

I have a Full-time job offer in hand, which is why I was going all ambitious, but now I'm kinda conflicted if I reall
y want the job. Let me know my realistic chances (I'll go anywhere I am accepted from these), you can be upfront xD. And
 feel free to suggest any more if their deadline hasn't already passed.

Thanks!
```
---

     
 
all -  [ There is no model that is phd level until it can write a paper that is accepted to Neurips on its ow ](https://www.reddit.com/r/ChatGPT/comments/1hopxko/there_is_no_model_that_is_phd_level_until_it_can/) , 2025-01-04-0912
```
Pretty much the title. But ultimately phd level means you can write a paper that gets accepted to a top conference. Unti
l it can come up with a new and novel idea to get submitted to a conference, I don’t think any model can claim phd level

```
---

     
 
all -  [ DeepSeek-v3 | Best open-source model on ProLLM ](https://www.reddit.com/r/LocalLLaMA/comments/1ho5ave/deepseekv3_best_opensource_model_on_prollm/) , 2025-01-04-0912
```
Hey everyone!

Just wanted to share some quick news -- the hype is real! DeepSeek-v3 is now the best open source model o
n our benchmark: [check it here](https://prollm.ai/leaderboard/stack-unseen). It's also the cheapest model in the top-10
 and shows a 20% improvement across our benchmarks compared to the previous best DeepSeek model.

If you're curious abou
t how we do our benchmarking, we published a [paper at NeurIPS](https://arxiv.org/abs/2412.05288) about our methodology.
 We share how we curated our datasets and conducted a thorough ablation on using LLMs for natural-language code evaluati
on. Some key takeaways:

* Without a reference answer, CoT leads to overthinking in LLM judges.
* LLM-as-a-Judge does no
t exhibit a self-preference bias in the coding domain.

We've also made some small updates to our leaderboard since our 
last post:

* Added new benchmarks (OpenBook-Q&A and Transcription)
* Added 15-20 new models across multiple of our benc
hmarks

Let me know if you have any questions or thoughts!

Leaderboard: [https://prollm.ai/leaderboard/stack-unseen](ht
tps://prollm.ai/leaderboard/stack-unseen)  
NeurIPS paper: [https://arxiv.org/abs/2412.05288](https://arxiv.org/abs/2412
.05288)

https://preview.redd.it/ibvp9yjk8l9e1.png?width=1060&format=png&auto=webp&s=1f638d3970baf912ae03b5f0073595ff033
be4ab
```
---

     
 
all -  [ Wyzwanie: sztuczna inteligencja! - z Barbarą Rychalską rozmawia Dobrosława Gogłoza ](https://www.reddit.com/r/libek/comments/1ho3uoy/wyzwanie_sztuczna_inteligencja_z_barbarą/) , 2025-01-04-0912
```
[Wyzwanie: sztuczna inteligencja! - z Barbarą Rychalską rozmawia Dobrosława Gogłoza - Liberté!](https://liberte.pl/wyzwa
nie-sztuczna-inteligencja-z-barbara-rychalska-rozmawia-dobroslawa-gogloza/)

**Dobrosława Gogłoza: Chciałabym zacząć od 
zadania sobie pytania o to, jak znaleźliśmy się w tym momencie w historii – jakie były największe przełomy w działaniu s
ztucznej inteligencji w ostatnich latach?**

Barbara Rychalska: Jeśli chodzi o językowe modele GenAI, czyli popularne LL
M-y (*large language models*, pol. wielkie modele językowe), przełomem było opracowanie nowego modelu nazwanego transfor
merem. Dokonał tego zespół Google’a – co ciekawe, z udziałem polskiego naukowca, Łukasza Kaisera. Przed opracowaniem tra
nsformera istniały już różne, zaawansowane modele języka naturalnego, oparte na sieciach neuronowych. Czytały one jednak
 zdania token po tokenie (w przybliżeniu – słowo po słowie), co powodowało stopniowe zanikanie informacji. W transformer
ze natomiast zastosowano nowatorski mechanizm atencji. Umożliwia on wczytywanie wielu tokenów naraz, w ramach jednej „po
rcji”. Ponieważ transformer analizuje relację każdego tokenu wobec każdego innego tokenu w przetwarzanym kontekście, two
rzy bardzo szczegółowy „rozkład uwagi”, który wskazuje, które tokeny są bardziej istotne dla danego kontekstu. W szczegó
lności pozwala to wykryć związki znaczeniowe pomiędzy słowami znajdującymi się daleko od siebie. Ma to również skutki wy
dajnościowe – operację jednoczesnej analizy par tokenów łatwiej jest zrównoleglić niż sekwencyjne czytanie tekstu. Oprac
owanie transformera wywołało lawinę – powstały oparte na tej architekturze modele takie jak BERT, RoBERTa, XLNet oraz wi
ele innych, które osiągnęły znacznie lepsze wyniki niż klasyczne modele sekwencyjne, praktycznie w każdym zadaniu dotycz
ącym języka naturalnego, takim jak wykrywanie emocji w tekście, podsumowywanie go, odpowiadanie na pytania czy tłumaczen
ia. Różnica w jakości ich działania w porównaniu do starszych metod była piorunująca – było widać, że to już nie inkreme
ntalne ulepszenie, a zupełnie nowa jakość.

Następnie okazało się, że w przeciwieństwie do wielu innych typów modeli, zw
iększanie ilości danych treningowych oraz rozmiaru modelu transformerowego nie powoduje szybkiego wypłaszczenia przyrost
ów jakości. Wprost przeciwnie, umiejętności i jakość modelu szybko rosły wraz z rozmiarem danych i liczbą parametrów, je
śli pomiędzy tymi wartościami zachowana była odpowiednia równowaga (tzw. „LLM scaling laws”). Na tym zjawisku opiera się
 sukces firmy OpenAI i modeli z rodziny GPT, a następnie kolejnych, o których słyszymy na co dzień – modeli Mistral, Cla
ude, Llama i innych.

**Czy możesz wyjaśnić, jak działają duże modele językowe (LLM) i jakie są ich obecne ograniczenia?
**

Nowoczesne modele LLM wciąż opierają się na architekturze transformera. Zostały w nich dodatkowo zastosowane liczne 
nowe techniki, takie jak *instruction tuning* czy RLHF (*reinforcement learning from human feedback*) – uczenie modelu r
ozpoznawania intencji pytań i próśb, jakie kieruje do niego użytkownik oraz wykrywania, które odpowiedzi będą bardziej p
ożądane. Pierwsze modele transformerowe uczone były przewidywania następnego tokenu (można sobie to wyobrazić w ten spos
ób, że pokazujemy modelowi niedokończony tekst i prosimy go o uzupełnienie, tak aby całość tekstu była logiczna). Modele
 były więc uczone określania, jakie słowa są najbardziej prawdopodobne w danym kontekście. Jak się okazało w praktyce, z
adanie to ściśle wiąże się z rozumieniem logiki języka, znaczenia słów oraz generowaniem płynnego i poprawnego tekstu. D
zięki metodom takim jak *instruction tuning*, modele są w stanie pójść jeszcze krok dalej – odczytywać intencję pytań uż
ytkownika i spełniać jego cele. 

Nieustannie powstające nowe techniki trenowania pozwalają na wzmocnienie różnych umiej
ętności LLM-ów. Na przykład wypuszczony niedawno model OpenAI „o1” wykazuje daleko wyższe niż wcześniej zdolności do log
icznego rozumowania – dzięki zastosowaniu metody *chain-of-thought*. Polega ona na rozbijaniu skomplikowanych zadań na s
ekwencję niezbędnych kroków rozumowania, które prowadzą do rozwiązania. Dzięki nauce dzielenia problemu na podproblemy, 
model jest w stanie zastosować zdobyte umiejętności do wielu nowych zadań. Oczywiście daleko tu do medialnego stwierdzen
ia, że „modele rozumują na poziomie dorosłego człowieka/studenta/doktoranta/itp.”. Takie wyrażenie jest problematyczne n
a wielu poziomach, nawet tym najbardziej podstawowym, ponieważ nie wiemy dokładnie, w jaki sposób rozumuje punkt odniesi
enia, czyli człowiek.

Nie wszystko jednak zależy od konkretnych celów trenowania. LLM-y mają ciekawą własność znaną jak
o „emergence”, czyli spontaniczne nabywanie nieoczekiwanych umiejętności, do których nie były bezpośrednio trenowane. *E
mergence* występuje przy zastosowaniu odpowiednio dużych danych i przy odpowiednio dużym rozmiarze modelu. Przykładem ta
kiej emergentnej umiejętności jest dokonywanie tłumaczeń czy wykonywanie prostych obliczeń. Przypuszcza się, że LLM-y ta
kie jak GPT nie były trenowane specjalnie do tych zadań, jednak ich ekspozycja na ogromne, wielojęzyczne dane spowodował
a, że są w stanie stworzyć ekwiwalentny znaczeniowo tekst w innym języku oraz rozumieć podstawowe znaczenie liczb. Nie w
iemy, jakie emergentne umiejętności modeli pojawią się w przyszłości.

Źródłem fascynacji LLM-ami są ich ogólnie bardzo 
wysokie zdolności językowe (płynność, poprawność gramatyczna wypowiedzi), duża zasobność pamięci (są w stanie odpowiedzi
eć na wiele pytań z najróżniejszych dziedzin, takich jak historia, prawo, chemia, botanika, tak naprawdę dowolne tematy)
, oraz dobre zrozumienie intencji pytań.

Natomiast braki i zagrożenia związane z LLM-ami to przede wszystkim halucynacj
e (wypowiedzi niepoprawne, zawierające błędy merytoryczne, jednak artykułowane z dużą pewnością siebie). Istnieją na to 
pewne środki zaradcze, jednak nie są w 100% skuteczne. Na przykład, LLM-y często mogą poprawić się i zadenuncjować swoją
 własną halucynację, jeśli dopytamy je, czy są pewne swojej odpowiedzi lub dopytamy je o fragment, który wydaje nam się 
zaskakujący lub dziwny. Warto zatem podważać odpowiedzi LLM-ów i zadawać pytania typu „Czy jesteś pewien, że…?” Niestety
, im bardziej specjalistyczne pytanie, tym większa podatność na błędy. Z moich obserwacji: zadając LLM-om pytania z pozy
cji laika, np. z dziedziny farmacji, możemy być pozytywnie zaskoczeni (dlatego, że nasze pytania będą dosyć proste meryt
orycznie). LLM ogólnego przeznaczenia, taki jak ChatGPT nie poradzi sobie jednak ze specjalistycznymi pytaniami, które z
adałby farmaceuta w związku ze swoją pracą codzienną. Do takich zastosowań warto wykorzystywać usługi wyspecjalizowane w
 odpowiadaniu na pytania, a najlepiej jeszcze podające źródła odpowiedzi (np. darmowe Perplexity) lub modele dziedzinowe
.

Niektórzy pokładają też zbyt dużo zaufania w kreatywność modeli, przypisując im na przykład zdolność tworzenia strate
gii rozwoju firm, umiejętność tworzenia zaskakujących kampanii marketingowych itp. LLM-y mogą na pewno podpowiedzieć wie
le interesujących pomysłów w tych tematach, jednak będą to bardziej propozycje „zdroworozsądkowe” i powtarzalne niż rewo
lucyjne. LLM-y nie są wystarczająco twórcze, poruszają się raczej w obrębie pewnych uśrednionych konceptów, które zaobse
rwowały w swoich danych treningowych. Nie mogą więc póki co przewyższyć człowieka. Inaczej mówiąc – biznesowa porada od 
LLM będzie brzmiała bardziej jak raport McKinsey’a niż strategia, którą sporządziłby osobiście geniusz biznesu, planując
 swój następny ruch.

**Jakie potencjalne korzyści dla społeczeństwa widzisz w dalszym rozwoju AI? A jakie zagrożenia?**


Korzyści widzę na co dzień w swoim własnym życiu, zaczynają się też pojawiać badania dokumentujące przydatność GenAI. 
Nie jest się jednak łatwo przebić przez szum informacyjny. Na początku fali zachwytu pierwszymi modelami GPT utrzymywała
m daleko idącą ostrożność. Zwłaszcza w obliczu pojawiających się jak grzyby po deszczu influencerów, obiecujących cuda. 
Następnie, w odpowiedzi na wszechobecny entuzjazm, pojawiły się grupy „hejterów”, twierdzących, że GenAI jest trendem, k
tóry szybko przeminie.

Tymczasem, w badaniu Harvard Business School pt. „Navigating the Jagged Technological Frontier” 
stwierdzono, że pracownicy są w stanie wykonać zadania szybciej i lepiej (z większą poprawnością) używając AI, jeśli spe
łniony jest warunek odpowiedniego z niej korzystania. Co to znaczy? Otóż zadania stawiane przed uczestnikami zostały spe
cjalnie stworzone w taki sposób, aby część z nich była trudna do wykonania lub „niekompatybilna” z AI (zadania „outside 
the frontier”), zaś część znajdowała się w obrębie możliwości modelu AI (zadania „inside the frontier”). Grupa pracownik
ów, która potrafiła krytycznie oceniać odpowiedzi modelu i z zaangażowaniem wchodzić w dyskusje z modelem, kwestionując 
jego oceny, dobrze radziła sobie z zdaniami „outside the frontier”. Pracownicy, którzy przyjmowali bezkrytycznie odpowie
dzi AI, wykonywali te zadania gorzej. Jeśli chodzi o zadania w obrębie możliwości AI, obserwowano wzrost metryk jakości 
pracy. Optymistyczne doniesienia pojawiają się również na temat wpływu GenAI na pracę programistów – zresztą większość m
i znanych korzysta już z asysty LLM-ów do rozwiązywania albo szczególnie powtarzalnych, bardzo prostych, ale czasochłonn
ych zadań, albo tych związanych z nowymi frameworkami czy narzędziami, które muszą szybko zastosować. W takich sytuacjac
h przed erą GenAI musieli przejść długą ścieżkę prób i błędów. 

Jednocześnie faktem jest to, co już wcześniej wspomniał
am – wiele specjalistycznych branż, takich jak medycyna, różne gałęzie inżynierii, farmacja czy prawo, najpewniej nie sk
orzysta z LLM-ów ogólnego przeznaczenia, bo są one po prostu zbyt słabe do tak specyficznych zastosowań. Branże te potrz
ebują dokładniejszych, dedykowanych im narzędzi.

Na tak fundamentalne przemiany należy jednak patrzeć szerzej niż tylko
 nasza satysfakcja (lub jej brak) ze sprawniejszego wykonywania zadań. GenAI może pomóc rodzicom stworzyć bajkową kontyn
uację przygód ulubionych bohaterów swoich dzieci. Ale tak samo może posłużyć do szybkiego tworzenia tysięcy tekstów, wyg
lądających na tweety pisane przez prawdziwych ludzi, które są gotowe do rozsyłania przez media społecznościowe w przecią
gu sekund. Niezwykle łatwo jest tworzyć nawet te najniebezpieczniejsze, jak agitujące politycznie i szerzące nienawiść w
pisy na forach lub nawet fałszywe zdjęcia i filmy – tzw. *deepfakes*. To już się dzieje – na przykład w Korei Południowe
j mamy w tym momencie do czynienia z masowym tworzeniem pornograficznych *deep fake’ów* z użyciem wizerunków istniejącyc
h osób, często nieletnich. Ich twarze są nakładane na wygenerowane sylwetki. Sprawcami są często szkolni koledzy prześla
dowanych dziewczynek. Wykrywane przez władze kanały na Telegramie, które rozpowszechniają te materiały, miewają po kilka
set tysięcy członków. Niestety, wydaje się, że w dobie generatywnej sztucznej inteligencji dobrze zaprojektowane regulac
je są niezbędne, aby zabezpieczyć nasze dane, prywatność oraz uczęszczane przez nas przestrzenie internetowe przed zalan
iem przez wygenerowane treści.

Często podnoszonym argumentem przeciwko AI jest też zanikanie miejsc pracy. Rzeczywiście
, może dojść do sytuacji, w której trudniej będzie znaleźć pracę osobom dopiero zaczynającym w danej branży, ponieważ pr
ostsze zadania będą rozwiązywane przez AI na zadowalającym poziomie. Uważam jednak, że ostatecznie nie da się polegać wy
łącznie na AI – mimo że nasza praca może być złożona ze względnie prostych podzadań, to kumulacja choćby małych błędów i
 przeoczeń spowoduje ostatecznie duże problemy. Ktoś musi tworzyć prompty i aktywnie pracować z AI, aby dojść do pożądan
ego rezultatu, a następnie weryfikować i ewentualnie poprawiać efekty. Również pracodawcy muszą zdać sobie sprawę, że br
ak edukacji młodszych pracowników spowoduje, że już za 3-5 lat zabraknie specjalistów.

Innym zagadnieniem jest zdolność
 AI do spełniania złożonych kryteriów wykonania danego zadania. Na przykład, AI potrafi generować robiące wrażenie obraz
y, jednak w pracy grafika ważne są konkretne wymagania stawiane przez zleceniodawcę (np. na temat skojarzeń, które mają 
wywoływać poszczególne elementy obrazu, liczby osób i przedmiotów na obrazie, itp.). Nie wystarczy po prostu wygenerowan
ie „ładnej” grafiki, a modele mają problemy z rygorystycznym spełnianiem złożonych wymagań. Nie bez znaczenia jest też k
westia prawnej odpowiedzialności za poprawność wyników naszej pracy. Uważam, że ostatecznie dojdziemy do paradygmatu pra
cownika szeroko wspieranego przez AI, jednak wciąż niezbędnego w swoim miejscu pracy.

**Jak oceniasz obecny stan badań 
nad interpretowalnością i wyjaśnialnością AI? Czy zbliżamy się do „otworzenia czarnej skrzynki” systemów AI?**

Modele L
LM należą do grupy modeli trudno wyjaśnialnych, co związane jest chociażby z ich rozmiarem – ciężko jest zrozumieć znacz
enie każdego parametru lub ich kombinacji, gdy mamy do czynienia z miliardami parametrów. Znacznie więcej wysiłków jest 
obecnie kierowanych w stronę osiągania coraz bardziej imponujących zdolności modeli niż prób zrozumienia tego, co dokład
nie dzieje się w ich „głowie”. Jednakże niektóre badania nad interpretowalnością modeli dostarczyły ciekawych wniosków n
a temat samego ich działania – na przykład udowodniły, że duża część parametrów modelu w rzeczywistości nie służy do nic
zego ważnego i może zostać usunięta. 

Często wspominamy o wyjaśnialności w kontekście weryfikacji luk i braków w modela
ch. Czyli tak naprawdę interesuje nas kwestia wycieków danych osobowych, biasów czy dyskryminacji dokonywanej przez mode
le. W celu walki z tymi problemami pojawiają się obecnie inne podejścia niż czysta wyjaśnialność – na przykład, szybko r
ozwijająca się dziedzina LLM Red Teaming pozwala na identyfikację luk bezpieczeństwa w modelach, za pomocą metod przypom
inających działania etycznych hakerów. W ramach Red Teamingu projektuje się specjalne prompty, mające na celu sprowokowa
nie modelu do zrobienia czegoś „złego”. Dzięki temu co prawda nie zrozumiemy dokładnego działania modelu, ale za to mamy
 szansę wykryć konkretne zagrożenia i luki.

Jednakże myślę, że problem braku wyjaśnialności boli wielu twórców AI. Ilya
 Sutskever, jeden z założycieli OpenAI, ogłosił ostatnio uruchomienie swojego startupu, którego celem jest stworzenie „s
afe superintelligence”. Można mieć nadzieję, że przyczyni się do rozwoju metod wyjaśnialności.

**Jakie kompetencje powi
nni rozwijać Polacy, aby być przygotowanymi na erę AI?**

Mamy tutaj co najmniej 2 perspektywy: osoby tworzącej lub wdra
żającej AI i osoby „nietechnicznej”, która chcąc nie chcąc, jest już wystawiona na działania AI każdego dnia.

Jeśli cho
dzi o praktyków AI, to podstawą ich pracy są dobre umiejętności programistyczne. Umiejętność sprawnego tworzenia dobrego
, skalowalnego kodu pozwala szybko przeprowadzać eksperymenty i oszczędzać zasoby obliczeniowe, które są drogie i potrze
ba ich coraz więcej. Według mnie łatwiej jest zdobyć umiejętności AI będąc dobrym programistą niż zdobyć umiejętności pr
ogramistyczne będąc niekodującym lub słabo kodującym praktykiem AI. Następnie należy się skupić na umiejętnościach związ
anych z samym uczeniem maszynowym. Wszystkie potrzebne materiały i kursy są dostępne w Internecie: na przykład, kursy de
eplearning.ai czy polski AI Devs, otwarte wykłady uczelni amerykańskich, jak na przykład MIT, rzesza kanałów na YouTube 
tłumaczących intuicyjnie zagadnienia matematyczne lub stricte dotyczące AI, na przykład 3Blue1Brown czy kanał Yannica Ki
lchera, repozytoria publikacji – [arxiv.com](http://arxiv.com/), otwarte zasoby publikacji z konferencji AI – ICLR, KDD,
 NeurIPS, ICML, SIGIR i wiele innych. Zwykle pojawia się jednak problem z dostępem do zasobów obliczeniowych. Dlatego ob
ecnie ciężko jest niestety rozwijać AI w przysłowiowym garażu, wskazane jest działanie w ramach jednostki badawczej lub 
firmy, która posiada wymagane zasoby lub korzystanie z grantów przyznawanych przez operatorów chmur komercyjnych.

Będąc
 osobą niezwiązaną profesjonalnie z AI, również musimy włożyć pewien wysiłek w edukację, aby zapewnić sobie z jednej str
ony komfort użycia narzędzi AI, a z drugiej strony bezpieczeństwo. Wskazane jest poznanie technik promptowania, na przyk
ład z pomocą poradnika i biblioteki promptów firmy Anthropic. Pojawiło się niedawno czasopismo poświęcone AI – hAI Magaz
ine, z artykułami na różnych poziomach trudności – od podstawowych do bardziej zaawansowanych. Wielka szkoda, że nie ma,
 póki co, oficjalnego szkolenia tworzonego przez instytucje państwowe, otwartego dla wszystkich, przedstawiającego korzy
ści i ryzyka. Istnieją poradniki dla konkretnych grup, np. administracji, ale to nie wystarczy.

Nawet jeśli zupełnie ni
e chcemy korzystać z AI, to treści przez nią wygenerowane znajdą nas wcześniej lub później. Niezależnie od wszystkiego, 
polecam zapoznać się z działalnością Instytutu NASK (Naukowa i Akademicka Sieć Komputerowa), który zajmuje się zagadnien
iami cyberbezpieczeństwa, w tym AI. NASK jest obecna w mediach społecznościowych i publikuje ciekawe zasoby edukacyjne, 
jak np. raport „Cyberbezpieczeństwo AI. AI w cyberbezpieczeństwie”.

**AI Act – co wiemy już teraz i czego możemy się sp
odziewać? Czy istnieje niebezpieczeństwo, że takie regulacje wpłyną na innowacyjność i konkurencyjność polskich firm tec
hnologicznych?**

Wspomniałyśmy już o zagrożeniach płynących z masowego generowania nieprawdziwych treści. Czy AI Act na
s przed tym ochroni? Na razie wiemy na pewno, że sama treść AI Actu zawiera niestety wiele niejasności i są one na tyle 
znaczące, że nie wiadomo, jaka będzie praktyka jego stosowania.

AI Act określa wiele dziedzin zastosowania AI jako syst
emy wysokiego ryzyka (*high risk*) – np. AI stosowane w edukacji, zatrudnieniu, zarządzaniu migracją, infrastrukturą kry
tyczną miast i państw i wielu innych dziedzin życia. Warunki nadzoru tych modeli są opisane bardzo ogólnie. Będą musiały
 zostać stworzone dla nich osobne instytucje w krajach członkowskich. Nie wiadomo, w jaki sposób będą one działać i jak 
bardzo kosztowny i czasochłonny będzie proces nadzoru. Można to sobie jednak wyobrazić, obserwując sytuację bieżącą. W t
ym momencie wymóg certyfikacji dotyczy modeli AI dokonujących diagnoz medycznych, które traktowane są jak wyrób medyczny
. Niedawno zrezygnowaliśmy z wykonania projektu z użyciem takiego modelu, ponieważ brakuje jednostek certyfikujących i n
a samo rozpoczęcie procesu musielibyśmy czekać ponad rok. Wyobraźmy sobie, co się stanie, jeśli większość dostawców syst
emów AI będzie musiała certyfikować swoje systemy. Rok w dziedzinie AI jest epoką. Jeżeli dojdzie do takiej blokady, cał
y świat nam ucieknie.

AI Act nakłada też dodatkowe obowiązki regulacyjne na tzw. „modele ogólnego przeznaczenia z ryzyk
iem systemowym”, do których zaliczają się wielkie modele językowe (LLM), które do trenowania potrzebują wystarczająco du
żej liczby operacji zmiennoprzecinkowych (FLOP). Definicja FLOP zawarta w AI Act moim zdaniem stwarza jednak ryzyko nadu
żyć. Nie wiadomo zatem, które modele realnie będą wpadać w tę kategorię, biorąc pod uwagę fakt, że twórcy będą mieli żyw
otny interes w wykorzystaniu każdego błędu w definicji do obniżenia raportowanej przez siebie liczby FLOP.

AI Act poroz
umiewa się z czytelnikiem kryteriami tak ogólnymi i wieloznacznymi, że nie da się udowodnić ich spełnienia, np.: „(…) po
dmiot ten zapewnia adekwatność i wystarczającą reprezentatywność danych wejściowych (…)”. Czym jest „adekwatność” i kto 
jest ostateczną instancją do jej oceny? Kary za niedopasowanie się do regulacji są już jednak niezwykle konkretne i bard
zo wysokie – np. 35 mln euro lub 7% rocznego obrotu przedsiębiorstwa.

Opisane przeze mnie problemy to tylko wierzchołek
 góry lodowej. Czy w obliczu takich niejasności można powiedzieć, że AI Act w tym momencie zwiększa nasz komfort, poczuc
ie bezpieczeństwa lub daje nadzieję na bardziej etyczny i zrównoważony rozwój? Niestety, nie. Niepewność przynosi już pi
erwsze owoce – na przykład, Meta nie udostępni w UE swojego modelu Multimodal Llama (jest to model unikalny, ponieważ pr
zetwarza dane wideo, audio, tekstowe i obrazowe). Mimo że model jest udostępniany w formie otwartego oprogramowania na l
icencji niekomercyjnej, ryzyko regulacyjne jest zbyt duże. Modele Llama, dzięki swojej otwartej licencji, są świetnym na
rzędziem do badań i wdrożeń.

**Czy Polska ma szansę stać się europejskim hubem AI? Jakie warunki musiałyby zostać spełn
ione? Czy istnieją jakieś unikalne cechy polskiego ekosystemu AI, które mogą być konkurencyjne na arenie międzynarodowej
?**

Polska ma potencjał, aby takim hubem się stać – w ciągu ostatnich lat branża IT stała się jednym z koni pociągowych
 polskiej gospodarki. W związku z tym zostało wykształcone (lub wykształciło samo siebie) szerokie grono specjalistów. C
hętnych do rozwoju w branży nie brakuje, kariera w IT jest nieustannie postrzegana jako pożądana. Polscy informatycy dal
i się poznać jako wiarygodni specjaliści o wysokich umiejętnościach, chętnie dokształcający się i dostarczający kod wyso
kiej jakości. Mamy więc solidne podstawy. Co do samego AI, uważam, że nie mamy jeszcze wystarczająco dobrej oferty uczel
ni wyższych. Nie jest to zarzut do wszystkich uczelni, ponieważ część z nich oferuje świetny poziom. Często jednak nawet
 na solidnych uczelniach zajęcia są na podstawowym poziomie, brakuje omawiania najnowszych trendów badawczych i projektó
w studenckich skupionych na problemach z bieżących publikacji. Omawiając metody sprzed 15 lat możemy położyć fundament d
la zrozumienia obecnych technologii, ale zbyt często te tradycyjne metody grają centralną rolę. 

Jeśli chodzi o badania
 – tworzone są obecnie polskie modele językowe, które będą lub są otwarte do użytku dla wszystkich: Bielik, stworzony pr
zez SpeakLeash, PLLuM, za który odpowiada konsorcjum instytucji badawczych pod przewodnictwem Politechniki Wrocławskiej,
 czy Qra z Politechniki Gdańskiej. Modele takie najczęściej tworzone są na bazie istniejących modeli *open source*, jak 
Llama czy Mistral. Jednostki badawcze, takie jak IDEAS NCBR, prowadzą badania fundamentalne nad AI, przyciągają światowe
 talenty, a ich prace są prezentowane na najlepszych konferencjach.

Mimo tego wszystkiego, brak jest AI w polskiej stra
tegii rozwoju. Ministerstwo Cyfryzacji rozpoczęło pewne inicjatywy związane z AI, takie jak powołanie zespołu doradczego
 PL/AI czy ogłoszenie Funduszu AI. Wygląda jednak na to, że wysiłki kierowane są w stronę wspierania wdrożeń AI, a nie t
worzenia nowych technologii. Jest to zasadnicza różnica. Wdrażając AI, jesteśmy w stanie wesprzeć konkretne obszary pańs
twa, usprawnić ich działanie, jednak narzędzia których użyjemy, siłą rzeczy będą kupowane od ich zagranicznych twórców. 
Wdrożenia jako takie nie wspierają rozwoju źródłowej technologii w Polsce. Inaczej mówiąc, nie zbudujemy w ten sposób na
stępcy dla dzisiejszych LLM-ów, tylko będziemy opakowywać i sprzedawać w Polsce technologie wymyślone przez innych. 

Na
 razie nie ma środków dedykowanych temu, abyśmy mieli choć szansę postawić krok przed OpenAI i podobnymi, nie ma też pla
nów lepszego rozliczania naukowców i wspierania doskonałości naukowej (bo jestem zdania, że samo dosypanie pieniędzy do 
wadliwego systemu nie pomoże).

**Jakie są twoim zdaniem najważniejsze globalne trendy w rozwoju AI, które będą kształto
wać tę dziedzinę i wpływać na polską gospodarkę w najbliższych latach?**

Najważniejsze trendy to moim zdaniem:

* syste
my wieloagentowe – złożone z wielu LLM-ów, które posiadają zwiększone zdolności do samodzielnej korekcji błędów i popraw
iania jakości swojego działania. Badania pokazują, że grupy nawet bardzo prostych agentów pracujących razem w ramach wię
kszego systemu osiągają jakość znacznie lepszą niż modele składowe. Być może będą mogły być używane do (prawie) autonomi
cznego wykonywania złożonych czynności, np. używania wielu programów w sekwencji lub nawet poprawy struktury własnego sy
stemu;
* systemy trenowane w większym kontakcie ze światem fizycznym, w schemacie takim jak np. proponowany przez Yanna 
LeCuna „World Model” – nauka przewidywania następnego stanu środowiska na podstawie obecnego stanu. Obecne duże modele n
ie są jeszcze do końca multimodalne (nie przetwarzają danych pochodzących ze wszystkich zmysłów) i trenują raczej na dan
ych statycznych;
* mniejsze modele o zdolnościach porównywanych z dzisiejszymi LLM-ami – trend minimalizacji jest wyraźn
y i bardzo potrzebny, gdyż fundusze niezbędne do trenowania wielkich modeli są ogromne. Podnoszone są nawet koszty środo
wiskowe trenowania LLM-ów. Historia minimalizacji modeli jest optymistyczna – np. model DistilBERT, zminimalizowana wers
ja transformerowego modelu BERT, zachowała 97% wydajności BERT-a przy redukcji rozmiaru o 60%. Wśród LLM-ów również widz
imy już pewne zwiastuny sukcesu – np. bardzo dobre działanie modelu GPT-4o mini. Niestety nie wiemy, o ile GPT-4o mini j
est mniejszy od GPT-4, ale z pewnością jest dużo mniejszy, co widać choćby po cenie (GPT-4o mini jest ok. 30 razy tańszy
, jeśli chodzi o cenę za tokeny wejściowe).

Myślę, że będziemy obserwować również trendy regionalne, związane np. z reg
ulacjami – czyli w kontekście europejskim zobaczymy jakąś odpowiedź twórców modeli na AI Act, np. dążenie do utrzymania 
się pod progami wyznaczającymi funkcjonalność lub wielkość modeli.

\[1\] [https://www.hbs.edu/ris/Publication%20Files/2
4-013\_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf](https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a
1c6-c72fb70c7282.pdf)

\[2\] [https://link.springer.com/chapter/10.1007/978-3-031-64881-6\_21](https://link.springer.com
/chapter/10.1007/978-3-031-64881-6_21)

\[3\] [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/p
rompt-generator](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator)

\[4\] [https
://haimagazine.com](https://haimagazine.com/)
```
---

     
 
all -  [ AMA: PhD Researcher in Computer Vision/Machine Learning ](https://www.reddit.com/r/Btechtards/comments/1hnvw3s/ama_phd_researcher_in_computer_visionmachine/) , 2025-01-04-0912
```
Hello! I am a doctoral researcher working at the intersection of computer vision and machine learning at UCF, one of the
 top vision research institutes in the US. I have four years of research experience in computer vision before joining UC
F.

Feel free to comment on this post if you seek career guidance in Vision/ML or relevant fields. Post your questions a
s comments to this thread, and I'll try to respond to everyone. This thread is aimed to guide students/aspirants, partic
ularly those pursuing/completing undergrad degrees and who want to get into Vision/ML research.

Note: Please don't use 
'Sir/Ma'am/XYZ' in your comments. Just use '**OP**.'

**Edit:** It is late night here in my timezone, and morning in Ind
ia. Sorry that, it had to be this way. So, I'll respond to every question I get in 24 hours.

# Resources/Roadmap to ML/
Vision:

**Prerequisites:**

* Linear Algebra: Use Dr. Gilbert Strang's book and lectures on YouTube.
* Calculus: Brush 
up your school-level calculus, and that would do for starters.
* Probability: I have used [probabilitycourse.com](https:
//www.probabilitycourse.com) and [statlect.com](https://www.statlect.com) but feel free to use any good resource you fin
d. MIT OCW lectures are good resources.
* Follow 3Blue1Brown for a lot of concepts.
* You may also want to learn the bas
ics of Information Theory or Coding Theory. Use MIT OCW lectures for that.

**Basic Machine Learning:**

* Start with th
e ML for Everyone course by Dr. Andrew Ng in Coursera if you are an absolute beginner, and if you're learning the prereq
uisites on the side. This used to be the absolute best (and probably the only good enough resource) back when I started.
 All the videos are on YouTube. I am not sure how good their new ML Specialization is, but I am assuming it would be pre
tty good.
* Your goal will be to go towards CS229 Stanford. Use their lecture notes. It is a very good resource.
* Refer
ence Books: Machine Learning and Pattern Recognition by Bishop & Machine Learning Trilogy by Kevin P. Murphy. All of the
se books are available in PDF copies on the internet.

**Deep Learning:**

* You may start with CS230 Stanford. It's a g
ood resource.
* You can try the Deep Learning Specialization in Coursera. It is decent enough to go through. Again, back
 in the day, when I started in 2017, it was one of the best.
* For generative models, you can start with the GAN Special
ization of Coursera. It teaches you GANs. Work your way towards VAEs and Diffusion models through papers and blogs.
* Tr
ansformers, you can start learning from Dr. Andrej Karpathy's blog and YouTube [channel](https://www.youtube.com/playlis
t?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ).
* Reference Books: Deep Learning by Ian Goodfellow. If you want to go toward
s the more obscure statistical part of it for deeper theoretical understanding, use Elements of Statistical Learning and
 Introduction to Statistical Learning by Tibshirani.
* HuggingFace [blog](https://huggingface.co/blog) is a very good pl
ace to learn. Particularly works by the Diffusers team.
* Another decent blog is [Lil'log](https://lilianweng.github.io/
) by Lilian Weng. She is very good at this.
* You can find more on Analytics Vidhya, Medium, and Towrads Data Science.


**Computer Vision:**

* Fundamental computer vision is very different from these. Use Tubingen [lectures](https://youtub
e.com/playlist?list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_) from YouTube. Their other lectures are very good as well.
* Refe
rence Books: Foundations of Computer Vision by Torralba, Isola, Freeman

**Programming Languages:**

* Python is absolut
ely necessary. Learn Numpy and Pandas well. Correy Schafer YouTube channel has a good Pandas [series](https://youtube.co
m/playlist?list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS). Scikit-Learn will satisfy the majority of the classical ML problems
 you'll approach
* Tensorflow is kind of outdated and hence, so is Keras. Learn PyTorch properly. And not just the [Fast
.ai](http://Fast.ai) API. HuggingFace API is good for engineers.
* Learn C++ if you are going towards GPU programming wi
th CUDA. A lot of theoretical ML researchers use it and it s needed for a lot of custom and efficient implementations in
 real-world applications. Triton is a new alternative, but it is to be seen how good it is as an alternative.

**Researc
h:**

The only thing you can do is read papers and blogs. Particularly, read top-venue A\*s (Vision - CVPR, ICCV, ECCV, 
TPAMI, IJCV; ML - ICLR, ICML, NeurIPS, TMLR, PMLR; Theoretical AI - AAAI, IJCAI, TAI; NLP - ACL, NAACL, EMNLP) Try to st
ick to paper, but if you are stuck somewhere find blogs that explain it. Not necessarily you'll find blogs always. Searc
h through arXiv and Google Scholar. Keep following people who work in this domain. Yannic Kilcher's [podcast](https://ww
w.youtube.com/@YannicKilcher), [Machine Learning Street Talk](https://www.youtube.com/@MachineLearningStreetTalk) are go
od YouTube channels to stay updated as well.

**Implementations:**

There are a few implementations that are easier to u
nderstand or use.

* For any paper or implementation, if you are seeing Meta's repository, it's the best thing out there
.
* Lucidrains has a good [profile](https://github.com/lucidrains) with repositories and lots of implementations.
* [Seq
2Seq](https://github.com/bentrevett/pytorch-seq2seq) is good for RNN to transformer explanations with codes.
* Find impl
ementations of any paper in [Papers with Code](https://paperswithcode.com/).
```
---

     
 
all -  [ # of papers vs. citations ](https://www.reddit.com/r/eb_1a/comments/1hm9j9b/of_papers_vs_citations/) , 2025-01-04-0912
```
How are the two compared?

My lawyers are aiming for EB1B for me but I’m worried that I might not have enough papers.

I
 have 4 publications, 1 thesis, 3 preprints (one of which was also presented as an industrial demo at CVPR). My works ar
e mostly focused in vision / nlp (CVPR, WACV, AACL). All first author except 1 publication and 1 preprint.

Now, I got l
ike 450 citations, and am getting cited rapidly - decent chance to be at 500 by the time the doc is ready. Have served a
s reviewer for all top tier conferences in ML/CV (NeurIPS, CVPR, ICLR, ICML, ICCV, etc.) for something like 50+ papers.


My big worry is having issues stemming from only 4 of my works having been actually published (although the non-publish
ed ones are also getting cited). What do you guys think?
```
---

     
 
all -  [ i sensed anxiety and frustration at NeurIPS 24 ](https://kyunghyuncho.me/i-sensed-anxiety-and-frustration-at-neurips24/) , 2025-01-04-0912
```

```
---

     
 
all -  [ [D] What would you like in a ML/ML-related course in university? ](https://www.reddit.com/r/MachineLearning/comments/1hhdch4/d_what_would_you_like_in_a_mlmlrelated_course_in/) , 2025-01-04-0912
```
Hi!

I'm invited to give a course in university (not really a university, it's a different educational system, they call
 it engineering school but it's equivalent) in ML or ML-related.

The course is 22 hours in total. Which is short. The c
ourse is divided in both theoretical classes and practices classes. But I can change the proportion of hours. When I say
 practice it's more like a project they can do and then I grade it.

It's not the only ML course the students have, I wa
s told the students already have a machine learning course where they cover all the basics in Machine Learning and some 
statistical models (the usual ones like random forests, SVMs etc.), and they also have an in-depth NLP course, so I don'
t think I'm going with that.

What bothers me is, how to balance the theory with practice. I don't want to cover some to
pic superficially but at the same time I don't know if it's worth it for the students to cover a specific topic too deep
ly.

I don't know if it's a good idea to do something like two topics, 11 hours each with like 5 hours of theory and 6 h
ours of practice. Or do I go with just one topic.

I was suggested to show them about MLOps and tooling like Git, Docker
, Mlflow, basically just a bit of Mlops, monitoring models, how to productionize them etc. But I don't know if it's wort
h it, I feel like it's superficial to teach them how to use these tools, and there are a lot of resources online anyways
 and I guess recruiters won't expect them to know that or have experience with for junior positions.

I was also suggest
ed time series as a course, but I don't know if going in-depth in them would be interesting to the students 😅 there's a 
lot of math, and though professors assured me that they have a good level in math, I don't know if they'll be interested
 in that.

Another drawback is that I don't have access to computational resources for this course so I'm a bit limited.
 I think if I were at their place I'd have loved a course in low-level stuff like how flash attention works, some distri
buted training mechanisms, cuda etc. But I don't have means to ensure that for them :(

Another thing I'd love to do is 
to take some of the best awards papers of this year or something and help them gain the knowledge and understanding nece
ssary to understand the paper and the topics around it. Or maybe have different sessions with different topics like, one
 about diffusion models, one about multi-modal models etc., like 'let's understand how they came about qwen2-vl', 'let's
 understand what's the main contribution and novelty of the best paper in neurips main track about var' etc.

So I'm a b
it lost and I'd love to have your ideas and suggestions. What I care about is giving the students enough knowledge about
 some topic(s) so they don't only have a high-level idea (I've had interns to which I asked what is a transformer and th
ey went 'we import a transformer from hugging face') but at the same time equip them with skills or knowledge that can h
elp them get recruited for junior positions

Thank you!

```
---

     
 
all -  [ Winning edge models from Neurips 2024 competition  ](https://www.reddit.com/r/LocalLLaMA/comments/1hhbl6a/winning_edge_models_from_neurips_2024_competition/) , 2025-01-04-0912
```
I have been following up the neurips edge llm competition for a while and recently they announced the winners. The compe
tition had two tracks. One was compression challenge and another was training from scratch. Though the models and associ
ated compression techniques are not yet made public, it is interesting to see the edge llm space getting more traction 


https://edge-llms-challenge.github.io/edge-llm-challenge.github.io/leaderboard
```
---

     
 
all -  [ NeurIPS 2024: Capital One showcases leading AI research ](https://www.reddit.com/r/u_CapitalOne/comments/1hh7gli/neurips_2024_capital_one_showcases_leading_ai/) , 2025-01-04-0912
```
# This past week, many Capital One associates were active participants at the r/NeurIPS conference. Explore our contribu
tions to the world's premier AI research conference, from papers and workshops to expert presentations.

The 38th Annual
 Conference on Neural Information Processing Systems ([NeurIPS](https://neurips.cc/)), returned this December, and Capit
al One is excited to be a part of it! As a company committed to [responsible and innovative AI](https://www.capitalone.c
om/tech/machine-learning/applied-ai-research/), we're eager to share our latest research, connect with fellow researcher
s and engage in the vibrant exchange of ideas that defines this event.

# Capital One's impact at NeurIPS 2024

At Capit
al One, we're [leveraging AI to unlock new possibilities in financial services](https://www.capitalone.com/tech/machine-
learning/machine-learning-research-roundup/) and deliver exceptional customer experiences. NeurIPS provides a crucial pl
atform to engage and exchange ideas with some of the best minds in AI and science. We're eager to engage with leading ac
ademics, researchers and industry experts to discuss the latest advancements and challenges in AI. Our research efforts 
are focused on applying AI/ML techniques to address real-world challenges in the financial domain, such as improving the
 efficiency and interpretability of models, developing advanced techniques for analyzing financial time series data and 
building transparent and understandable AI systems. This work is critical to developing trustworthy AI solutions, and we
're particularly excited to share our progress in areas like deep learning, sequence modeling, explainable AI and genera
tive AI.

At [NeurIPS 2023](https://www.capitalone.com/tech/machine-learning/neurips-applied-research/), our Applied Res
earch team had several works accepted. This year, we’re continuing to showcase our research, foster collaboration and su
pport the next generation of AI talent

# Advancing AI research: Main conference papers

We have contributed to three pa
pers accepted to the main conference track:

* [**Distributional Preference Alignment of LLMs via Optimal Transport**](h
ttps://neurips.cc/virtual/2024/poster/96822): Distinguished Applied Researcher, [Igor Melnyk](https://scholar.google.com
/citations?user=4vDRTWwAAAAJ&hl=en), takes the lead as first-author exploring novel methods for aligning large language 
models (LLMs) with user preferences using optimal transport theory. This work enhances LLMs' ability to generate content
 that aligns with specific needs and values. 
* [**Searching for Efficient Linear Layers over a Continuous Space of Stru
ctured Matrices**](https://neurips.cc/virtual/2024/poster/94195): VP of AI Research [Bayan Bruss](https://scholar.google
.com/citations?user=ClqvGRQAAAAJ&hl=en) and Senior Machine Learning Engineer [Christopher Ferri](https://scholar.google.
co.uk/citations?hl=en&user=rGuDTOIAAAAJ) contribute their expertise to this collaborative research with NYU, which focus
es on optimizing the efficiency of neural networks by exploring a continuous space of structured matrices for linear lay
ers. This has the potential to lead to faster and more efficient models.
* [**Tiny Time Mixers (TTMs): Fast Pre-trained 
Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series**](https://neurips.cc/virtual/2024/poster/9674
8): In partnership with IBM Corp, Senior Distinguished Applied Researcher [Nam Nguyen](https://scholar.google.com/citati
ons?user=zzBcUpEAAAAJ&hl=en) helps to introduce innovative techniques for time series forecasting with Tiny Time Mixers 
(TTMs). These pre-trained models are optimized for fast and accurate predictions, even with limited fine-tuning data.

[
Dr. Furong Huang](https://scholar.google.com/citations?user=13yyuCcAAAAJ&hl=en), our inaugural Visiting Scholar and an A
ssociate Professor in Computer Science at the University of Maryland, also contributed to [six additional NeurIPS papers
](https://nips.cc/virtual/2024/papers.html?filter=authors&search=Furong+Huang). Dr. Huang’s expertise in trustworthy mac
hine learning strengthens our research collaborations and reflects our commitment to bridging academia and industry. 

#
 Fostering the next generation of AI talent: Workshop papers

Capital One's presence at NeurIPS extends beyond the main 
conference with seven accepted workshop papers, further demonstrating our commitment to advancing AI research and foster
ing the next generation of AI talent through our internship programs. 

# Applied Research Internship Program (ARIP)

Fi
ve of the accepted papers are authored by talented individuals from our 2024 [Applied Research Internship Program](https
://www.capitalonecareers.com/upgrade-ai-work-with-the-applied-research-internship-students-tech) (ARIP), a program desig
ned to provide PhD students with hands-on experience tackling real-world AI challenges in the financial sector. 

* [**R
efusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models**](https://scholar.google.com/citations?view_
op=view_citation&hl=en&user=nSn7jtIAAAAJ&citation_for_view=nSn7jtIAAAAJ:4DMP91E08xMC): This paper explores a novel metho
d for controlling the refusal behavior of large language models by introducing 'refusal tokens' during training. This te
chnique allows for fine-grained control over the model's tendency to refuse certain prompts or questions, enhancing safe
ty and reliability.
* [**Dense Backpropagation Improves Routing for Sparsely-Gated Mixture of Experts**](https://openrev
iew.net/pdf?id=huy8g3iKy0): This paper investigates the use of dense backpropagation in Mixture of Experts (MoE) models,
 demonstrating its effectiveness in improving routing decisions and overall model performance. This work contributes to 
the advancement of MoE models, which are known for their efficiency and scalability.
* [**Language Model Scaling Laws an
d Zero-sum Learning**](https://openreview.net/forum?id=yBq2g832Go&referrer=%5Bthe%20profile%20of%20Irina%20Rish%5D(%2Fpr
ofile%3Fid%3D~Irina_Rish1))**:** This research delves into the relationship between language model scaling laws and zero
-sum learning, exploring how the competitive dynamics of zero-sum games can influence the scaling behavior and performan
ce of large language models.
* [**StructMoE: Augmenting MoEs with Hierarchically Routed Low Rank Experts**](https://open
review.net/pdf?id=v71Nsh6R7m): This paper proposes StructMoE, a novel architecture that enhances Mixture of Experts (MoE
) models by incorporating hierarchically routed low-rank experts. This approach aims to improve the efficiency and expre
ssiveness of MoE models, further advancing their capabilities in various applications.

# Data Science Internship Progra
m (DSIP)

We are also proud to highlight a workshop paper authored by a former intern from our [Data Science Internship 
Program](https://campus.capitalone.com/employment/new-york-data-science-jobs/1786/8161296/6252001-5128638-5128581/4) (DS
IP), which offers aspiring data scientists the opportunity to contribute to cutting-edge research and development.

* [*
*Enhancing Table Representations with LLM-powered Synthetic Data Generation**](https://openreview.net/forum?id=h9465s0xu
7#discussion): This paper explores the use of large language models (LLMs) to generate synthetic tabular data for improv
ing table representations. This approach aims to enhance the performance of similar table recommendation systems, which 
are crucial for efficient data management and analysis in data-driven enterprises. The research introduces a novel synth
etic data generation pipeline that leverages LLMs to create a large-scale dataset tailored for table-level representatio
n learning, leading to improved accuracy in recommending similar tables.

# Collaborative research

Finally, we have wor
kshop papers co-authored by Senior Distinguished Applied Researcher [Nam Nguyen](https://scholar.google.com/citations?us
er=zzBcUpEAAAAJ&hl=en), and Distinguished Applied Researcher [Supriyo Chakraborty](https://scholar.google.com/citations?
user=UIM7nGwAAAAJ&hl=en) further demonstrating our commitment to collaborative research and knowledge sharing within the
 AI community.

* [**Scaling-laws for Large Time-series Models**](https://arxiv.org/pdf/2405.13867): This research, cond
ucted in collaboration with Johns Hopkins University, explores the scaling laws that govern the performance of large tim
e-series models. By examining the relationships among model size, data volume, and computational resources, this study o
ffers valuable insights into the efficient training and deployment of these models for diverse time-series forecasting t
asks in finance and other domains.
* [**MyCroft: Towards Effective and Efficient External Data Augmentation**](https://s
cholar.google.com/citations?view_op=view_citation&hl=en&user=QOsVyPMAAAAJ&citation_for_view=QOsVyPMAAAAJ:LkGwnXOMwfcC): 
This research introduces MyCroft, a new data-efficient framework implementing techniques to evaluate relative utility of
  relevant external data sources that can augment internal data to improve model performance. These techniques leverage 
feature space distances and gradient matching to identify small but informative data subsets to maximize performance wit
h minimal data [exposure.Capital](http://exposure.Capital) One's presence at NeurIPS extends beyond the main conference 
with seven accepted workshop papers, further demonstrating our commitment to advancing AI research and fostering the nex
t generation of AI talent through our internship programs. 

# Our two engaging expo talks

# Sequence Modeling in Finan
cial Services

Led by Senior Distinguished Applied Researcher [Nam Nguyen](https://scholar.google.com/citations?user=zzB
cUpEAAAAJ&hl=en), this talk delves into the intricacies of applying sequence modeling to financial data. Learn about cut
ting-edge research, including novel approaches for leveraging powerful transformer models for enhanced insights and pred
ictions.

* **Date/Time:** Tuesday, December 10th, 4:00 PM local time
* **Location:** West Ballroom B

# Deep Tabular Da
ta

Led by Distinguished Machine Learning Engineer [Doron Bergman](https://scholar.google.com/citations?user=FeCagRUAAAA
J&hl=en), this talk explores the challenges and opportunities of deep learning for tabular data in finance. Discover how
 deep learning can surpass traditional methods and unlock new possibilities for financial modeling.

* **Date/Time:** We
dnesday, December 11th, 1:00 PM local time
* **Location:** West Meeting Room 109/110

# Connect with Capital One at Neur
IPS 2024!

We're excited to connect with you at NeurIPS 2024! Come visit us at booth #315 where you can:

* **Explore ou
r research**: Dive deep into our latest [advancements in AI](https://www.capitalone.com/tech/ai/) and machine learning.

* **Discover career opportunities**: Learn about exciting [applied research career paths](https://www.capitalonecareers.
com/search-jobs/applied%20research/234/1) at Capital One for researchers and engineers passionate about AI and join our 
world-class team.
* **Engage with our team**: Meet our researchers and AI experts, ask questions and discuss the [future
 of AI in finance](https://www.capitalone.com/tech/ai-research/).

This Reddit post is a [\~repurposed Tech blog\~](http
s://www.capitalone.com/tech/ai-research/?utm_campaign=always-on&utm_source=reddit&utm_medium=organic-social&utm_content=
neur-ips). For more learning opportunities check out the [\~Capital One Tech blog\~](https://www.capitalone.com/tech/blo
g/) and keep the progress going! 
```
---

     
 
all -  [ Can o1-preview find major mistakes amongst 59 NeurIPS '24 MLSB papers? ](https://www.reddit.com/r/slatestarcodex/comments/1hh25xz/can_o1preview_find_major_mistakes_amongst_59/) , 2025-01-04-0912
```
[Link to the essay](https://www.owlposting.com/p/can-o1-preview-find-major-mistakes)

  
Summary: I saw this Twitter thr
ead recently about how o1 [was able to find a major error in a scientific paper. ](https://x.com/emollick/status/1868329
599438037491)I wondered: could it do something similar in my own field of biology x ML? I downloaded 59 papers from [Neu
rIPS '24 MLSB](https://www.mlsb.io/), a structural biology + chemistry + AI workshop that happened just last week, pushe
d them through o1 to ask if there are any errors, and interpreted its response. Of the 59, o1 said 3 have major errors. 
Upon reviewing the 3, none of the complaints seem well-founded. But all were intelligent and fun to grapple with! But fo
r at least one of the papers, it took quite a bit of effort (contacting the authors) to disprove. All this to say, o1 is
n't a drop in replacement for an academic reviewer, but its critiques are still often interesting and useful. 
```
---

     
 
all -  [ Influential creators at tech conference: 'Don't say AI democratizes art-making. Should we democratiz ](https://www.reddit.com/r/aiwars/comments/1hgq91u/influential_creators_at_tech_conference_dont_say/) , 2025-01-04-0912
```
Despite being mostly public figures, names still censored as per rules.

https://preview.redd.it/x3x3wzuubi7e1.png?width
=721&format=png&auto=webp&s=b61a67d70cba854f9aa57743566211748ae073f9

Here's the issue.

You can't democratize marathon 
running with mopeds, because then it would no longer be running. Marathon running is a specific activity performed for a
 specific purpose.

What you *can* democratize -- and what we have successfully democratized to everyone's benefit -- is
 getting from place-to-place quickly. For this you can give people mopeds, electric wheelchairs, cars, planes, whatever.
 Because the goal of the activity is not to use your legs to run, but simply to get from one place to another. There was
 a time when those who couldn't walk were mostly out of luck. There were times when there weren't ramps for wheelchairs 
to get into most buildings. But we've taken steps to make it easier for everyone to get around; we've democratized trave
l.

  
To take all this back to AI, AI doesn't democratize drawing or painting, because those are specific activities wh
ich AI is not. But it does democratize art-making. If you don't like calling it art, fine, it democratizes the ability t
o get ideas from your head into imagery which can be enjoyed or shared. If you'd say it didn't need to be democratized b
ecause anyone can draw, well sure, and anyone can walk. But we have technology like bicycles and mopeds to get to places
 faster...or Photoshop with all its conveniences like layering and undo...and as long as you're not in a context where t
hose tools don't match the activity, there's nothing wrong with that.
```
---

     
 
all -  [ Am I Making the Right Choice? Masters in ML, Research Lab Experience, and Building Things That Matte ](https://www.reddit.com/r/learnmachinelearning/comments/1hgng0o/am_i_making_the_right_choice_masters_in_ml/) , 2025-01-04-0912
```
I’m currently pursuing my master’s in machine learning, and I love building things — that’s how I understand concepts be
st. But my first semester hit me with a tough realization: I joined a research lab way too early, and it just wasn’t the
 right fit for me.

The lab’s environment felt off. The code was sloppy, results were rushed, and even the smallest net-
positive outcome led to a question: “Which journal should we target?” Maybe this is how it works in many research labs —
 I don’t know, this was my first experience. But the emphasis on quick publication, without deeper exploration or clean 
fundamentals, didn’t sit well with me.

For context, I was working with LLMs. What surprised me is how many papers get p
ublished even when they’re essentially hacks — a lot of prompt engineering, and observations that openly admit “We don’t
 know why this works, but it does.” I respect research, but it started to feel… unfulfilling. I wasn’t enjoying it, and 
I had no time to work on projects of my own.

Now I’m planning to quit the lab. But here’s where I’m conflicted: it seem
s like most ML jobs require a strong research profile — X papers in NeurIPS, ICML, etc. Part of me wonders if I should s
tick with it, keep my head down, and publish papers just to “check the box.” But then I remember why I’m here in the fir
st place: I genuinely enjoy ML, especially when I’m building things that matter, not just chasing publications.

To thos
e who’ve been down this road: Am I sabotaging my career prospects by walking away from research so early? Is it better t
o focus on building meaningful projects, even if they don’t come with a DOI? Or am I missing something about the value o
f sticking it out in the lab?
```
---

     
 
all -  [ Legal Tech’s Data Dilemma: Trust, Betrayal, and Competition. ](https://www.reddit.com/r/legaltech/comments/1hgmxc4/legal_techs_data_dilemma_trust_betrayal_and/) , 2025-01-04-0912
```
Ilya Sutskever, co-founder of OpenAI, recently highlighted a critical issue at the NeurIPS 2024 conference: the AI indus
try is facing a data scarcity problem, often referred to as 'peak data.' Despite advancements in computing power, the av
ailability of high-quality training data is becoming a bottleneck for AI development. Sutskever emphasized that syntheti
c data, while a potential solution, does not fully address this challenge.

In this landscape, companies promising not t
o mine your data face immense pressure to break that pledge. The competitive advantage of leveraging vast, real-world da
tasets is simply too great to ignore. Discarding millions of dollars’ worth of high-quality data—data that could refine 
models, boost performance, and outpace competitors—is a hard sell for any profit-driven firm.

And here lies the uncomfo
rtable truth: no amount of compliance paperwork, signed audits, or certifications can fully guarantee your data’s safety
. Unless you examine production code directly, there’s no way to ensure that your data isn’t being anonymized and quietl
y used to train systems. Unlike static cloud storage, generative AI operates on a completely different scale. Its rapid 
feedback loops and massive bandwidth allow companies to quickly organize and refine reinforcement-learning-grade dataset
s—even with anonymized or de-identified data.

We’re decisively moving from the compute era to the data era of AI, where
 success is no longer about the size of your GPU cluster but the quality of your post-training data. In this new paradig
m, aligning models with the correct data is essential—placing tools for data curation, human supervision, and evaluation
 at the heart of AI development.

The legal tech industry must take heed: make sure you own your AI. AI in the cloud is 
not aligned with you—it’s aligned with the company that owns it. To protect sensitive data and retain control, on-premis
e solutions and transparent practices are no longer optional—they are imperative.

[NeurIPS 2024 conference](https://pre
view.redd.it/k32axcw1lh7e1.jpg?width=2048&format=pjpg&auto=webp&s=3887497b862f3190153705674c943c94697a7bd3)


```
---

     
 
all -  [ ChanceMe : Asian Male CS 🙏🙏🙏 ](https://www.reddit.com/r/chanceme/comments/1hgkgzi/chanceme_asian_male_cs/) , 2025-01-04-0912
```
I think its a strong application but holy shit my GPA is eating at my confidence rn, please chanceme would be much appre
ciated

Currently a junior - any senior related stuff is likely predictions - chanceme as such

**Demographics:**   
Mal
e, Indian, Southern USA, Looking at top CS programs, potentially recruited for d2 level swim, upper middle class income


**Intended major(s):**

CS w a focus on AI/ML, EECS

**Academics:**

* **SAT:** 1600 superscore
* **Class rank:** top 2
0% atleast, most likely top 15%
* **UW/W GPA: 3.9/6.05 on a 6.7 scale**
   * My school grades by semester, so i have 63 
total semester grades of which 6 are B's.
* **Coursework:** 
   * AP Human geo, Discrete Math
   * AP physics, AP CSA, A
P Precalc, AP World history, Linear Algebra
   * AP Lang, AP Physics C:Mech, AP Calc AB, AP Stats, APUSH, AP CSP
   * AP
 Lit, AP Physics C: EM, AP Calc BC, Multivariable Calc & Differential EQ, AP Macro/Micro, AP Gov
      * 5's on all exce
pt a 4 on AP human geo
* **Awards:**
   * USACO Plat
   * USAJMO Qual
   * ISEF grand award
   * USAPhO Medallist (Not g
old)
   * Published research in a major CS conference (Neurips level but not neurips)
   * Deciding between All state ja
zz saxophone or a business related award,

**Extracurriculars:**

* AI/ML startup - 5 figure revenue, interviewed by Y c
ombinator, abt 15k users
* Software/CSE Intern at a major company working in lawtech
* Nonprofit that provides olympiad 
tutoring and classes for free, about 500 hours taught total, 15 volunteers, personally taught about 100 hours early on
*
 d2/3 swim, some minor schools reaching out, 25 second 50 free
* Math competition club president - major club with about
 100 members, schools ranked pretty high nationally in this one competition
* Speech and debate congressional debate com
petitor, finaled some pretty big tournaments and have accumulated 15 bids to TOC
* AI ML publication - if not in awards 
ill mention here, did prompting research and technical research with a PhD at berkeley - through cold emailing
* Patent 
for a novel bio-based material, considering commercializing but probably not
* Jazz saxophone player for abt 8 years, pl
ay a lot for fun and am decent at it, considering putting it on like a portfolio
* Youtube channel with 1.1 million life
time views and about 5k subscribers, where I post music and education content

**Schools:**

* Basically the top 20 CS s
chools, preference on MIT/CMU/Stanford

LOR

10/10 - research teacher

10/10 - Math teacher

7/10 - Saxophone tutor

Ess
ays  
Common app - 8/10 - not an amazing writer but fairly good feeling about it  
Supps - didnt write yet - re: current
 junior

**H**ad a shitty GPA because close family member died first semester of sophomore year, so I got some B's, and 
got into some disciplinary trouble around that time with the school, nothing major, no suspension etc. all the classes w
ith B's were A+ next semester.

I feel decently confident, but im fairly worried about my GPA and how it measures up her
e, I go to a rich private school full of tryhards :()

Would really appreciate yall's thoughts on this :D
```
---

     
 
all -  [ Valence & Recursion Sweep Awards at Foundation Models for Science Workshop at NeurIPS ](https://www.reddit.com/r/RecursionPharma/comments/1hfo8d6/valence_recursion_sweep_awards_at_foundation/) , 2025-01-04-0912
```
https://preview.redd.it/0jg8zf1sv87e1.png?width=1200&format=png&auto=webp&s=951c8af14784f575ef01a613f15a8c80e2f73d17

Th
is past weekend at NeurIPS, Valence Labs and Recursion won first, second and third place awards at the Foundation Models
 for Science workshop. These foundation models offer breakthroughs in leveraging machine learning to better model the in
tersection of biology and chemistry necessary to improve and scale AI drug discovery. 

**First place** was awarded to a
 paper on MolPhenix, a foundation model that can predict the effect of any given molecule and concentration pair on phen
otypic cell assays and cell morphology by integrating phenomics data with chemistry data. Over the past decade, Recursio
n has generated billions of phenomics images through automated, high-throughput experiments. Paired with new phenomics f
oundation models like Phenom-1, Recursion can extract meaningful representations from these high-dimensional images to b
uild Maps of Biology, allowing them to navigate which molecules and genes map to the same space of morphological changes
. MolPhenix mines that rich data and delivers 10X improvement over previous methods – from 7.9% to 77.3% on the Top 1% r
ecall of active molecules. **Paper:** [https://www.arxiv.org/abs/2409.08302](https://www.arxiv.org/abs/2409.08302?fbclid
=IwZXh0bgNhZW0CMTAAAR1DNiyBVjVqVLuNtS9J5NV1MFqG6gxWIEMc8VxgmY_h1F61pVC1QMYBx5E_aem_eMEc0w9oQCwp4X5KhTwjSA)

**Second pla
ce** was awarded to a paper on Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as
 building blocks to explore drug-like chemical space more comprehensively. The model uses an unsupervised pre-training a
pproach using offline drug-like molecule datasets, conditioning A-GFNs on inexpensive yet informative molecular descript
ors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve 
as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. **
Paper:** [https://arxiv.org/abs/2409.09702](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.org%2Fabs%2F2409.09702%3F
fbclid%3DIwZXh0bgNhZW0CMTAAAR1TFhx9hYXN7h0fUw0yz-jj1XbKnG5VVmeGbhBjXq3D_mI-0EaoAFSuPdM_aem_DSvWO2X4lUCyArxe25tAxg&h=AT3p
MSPJmPbXaJMGgs_or31Oprx-0d6DEJfLPfDpG7eaT2mbKba97Q38wnlIRWtiiybJk6-ebgL6mKUxVGSjRbxBmELjxYpjXNjdHgL29VeAnS-hGjDCqAKKMQMj
w7aG1g&__tn__=-UK-R&c[0]=AT0iM8paHD3SnWn3-5OATLDMYPKlw0l87qqsKblkdsjHHaCw_MGxpfhRaSEvxsy0AUorpVBjv6c2ceWmePe-Jl-BcrAc3Ma
QXhUG9VtgoQ9q8Jm8kZAX4gjgN8BWRSb9QeIYE2XwWCLl8bqvpU0AKz5RAGcDYz5s2BukDaagpBsjOI07tAnlVJ-4_HtQZ-b4sEweKQ) 

**Third place
** was awarded to a paper on the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter 
ViT-G/8 MAE trained on over 8 billion microscopy images that achieves a 60% improvement in linear separability of geneti
c perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate co
nsistency benchmarks. **Paper:** [https://arxiv.org/abs/2411.02572](https://l.facebook.com/l.php?u=https%3A%2F%2Farxiv.o
rg%2Fabs%2F2411.02572%3Ffbclid%3DIwZXh0bgNhZW0CMTAAAR1TFhx9hYXN7h0fUw0yz-jj1XbKnG5VVmeGbhBjXq3D_mI-0EaoAFSuPdM_aem_DSvWO
2X4lUCyArxe25tAxg&h=AT2mulqFETtdCih5m7gG70wHri8uodEz6hXHak0aAKc6GwUbm4BJnRDlDjN6L8dYBc_aXrweThNM8xEMPZ5GqovHoSWE3w2YETVv
k99BY8TWSX6EyOpJw5-OvDXcHdj2fw&__tn__=-UK-R&c[0]=AT0iM8paHD3SnWn3-5OATLDMYPKlw0l87qqsKblkdsjHHaCw_MGxpfhRaSEvxsy0AUorpVB
jv6c2ceWmePe-Jl-BcrAc3MaQXhUG9VtgoQ9q8Jm8kZAX4gjgN8BWRSb9QeIYE2XwWCLl8bqvpU0AKz5RAGcDYz5s2BukDaagpBsjOI07tAnlVJ-4_HtQZ-b
4sEweKQ)

\#NeurIPS #NeurIPS2024 #ML
```
---

     
 
all -  [ NeurIPS conference in Vancouver draws 16,000 AI researchers $META ](https://www.reddit.com/r/alertscreener/comments/1hfitr5/neurips_conference_in_vancouver_draws_16000_ai/) , 2025-01-04-0912
```
With major companies like Meta $META, Alphabet $GOOGL, and Microsoft $MSFT showcasing their latest AI advancements and p
roducts.

### Follow [@alertscreener](https://x.com/intent/user?screen_name=alertscreener) for more
```
---

     
 
all -  [ Last Evening in Vancouver: Must-See Experiences Before Heading Back? ](https://www.reddit.com/r/askvan/comments/1hf3fah/last_evening_in_vancouver_mustsee_experiences/) , 2025-01-04-0912
```
I've been here for a week attending NeurIPS 2024 and will be heading back Toronto tomorrow. I only have this evening lef
t—what's the must-see or unique experience I shouldn’t miss to avoid regretting it later
```
---

     
 
all -  [ Last Evening in Vancouver: Must-See Experiences Before Heading Back? ](https://www.reddit.com/r/canadatravel/comments/1hf37do/last_evening_in_vancouver_mustsee_experiences/) , 2025-01-04-0912
```
I've been here for a week attending NeurIPS 2024 and will be heading back Toronto tomorrow. I only have this evening lef
t—what's the must-see or unique experience I shouldn’t miss to avoid regretting it later


```
---

     
 
all -  [ [D] Are We Okay With This? Questionable Poster Behavior at NeurIPS ](https://www.reddit.com/r/MachineLearning/comments/1heo36q/d_are_we_okay_with_this_questionable_poster/) , 2025-01-04-0912
```
This was my first year at NeurIPS. It’s inspiring to see so much cutting-edge research being presented, but something tr
oubling caught my attention during the poster sessions that I feel compelled to share, especially given [the recent inci
dent with Rosalind Picard](https://www.reddit.com/r/MachineLearning/comments/1hdxbru/d_what_happened_at_neurips/).

Gett
ing a paper accepted at NeurIPS is a huge achievement. Each poster spot represents so much hard work and is highly covet
ed.

I saw two posters that *shouldn’t* have been there, and it has left me wondering about the exploitation of these sp
aces.

**Illegal Poster #1:** [Generative Boba](https://x.com/BoyuanChen0/status/1778565953627775453). This was a “cute,
 look at me” poster, but it also featured a QR code linking to the creator’s X/Twitter. While the poster itself was plac
ed on a side wall in the exhibition hall and not in an official poster spot (when I saw it anyway), it still felt odd. W
hy did they make this poster? Was this about sparking joy, or gaining attention and followers?

[Illegal Poster #1: Gene
rative Boba.](https://preview.redd.it/u3vfvszkoy6e1.jpg?width=3363&format=pjpg&auto=webp&s=8c09ddda45e0ac002223dadf0eac4
165bfdc0433)

**Illegal Poster #2:** [Benchmarkthing](https://x.com/xdotli/status/1867823150068535797)**.** This was far
 more concerning. It blatantly promoted a new AI startup, mentioning funding by a prominent figure in our field, Jeff De
an. Unlike the boba poster, this could visually pass as a real NeurIPS poster. Probably most passersby didn’t give it a 
second thought, but the poster's presenter (who is also the company’s founder) was essentially promoting his new startup
, sometimes to a significant audience size AND across *multiple* poster sessions. This feels deceptive and exploitative 
— gaming the trust of the community to cheatingly gain visibility in a sacred academic space.

[Illegal Poster #2: Bench
markthing.](https://preview.redd.it/qn7vpos4py6e1.jpg?width=2646&format=pjpg&auto=webp&s=4cfd1aa535bdf74cdb57ba8e44f1fa8
13b9d28a7)

A different type of gaming involves authors putting up their poster at unused spots while leaving a sign in 
their formally assigned location that says “See poster at #{better spot}”. If the authors for the unused spot arrived, t
hey’d just move their poster back — but if not, they would presumably revel in the extra attention from being located, f
or example, closer to the hall’s entrance with more foot traffic.

Relocating posters still seems problematic, but at le
ast the posters *belong* at the conference. On the other hand, I feel much more strongly that unauthorized posters for p
ersonal or commercial promotion hurts the integrity of the space, disrespects the presenters whose posters truly belong 
there, and undermines the conference overall.

Questions for the community:

1. Should there be stricter policies or bet
ter enforcement for poster sessions?
2. How do we differentiate between minor gaming (e.g. relocating posters) and outri
ght exploitation (e.g. unauthorized posters)?
3. Is it fair to tolerate some flexibility as long as the intentions are l
ighthearted or still academic? 
4. How do we address these behaviors moving forward? Should there be consequences?
```
---

     
 
all -  [ A little bit of drama: Pre-training is only over if you have no imagination - Logan Kilpatrick ](https://www.reddit.com/r/singularity/comments/1he9tsn/a_little_bit_of_drama_pretraining_is_only_over_if/) , 2025-01-04-0912
```
https://x.com/OfficialLoganK/status/1868002617311596552?t=uNazJ-3HPuWlBrXGagkAag&s=19

It's a slow saturday so why not s
hitpost a little.
This is in response to Ilya Sutskever's talk during NeurIPS 2024.
```
---

     
 
all -  [ A Perfect Storm for AI Inference TPU will be new king  ](https://www.reddit.com/r/Bard/comments/1he4e14/a_perfect_storm_for_ai_inference_tpu_will_be_new/) , 2025-01-04-0912
```
Ilya Sutskever's recent bombshell at NeurIPS – [https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-op
enai-model-data-training](https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-traini
ng) that we've reached 'peak data' and the era of pre-training as we know it is ending – has sent ripples through the AI
 world. His vision of a future dominated by 'agentic,' reasoning AI, capable of learning from limited data, sets the sta
ge for a fundamental shift in how we develop and deploy artificial intelligence. This shift, it turns out, might just be
 the perfect storm for the rise of the TPU and Broadcom's latest chip announcement could be the catalyst. [https://www.b
roadcom.com/company/news/product-releases/62691](https://www.broadcom.com/company/news/product-releases/62691)

**Why TP
Us are Poised to Shine in a Post-'Peak Data' World:**

1. **Inference, Inference, Inference:** Sutskever's emphasis on a
 future where AI is smarter, not just bigger, puts the spotlight squarely on **inference**. This is where AI applies its
 learned knowledge to make predictions and decisions in real-world scenarios. And this is precisely where TPUs have a di
stinct advantage.
2. **Enter Broadcom: The 3.5D XDSiP and the TPU Advantage:** Broadcom's new 3.5D chip isn't just anoth
er incremental improvement; it's a potential game-changer, especially for TPUs. Its innovative design, featuring vertica
l die stacking and face-to-face interconnects, directly addresses the key challenges of inference:
   * **Latency Killer
:** By drastically reducing the distance data needs to travel, Broadcom's chip minimizes latency, enabling the rapid-fir
e calculations that TPUs are built for. This is crucial for real-time inference applications.
   * **Power Saver:** The 
3.5D architecture slashes power consumption, a critical factor for deploying TPUs in data centers and edge devices where
 energy efficiency is paramount.
   * **Density Champion:** The compact form factor allows for denser packing of TPUs, p
aving the way for more powerful and efficient inference systems.

The potential rise of TPUs, spurred by the need for mo
re efficient inference and enabled by innovations like Broadcom's, could trigger a paradigm shift, compelling all major 
players in the AI field to develop their own specialized chips and hardware solutions to remain competitive in this rapi
dly evolving landscape. This may be the dawn of the age of custom AI silicon, and potentially the beginning of the TPU e
ra.
```
---

     
 
all -  [ Ilya Sutskever, cofondateur et ancien directeur scientifique d'OpenAI, a fait une rare apparition pu ](https://www.reddit.com/r/actutech/comments/1hdxdjs/ilya_sutskever_cofondateur_et_ancien_directeur/) , 2025-01-04-0912
```
Il a notamment affirmé que le pré-entraînement des modèles tel que nous le connaissons va inévitablement prendre fin, co
mparant les données à un 'combustible fossile' limité. Selon lui, nous avons atteint un pic des données disponibles, car
 il n'existe qu'un seul internet.  
  
Pour l'avenir, il prédit que les prochaines générations d'IA seront plus 'agentiq
ues' et capables de raisonner véritablement, contrairement aux systèmes actuels qui se contentent principalement de reco
nnaître des motifs. Ces systèmes deviendront plus imprévisibles à mesure qu'ils développeront leur capacité.  
  
Il a é
galement établi un parallèle intéressant entre l'évolution de l'IA et la biologie évolutive, suggérant que l'IA pourrait
 découvrir de nouvelles approches de mise à l'échelle, tout comme l'évolution a trouvé un nouveau modèle pour le cerveau
 des hominidés.

https://preview.redd.it/2ly7b8fejr6e1.jpg?width=960&format=pjpg&auto=webp&s=ae6cfe8241a9fc37bf648d189d8
2908a8624094c


```
---

     
 
MachineLearning -  [ [D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams ](https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/) , 2025-01-04-0912
```
Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the
 other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his col
leagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.git
hub.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is
 asserted and if this is true this would be nice to confirm.
```
---

     
 
MachineLearning -  [ [D] How to make friends and network at NeurIPS? ](https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/) , 2025-01-04-0912
```
I’m attending NeurIPS for the first time and it’s quite overwhelming seeing the amount of people and so many recruiters.
 I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is
 here.

I didn’t really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach
 others who are in big groups of people and (2) I’m feeling strong imposter syndrome and under-qualified for the jobs re
cruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other 
students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to
 potentially meet up and have a chat? I’m a 3rd year PhD student from the UK, but from Vancouver myself so know lots of 
stuff going on in the area. Cheers!
```
---

     
 
MachineLearning -  [ [R] Improving robustness to corruptions with multiplicative weight perturbations - A simple yet effe ](https://www.reddit.com/r/MachineLearning/comments/1hap6gx/r_improving_robustness_to_corruptions_with/) , 2025-01-04-0912
```
We would like to share and discuss this NeurIPS spotlight paper (disclaimer: I am a co-author).

**Paper**: [https://arx
iv.org/abs/2406.16540](https://arxiv.org/abs/2406.16540)  
**GitHub**: [https://github.com/trungtrinh44/DAMP](https://gi
thub.com/trungtrinh44/DAMP)  
**DAMP** (Data augmentation via multiplicative perturbations) is a simple yet effective ap
proach to improving neural network robustness through multiplicative weight perturbations. Unlike traditional data augme
ntation methods, DAMP operates directly on model weights during training, enabling improved corruption robustness withou
t compromising clean image performance or increasing computational cost.  
  
**Key Highlights:**

* **Theoretical Found
ation**: DAMP demonstrates that input corruptions can be equivalently represented as multiplicative weight perturbations
, providing a theoretical basis for weight-space data augmentation.
* **Simple Implementation**: The method requires onl
y random Gaussian sampling and pointwise multiplication, maintaining almost the same training cost as standard SGD while
 being fully compatible with data parallelism.
* **Breakthrough in ViT Training**: Successfully trains Vision Transforme
rs from scratch using only basic preprocessing, achieving ResNet50-level performance (23.7% top-1 error) on ImageNet wit
hout complex augmentations.
* **Advanced Integration**: When combined with MixUp and RandAugment, DAMP significantly imp
roves both clean and corruption performance:
   * ViT-S/16: 20.09% clean error (vs 20.25% baseline), 58.30% avg corrupti
on error (vs 60.07% baseline)
   * ViT-B/16: 19.36% clean error (vs 20.41% baseline), 56.76% avg corruption error (vs 58
.83% baseline)

**Why DAMP?** Unlike traditional approaches that rely on complex data augmentation pipelines or computat
ionally expensive ensemble methods, DAMP provides a simple, theoretically-grounded solution to improving model robustnes
s. Its ability to train Vision Transformers from scratch without advanced augmentations and compatibility with existing 
techniques makes it a practical choice for developing robust vision models.  
**Since DAMP has minimal overhead over sta
ndard training, it is particularly effective when applied to large models and datasets.**  
  
We welcome technical disc
ussions, particularly regarding theoretical connections to other robustness methods and potential applications beyond co
mputer vision!
```
---

     
