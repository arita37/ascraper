""	"title"	"body"	"url"	"comms_num"	"dt"	"score"	"id"
0	"[D] Our community must get serious about opposing OpenAI"	"OpenAI was founded for the explicit purpose of democratizing
 access to AI and acting as a counterbalance to the closed o
ff world of big tech by developing open source tools.

They 
have abandoned this idea entirely.

Today, with the release 
of GPT4 and their direct statement that they will not releas
e details of the model creation due to 'safety concerns' and
 the competitive environment, they have created a precedent 
worse than those that existed before they entered the field.
 We're at risk now of other major players, who previously at
 least published their work and contributed to open source t
ools, close themselves off as well.

AI alignment is a serio
us issue that we definitely have not solved. Its a huge fiel
d with a dizzying array of ideas, beliefs and approaches. We
're talking about trying to capture the interests and goals 
of all humanity, after all. In this space, the one approach 
that is horrifying (and the one that OpenAI was LITERALLY cr
eated to prevent) is a singular or oligarchy of for profit c
orporations making this decision for us. This is exactly wha
t OpenAI plans to do.

I get it, GPT4 is incredible. However
, we are talking about the single most transformative techno
logy and societal change that humanity has ever made. It nee
ds to be for everyone or else the average person is going to
 be left behind.

We need to unify around open source develo
pment; choose companies that contribute to science, and cond
emn the ones that don't.

This conversation will only ever g
et more important."	"https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/"	"464"	"1678919641.0"	"2859"	"11sboh1"
1	"[D] I don't really trust papers out of 'Top Labs' anymore"	"I mean, I trust that the numbers they got are accurate and t
hat they really did the work and got the results. I believe 
those. It's just that, take the recent 'An Evolutionary Appr
oach to Dynamic Introduction of Tasks in Large-scale Multita
sk Learning Systems' paper. It's 18 pages of talking through
 this pretty convoluted evolutionary and multitask learning 
algorithm, it's pretty interesting, solves a bunch of proble
ms. But two notes. 

One, the big number they cite as the su
ccess metric is 99.43 on CIFAR-10, against a SotA of 99.40, 
so woop-de-fucking-doo in the grand scheme of things.

Two, 
there's a chart towards the end of the paper that details ho
w many TPU core-hours were used for just the training regime
ns that results in the final results. The sum total is 17,81
0 core-hours. Let's assume that for someone who doesn't work
 at Google, you'd have to use on-demand pricing of $3.22/hr.
 This means that these trained models cost $57,348. 

Strict
ly speaking, throwing enough compute at a general enough gen
etic algorithm will eventually produce arbitrarily good perf
ormance, so while you can absolutely read this paper and col
lect interesting ideas about how to use genetic algorithms t
o accomplish multitask learning by having each new task leve
rage learned weights from previous tasks by defining modific
ations to a subset of components of a pre-existing model, th
ere's a meta-textual level on which this paper is just 'Jeff
 Dean spent enough money to feed a family of four for half a
 decade to get a 0.03% improvement on CIFAR-10.'

OpenAI is 
far and away the worst offender here, but it seems like ever
yone's doing it. You throw a fuckton of compute and a light 
ganache of new ideas at an existing problem with existing da
ta and existing benchmarks, and then if your numbers are inf
initesimally higher than their numbers, you get to put a lil
' sticker on your CV. Why should I trust that your ideas are
 even any good? I can't check them, I can't apply them to my
 own projects. 

Is this really what we're comfortable with 
as a community? A handful of corporations and the occasional
 university waving their dicks at everyone because they've g
ot the compute to burn and we don't? There's a level at whic
h I think there should be a new journal, exclusively for pap
ers in which you can replicate their experimental results in
 under eight hours on a single consumer GPU."	"https://www.reddit.com/r/MachineLearning/comments/uyratt/d_i_dont_really_trust_papers_out_of_top_labs/"	"264"	"1653630414.0"	"1670"	"uyratt"
2	"[D] Why can't you guys comment your fucking code?"	"Seriously.

I spent the last few years doing web app develop
ment. Dug into DL a couple months ago. Supposedly, compared 
to the post-post-post-docs doing AI stuff, JavaScript develo
pers should be inbred peasants. But every project these peas
ants release, even a fucking library that colorizes CLI outp
ut, has a catchy name, extensive docs, shitloads of comments
, fuckton of tests, semantic versioning, changelog, and, oh 
my god, better variable names than `ctx_h` or `lang_hs` or `
fuck_you_for_trying_to_understand`.

The concepts and ideas 
behind DL, GANs, LSTMs, CNNs, whatever – it's clear, it's si
mple, it's intuitive. The slog is to go through the jargon (
that keeps changing beneath your feet - what's the point of 
using fancy words if you can't keep them consistent?), the u
nnecessary equations, trying to squeeze meaning from bullshi
t language used in papers, figuring out the super important 
steps, preprocessing, hyperparameters optimization that the 
authors, oops, failed to mention.

Sorry for singling out, b
ut [look at this](https://github.com/facebookresearch/end-to
-end-negotiator/blob/master/src/agent.py) - what the fuck? I
f a developer anywhere else at Facebook would get this code 
for a review they would throw up.

- Do you intentionally tr
y to obfuscate your papers? Is pseudo-code a fucking premium
? Can you at least try to give some intuition before showeri
ng the reader with equations?

- How the fuck do you dare to
 release a paper without source code?

- Why the fuck do you
 never ever add comments to you code?

- When naming things,
 are you charged by the character? Do you get a bonus for ac
ronyms?

- Do you realize that OpenAI having needed to relea
se a 'baseline' TRPO implementation is a fucking disgrace to
 your profession?

- Jesus christ, who decided to name a ten
sor concatenation function `cat`?
"	"https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/"	"478"	"1499113449.0"	"1650"	"6l2esd"
3	"[D] The current and future state of AI/ML is shockingly demoralizing with little hope of redemption"	"I recently encountered the PaLM (Scaling Language Modeling w
ith Pathways) paper from Google Research and it opened up a 
can of worms of ideas I’ve felt I’ve intuitively had for a w
hile, but have been unable to express – and I know I can’t b
e the only one. Sometimes I wonder what the original pioneer
s of AI – Turing, Neumann, McCarthy, etc. – would think if t
hey could see the state of AI that we’ve gotten ourselves in
to. 67 authors, 83 pages, 540B parameters in a model, the in
ternals of which no one can say they comprehend with a strai
ght face, 6144 TPUs in a commercial lab that no one has acce
ss to, on a rig that no one can afford, trained on a volume 
of data that a human couldn’t process in a lifetime, 1 page 
on ethics with the same ideas that have been rehashed over a
nd over elsewhere with no attempt at a solution – bias, raci
sm, malicious use, etc. – for purposes that who asked for?


When I started my career as an AI/ML research engineer 2016,
 I was most interested in two types of tasks – 1.) those tha
t most humans could do but that would universally be conside
red tedious and non-scalable. I’m talking image classificati
on, sentiment analysis, even document summarization, etc. 2.
) tasks that humans lack the capacity to perform as well as 
computers for various reasons – forecasting, risk analysis, 
game playing, and so forth. I still love my career, and I tr
y to only work on projects in these areas, but it’s getting 
harder and harder.

This is because, somewhere along the way
, it became popular and unquestionably acceptable to push AI
 into domains that were originally uniquely human, those are
as that sit at the top of Maslows’s hierarchy of needs in te
rms of self-actualization – art, music, writing, singing, pr
ogramming, and so forth. These areas of endeavor have negati
ve logarithmic ability curves – the vast majority of people 
cannot do them well at all, about 10% can do them decently, 
and 1% or less can do them extraordinarily. The little discu
ssed problem with AI-generation is that, without extreme det
errence, we will sacrifice human achievement at the top perc
entile in the name of lowering the bar for a larger volume o
f people, until the AI ability range is the norm. This is be
cause relative to humans, AI is cheap, fast, and infinite, t
o the extent that investments in human achievement will be w
atered down at the societal, educational, and individual lev
el with each passing year. And unlike AI gameplay which supe
rseded humans decades ago, we won’t be able to just disquali
fy the machines and continue to play as if they didn’t exist
.

Almost everywhere I go, even this forum, I encounter almo
st universal deference given to current SOTA AI generation s
ystems like GPT-3, CODEX, DALL-E, etc., with almost no one e
xtending their implications to its logical conclusion, which
 is long-term convergence to the mean, to mediocrity, in the
 fields they claim to address or even enhance. If you’re an 
artist or writer and you’re using DALL-E or GPT-3 to “enhanc
e” your work, or if you’re a programmer saying, “GitHub Co-P
ilot makes me a better programmer?”, then how could you poss
ibly know? You’ve disrupted and bypassed your own creative p
rocess, which is thoughts -> (optionally words) -> actions -
> feedback -> repeat, and instead seeded your canvas with id
eas from a machine, the provenance of which you can’t unders
tand, nor can the machine reliably explain. And the more you
 do this, the more you make your creative processes dependen
t on said machine, until you must question whether or not yo
u could work at the same level without it.

When I was a col
lege student, I often dabbled with weed, LSD, and mushrooms,
 and for a while, I thought the ideas I was having while und
er the influence were revolutionary and groundbreaking – tha
t is until took it upon myself to actually start writing dow
n those ideas and then reviewing them while sober, when I re
alized they weren’t that special at all. What I eventually d
etermined is that, under the influence, it was impossible fo
r me to accurately evaluate the drug-induced ideas I was hav
ing because the influencing agent the generates the ideas th
emselves was disrupting the same frame of reference that is 
responsible evaluating said ideas. This is the same principl
e of – if you took a pill and it made you stupider, would ev
en know it? I believe that, especially over the long-term ti
meframe that crosses generations, there’s significant risk t
hat current AI-generation developments produces a similar ef
fect on humanity, and we mostly won’t even realize it has ha
ppened, much like a frog in boiling water. If you have child
ren like I do, how can you be aware of the the current SOTA 
in these areas, project that 20 to 30 years, and then and te
ll them with a straight face that it is worth them pursuing 
their talent in art, writing, or music? How can you be hones
t and still say that widespread implementation of auto-corre
ction hasn’t made you and others worse and worse at spelling
 over the years (a task that even I believe most would agree
 is tedious and worth automating).

Furthermore, I’ve yet to
 set anyone discuss the train – generate – train - generate 
feedback loop that long-term application of AI-generation sy
stems imply. The first generations of these models were trai
ned on wide swaths of web data generated by humans, but if t
hese systems are permitted to continually spit out content w
ithout restriction or verification, especially to the extent
 that it reduces or eliminates development and investment in
 human talent over the long term, then what happens to the 4
th or 5th generation of models? Eventually we encounter this
 situation where the AI is being trained almost exclusively 
on AI-generated content, and therefore with each generation,
 it settles more and more into the mean and mediocrity with 
no way out using current methods. By the time that happens, 
what will we have lost in terms of the creative capacity of 
people, and will we be able to get it back?

By relentlessly
 pursuing this direction so enthusiastically, I’m convinced 
that we as AI/ML developers, companies, and nations are past
 the point of no return, and it mostly comes down the invest
ments in time and money that we’ve made, as well as a prison
er’s dilemma with our competitors. As a society though, this
 direction we’ve chosen for short-term gains will almost cer
tainly make humanity worse off, mostly for those who are pow
erless to do anything about it – our children, our grandchil
dren, and generations to come.

If you’re an AI researcher o
r a data scientist like myself, how do you turn things back 
for yourself when you’ve spent years on years building your 
career in this direction? You’re likely making near or north
 of $200k annually TC and have a family to support, and so i
t’s too late, no matter how you feel about the direction the
 field has gone. If you’re a company, how do you standby and
 let your competitors aggressively push their AutoML solutio
ns into more and more markets without putting out your own? 
Moreover, if you’re a manager or thought leader in this fiel
d like Jeff Dean how do you justify to your own boss and you
r shareholders your team’s billions of dollars in AI investm
ent while simultaneously balancing ethical concerns? You can
’t – the only answer is bigger and bigger models, more and m
ore applications, more and more data, and more and more auto
mation, and then automating that even further. If you’re a c
ountry like the US, how do responsibly develop AI while your
 competitors like China single-mindedly push full steam ahea
d without an iota of ethical concern to replace you in numer
ous areas in global power dynamics? Once again, failing to c
ompete would be pre-emptively admitting defeat.

Even assumi
ng that none of what I’ve described here happens to such an 
extent, how are so few people not taking this seriously and 
discounting this possibility? If everything I’m saying is fe
ar-mongering and non-sense, then I’d be interested in hearin
g what you think human-AI co-existence looks like in 20 to 3
0 years and why it isn’t as demoralizing as I’ve made it out
 to be.

&#x200B;

EDIT: Day after posting this -- this post
 took off way more than I expected. Even if I received 20 - 
25 comments, I would have considered that a success, but thi
s went much further. Thank you to each one of you that has r
ead this post, even more so if you left a comment, and tripl
y so for those who gave awards! I've read almost every comme
nt that has come in (even the troll ones), and am truly grat
eful for each one, including those in sharp disagreement. I'
ve learned much more from this discussion with the sub than 
I could have imagined on this topic, from so many perspectiv
es. While I will try to reply as many comments as I can, the
 sheer comment volume combined with limited free time betwee
n work and family unfortunately means that there are many th
at I likely won't be able to get to. That will invariably in
clude some that I would love respond to under the assumption
 of infinite time, but I will do my best, even if the latenc
y stretches into days. Thank you all once again!"	"https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/"	"396"	"1659907526.0"	"1392"	"wiqjxv"
4	"[D] Does anybody else despise OpenAI?"	" I  mean, don't get me started with the closed source models
 they have that were trained using the work of unassuming in
dividuals who will never  see a penny for it. Put it up on G
ithub they said. I'm all for  open-source, but when a compan
y turns around and charges you for a  product they made with
 freely and publicly made content, while forbidding you from
 using the output to create competing models, that is where 
I  draw the line. It is simply ridiculous. 

Sam Altman coul
dn't be anymore predictable with his recent attempts to get 
the government to start regulating AI.

What  risks? The AI 
is just a messenger for information that is already out  the
re if one knows how/where to look. You don't need AI to lear
n how to  hack, to learn how to make weapons, etc. Fake news
/propaganda? The  internet has all of that covered. LLMs are
 no where near the level of AI  you see in sci-fi. I mean, a
re people really afraid of text? Yes, I  know that text can 
sometimes be malicious code such as viruses, but  those can 
be found on github as well.  If they fall for this they migh
t  as well shutdown the internet while they're at it.

He  i
s simply blowing things out of proportion and using fear to 
increase  the likelihood that they do what he wants, hurt th
e competition. I  bet he is probably teething with bitternes
s everytime a new huggingface  model comes out. The thought 
of us peasants being able to use AI  privately is too danger
ous. No, instead we must be fed scraps while they  slowly ta
ke away our jobs and determine our future.

This  is not a d
oomer post, as I am all in favor of the advancement of AI.  
However, the real danger here lies in having a company like 
OpenAI  dictate the future of humanity. I get it, the writin
g is on the wall;  the cost of human intelligence will go do
wn, but if everyone has their  personal AI then it wouldn't 
seem so bad or unfair would it? Listen,  something that has 
the power to render a college degree that costs  thousands o
f dollars worthless should be available to the public. This 
 is to offset the damages and job layoffs that will come as 
a result of  such an entity. It wouldn't be as bitter of a t
aste as it would if you were replaced by it while still not 
being able to access it. Everyone should be able to use it a
s leverage, it is the only fair solution.

If  we don't take
 action now, a company like ClosedAI will, and they are  not
 in favor of the common folk. Sam Altman is so calculated to
 the  point where there were times when he seemed to be shoo
ting OpenAI in the foot during his talk.  This move is to si
mply conceal his real intentions, to climb the ladder and ta
ke it with him. If he didn't include his company in his  ram
blings, he would be easily read. So instead, he pretends to 
be scared of his own product, in an effort to legitimize his
 claim. Don't fall  for it.

They are slowly making a  reput
ation as one the most hated tech companies, right up there w
ith  Adobe, and they don't show any sign of change. They hav
e no moat,  othewise they wouldn't feel so threatened to the
 point where they would have to resort to creating barriers 
of entry via regulation. This only  means one thing, we are 
slowly catching up. We just need someone to  vouch for human
ity's well-being, while acting as an opposing force to the  
evil corporations who are only looking out for themselves. Q
uestion is,  who would be a good candidate?"	"https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/"	"415"	"1684361728.0"	"1319"	"13kfxzy"
5	"[P] Landing the Falcon booster with Reinforcement Learning in OpenAI"	""	"https://gfycat.com/CoarseEmbellishedIsopod"	"55"	"1518871530.0"	"1292"	"7y6g79"
6	"[P] OpenAssistant - The world's largest open-source replication of ChatGPT"	"We’re excited to announce the release of OpenAssistant.

The
 future of AI development depends heavily on high quality da
tasets and models being made publicly available, and that’s 
exactly what this project does.

Watch the annoucement video
:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4
Kk)

&#x200B;

Our team has worked tirelessly over the past 
several months collecting large amounts of text-based input 
and feedback to create an incredibly diverse and unique data
set designed specifically for training language models or ot
her AI applications.

With over 600k human-generated data po
ints covering a wide range of topics and styles of writing, 
our dataset will be an invaluable tool for any developer loo
king to create state-of-the-art instruction models!

To make
 things even better, we are making this entire dataset free 
and accessible to all who wish to use it. Check it out today
 at our HF org: OpenAssistant

On top of that, we've trained
 very powerful models that you can try right now at: [open-a
ssistant.io/chat](https://open-assistant.io/chat) !"	"https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/"	"175"	"1681578898.0"	"1266"	"12nbixk"
7	"[D] Google 'We Have No Moat, And Neither Does OpenAI': Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI"	""	"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"	"205"	"1683216810.0"	"1169"	"137rxgw"
8	"We are Oriol Vinyals and David Silver from DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything"	"Hi there! We are Oriol Vinyals (/u/OriolVinyals) and David S
ilver (/u/David_Silver), lead researchers on DeepMind’s Alph
aStar team, joined by StarCraft II pro players TLO, and MaNa
.

This evening at DeepMind HQ we held a livestream demonstr
ation of AlphaStar playing against TLO and MaNa - you can re
ad more about the matches [here](https://deepmind.com/blog/a
lphastar-mastering-real-time-strategy-game-starcraft-ii/) or
 re-watch the stream on YouTube [here](https://www.youtube.c
om/watch?v=cUTMhmVh1qs).

Now, we’re excited to talk with yo
u about AlphaStar, the challenge of real-time strategy games
 for AI research, the matches themselves, and anything you’d
 like to know from TLO and MaNa about their experience playi
ng against AlphaStar! :)

We are opening this thread now and
 will be here at **16:00 GMT / 11:00 ET / 08:00PT** on Frida
y, 25 January to answer your questions.

&#x200B;

EDIT: Tha
nks everyone for your great questions. It was a blast, hope 
you enjoyed it as well!"	"https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/"	"1011"	"1548363323.0"	"1168"	"ajgzoc"
9	"[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data"	"[GPT-4 and professional benchmarks: the wrong answer to the 
wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-
professional-benchmarks)

*OpenAI may have tested on the tra
ining data. Besides, human benchmarks are meaningless for bo
ts.*

 **Problem 1: training data contamination**

To benchm
ark GPT-4’s coding ability, OpenAI evaluated it on problems 
from Codeforces, a website that hosts coding competitions. S
urprisingly, Horace He pointed out that GPT-4 solved 10/10 p
re-2021 problems and 0/10 recent problems in the easy catego
ry. The training data cutoff for GPT-4 is September 2021. Th
is strongly suggests that the model is able to memorize solu
tions from its training set — or at least partly memorize th
em, enough that it can fill in what it can’t recall.

As fur
ther evidence for this hypothesis, we tested it on Codeforce
s problems from different times in 2021. We found that it co
uld regularly solve problems in the easy category before Sep
tember 5, but none of the problems after September 12.

In f
act, we can definitively show that it has memorized problems
 in its training set: when prompted with the title of a Code
forces problem, GPT-4 includes a link to the exact contest w
here the problem appears (and the round number is almost cor
rect: it is off by one). Note that GPT-4 cannot access the I
nternet, so memorization is the only explanation."	"https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/"	"135"	"1679983023.0"	"995"	"124eyso"
10	"[D] Our community must get serious about opposing OpenAI"	"OpenAI was founded for the explicit purpose of democratizing
 access to AI and acting as a counterbalance to the closed o
ff world of big tech by developing open source tools.

They 
have abandoned this idea entirely.

Today, with the release 
of GPT4 and their direct statement that they will not releas
e details of the model creation due to 'safety concerns' and
 the competitive environment, they have created a precedent 
worse than those that existed before they entered the field.
 We're at risk now of other major players, who previously at
 least published their work and contributed to open source t
ools, close themselves off as well.

AI alignment is a serio
us issue that we definitely have not solved. Its a huge fiel
d with a dizzying array of ideas, beliefs and approaches. We
're talking about trying to capture the interests and goals 
of all humanity, after all. In this space, the one approach 
that is horrifying (and the one that OpenAI was LITERALLY cr
eated to prevent) is a singular or oligarchy of for profit c
orporations making this decision for us. This is exactly wha
t OpenAI plans to do.

I get it, GPT4 is incredible. However
, we are talking about the single most transformative techno
logy and societal change that humanity has ever made. It nee
ds to be for everyone or else the average person is going to
 be left behind.

We need to unify around open source develo
pment; choose companies that contribute to science, and cond
emn the ones that don't.

This conversation will only ever g
et more important."	"https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/"	"464"	"1678919641.0"	"2862"	"11sboh1"
11	"[D] I don't really trust papers out of 'Top Labs' anymore"	"I mean, I trust that the numbers they got are accurate and t
hat they really did the work and got the results. I believe 
those. It's just that, take the recent 'An Evolutionary Appr
oach to Dynamic Introduction of Tasks in Large-scale Multita
sk Learning Systems' paper. It's 18 pages of talking through
 this pretty convoluted evolutionary and multitask learning 
algorithm, it's pretty interesting, solves a bunch of proble
ms. But two notes. 

One, the big number they cite as the su
ccess metric is 99.43 on CIFAR-10, against a SotA of 99.40, 
so woop-de-fucking-doo in the grand scheme of things.

Two, 
there's a chart towards the end of the paper that details ho
w many TPU core-hours were used for just the training regime
ns that results in the final results. The sum total is 17,81
0 core-hours. Let's assume that for someone who doesn't work
 at Google, you'd have to use on-demand pricing of $3.22/hr.
 This means that these trained models cost $57,348. 

Strict
ly speaking, throwing enough compute at a general enough gen
etic algorithm will eventually produce arbitrarily good perf
ormance, so while you can absolutely read this paper and col
lect interesting ideas about how to use genetic algorithms t
o accomplish multitask learning by having each new task leve
rage learned weights from previous tasks by defining modific
ations to a subset of components of a pre-existing model, th
ere's a meta-textual level on which this paper is just 'Jeff
 Dean spent enough money to feed a family of four for half a
 decade to get a 0.03% improvement on CIFAR-10.'

OpenAI is 
far and away the worst offender here, but it seems like ever
yone's doing it. You throw a fuckton of compute and a light 
ganache of new ideas at an existing problem with existing da
ta and existing benchmarks, and then if your numbers are inf
initesimally higher than their numbers, you get to put a lil
' sticker on your CV. Why should I trust that your ideas are
 even any good? I can't check them, I can't apply them to my
 own projects. 

Is this really what we're comfortable with 
as a community? A handful of corporations and the occasional
 university waving their dicks at everyone because they've g
ot the compute to burn and we don't? There's a level at whic
h I think there should be a new journal, exclusively for pap
ers in which you can replicate their experimental results in
 under eight hours on a single consumer GPU."	"https://www.reddit.com/r/MachineLearning/comments/uyratt/d_i_dont_really_trust_papers_out_of_top_labs/"	"264"	"1653630414.0"	"1670"	"uyratt"
12	"[D] Why can't you guys comment your fucking code?"	"Seriously.

I spent the last few years doing web app develop
ment. Dug into DL a couple months ago. Supposedly, compared 
to the post-post-post-docs doing AI stuff, JavaScript develo
pers should be inbred peasants. But every project these peas
ants release, even a fucking library that colorizes CLI outp
ut, has a catchy name, extensive docs, shitloads of comments
, fuckton of tests, semantic versioning, changelog, and, oh 
my god, better variable names than `ctx_h` or `lang_hs` or `
fuck_you_for_trying_to_understand`.

The concepts and ideas 
behind DL, GANs, LSTMs, CNNs, whatever – it's clear, it's si
mple, it's intuitive. The slog is to go through the jargon (
that keeps changing beneath your feet - what's the point of 
using fancy words if you can't keep them consistent?), the u
nnecessary equations, trying to squeeze meaning from bullshi
t language used in papers, figuring out the super important 
steps, preprocessing, hyperparameters optimization that the 
authors, oops, failed to mention.

Sorry for singling out, b
ut [look at this](https://github.com/facebookresearch/end-to
-end-negotiator/blob/master/src/agent.py) - what the fuck? I
f a developer anywhere else at Facebook would get this code 
for a review they would throw up.

- Do you intentionally tr
y to obfuscate your papers? Is pseudo-code a fucking premium
? Can you at least try to give some intuition before showeri
ng the reader with equations?

- How the fuck do you dare to
 release a paper without source code?

- Why the fuck do you
 never ever add comments to you code?

- When naming things,
 are you charged by the character? Do you get a bonus for ac
ronyms?

- Do you realize that OpenAI having needed to relea
se a 'baseline' TRPO implementation is a fucking disgrace to
 your profession?

- Jesus christ, who decided to name a ten
sor concatenation function `cat`?
"	"https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/"	"478"	"1499113449.0"	"1646"	"6l2esd"
13	"[D] The current and future state of AI/ML is shockingly demoralizing with little hope of redemption"	"I recently encountered the PaLM (Scaling Language Modeling w
ith Pathways) paper from Google Research and it opened up a 
can of worms of ideas I’ve felt I’ve intuitively had for a w
hile, but have been unable to express – and I know I can’t b
e the only one. Sometimes I wonder what the original pioneer
s of AI – Turing, Neumann, McCarthy, etc. – would think if t
hey could see the state of AI that we’ve gotten ourselves in
to. 67 authors, 83 pages, 540B parameters in a model, the in
ternals of which no one can say they comprehend with a strai
ght face, 6144 TPUs in a commercial lab that no one has acce
ss to, on a rig that no one can afford, trained on a volume 
of data that a human couldn’t process in a lifetime, 1 page 
on ethics with the same ideas that have been rehashed over a
nd over elsewhere with no attempt at a solution – bias, raci
sm, malicious use, etc. – for purposes that who asked for?


When I started my career as an AI/ML research engineer 2016,
 I was most interested in two types of tasks – 1.) those tha
t most humans could do but that would universally be conside
red tedious and non-scalable. I’m talking image classificati
on, sentiment analysis, even document summarization, etc. 2.
) tasks that humans lack the capacity to perform as well as 
computers for various reasons – forecasting, risk analysis, 
game playing, and so forth. I still love my career, and I tr
y to only work on projects in these areas, but it’s getting 
harder and harder.

This is because, somewhere along the way
, it became popular and unquestionably acceptable to push AI
 into domains that were originally uniquely human, those are
as that sit at the top of Maslows’s hierarchy of needs in te
rms of self-actualization – art, music, writing, singing, pr
ogramming, and so forth. These areas of endeavor have negati
ve logarithmic ability curves – the vast majority of people 
cannot do them well at all, about 10% can do them decently, 
and 1% or less can do them extraordinarily. The little discu
ssed problem with AI-generation is that, without extreme det
errence, we will sacrifice human achievement at the top perc
entile in the name of lowering the bar for a larger volume o
f people, until the AI ability range is the norm. This is be
cause relative to humans, AI is cheap, fast, and infinite, t
o the extent that investments in human achievement will be w
atered down at the societal, educational, and individual lev
el with each passing year. And unlike AI gameplay which supe
rseded humans decades ago, we won’t be able to just disquali
fy the machines and continue to play as if they didn’t exist
.

Almost everywhere I go, even this forum, I encounter almo
st universal deference given to current SOTA AI generation s
ystems like GPT-3, CODEX, DALL-E, etc., with almost no one e
xtending their implications to its logical conclusion, which
 is long-term convergence to the mean, to mediocrity, in the
 fields they claim to address or even enhance. If you’re an 
artist or writer and you’re using DALL-E or GPT-3 to “enhanc
e” your work, or if you’re a programmer saying, “GitHub Co-P
ilot makes me a better programmer?”, then how could you poss
ibly know? You’ve disrupted and bypassed your own creative p
rocess, which is thoughts -> (optionally words) -> actions -
> feedback -> repeat, and instead seeded your canvas with id
eas from a machine, the provenance of which you can’t unders
tand, nor can the machine reliably explain. And the more you
 do this, the more you make your creative processes dependen
t on said machine, until you must question whether or not yo
u could work at the same level without it.

When I was a col
lege student, I often dabbled with weed, LSD, and mushrooms,
 and for a while, I thought the ideas I was having while und
er the influence were revolutionary and groundbreaking – tha
t is until took it upon myself to actually start writing dow
n those ideas and then reviewing them while sober, when I re
alized they weren’t that special at all. What I eventually d
etermined is that, under the influence, it was impossible fo
r me to accurately evaluate the drug-induced ideas I was hav
ing because the influencing agent the generates the ideas th
emselves was disrupting the same frame of reference that is 
responsible evaluating said ideas. This is the same principl
e of – if you took a pill and it made you stupider, would ev
en know it? I believe that, especially over the long-term ti
meframe that crosses generations, there’s significant risk t
hat current AI-generation developments produces a similar ef
fect on humanity, and we mostly won’t even realize it has ha
ppened, much like a frog in boiling water. If you have child
ren like I do, how can you be aware of the the current SOTA 
in these areas, project that 20 to 30 years, and then and te
ll them with a straight face that it is worth them pursuing 
their talent in art, writing, or music? How can you be hones
t and still say that widespread implementation of auto-corre
ction hasn’t made you and others worse and worse at spelling
 over the years (a task that even I believe most would agree
 is tedious and worth automating).

Furthermore, I’ve yet to
 set anyone discuss the train – generate – train - generate 
feedback loop that long-term application of AI-generation sy
stems imply. The first generations of these models were trai
ned on wide swaths of web data generated by humans, but if t
hese systems are permitted to continually spit out content w
ithout restriction or verification, especially to the extent
 that it reduces or eliminates development and investment in
 human talent over the long term, then what happens to the 4
th or 5th generation of models? Eventually we encounter this
 situation where the AI is being trained almost exclusively 
on AI-generated content, and therefore with each generation,
 it settles more and more into the mean and mediocrity with 
no way out using current methods. By the time that happens, 
what will we have lost in terms of the creative capacity of 
people, and will we be able to get it back?

By relentlessly
 pursuing this direction so enthusiastically, I’m convinced 
that we as AI/ML developers, companies, and nations are past
 the point of no return, and it mostly comes down the invest
ments in time and money that we’ve made, as well as a prison
er’s dilemma with our competitors. As a society though, this
 direction we’ve chosen for short-term gains will almost cer
tainly make humanity worse off, mostly for those who are pow
erless to do anything about it – our children, our grandchil
dren, and generations to come.

If you’re an AI researcher o
r a data scientist like myself, how do you turn things back 
for yourself when you’ve spent years on years building your 
career in this direction? You’re likely making near or north
 of $200k annually TC and have a family to support, and so i
t’s too late, no matter how you feel about the direction the
 field has gone. If you’re a company, how do you standby and
 let your competitors aggressively push their AutoML solutio
ns into more and more markets without putting out your own? 
Moreover, if you’re a manager or thought leader in this fiel
d like Jeff Dean how do you justify to your own boss and you
r shareholders your team’s billions of dollars in AI investm
ent while simultaneously balancing ethical concerns? You can
’t – the only answer is bigger and bigger models, more and m
ore applications, more and more data, and more and more auto
mation, and then automating that even further. If you’re a c
ountry like the US, how do responsibly develop AI while your
 competitors like China single-mindedly push full steam ahea
d without an iota of ethical concern to replace you in numer
ous areas in global power dynamics? Once again, failing to c
ompete would be pre-emptively admitting defeat.

Even assumi
ng that none of what I’ve described here happens to such an 
extent, how are so few people not taking this seriously and 
discounting this possibility? If everything I’m saying is fe
ar-mongering and non-sense, then I’d be interested in hearin
g what you think human-AI co-existence looks like in 20 to 3
0 years and why it isn’t as demoralizing as I’ve made it out
 to be.

&#x200B;

EDIT: Day after posting this -- this post
 took off way more than I expected. Even if I received 20 - 
25 comments, I would have considered that a success, but thi
s went much further. Thank you to each one of you that has r
ead this post, even more so if you left a comment, and tripl
y so for those who gave awards! I've read almost every comme
nt that has come in (even the troll ones), and am truly grat
eful for each one, including those in sharp disagreement. I'
ve learned much more from this discussion with the sub than 
I could have imagined on this topic, from so many perspectiv
es. While I will try to reply as many comments as I can, the
 sheer comment volume combined with limited free time betwee
n work and family unfortunately means that there are many th
at I likely won't be able to get to. That will invariably in
clude some that I would love respond to under the assumption
 of infinite time, but I will do my best, even if the latenc
y stretches into days. Thank you all once again!"	"https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/"	"396"	"1659907526.0"	"1392"	"wiqjxv"
14	"[D] Does anybody else despise OpenAI?"	" I  mean, don't get me started with the closed source models
 they have that were trained using the work of unassuming in
dividuals who will never  see a penny for it. Put it up on G
ithub they said. I'm all for  open-source, but when a compan
y turns around and charges you for a  product they made with
 freely and publicly made content, while forbidding you from
 using the output to create competing models, that is where 
I  draw the line. It is simply ridiculous. 

Sam Altman coul
dn't be anymore predictable with his recent attempts to get 
the government to start regulating AI.

What  risks? The AI 
is just a messenger for information that is already out  the
re if one knows how/where to look. You don't need AI to lear
n how to  hack, to learn how to make weapons, etc. Fake news
/propaganda? The  internet has all of that covered. LLMs are
 no where near the level of AI  you see in sci-fi. I mean, a
re people really afraid of text? Yes, I  know that text can 
sometimes be malicious code such as viruses, but  those can 
be found on github as well.  If they fall for this they migh
t  as well shutdown the internet while they're at it.

He  i
s simply blowing things out of proportion and using fear to 
increase  the likelihood that they do what he wants, hurt th
e competition. I  bet he is probably teething with bitternes
s everytime a new huggingface  model comes out. The thought 
of us peasants being able to use AI  privately is too danger
ous. No, instead we must be fed scraps while they  slowly ta
ke away our jobs and determine our future.

This  is not a d
oomer post, as I am all in favor of the advancement of AI.  
However, the real danger here lies in having a company like 
OpenAI  dictate the future of humanity. I get it, the writin
g is on the wall;  the cost of human intelligence will go do
wn, but if everyone has their  personal AI then it wouldn't 
seem so bad or unfair would it? Listen,  something that has 
the power to render a college degree that costs  thousands o
f dollars worthless should be available to the public. This 
 is to offset the damages and job layoffs that will come as 
a result of  such an entity. It wouldn't be as bitter of a t
aste as it would if you were replaced by it while still not 
being able to access it. Everyone should be able to use it a
s leverage, it is the only fair solution.

If  we don't take
 action now, a company like ClosedAI will, and they are  not
 in favor of the common folk. Sam Altman is so calculated to
 the  point where there were times when he seemed to be shoo
ting OpenAI in the foot during his talk.  This move is to si
mply conceal his real intentions, to climb the ladder and ta
ke it with him. If he didn't include his company in his  ram
blings, he would be easily read. So instead, he pretends to 
be scared of his own product, in an effort to legitimize his
 claim. Don't fall  for it.

They are slowly making a  reput
ation as one the most hated tech companies, right up there w
ith  Adobe, and they don't show any sign of change. They hav
e no moat,  othewise they wouldn't feel so threatened to the
 point where they would have to resort to creating barriers 
of entry via regulation. This only  means one thing, we are 
slowly catching up. We just need someone to  vouch for human
ity's well-being, while acting as an opposing force to the  
evil corporations who are only looking out for themselves. Q
uestion is,  who would be a good candidate?"	"https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/"	"415"	"1684361728.0"	"1313"	"13kfxzy"
15	"[P] Landing the Falcon booster with Reinforcement Learning in OpenAI"	""	"https://gfycat.com/CoarseEmbellishedIsopod"	"55"	"1518871530.0"	"1290"	"7y6g79"
16	"[P] OpenAssistant - The world's largest open-source replication of ChatGPT"	"We’re excited to announce the release of OpenAssistant.

The
 future of AI development depends heavily on high quality da
tasets and models being made publicly available, and that’s 
exactly what this project does.

Watch the annoucement video
:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4
Kk)

&#x200B;

Our team has worked tirelessly over the past 
several months collecting large amounts of text-based input 
and feedback to create an incredibly diverse and unique data
set designed specifically for training language models or ot
her AI applications.

With over 600k human-generated data po
ints covering a wide range of topics and styles of writing, 
our dataset will be an invaluable tool for any developer loo
king to create state-of-the-art instruction models!

To make
 things even better, we are making this entire dataset free 
and accessible to all who wish to use it. Check it out today
 at our HF org: OpenAssistant

On top of that, we've trained
 very powerful models that you can try right now at: [open-a
ssistant.io/chat](https://open-assistant.io/chat) !"	"https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/"	"175"	"1681578898.0"	"1259"	"12nbixk"
17	"[D] Google 'We Have No Moat, And Neither Does OpenAI': Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI"	""	"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"	"205"	"1683216810.0"	"1172"	"137rxgw"
18	"We are Oriol Vinyals and David Silver from DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything"	"Hi there! We are Oriol Vinyals (/u/OriolVinyals) and David S
ilver (/u/David_Silver), lead researchers on DeepMind’s Alph
aStar team, joined by StarCraft II pro players TLO, and MaNa
.

This evening at DeepMind HQ we held a livestream demonstr
ation of AlphaStar playing against TLO and MaNa - you can re
ad more about the matches [here](https://deepmind.com/blog/a
lphastar-mastering-real-time-strategy-game-starcraft-ii/) or
 re-watch the stream on YouTube [here](https://www.youtube.c
om/watch?v=cUTMhmVh1qs).

Now, we’re excited to talk with yo
u about AlphaStar, the challenge of real-time strategy games
 for AI research, the matches themselves, and anything you’d
 like to know from TLO and MaNa about their experience playi
ng against AlphaStar! :)

We are opening this thread now and
 will be here at **16:00 GMT / 11:00 ET / 08:00PT** on Frida
y, 25 January to answer your questions.

&#x200B;

EDIT: Tha
nks everyone for your great questions. It was a blast, hope 
you enjoyed it as well!"	"https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/"	"1011"	"1548363323.0"	"1169"	"ajgzoc"
19	"[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data"	"[GPT-4 and professional benchmarks: the wrong answer to the 
wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-
professional-benchmarks)

*OpenAI may have tested on the tra
ining data. Besides, human benchmarks are meaningless for bo
ts.*

 **Problem 1: training data contamination**

To benchm
ark GPT-4’s coding ability, OpenAI evaluated it on problems 
from Codeforces, a website that hosts coding competitions. S
urprisingly, Horace He pointed out that GPT-4 solved 10/10 p
re-2021 problems and 0/10 recent problems in the easy catego
ry. The training data cutoff for GPT-4 is September 2021. Th
is strongly suggests that the model is able to memorize solu
tions from its training set — or at least partly memorize th
em, enough that it can fill in what it can’t recall.

As fur
ther evidence for this hypothesis, we tested it on Codeforce
s problems from different times in 2021. We found that it co
uld regularly solve problems in the easy category before Sep
tember 5, but none of the problems after September 12.

In f
act, we can definitively show that it has memorized problems
 in its training set: when prompted with the title of a Code
forces problem, GPT-4 includes a link to the exact contest w
here the problem appears (and the round number is almost cor
rect: it is off by one). Note that GPT-4 cannot access the I
nternet, so memorization is the only explanation."	"https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/"	"135"	"1679983023.0"	"994"	"124eyso"
20	"[D] Our community must get serious about opposing OpenAI"	"OpenAI was founded for the explicit purpose of democratizing
 access to AI and acting as a counterbalance to the closed o
ff world of big tech by developing open source tools.

They 
have abandoned this idea entirely.

Today, with the release 
of GPT4 and their direct statement that they will not releas
e details of the model creation due to 'safety concerns' and
 the competitive environment, they have created a precedent 
worse than those that existed before they entered the field.
 We're at risk now of other major players, who previously at
 least published their work and contributed to open source t
ools, close themselves off as well.

AI alignment is a serio
us issue that we definitely have not solved. Its a huge fiel
d with a dizzying array of ideas, beliefs and approaches. We
're talking about trying to capture the interests and goals 
of all humanity, after all. In this space, the one approach 
that is horrifying (and the one that OpenAI was LITERALLY cr
eated to prevent) is a singular or oligarchy of for profit c
orporations making this decision for us. This is exactly wha
t OpenAI plans to do.

I get it, GPT4 is incredible. However
, we are talking about the single most transformative techno
logy and societal change that humanity has ever made. It nee
ds to be for everyone or else the average person is going to
 be left behind.

We need to unify around open source develo
pment; choose companies that contribute to science, and cond
emn the ones that don't.

This conversation will only ever g
et more important."	"https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/"	"464"	"1678919641.0"	"2862"	"11sboh1"
21	"[D] I don't really trust papers out of 'Top Labs' anymore"	"I mean, I trust that the numbers they got are accurate and t
hat they really did the work and got the results. I believe 
those. It's just that, take the recent 'An Evolutionary Appr
oach to Dynamic Introduction of Tasks in Large-scale Multita
sk Learning Systems' paper. It's 18 pages of talking through
 this pretty convoluted evolutionary and multitask learning 
algorithm, it's pretty interesting, solves a bunch of proble
ms. But two notes. 

One, the big number they cite as the su
ccess metric is 99.43 on CIFAR-10, against a SotA of 99.40, 
so woop-de-fucking-doo in the grand scheme of things.

Two, 
there's a chart towards the end of the paper that details ho
w many TPU core-hours were used for just the training regime
ns that results in the final results. The sum total is 17,81
0 core-hours. Let's assume that for someone who doesn't work
 at Google, you'd have to use on-demand pricing of $3.22/hr.
 This means that these trained models cost $57,348. 

Strict
ly speaking, throwing enough compute at a general enough gen
etic algorithm will eventually produce arbitrarily good perf
ormance, so while you can absolutely read this paper and col
lect interesting ideas about how to use genetic algorithms t
o accomplish multitask learning by having each new task leve
rage learned weights from previous tasks by defining modific
ations to a subset of components of a pre-existing model, th
ere's a meta-textual level on which this paper is just 'Jeff
 Dean spent enough money to feed a family of four for half a
 decade to get a 0.03% improvement on CIFAR-10.'

OpenAI is 
far and away the worst offender here, but it seems like ever
yone's doing it. You throw a fuckton of compute and a light 
ganache of new ideas at an existing problem with existing da
ta and existing benchmarks, and then if your numbers are inf
initesimally higher than their numbers, you get to put a lil
' sticker on your CV. Why should I trust that your ideas are
 even any good? I can't check them, I can't apply them to my
 own projects. 

Is this really what we're comfortable with 
as a community? A handful of corporations and the occasional
 university waving their dicks at everyone because they've g
ot the compute to burn and we don't? There's a level at whic
h I think there should be a new journal, exclusively for pap
ers in which you can replicate their experimental results in
 under eight hours on a single consumer GPU."	"https://www.reddit.com/r/MachineLearning/comments/uyratt/d_i_dont_really_trust_papers_out_of_top_labs/"	"264"	"1653630414.0"	"1669"	"uyratt"
22	"[D] Why can't you guys comment your fucking code?"	"Seriously.

I spent the last few years doing web app develop
ment. Dug into DL a couple months ago. Supposedly, compared 
to the post-post-post-docs doing AI stuff, JavaScript develo
pers should be inbred peasants. But every project these peas
ants release, even a fucking library that colorizes CLI outp
ut, has a catchy name, extensive docs, shitloads of comments
, fuckton of tests, semantic versioning, changelog, and, oh 
my god, better variable names than `ctx_h` or `lang_hs` or `
fuck_you_for_trying_to_understand`.

The concepts and ideas 
behind DL, GANs, LSTMs, CNNs, whatever – it's clear, it's si
mple, it's intuitive. The slog is to go through the jargon (
that keeps changing beneath your feet - what's the point of 
using fancy words if you can't keep them consistent?), the u
nnecessary equations, trying to squeeze meaning from bullshi
t language used in papers, figuring out the super important 
steps, preprocessing, hyperparameters optimization that the 
authors, oops, failed to mention.

Sorry for singling out, b
ut [look at this](https://github.com/facebookresearch/end-to
-end-negotiator/blob/master/src/agent.py) - what the fuck? I
f a developer anywhere else at Facebook would get this code 
for a review they would throw up.

- Do you intentionally tr
y to obfuscate your papers? Is pseudo-code a fucking premium
? Can you at least try to give some intuition before showeri
ng the reader with equations?

- How the fuck do you dare to
 release a paper without source code?

- Why the fuck do you
 never ever add comments to you code?

- When naming things,
 are you charged by the character? Do you get a bonus for ac
ronyms?

- Do you realize that OpenAI having needed to relea
se a 'baseline' TRPO implementation is a fucking disgrace to
 your profession?

- Jesus christ, who decided to name a ten
sor concatenation function `cat`?
"	"https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/"	"478"	"1499113449.0"	"1646"	"6l2esd"
23	"[D] The current and future state of AI/ML is shockingly demoralizing with little hope of redemption"	"I recently encountered the PaLM (Scaling Language Modeling w
ith Pathways) paper from Google Research and it opened up a 
can of worms of ideas I’ve felt I’ve intuitively had for a w
hile, but have been unable to express – and I know I can’t b
e the only one. Sometimes I wonder what the original pioneer
s of AI – Turing, Neumann, McCarthy, etc. – would think if t
hey could see the state of AI that we’ve gotten ourselves in
to. 67 authors, 83 pages, 540B parameters in a model, the in
ternals of which no one can say they comprehend with a strai
ght face, 6144 TPUs in a commercial lab that no one has acce
ss to, on a rig that no one can afford, trained on a volume 
of data that a human couldn’t process in a lifetime, 1 page 
on ethics with the same ideas that have been rehashed over a
nd over elsewhere with no attempt at a solution – bias, raci
sm, malicious use, etc. – for purposes that who asked for?


When I started my career as an AI/ML research engineer 2016,
 I was most interested in two types of tasks – 1.) those tha
t most humans could do but that would universally be conside
red tedious and non-scalable. I’m talking image classificati
on, sentiment analysis, even document summarization, etc. 2.
) tasks that humans lack the capacity to perform as well as 
computers for various reasons – forecasting, risk analysis, 
game playing, and so forth. I still love my career, and I tr
y to only work on projects in these areas, but it’s getting 
harder and harder.

This is because, somewhere along the way
, it became popular and unquestionably acceptable to push AI
 into domains that were originally uniquely human, those are
as that sit at the top of Maslows’s hierarchy of needs in te
rms of self-actualization – art, music, writing, singing, pr
ogramming, and so forth. These areas of endeavor have negati
ve logarithmic ability curves – the vast majority of people 
cannot do them well at all, about 10% can do them decently, 
and 1% or less can do them extraordinarily. The little discu
ssed problem with AI-generation is that, without extreme det
errence, we will sacrifice human achievement at the top perc
entile in the name of lowering the bar for a larger volume o
f people, until the AI ability range is the norm. This is be
cause relative to humans, AI is cheap, fast, and infinite, t
o the extent that investments in human achievement will be w
atered down at the societal, educational, and individual lev
el with each passing year. And unlike AI gameplay which supe
rseded humans decades ago, we won’t be able to just disquali
fy the machines and continue to play as if they didn’t exist
.

Almost everywhere I go, even this forum, I encounter almo
st universal deference given to current SOTA AI generation s
ystems like GPT-3, CODEX, DALL-E, etc., with almost no one e
xtending their implications to its logical conclusion, which
 is long-term convergence to the mean, to mediocrity, in the
 fields they claim to address or even enhance. If you’re an 
artist or writer and you’re using DALL-E or GPT-3 to “enhanc
e” your work, or if you’re a programmer saying, “GitHub Co-P
ilot makes me a better programmer?”, then how could you poss
ibly know? You’ve disrupted and bypassed your own creative p
rocess, which is thoughts -> (optionally words) -> actions -
> feedback -> repeat, and instead seeded your canvas with id
eas from a machine, the provenance of which you can’t unders
tand, nor can the machine reliably explain. And the more you
 do this, the more you make your creative processes dependen
t on said machine, until you must question whether or not yo
u could work at the same level without it.

When I was a col
lege student, I often dabbled with weed, LSD, and mushrooms,
 and for a while, I thought the ideas I was having while und
er the influence were revolutionary and groundbreaking – tha
t is until took it upon myself to actually start writing dow
n those ideas and then reviewing them while sober, when I re
alized they weren’t that special at all. What I eventually d
etermined is that, under the influence, it was impossible fo
r me to accurately evaluate the drug-induced ideas I was hav
ing because the influencing agent the generates the ideas th
emselves was disrupting the same frame of reference that is 
responsible evaluating said ideas. This is the same principl
e of – if you took a pill and it made you stupider, would ev
en know it? I believe that, especially over the long-term ti
meframe that crosses generations, there’s significant risk t
hat current AI-generation developments produces a similar ef
fect on humanity, and we mostly won’t even realize it has ha
ppened, much like a frog in boiling water. If you have child
ren like I do, how can you be aware of the the current SOTA 
in these areas, project that 20 to 30 years, and then and te
ll them with a straight face that it is worth them pursuing 
their talent in art, writing, or music? How can you be hones
t and still say that widespread implementation of auto-corre
ction hasn’t made you and others worse and worse at spelling
 over the years (a task that even I believe most would agree
 is tedious and worth automating).

Furthermore, I’ve yet to
 set anyone discuss the train – generate – train - generate 
feedback loop that long-term application of AI-generation sy
stems imply. The first generations of these models were trai
ned on wide swaths of web data generated by humans, but if t
hese systems are permitted to continually spit out content w
ithout restriction or verification, especially to the extent
 that it reduces or eliminates development and investment in
 human talent over the long term, then what happens to the 4
th or 5th generation of models? Eventually we encounter this
 situation where the AI is being trained almost exclusively 
on AI-generated content, and therefore with each generation,
 it settles more and more into the mean and mediocrity with 
no way out using current methods. By the time that happens, 
what will we have lost in terms of the creative capacity of 
people, and will we be able to get it back?

By relentlessly
 pursuing this direction so enthusiastically, I’m convinced 
that we as AI/ML developers, companies, and nations are past
 the point of no return, and it mostly comes down the invest
ments in time and money that we’ve made, as well as a prison
er’s dilemma with our competitors. As a society though, this
 direction we’ve chosen for short-term gains will almost cer
tainly make humanity worse off, mostly for those who are pow
erless to do anything about it – our children, our grandchil
dren, and generations to come.

If you’re an AI researcher o
r a data scientist like myself, how do you turn things back 
for yourself when you’ve spent years on years building your 
career in this direction? You’re likely making near or north
 of $200k annually TC and have a family to support, and so i
t’s too late, no matter how you feel about the direction the
 field has gone. If you’re a company, how do you standby and
 let your competitors aggressively push their AutoML solutio
ns into more and more markets without putting out your own? 
Moreover, if you’re a manager or thought leader in this fiel
d like Jeff Dean how do you justify to your own boss and you
r shareholders your team’s billions of dollars in AI investm
ent while simultaneously balancing ethical concerns? You can
’t – the only answer is bigger and bigger models, more and m
ore applications, more and more data, and more and more auto
mation, and then automating that even further. If you’re a c
ountry like the US, how do responsibly develop AI while your
 competitors like China single-mindedly push full steam ahea
d without an iota of ethical concern to replace you in numer
ous areas in global power dynamics? Once again, failing to c
ompete would be pre-emptively admitting defeat.

Even assumi
ng that none of what I’ve described here happens to such an 
extent, how are so few people not taking this seriously and 
discounting this possibility? If everything I’m saying is fe
ar-mongering and non-sense, then I’d be interested in hearin
g what you think human-AI co-existence looks like in 20 to 3
0 years and why it isn’t as demoralizing as I’ve made it out
 to be.

&#x200B;

EDIT: Day after posting this -- this post
 took off way more than I expected. Even if I received 20 - 
25 comments, I would have considered that a success, but thi
s went much further. Thank you to each one of you that has r
ead this post, even more so if you left a comment, and tripl
y so for those who gave awards! I've read almost every comme
nt that has come in (even the troll ones), and am truly grat
eful for each one, including those in sharp disagreement. I'
ve learned much more from this discussion with the sub than 
I could have imagined on this topic, from so many perspectiv
es. While I will try to reply as many comments as I can, the
 sheer comment volume combined with limited free time betwee
n work and family unfortunately means that there are many th
at I likely won't be able to get to. That will invariably in
clude some that I would love respond to under the assumption
 of infinite time, but I will do my best, even if the latenc
y stretches into days. Thank you all once again!"	"https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/"	"396"	"1659907526.0"	"1392"	"wiqjxv"
24	"[D] Does anybody else despise OpenAI?"	" I  mean, don't get me started with the closed source models
 they have that were trained using the work of unassuming in
dividuals who will never  see a penny for it. Put it up on G
ithub they said. I'm all for  open-source, but when a compan
y turns around and charges you for a  product they made with
 freely and publicly made content, while forbidding you from
 using the output to create competing models, that is where 
I  draw the line. It is simply ridiculous. 

Sam Altman coul
dn't be anymore predictable with his recent attempts to get 
the government to start regulating AI.

What  risks? The AI 
is just a messenger for information that is already out  the
re if one knows how/where to look. You don't need AI to lear
n how to  hack, to learn how to make weapons, etc. Fake news
/propaganda? The  internet has all of that covered. LLMs are
 no where near the level of AI  you see in sci-fi. I mean, a
re people really afraid of text? Yes, I  know that text can 
sometimes be malicious code such as viruses, but  those can 
be found on github as well.  If they fall for this they migh
t  as well shutdown the internet while they're at it.

He  i
s simply blowing things out of proportion and using fear to 
increase  the likelihood that they do what he wants, hurt th
e competition. I  bet he is probably teething with bitternes
s everytime a new huggingface  model comes out. The thought 
of us peasants being able to use AI  privately is too danger
ous. No, instead we must be fed scraps while they  slowly ta
ke away our jobs and determine our future.

This  is not a d
oomer post, as I am all in favor of the advancement of AI.  
However, the real danger here lies in having a company like 
OpenAI  dictate the future of humanity. I get it, the writin
g is on the wall;  the cost of human intelligence will go do
wn, but if everyone has their  personal AI then it wouldn't 
seem so bad or unfair would it? Listen,  something that has 
the power to render a college degree that costs  thousands o
f dollars worthless should be available to the public. This 
 is to offset the damages and job layoffs that will come as 
a result of  such an entity. It wouldn't be as bitter of a t
aste as it would if you were replaced by it while still not 
being able to access it. Everyone should be able to use it a
s leverage, it is the only fair solution.

If  we don't take
 action now, a company like ClosedAI will, and they are  not
 in favor of the common folk. Sam Altman is so calculated to
 the  point where there were times when he seemed to be shoo
ting OpenAI in the foot during his talk.  This move is to si
mply conceal his real intentions, to climb the ladder and ta
ke it with him. If he didn't include his company in his  ram
blings, he would be easily read. So instead, he pretends to 
be scared of his own product, in an effort to legitimize his
 claim. Don't fall  for it.

They are slowly making a  reput
ation as one the most hated tech companies, right up there w
ith  Adobe, and they don't show any sign of change. They hav
e no moat,  othewise they wouldn't feel so threatened to the
 point where they would have to resort to creating barriers 
of entry via regulation. This only  means one thing, we are 
slowly catching up. We just need someone to  vouch for human
ity's well-being, while acting as an opposing force to the  
evil corporations who are only looking out for themselves. Q
uestion is,  who would be a good candidate?"	"https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/"	"415"	"1684361728.0"	"1317"	"13kfxzy"
25	"[P] Landing the Falcon booster with Reinforcement Learning in OpenAI"	""	"https://gfycat.com/CoarseEmbellishedIsopod"	"55"	"1518871530.0"	"1294"	"7y6g79"
26	"[P] OpenAssistant - The world's largest open-source replication of ChatGPT"	"We’re excited to announce the release of OpenAssistant.

The
 future of AI development depends heavily on high quality da
tasets and models being made publicly available, and that’s 
exactly what this project does.

Watch the annoucement video
:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4
Kk)

&#x200B;

Our team has worked tirelessly over the past 
several months collecting large amounts of text-based input 
and feedback to create an incredibly diverse and unique data
set designed specifically for training language models or ot
her AI applications.

With over 600k human-generated data po
ints covering a wide range of topics and styles of writing, 
our dataset will be an invaluable tool for any developer loo
king to create state-of-the-art instruction models!

To make
 things even better, we are making this entire dataset free 
and accessible to all who wish to use it. Check it out today
 at our HF org: OpenAssistant

On top of that, we've trained
 very powerful models that you can try right now at: [open-a
ssistant.io/chat](https://open-assistant.io/chat) !"	"https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/"	"175"	"1681578898.0"	"1256"	"12nbixk"
27	"[D] Google 'We Have No Moat, And Neither Does OpenAI': Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI"	""	"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"	"205"	"1683216810.0"	"1171"	"137rxgw"
28	"We are Oriol Vinyals and David Silver from DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything"	"Hi there! We are Oriol Vinyals (/u/OriolVinyals) and David S
ilver (/u/David_Silver), lead researchers on DeepMind’s Alph
aStar team, joined by StarCraft II pro players TLO, and MaNa
.

This evening at DeepMind HQ we held a livestream demonstr
ation of AlphaStar playing against TLO and MaNa - you can re
ad more about the matches [here](https://deepmind.com/blog/a
lphastar-mastering-real-time-strategy-game-starcraft-ii/) or
 re-watch the stream on YouTube [here](https://www.youtube.c
om/watch?v=cUTMhmVh1qs).

Now, we’re excited to talk with yo
u about AlphaStar, the challenge of real-time strategy games
 for AI research, the matches themselves, and anything you’d
 like to know from TLO and MaNa about their experience playi
ng against AlphaStar! :)

We are opening this thread now and
 will be here at **16:00 GMT / 11:00 ET / 08:00PT** on Frida
y, 25 January to answer your questions.

&#x200B;

EDIT: Tha
nks everyone for your great questions. It was a blast, hope 
you enjoyed it as well!"	"https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/"	"1011"	"1548363323.0"	"1167"	"ajgzoc"
29	"[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data"	"[GPT-4 and professional benchmarks: the wrong answer to the 
wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-
professional-benchmarks)

*OpenAI may have tested on the tra
ining data. Besides, human benchmarks are meaningless for bo
ts.*

 **Problem 1: training data contamination**

To benchm
ark GPT-4’s coding ability, OpenAI evaluated it on problems 
from Codeforces, a website that hosts coding competitions. S
urprisingly, Horace He pointed out that GPT-4 solved 10/10 p
re-2021 problems and 0/10 recent problems in the easy catego
ry. The training data cutoff for GPT-4 is September 2021. Th
is strongly suggests that the model is able to memorize solu
tions from its training set — or at least partly memorize th
em, enough that it can fill in what it can’t recall.

As fur
ther evidence for this hypothesis, we tested it on Codeforce
s problems from different times in 2021. We found that it co
uld regularly solve problems in the easy category before Sep
tember 5, but none of the problems after September 12.

In f
act, we can definitively show that it has memorized problems
 in its training set: when prompted with the title of a Code
forces problem, GPT-4 includes a link to the exact contest w
here the problem appears (and the round number is almost cor
rect: it is off by one). Note that GPT-4 cannot access the I
nternet, so memorization is the only explanation."	"https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/"	"135"	"1679983023.0"	"989"	"124eyso"
30	"[D] Our community must get serious about opposing OpenAI"	"OpenAI was founded for the explicit purpose of democratizing
 access to AI and acting as a counterbalance to the closed o
ff world of big tech by developing open source tools.

They 
have abandoned this idea entirely.

Today, with the release 
of GPT4 and their direct statement that they will not releas
e details of the model creation due to 'safety concerns' and
 the competitive environment, they have created a precedent 
worse than those that existed before they entered the field.
 We're at risk now of other major players, who previously at
 least published their work and contributed to open source t
ools, close themselves off as well.

AI alignment is a serio
us issue that we definitely have not solved. Its a huge fiel
d with a dizzying array of ideas, beliefs and approaches. We
're talking about trying to capture the interests and goals 
of all humanity, after all. In this space, the one approach 
that is horrifying (and the one that OpenAI was LITERALLY cr
eated to prevent) is a singular or oligarchy of for profit c
orporations making this decision for us. This is exactly wha
t OpenAI plans to do.

I get it, GPT4 is incredible. However
, we are talking about the single most transformative techno
logy and societal change that humanity has ever made. It nee
ds to be for everyone or else the average person is going to
 be left behind.

We need to unify around open source develo
pment; choose companies that contribute to science, and cond
emn the ones that don't.

This conversation will only ever g
et more important."	"https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/"	"464"	"1678919641.0"	"2866"	"11sboh1"
31	"[D] I don't really trust papers out of 'Top Labs' anymore"	"I mean, I trust that the numbers they got are accurate and t
hat they really did the work and got the results. I believe 
those. It's just that, take the recent 'An Evolutionary Appr
oach to Dynamic Introduction of Tasks in Large-scale Multita
sk Learning Systems' paper. It's 18 pages of talking through
 this pretty convoluted evolutionary and multitask learning 
algorithm, it's pretty interesting, solves a bunch of proble
ms. But two notes. 

One, the big number they cite as the su
ccess metric is 99.43 on CIFAR-10, against a SotA of 99.40, 
so woop-de-fucking-doo in the grand scheme of things.

Two, 
there's a chart towards the end of the paper that details ho
w many TPU core-hours were used for just the training regime
ns that results in the final results. The sum total is 17,81
0 core-hours. Let's assume that for someone who doesn't work
 at Google, you'd have to use on-demand pricing of $3.22/hr.
 This means that these trained models cost $57,348. 

Strict
ly speaking, throwing enough compute at a general enough gen
etic algorithm will eventually produce arbitrarily good perf
ormance, so while you can absolutely read this paper and col
lect interesting ideas about how to use genetic algorithms t
o accomplish multitask learning by having each new task leve
rage learned weights from previous tasks by defining modific
ations to a subset of components of a pre-existing model, th
ere's a meta-textual level on which this paper is just 'Jeff
 Dean spent enough money to feed a family of four for half a
 decade to get a 0.03% improvement on CIFAR-10.'

OpenAI is 
far and away the worst offender here, but it seems like ever
yone's doing it. You throw a fuckton of compute and a light 
ganache of new ideas at an existing problem with existing da
ta and existing benchmarks, and then if your numbers are inf
initesimally higher than their numbers, you get to put a lil
' sticker on your CV. Why should I trust that your ideas are
 even any good? I can't check them, I can't apply them to my
 own projects. 

Is this really what we're comfortable with 
as a community? A handful of corporations and the occasional
 university waving their dicks at everyone because they've g
ot the compute to burn and we don't? There's a level at whic
h I think there should be a new journal, exclusively for pap
ers in which you can replicate their experimental results in
 under eight hours on a single consumer GPU."	"https://www.reddit.com/r/MachineLearning/comments/uyratt/d_i_dont_really_trust_papers_out_of_top_labs/"	"264"	"1653630414.0"	"1666"	"uyratt"
32	"[D] Why can't you guys comment your fucking code?"	"Seriously.

I spent the last few years doing web app develop
ment. Dug into DL a couple months ago. Supposedly, compared 
to the post-post-post-docs doing AI stuff, JavaScript develo
pers should be inbred peasants. But every project these peas
ants release, even a fucking library that colorizes CLI outp
ut, has a catchy name, extensive docs, shitloads of comments
, fuckton of tests, semantic versioning, changelog, and, oh 
my god, better variable names than `ctx_h` or `lang_hs` or `
fuck_you_for_trying_to_understand`.

The concepts and ideas 
behind DL, GANs, LSTMs, CNNs, whatever – it's clear, it's si
mple, it's intuitive. The slog is to go through the jargon (
that keeps changing beneath your feet - what's the point of 
using fancy words if you can't keep them consistent?), the u
nnecessary equations, trying to squeeze meaning from bullshi
t language used in papers, figuring out the super important 
steps, preprocessing, hyperparameters optimization that the 
authors, oops, failed to mention.

Sorry for singling out, b
ut [look at this](https://github.com/facebookresearch/end-to
-end-negotiator/blob/master/src/agent.py) - what the fuck? I
f a developer anywhere else at Facebook would get this code 
for a review they would throw up.

- Do you intentionally tr
y to obfuscate your papers? Is pseudo-code a fucking premium
? Can you at least try to give some intuition before showeri
ng the reader with equations?

- How the fuck do you dare to
 release a paper without source code?

- Why the fuck do you
 never ever add comments to you code?

- When naming things,
 are you charged by the character? Do you get a bonus for ac
ronyms?

- Do you realize that OpenAI having needed to relea
se a 'baseline' TRPO implementation is a fucking disgrace to
 your profession?

- Jesus christ, who decided to name a ten
sor concatenation function `cat`?
"	"https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/"	"478"	"1499113449.0"	"1643"	"6l2esd"
33	"[D] The current and future state of AI/ML is shockingly demoralizing with little hope of redemption"	"I recently encountered the PaLM (Scaling Language Modeling w
ith Pathways) paper from Google Research and it opened up a 
can of worms of ideas I’ve felt I’ve intuitively had for a w
hile, but have been unable to express – and I know I can’t b
e the only one. Sometimes I wonder what the original pioneer
s of AI – Turing, Neumann, McCarthy, etc. – would think if t
hey could see the state of AI that we’ve gotten ourselves in
to. 67 authors, 83 pages, 540B parameters in a model, the in
ternals of which no one can say they comprehend with a strai
ght face, 6144 TPUs in a commercial lab that no one has acce
ss to, on a rig that no one can afford, trained on a volume 
of data that a human couldn’t process in a lifetime, 1 page 
on ethics with the same ideas that have been rehashed over a
nd over elsewhere with no attempt at a solution – bias, raci
sm, malicious use, etc. – for purposes that who asked for?


When I started my career as an AI/ML research engineer 2016,
 I was most interested in two types of tasks – 1.) those tha
t most humans could do but that would universally be conside
red tedious and non-scalable. I’m talking image classificati
on, sentiment analysis, even document summarization, etc. 2.
) tasks that humans lack the capacity to perform as well as 
computers for various reasons – forecasting, risk analysis, 
game playing, and so forth. I still love my career, and I tr
y to only work on projects in these areas, but it’s getting 
harder and harder.

This is because, somewhere along the way
, it became popular and unquestionably acceptable to push AI
 into domains that were originally uniquely human, those are
as that sit at the top of Maslows’s hierarchy of needs in te
rms of self-actualization – art, music, writing, singing, pr
ogramming, and so forth. These areas of endeavor have negati
ve logarithmic ability curves – the vast majority of people 
cannot do them well at all, about 10% can do them decently, 
and 1% or less can do them extraordinarily. The little discu
ssed problem with AI-generation is that, without extreme det
errence, we will sacrifice human achievement at the top perc
entile in the name of lowering the bar for a larger volume o
f people, until the AI ability range is the norm. This is be
cause relative to humans, AI is cheap, fast, and infinite, t
o the extent that investments in human achievement will be w
atered down at the societal, educational, and individual lev
el with each passing year. And unlike AI gameplay which supe
rseded humans decades ago, we won’t be able to just disquali
fy the machines and continue to play as if they didn’t exist
.

Almost everywhere I go, even this forum, I encounter almo
st universal deference given to current SOTA AI generation s
ystems like GPT-3, CODEX, DALL-E, etc., with almost no one e
xtending their implications to its logical conclusion, which
 is long-term convergence to the mean, to mediocrity, in the
 fields they claim to address or even enhance. If you’re an 
artist or writer and you’re using DALL-E or GPT-3 to “enhanc
e” your work, or if you’re a programmer saying, “GitHub Co-P
ilot makes me a better programmer?”, then how could you poss
ibly know? You’ve disrupted and bypassed your own creative p
rocess, which is thoughts -> (optionally words) -> actions -
> feedback -> repeat, and instead seeded your canvas with id
eas from a machine, the provenance of which you can’t unders
tand, nor can the machine reliably explain. And the more you
 do this, the more you make your creative processes dependen
t on said machine, until you must question whether or not yo
u could work at the same level without it.

When I was a col
lege student, I often dabbled with weed, LSD, and mushrooms,
 and for a while, I thought the ideas I was having while und
er the influence were revolutionary and groundbreaking – tha
t is until took it upon myself to actually start writing dow
n those ideas and then reviewing them while sober, when I re
alized they weren’t that special at all. What I eventually d
etermined is that, under the influence, it was impossible fo
r me to accurately evaluate the drug-induced ideas I was hav
ing because the influencing agent the generates the ideas th
emselves was disrupting the same frame of reference that is 
responsible evaluating said ideas. This is the same principl
e of – if you took a pill and it made you stupider, would ev
en know it? I believe that, especially over the long-term ti
meframe that crosses generations, there’s significant risk t
hat current AI-generation developments produces a similar ef
fect on humanity, and we mostly won’t even realize it has ha
ppened, much like a frog in boiling water. If you have child
ren like I do, how can you be aware of the the current SOTA 
in these areas, project that 20 to 30 years, and then and te
ll them with a straight face that it is worth them pursuing 
their talent in art, writing, or music? How can you be hones
t and still say that widespread implementation of auto-corre
ction hasn’t made you and others worse and worse at spelling
 over the years (a task that even I believe most would agree
 is tedious and worth automating).

Furthermore, I’ve yet to
 set anyone discuss the train – generate – train - generate 
feedback loop that long-term application of AI-generation sy
stems imply. The first generations of these models were trai
ned on wide swaths of web data generated by humans, but if t
hese systems are permitted to continually spit out content w
ithout restriction or verification, especially to the extent
 that it reduces or eliminates development and investment in
 human talent over the long term, then what happens to the 4
th or 5th generation of models? Eventually we encounter this
 situation where the AI is being trained almost exclusively 
on AI-generated content, and therefore with each generation,
 it settles more and more into the mean and mediocrity with 
no way out using current methods. By the time that happens, 
what will we have lost in terms of the creative capacity of 
people, and will we be able to get it back?

By relentlessly
 pursuing this direction so enthusiastically, I’m convinced 
that we as AI/ML developers, companies, and nations are past
 the point of no return, and it mostly comes down the invest
ments in time and money that we’ve made, as well as a prison
er’s dilemma with our competitors. As a society though, this
 direction we’ve chosen for short-term gains will almost cer
tainly make humanity worse off, mostly for those who are pow
erless to do anything about it – our children, our grandchil
dren, and generations to come.

If you’re an AI researcher o
r a data scientist like myself, how do you turn things back 
for yourself when you’ve spent years on years building your 
career in this direction? You’re likely making near or north
 of $200k annually TC and have a family to support, and so i
t’s too late, no matter how you feel about the direction the
 field has gone. If you’re a company, how do you standby and
 let your competitors aggressively push their AutoML solutio
ns into more and more markets without putting out your own? 
Moreover, if you’re a manager or thought leader in this fiel
d like Jeff Dean how do you justify to your own boss and you
r shareholders your team’s billions of dollars in AI investm
ent while simultaneously balancing ethical concerns? You can
’t – the only answer is bigger and bigger models, more and m
ore applications, more and more data, and more and more auto
mation, and then automating that even further. If you’re a c
ountry like the US, how do responsibly develop AI while your
 competitors like China single-mindedly push full steam ahea
d without an iota of ethical concern to replace you in numer
ous areas in global power dynamics? Once again, failing to c
ompete would be pre-emptively admitting defeat.

Even assumi
ng that none of what I’ve described here happens to such an 
extent, how are so few people not taking this seriously and 
discounting this possibility? If everything I’m saying is fe
ar-mongering and non-sense, then I’d be interested in hearin
g what you think human-AI co-existence looks like in 20 to 3
0 years and why it isn’t as demoralizing as I’ve made it out
 to be.

&#x200B;

EDIT: Day after posting this -- this post
 took off way more than I expected. Even if I received 20 - 
25 comments, I would have considered that a success, but thi
s went much further. Thank you to each one of you that has r
ead this post, even more so if you left a comment, and tripl
y so for those who gave awards! I've read almost every comme
nt that has come in (even the troll ones), and am truly grat
eful for each one, including those in sharp disagreement. I'
ve learned much more from this discussion with the sub than 
I could have imagined on this topic, from so many perspectiv
es. While I will try to reply as many comments as I can, the
 sheer comment volume combined with limited free time betwee
n work and family unfortunately means that there are many th
at I likely won't be able to get to. That will invariably in
clude some that I would love respond to under the assumption
 of infinite time, but I will do my best, even if the latenc
y stretches into days. Thank you all once again!"	"https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/"	"396"	"1659907526.0"	"1396"	"wiqjxv"
34	"[D] Does anybody else despise OpenAI?"	" I  mean, don't get me started with the closed source models
 they have that were trained using the work of unassuming in
dividuals who will never  see a penny for it. Put it up on G
ithub they said. I'm all for  open-source, but when a compan
y turns around and charges you for a  product they made with
 freely and publicly made content, while forbidding you from
 using the output to create competing models, that is where 
I  draw the line. It is simply ridiculous. 

Sam Altman coul
dn't be anymore predictable with his recent attempts to get 
the government to start regulating AI.

What  risks? The AI 
is just a messenger for information that is already out  the
re if one knows how/where to look. You don't need AI to lear
n how to  hack, to learn how to make weapons, etc. Fake news
/propaganda? The  internet has all of that covered. LLMs are
 no where near the level of AI  you see in sci-fi. I mean, a
re people really afraid of text? Yes, I  know that text can 
sometimes be malicious code such as viruses, but  those can 
be found on github as well.  If they fall for this they migh
t  as well shutdown the internet while they're at it.

He  i
s simply blowing things out of proportion and using fear to 
increase  the likelihood that they do what he wants, hurt th
e competition. I  bet he is probably teething with bitternes
s everytime a new huggingface  model comes out. The thought 
of us peasants being able to use AI  privately is too danger
ous. No, instead we must be fed scraps while they  slowly ta
ke away our jobs and determine our future.

This  is not a d
oomer post, as I am all in favor of the advancement of AI.  
However, the real danger here lies in having a company like 
OpenAI  dictate the future of humanity. I get it, the writin
g is on the wall;  the cost of human intelligence will go do
wn, but if everyone has their  personal AI then it wouldn't 
seem so bad or unfair would it? Listen,  something that has 
the power to render a college degree that costs  thousands o
f dollars worthless should be available to the public. This 
 is to offset the damages and job layoffs that will come as 
a result of  such an entity. It wouldn't be as bitter of a t
aste as it would if you were replaced by it while still not 
being able to access it. Everyone should be able to use it a
s leverage, it is the only fair solution.

If  we don't take
 action now, a company like ClosedAI will, and they are  not
 in favor of the common folk. Sam Altman is so calculated to
 the  point where there were times when he seemed to be shoo
ting OpenAI in the foot during his talk.  This move is to si
mply conceal his real intentions, to climb the ladder and ta
ke it with him. If he didn't include his company in his  ram
blings, he would be easily read. So instead, he pretends to 
be scared of his own product, in an effort to legitimize his
 claim. Don't fall  for it.

They are slowly making a  reput
ation as one the most hated tech companies, right up there w
ith  Adobe, and they don't show any sign of change. They hav
e no moat,  othewise they wouldn't feel so threatened to the
 point where they would have to resort to creating barriers 
of entry via regulation. This only  means one thing, we are 
slowly catching up. We just need someone to  vouch for human
ity's well-being, while acting as an opposing force to the  
evil corporations who are only looking out for themselves. Q
uestion is,  who would be a good candidate?"	"https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/"	"415"	"1684361728.0"	"1320"	"13kfxzy"
35	"[P] Landing the Falcon booster with Reinforcement Learning in OpenAI"	""	"https://gfycat.com/CoarseEmbellishedIsopod"	"55"	"1518871530.0"	"1291"	"7y6g79"
36	"[P] OpenAssistant - The world's largest open-source replication of ChatGPT"	"We’re excited to announce the release of OpenAssistant.

The
 future of AI development depends heavily on high quality da
tasets and models being made publicly available, and that’s 
exactly what this project does.

Watch the annoucement video
:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4
Kk)

&#x200B;

Our team has worked tirelessly over the past 
several months collecting large amounts of text-based input 
and feedback to create an incredibly diverse and unique data
set designed specifically for training language models or ot
her AI applications.

With over 600k human-generated data po
ints covering a wide range of topics and styles of writing, 
our dataset will be an invaluable tool for any developer loo
king to create state-of-the-art instruction models!

To make
 things even better, we are making this entire dataset free 
and accessible to all who wish to use it. Check it out today
 at our HF org: OpenAssistant

On top of that, we've trained
 very powerful models that you can try right now at: [open-a
ssistant.io/chat](https://open-assistant.io/chat) !"	"https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/"	"175"	"1681578898.0"	"1260"	"12nbixk"
37	"[D] Google 'We Have No Moat, And Neither Does OpenAI': Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI"	""	"https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"	"205"	"1683216810.0"	"1168"	"137rxgw"
38	"We are Oriol Vinyals and David Silver from DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything"	"Hi there! We are Oriol Vinyals (/u/OriolVinyals) and David S
ilver (/u/David_Silver), lead researchers on DeepMind’s Alph
aStar team, joined by StarCraft II pro players TLO, and MaNa
.

This evening at DeepMind HQ we held a livestream demonstr
ation of AlphaStar playing against TLO and MaNa - you can re
ad more about the matches [here](https://deepmind.com/blog/a
lphastar-mastering-real-time-strategy-game-starcraft-ii/) or
 re-watch the stream on YouTube [here](https://www.youtube.c
om/watch?v=cUTMhmVh1qs).

Now, we’re excited to talk with yo
u about AlphaStar, the challenge of real-time strategy games
 for AI research, the matches themselves, and anything you’d
 like to know from TLO and MaNa about their experience playi
ng against AlphaStar! :)

We are opening this thread now and
 will be here at **16:00 GMT / 11:00 ET / 08:00PT** on Frida
y, 25 January to answer your questions.

&#x200B;

EDIT: Tha
nks everyone for your great questions. It was a blast, hope 
you enjoyed it as well!"	"https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/"	"1011"	"1548363323.0"	"1164"	"ajgzoc"
39	"[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data"	"[GPT-4 and professional benchmarks: the wrong answer to the 
wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-
professional-benchmarks)

*OpenAI may have tested on the tra
ining data. Besides, human benchmarks are meaningless for bo
ts.*

 **Problem 1: training data contamination**

To benchm
ark GPT-4’s coding ability, OpenAI evaluated it on problems 
from Codeforces, a website that hosts coding competitions. S
urprisingly, Horace He pointed out that GPT-4 solved 10/10 p
re-2021 problems and 0/10 recent problems in the easy catego
ry. The training data cutoff for GPT-4 is September 2021. Th
is strongly suggests that the model is able to memorize solu
tions from its training set — or at least partly memorize th
em, enough that it can fill in what it can’t recall.

As fur
ther evidence for this hypothesis, we tested it on Codeforce
s problems from different times in 2021. We found that it co
uld regularly solve problems in the easy category before Sep
tember 5, but none of the problems after September 12.

In f
act, we can definitively show that it has memorized problems
 in its training set: when prompted with the title of a Code
forces problem, GPT-4 includes a link to the exact contest w
here the problem appears (and the round number is almost cor
rect: it is off by one). Note that GPT-4 cannot access the I
nternet, so memorization is the only explanation."	"https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/"	"135"	"1679983023.0"	"995"	"124eyso"
40	"Meta’s LLaMa weights leaked on torrent... and the best thing about it is someone put up a PR to replace the google form in the repo with it 😂"	""	"https://i.redd.it/olnsv438alla1.jpg"	"23"	"1677877700.0"	"185"	"11hezvk"
41	"⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI"	"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of
 The Decade](https://preview.redd.it/sg24cw3zekea1.png?width
=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2
fff989a1)

Microsoft is investing $10B into OpenAI!

There i
s lots of frustration in the community about OpenAI not bein
g all that open anymore. They appear to abandon their ethos 
of developing AI for everyone, [free](https://openai.com/blo
g/introducing-openai/) of economic pressures.

The fear is t
hat OpenAI’s models are going to become fancy MS Office plug
ins. Gone would be the days of open research and innovation.


However, the specifics of the deal tell a different story.


To understand what is going on, we need to peek behind the
 curtain of the tough business of machine learning. We will 
find that Sam Altman might have just orchestrated the coup o
f the decade!

To appreciate better why there is some three-
dimensional chess going on, let’s first look at Sam Altman’s
 backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sa
m Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt
) and was part of the first-ever YC batch. He raised a total
 of $30M in funding, but the company failed to gain traction
. Seven years into the business Loopt was basically dead in 
the water and had to be shut down.

Instead of caving, he ma
naged to sell his startup for $[43M](https://golden.com/wiki
/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https
://www.greendot.com/). Investors got their money back and he
 personally made $5M from the sale.

By YC standards, this w
as a pretty unimpressive outcome.

However, people took note
 that the fire between his ears was burning hotter than that
 of most people. So hot in fact that Paul Graham included hi
m in his 2009 [essay](http://www.paulgraham.com/5founders.ht
ml?viewfullsite=1) about the five founders who influenced hi
m the most.

He listed young Sam Altman next to Steve Jobs, 
Larry & Sergey from Google, and Paul Buchheit (creator of GM
ail and AdSense). He went on to describe him as a strategic 
mastermind whose sheer force of will was going to get him wh
atever he wanted.

And Sam Altman played his hand well!

He 
parleyed his new connections into raising $21M from Peter Th
iel and others to start investing. Within four years he 10x-
ed the money \[2\]. In addition, Paul Graham made him his su
ccessor as president of YC in 2014.

Within one decade of se
lling his first startup for $5M, he grew his net worth to a 
mind-bending $250M and rose to the circle of the most influe
ntial people in Silicon Valley.

Today, he is the CEO of Ope
nAI — one of the most exciting and impactful organizations i
n all of tech.

However, OpenAI — the rocket ship of AI inno
vation — is in dire straights.

# OpenAI is Bleeding Cash

B
ack in 2015, OpenAI was kickstarted with $1B in donations fr
om famous donors such as Elon Musk.

That money is long gone
.

In 2022 OpenAI is projecting a revenue of $36M. At the sa
me time, they spent roughly $544M. Hence the company has los
t >$500M over the last year alone.

This is probably not an 
outlier year. OpenAI is headquartered in San Francisco and h
as a stable of 375 employees of mostly machine learning rock
stars. Hence, salaries alone probably come out to be roughly
 $200M p.a.

In addition to high salaries their compute cost
s are stupendous. Considering it cost them $4.6M to train GP
T3 once, it is likely that their cloud bill is in a very hea
lthy nine-figure range as well \[4\].

So, where does this l
eave them today?

Before the Microsoft investment of $10B, O
penAI had received a total of $4B over its lifetime. With $4
B in funding, a burn rate of $0.5B, and eight years of compa
ny history it doesn’t take a genius to figure out that they 
are running low on cash.

It would be reasonable to think: O
penAI is sitting on ChatGPT and other great models. Can’t th
ey just lease them and make a killing?

Yes and no. OpenAI i
s projecting a revenue of $1B for 2024. However, it is unlik
ely that they could pull this off without significantly incr
easing their costs as well.

*Here are some reasons why!*

#
 The Tough Business Of Machine Learning

Machine learning co
mpanies are distinct from regular software companies. On the
 outside they look and feel similar: people are creating pro
ducts using code, but on the inside things can be very diffe
rent.

To start off, machine learning companies are usually 
way less profitable. Their gross margins land in the 50%-60%
 range, much lower than those of SaaS businesses, which can 
be as high as 80% \[7\].

On the one hand, the massive compu
te requirements and thorny data management problems drive up
 costs.

On the other hand, the work itself can sometimes re
semble consulting more than it resembles software engineerin
g. Everyone who has worked in the field knows that training 
models requires deep domain knowledge and loads of manual wo
rk on data.

To illustrate the latter point, imagine the uns
peakable complexity of performing content moderation on Chat
GPT’s outputs. If OpenAI scales the usage of GPT in producti
on, they will need large teams of moderators to filter and l
abel hate speech, slurs, tutorials on killing people, you na
me it.

*Alright, alright, alright! Machine learning is hard
.*

*OpenAI already has ChatGPT working. That’s gotta be wor
th something?*

# Foundation Models Might Become Commodities
:

In order to monetize GPT or any of their other models, Op
enAI can go two different routes.

First, they could pick on
e or more verticals and sell directly to consumers. They cou
ld for example become the ultimate copywriting tool and blow
 [Jasper](https://app.convertkit.com/campaigns/10748016/jasp
er.ai) or [copy.ai](https://app.convertkit.com/campaigns/107
48016/copy.ai) out of the water.

This is not going to happe
n. Reasons for it include:

1. To support their mission of b
uilding competitive foundational AI tools, and their huge(!)
 burn rate, they would need to capture one or more very larg
e verticals.
2. They fundamentally need to re-brand themselv
es and diverge from their original mission. This would likel
y scare most of the talent away.
3. They would need to build
 out sales and marketing teams. Such a step would fundamenta
lly change their culture and would inevitably dilute their f
ocus on research.

The second option OpenAI has is to keep d
oing what they are doing and monetize access to their models
 via API. Introducing a [pro version](https://www.searchengi
nejournal.com/openai-chatgpt-professional/476244/) of ChatGP
T is a step in this direction.

This approach has its own ch
allenges. Models like GPT do have a defensible moat. They ar
e just large transformer models trained on very large open-s
ource datasets.

As an example, last week Andrej Karpathy re
leased a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY
) of him coding up a version of GPT in an afternoon. Nothing
 could stop e.g. Google, StabilityAI, or HuggingFace from op
en-sourcing their own GPT.

As a result GPT inference would 
become a common good. This would melt OpenAI’s profits down 
to a tiny bit of nothing.

In this scenario, they would also
 have a very hard time leveraging their branding to generate
 returns. Since companies that integrate with OpenAI’s API c
ontrol the interface to the customer, they would likely end 
up capturing all of the value.

An argument can be made that
 this is a general problem of foundation models. Their high 
fixed costs and lack of differentiation could end up making 
them akin to the [steel industry](https://www.thediff.co/arc
hive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum
 it up:

* They don’t have a way to sustainably monetize the
ir models.
* They do not want and probably should not build 
up internal sales and marketing teams to capture verticals
*
 They need a lot of money to keep funding their research wit
hout getting bogged down by details of specific product deve
lopment

*So, what should they do?*

# The Microsoft Deal

O
penAI and Microsoft [announced](https://blogs.microsoft.com/
blog/2023/01/23/microsoftandopenaiextendpartnership/) the ex
tension of their partnership with a $10B investment, on Mond
ay.

At this point, Microsoft will have invested a total of 
$13B in OpenAI. Moreover, new VCs are in on the deal by buyi
ng up shares of employees that want to take some chips off t
he table.

However, the astounding size is not the only extr
aordinary thing about this deal.

First off, the ownership w
ill be split across three groups. Microsoft will hold 49%, V
Cs another 49%, and the OpenAI foundation will control the r
emaining 2% of shares.

If OpenAI starts making money, the p
rofits are distributed differently across four stages:

1. F
irst, early investors (probably Khosla Ventures and Reid Hof
fman’s foundation) get their money back with interest.
2. Af
ter that Microsoft is entitled to 75% of profits until the $
13B of funding is repaid
3. When the initial funding is repa
id, Microsoft and the remaining VCs each get 49% of profits.
 This continues until another $92B and $150B are paid out to
 Microsoft and the VCs, respectively.
4. Once the aforementi
oned money is paid to investors, 100% of shares return to th
e foundation, which regains total control over the company. 
\[3\]

# What This Means

This is absolutely crazy!

OpenAI 
managed to solve all of its problems at once. They raised a 
boatload of money and have access to all the compute they ne
ed.

On top of that, they solved their distribution problem.
 They now have access to Microsoft’s sales teams and their m
odels will be integrated into MS Office products.

Microsoft
 also benefits heavily. They can play at the forefront AI, b
rush up their tools, and have OpenAI as an exclusive partner
 to further compete in a [bitter cloud war](https://www.proj
ectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-
cloud-war/401) against AWS.

The synergies do not stop there
.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. 
will likely benefit heavily from the partnership as they con
tinue to develop[ GitHub Copilot](https://github.com/feature
s/copilot).

The deal creates a beautiful win-win situation,
 but that is not even the best part.

Sam Altman and his tea
m at OpenAI essentially managed to place a giant hedge. If O
penAI does not manage to create anything meaningful or we en
ter a new AI winter, Microsoft will have paid for the party.


However, if OpenAI creates something in the direction of A
GI — whatever that looks like — the value of it will likely 
be huge.

In that case, OpenAI will quickly repay the dept t
o Microsoft and the foundation will control 100% of whatever
 was created.

*Wow!*

Whether you agree with the path OpenA
I has chosen or would have preferred them to stay donation-b
ased, you have to give it to them.

*This deal is an absolut
e power move!*

I look forward to the future. Such exciting 
times to be alive!

As always, I really enjoyed making this 
for you and I sincerely hope you found it useful!

*Thank yo
u for reading!*

Would you like to receive an article such a
s this one straight to your inbox every Thursday? Consider s
igning up for **The Decoding** ⭕.

I send out a thoughtful n
ewsletter about ML research and the data economy once a week
. No Spam. No Nonsense. [Click here to sign up!](https://the
decoding.net/)

**References:**

\[1\] [https://golden.com/w
iki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J
5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/1
0/sam-altmans-manifest-destiny](https://www.newyorker.com/ma
gazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Art
icle in Fortune magazine ](https://fortune.com/2023/01/11/st
ructure-openai-investment-microsoft/?verification_code=DOVCV
S8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbG
Q2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMD
AzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRT
kxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW
55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6Nj
MyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/210
4.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5
\] [https://www.crunchbase.com/organization/openai/company\_
financials](https://www.crunchbase.com/organization/openai/c
ompany_financials)​

\[6\] Elon Musk donation [https://www.i
nverse.com/article/52701-openai-documents-elon-musk-donation
-a-i-research](https://www.inverse.com/article/52701-openai-
documents-elon-musk-donation-a-i-research)​

\[7\] [https://
a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-diffe
rent-from-traditional-software-2/](https://a16z.com/2020/02/
16/the-new-business-of-ai-and-how-its-different-from-traditi
onal-software-2/)"	"https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/"	"16"	"1674816348.0"	"117"	"10mhyek"
42	"image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model"	""	"https://www.youtube.com/watch?v=FwXQ568_io0"	"6"	"1596625086.0"	"96"	"i437pt"
43	"I wrote a program with OpenAI's Codex that fixes errors"	""	"https://v.redd.it/jupdtry6vf881"	"6"	"1640765002.0"	"95"	"rr2wme"
44	"Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT"	"Vicuna : ChatGPT Alternative, Open-Source, High Quality and 
Low Cost 

&#x200B;

[ Relative Response Quality Assessed by
 GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599
&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a07
2b91)

Vicuna-13B has demonstrated competitive performance a
gainst other open-source models, such as Stanford Alpaca, by
 fine-tuning a LLaMA base model on user-shared conversations
 collected from ShareGPT.

Evaluation using GPT-4 as a judge
 shows that Vicuna-13B achieves more than 90% of the quality
 of OpenAI ChatGPT and Google Bard AI, while outperforming o
ther models such as Meta LLaMA (Large Language Model Meta AI
) and Stanford Alpaca in more than 90% of cases.

The cost o
f training Vicuna-13B is approximately $300.

The training a
nd serving code, along with an online demo, are publicly ava
ilable for non-commercial use.

&#x200B;

More Information :
 [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-sourc
e-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT]
(https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source
-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)


Discord Server : [https://discord.gg/h6kCZb72G7](https://di
scord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysor
g](https://twitter.com/lmsysorg)"	"https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/"	"19"	"1680658600.0"	"84"	"12c43uu"
45	"GPT-4 Will Be 500x Smaller Than People Think - Here Is Why"	"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://pre
view.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=web
p&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mil
l is buzzing around the release of GPT-4.

People are predic
ting the model will have 100 trillion parameters. That’s a *
trillion* with a “t”.

The often-used graphic above makes GP
T-3 look like a cute little breadcrumb that is about to have
 a live-ending encounter with a bowling ball.

Sure, OpenAI’
s new brainchild will certainly be mind-bending and language
 models have been getting bigger — fast!

But this time migh
t be different and it makes for a good opportunity to look a
t the research on scaling large language models (LLMs).

*Le
t’s go!*

Training 100 Trillion Parameters

The creation of 
GPT-3 was a marvelous feat of engineering. The training was 
done on 1024 GPUs, took 34 days, and cost $4.6M in compute a
lone \[1\].

Training a 100T parameter model on the same dat
a, using 10000 GPUs, would take 53 Years. To avoid overfitti
ng such a huge model the dataset would also need to be much(
!) larger.

So, where is this rumor coming from?

The Source
 Of The Rumor:

It turns out OpenAI itself might be the sour
ce of it.

In August 2021 the CEO of Cerebras told [wired](h
ttps://www.wired.com/story/cerebras-chip-cluster-neural-netw
orks-ai/): “From talking to OpenAI, GPT-4 will be about 100 
trillion parameters”.

A the time, that was most likely what
 they believed, but that was in 2021. So, basically forever 
ago when machine learning research is concerned.

Things hav
e changed a lot since then!

To understand what happened we 
first need to look at how people decide the number of parame
ters in a model.

Deciding The Number Of Parameters:

The en
ormous hunger for resources typically makes it feasible to t
rain an LLM only once.

In practice, the available compute b
udget (how much money will be spent, available GPUs, etc.) i
s known in advance. Before the training is started, research
ers need to accurately predict which hyperparameters will re
sult in the best model.

*But there’s a catch!*

Most resear
ch on neural networks is empirical. People typically run hun
dreds or even thousands of training experiments until they f
ind a good model with the right hyperparameters.

With LLMs 
we cannot do that. Training 200 GPT-3 models would set you b
ack roughly a billion dollars. Not even the deep-pocketed te
ch giants can spend this sort of money.

Therefore, research
ers need to work with what they have. Either they investigat
e the few big models that have been trained or they train sm
aller models in the hope of learning something about how to 
scale the big ones.

This process can very noisy and the com
munity’s understanding has evolved a lot over the last few y
ears.

What People Used To Think About Scaling LLMs

In 2020
, a team of researchers from OpenAI released a [paper](https
://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For N
eural Language Models”.

They observed a predictable decreas
e in training loss when increasing the model size over multi
ple orders of magnitude.

So far so good. But they made two 
other observations, which resulted in the model size balloon
ing rapidly.

1. To scale models optimally the parameters sh
ould scale quicker than the dataset size. To be exact, their
 analysis showed when increasing the model size 8x the datas
et only needs to be increased 5x.
2. Full model convergence 
is not compute-efficient. Given a fixed compute budget it is
 better to train large models shorter than to use a smaller 
model and train it longer.

Hence, it seemed as if the way t
o improve performance was to scale models faster than the da
taset size \[2\].

And that is what people did. The models g
ot larger and larger with GPT-3 (175B), [Gopher](https://arx
iv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](htt
ps://arxiv.org/pdf/2201.11990) (530B) just to name a few.

B
ut the bigger models failed to deliver on the promise.

*Rea
d on to learn why!*

What We know About Scaling Models Today


It turns out you need to scale training sets and models in
 equal proportions. So, every time the model size doubles, t
he number of training tokens should double as well.

This wa
s published in DeepMind’s 2022 [paper](https://arxiv.org/pdf
/2203.15556.pdf): “Training Compute-Optimal Large Language M
odels”

The researchers fitted over 400 language models rang
ing from 70M to over 16B parameters. To assess the impact of
 dataset size they also varied the number of training tokens
 from 5B-500B tokens.

The findings allowed them to estimate
 that a compute-optimal version of GPT-3 (175B) should be tr
ained on roughly 3.7T tokens. That is more than 10x the data
 that the original model was trained on.

To verify their re
sults they trained a fairly small model on vastly more data.
 Their model, called Chinchilla, has 70B parameters and is t
rained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 b
ut trained on almost 5x the data.

Chinchilla outperforms GP
T-3 and other much larger models by a fair margin \[3\].

Th
is was a great breakthrough!The model is not just better, bu
t its smaller size makes inference cheaper and finetuning ea
sier.

*So What Will Happen?*

What GPT-4 Might Look Like:


To properly fit a model with 100T parameters, open OpenAI ne
eds a dataset of roughly 700T tokens. Given 1M GPUs and usin
g the calculus from above, it would still take roughly 2650 
years to train the model \[1\].

So, here is what GPT-4 coul
d look like:

* Similar size to GPT-3, but trained optimally
 on 10x more data
* ​[Multi-modal](https://thealgorithmicbri
dge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputt
ing text, images, and sound
* Output conditioned on document
 chunks from a memory bank that the model has access to duri
ng prediction \[4\]
* Doubled context size allows longer pre
dictions before the model starts going off the rails​

Regar
dless of the exact design, it will be a solid step forward. 
However, it will not be the 100T token human-brain-like AGI 
that people make it out to be.

Whatever it will look like, 
I am sure it will be amazing and we can all be excited about
 the release.

Such exciting times to be alive!

If you got 
down here, thank you! It was a privilege to make this for yo
u. At **TheDecoding** ⭕, I send out a thoughtful newsletter 
about ML research and the data economy once a week. No Spam.
 No Nonsense. [Click here to sign up!](https://thedecoding.n
et/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Ca
sper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbran
d, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee
 , M. Zaharia, [Efficient Large-Scale Language Model Trainin
g on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2
104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. 
Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Sc
aling laws for neural language model](https://arxiv.org/abs/
2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. B
orgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D
. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Tra
ining Compute-Optimal Large Language Models](https://arxiv.o
rg/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*
.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rut
herford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A.
 Clark, D. Casas, [Improving language models by retrieving f
rom trillions of tokens](https://arxiv.org/abs/2112.04426) (
2021). *arXiv preprint arXiv:2112.04426*.Vancouver"	"https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/"	"11"	"1674114949.0"	"68"	"10fw22o"
46	"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks"	"OpenAI released their newest language, [Triton](https://gith
ub.com/openai/triton). This open-source programming language
 that enables researchers to write highly efficient GPU code
 for AI workloads is Python-compatible and comes with the ab
ility of a user to write in as few as 25 lines, something on
 par with what an expert could achieve. OpenAI claims this m
akes it possible to reach peak hardware performance without 
much effort, making creating more complex workflows easier t
han ever before!

Researchers in the field of Deep Learning 
often rely on native framework operators. However, this can 
be problematic because it requires many temporary tensors to
 work, which may hurt performance at scale for neural networ
ks. Writing specialized GPU kernels is a more convenient sol
ution, but surprisingly difficult due to intricacies when pr
ogramming them according to GPUs. It was challenging to find
 a system that provides the flexibility and speed required w
hile also being easy enough for developers to understand. Th
is has led researchers at OpenAI in improving Triton, which 
was initially founded by one of their teammates.

Quick Read
: [https://www.marktechpost.com/2021/07/28/openai-releases-t
riton-an-open-source-python-like-gpu-programming-language-fo
r-neural-networks/](https://www.marktechpost.com/2021/07/28/
openai-releases-triton-an-open-source-python-like-gpu-progra
mming-language-for-neural-networks/) 

Paper: http://www.eec
s.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pd
f

Github: https://github.com/openai/triton"	"https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/"	"5"	"1627494357.0"	"71"	"otf0fs"
47	"Open AI and Microsoft Can Generate Python Code"	""	"https://youtu.be/y5-wzgIySb4"	"6"	"1590161008.0"	"70"	"golbq4"
48	"Everything you need to know about computer vision in one repo"	"*This post was co-authored by JS Tan, Patrick Buehler, Anupa
m Sharma and Jun Ki Min.*

In recent years, we’ve seen extra
ordinary growth in Computer Vision, with applications in ima
ge understanding, search, mapping, semi-autonomous or autono
mous vehicles and many more .

The ability for models to und
erstand actions in a video , a task that was unthinkable jus
t a few years ago , is now something that we can achieve wit
h relatively high accuracy and in near real-time.

However, 
the field is not particularly welcoming for newcomers. Witho
ut prior experience or guidance, building an accurate classi
fier can easily take weeks. Unless you’re ready to spend a l
ong-time learning computer vision, it’s extremely hard to ma
ster the basics, let alone begin to explore some of the cutt
ing-edge technologies in the field. Even for computer vision
 experts, building a quick Proof of Concept (POC) can be non
 trivial and could easily end up taking many days to put tog
ether.

At [Microsoft ](https://docs.microsoft.com/en-us/azu
re/machine-learning/?WT.mc_id=medium-article-lazzeri), we ha
ve been working for many years on diverse Computer Vision so
lutions for our customers and collected our learning into ou
r new public [Microsoft](https://docs.microsoft.com/en-us/az
ure/machine-learning/?WT.mc_id=medium-article-lazzeri) repos
itory: [Custom vision repo](https://github.com/microsoft/Com
puterVision-recipes?WT.mc_id=medium-article-lazzeri).

The g
oal of [this repository](https://github.com/microsoft/Comput
erVision-recipes?WT.mc_id=medium-article-lazzeri) is to prov
ide examples and best practice guidelines for building compu
ter vision systems on [Azure](https://docs.microsoft.com/en-
us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) 
, and to share this with the open-source community . More sp
ecifically, our goal was to create a [repository](https://do
cs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medi
um-article-lazzeri) that will help us to provide solutions r
apidly to the community and to customers that we work with ,
 or with on-boarding new team members who may have expertise
 in data science, but not specifically in computer vision. F
rom mastering some of the most common scenarios in the field
, like image classification, object detection , and image si
milarity, to exploring cutting edge scenarios like activity 
recognition and crowd counting, [this repo](https://docs.mic
rosoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-art
icle-lazzeri) will guide you through building models, fine-t
uning them, and using them in real-world scenarios.

We’re k
icking off our repo with **5 scenarios.** You can find the l
inks to the repos here:

* [Classification](https://github.c
om/microsoft/computervision-recipes/tree/master/scenarios/cl
assification?WT.mc_id=medium-article-lazzeri)
* [Similarity]
(https://github.com/microsoft/computervision-recipes/tree/ma
ster/scenarios/similarity?WT.mc_id=medium-article-lazzeri)
*
 [Detection](https://github.com/microsoft/computervision-rec
ipes/tree/master/scenarios/detection?WT.mc_id=medium-article
-lazzeri)
* [Action Recognition](https://github.com/microsof
t/computervision-recipes/tree/master/contrib/action_recognit
ion?WT.mc_id=medium-article-lazzeri)
* [Crowd Counting](http
s://github.com/microsoft/computervision-recipes/tree/master/
contrib/crowd_counting?WT.mc_id=medium-article-lazzeri)

Rat
her than creating implementations from scratch, we draw from
 popular state-of-the-art libraries (e.g. fast.ai and [torch
vision ](https://pytorch.org/docs/stable/torchvision/index.h
tml)), and we build additional utility around loading image 
data, optimizing models , and evaluating models. In addition
, we aim to answer the frequently asked questions, try to ex
plain the deep learning intuitions, and highlight common pit
falls.

Whether you a re an expert in computer vision or jus
t getting your hands wet, we believe [this repository](https
://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id
=medium-article-lazzeri) offers something for you . For the 
beginner, [this repo](https://docs.microsoft.com/en-us/azure
/machine-learning/?WT.mc_id=medium-article-lazzeri) will gui
de you through building a state-of-the-art model and help yo
u develop an intuition for the craft. For the experts, this 
repository can quickly get you to a strong baseline model wh
ich is easy to extend using custom Python/PyTorch code. In a
ddition, the repository also aims to provide support with:


1. [The full data science process](https://docs.microsoft.co
m/azure/machine-learning/team-data-science-process/overview?
WT.mc_id=medium-article-lazzeri).
2. [The tooling to succeed
 on Azure](https://docs.microsoft.com/en-us/azure/machine-le
arning/?WT.mc_id=medium-article-lazzeri).

We hope that thes
e examples and utilities will make it easier and faster for 
developers to create custom vision applications.

# The Data
 Science Process

The [Computer Vision Recipes GitHub reposi
tory](https://github.com/microsoft/ComputerVision-recipes?WT
.mc_id=medium-article-lazzeri) shows you how to approach the
 five key steps of the data science process and provides uti
lities to enrich each of the steps :

1. **Evaluating** — Ev
aluate your model. Depending on the metric you’re interested
 in optimizing, you may want to explore different methods of
 evaluation.
2. **Model selection and optimization** — Tun e
 and optimize hyperparameters to get the highest performing 
model. Because Computer Vision models are often computationa
lly costly, we show you how to seamlessly scale your paramet
er tuning into Azure .
3. **Operationalizing** — Operational
ize models in a production environment on Azure by deploying
 it onto Kubernetes.

Inside the computer vision recipes [re
po,](https://github.com/microsoft/ComputerVision-recipes?WT.
mc_id=medium-article-lazzeri) we have added a lot of utility
 to support common tasks such as loading data sets in the fo
rmat expected by different algorithms, splitting training/te
st data, and evaluating model outputs .

This computer visio
n repository also has deep integration with the [Azure Machi
ne Learning](https://docs.microsoft.com/en-us/azure/machine-
learning/?WT.mc_id=medium-article-lazzeri) to complement you
r work locally. We provide code examples on how you can opti
onally and easily scale your training into the cloud, and ho
w you can deploy your models for production workloads.

**Az
ure Cognitive Services**

Note that for certain computer vis
ion problems, you may not need to build your own models. Ins
tead, pre-built or easily customizable solutions exist which
 do not require any custom coding or machine learning expert
ise.

* [Vision Services](https://docs.microsoft.com/en-us/a
zure/cognitive-services/computer-vision/?WT.mc_id=medium-art
icle-lazzeri) are a set of pre-trained REST APIs which can b
e called for image tagging, OCR, video analytics, and more. 
These APIs work out of the box and require minimal expertise
 in machine learning but have limited customization capabili
ties. See the various demos available to get a feel for the 
functionality (e.g. Computer Vision).
* [Custom Vision](http
s://docs.microsoft.com/en-us/azure/cognitive-services/custom
-vision-service/?WT.mc_id=medium-article-lazzeri) is a SaaS 
service to train and deploy a model as a REST API given a us
er-provided training set. All steps including image upload, 
annotation, and model deployment can be performed using eith
er the UI or a Python SDK. Training image classification or 
object detection models can be achieved with minimal machine
 learning expertise. The Custom Vision offers more flexibili
ty than using the pre-trained cognitive services APIs but re
quires the user to bring and annotate their own data.

Befor
e using the Computer Vision repository, we strongly recommen
d evaluating if these can sufficiently solve your problem.


To give you a sense of how you can use our repo to build a s
tate of the art (SOTA) model, here is a preview of how simpl
e it is to create an Object Detection model. Of course, you 
can go much deeper and add custom PyTorch code, but getting 
started is as simple as this :

**1. Load your data**

The f
irst step is to load your data — we help you do this with a 
simple object that automatically parses your data and the an
notations:

`from utils_cv.detection.data import DetectionLo
ader data = DetectionLoader('path/to/data')`

**2. Train/fin
e-tune your model**

Then we create a ‘learner’ object that 
helps you manage and train your model. By default, it will u
se torchvision’s Faster R-CNN model. But you can easily swit
ch it out.

`from utils_cv.detection.model import DetectionL
earner detector = DetectionLearner(data) detector.fit()`

**
3. Evaluate**

Finally, lets evaluate our model using the bu
ilt-in helper functions. We can look at the precision and re
call curves to give us a sense of how our model is performin
g.

`from utils_cv.detection.plot import plot_pr_curves eval
 = detector.evaluate() plot_pr_curves(eval)`

As we continue
 to build out of repository, we will be looking for new comp
uter vision scenarios to unlock . Feel free to reach out to 
[cvbp@microsoft.com](mailto:cvbp@microsoft.com) or post an i
ssue if you wish to see us cover a scenario .

# Additional 
resources to learn more

To learn more, you can read the fol
lowing articles and notebooks:

* [Custom vision repo](https
://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medi
um-article-lazzeri)
* Original article: [https://techcommuni
ty.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-k
now-about-computer-vision-in-one/ba-p/1070311](https://techc
ommunity.microsoft.com/t5/azure-ai/nearly-everything-you-nee
d-to-know-about-computer-vision-in-one/ba-p/1070311)
* [Visi
on Services](https://docs.microsoft.com/en-us/azure/cognitiv
e-services/computer-vision/?WT.mc_id=medium-article-lazzeri)
 on Azure
* [Custom Vision](https://docs.microsoft.com/en-us
/azure/cognitive-services/custom-vision-service/?WT.mc_id=me
dium-article-lazzeri) on Azure
* Portfolio of Azure Machine 
Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://ak
a.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms
/AzureMLservice](https://aka.ms/AzureMLservice)
* Get starte
d with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/G
etStartedAzureML)
* Automated Machine Learning Documentation
: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
*
 What is Automated Machine Learning? [aka.ms/AutomatedML](ht
tps://aka.ms/AutomatedML)
* Python Microsoft: [aka.ms/Python
MS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms
/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)"	"https://www.reddit.com/r/deeplearning/comments/f8t2cf/everything_you_need_to_know_about_computer_vision/"	"5"	"1582559065.0"	"69"	"f8t2cf"
49	"*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!)"	""	"https://i.redd.it/as278qmzuqt61.gif"	"9"	"1618669867.0"	"65"	"msrsc4"
50	"Meta’s LLaMa weights leaked on torrent... and the best thing about it is someone put up a PR to replace the google form in the repo with it 😂"	""	"https://i.redd.it/olnsv438alla1.jpg"	"23"	"1677877700.0"	"185"	"11hezvk"
51	"⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI"	"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of
 The Decade](https://preview.redd.it/sg24cw3zekea1.png?width
=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2
fff989a1)

Microsoft is investing $10B into OpenAI!

There i
s lots of frustration in the community about OpenAI not bein
g all that open anymore. They appear to abandon their ethos 
of developing AI for everyone, [free](https://openai.com/blo
g/introducing-openai/) of economic pressures.

The fear is t
hat OpenAI’s models are going to become fancy MS Office plug
ins. Gone would be the days of open research and innovation.


However, the specifics of the deal tell a different story.


To understand what is going on, we need to peek behind the
 curtain of the tough business of machine learning. We will 
find that Sam Altman might have just orchestrated the coup o
f the decade!

To appreciate better why there is some three-
dimensional chess going on, let’s first look at Sam Altman’s
 backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sa
m Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt
) and was part of the first-ever YC batch. He raised a total
 of $30M in funding, but the company failed to gain traction
. Seven years into the business Loopt was basically dead in 
the water and had to be shut down.

Instead of caving, he ma
naged to sell his startup for $[43M](https://golden.com/wiki
/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https
://www.greendot.com/). Investors got their money back and he
 personally made $5M from the sale.

By YC standards, this w
as a pretty unimpressive outcome.

However, people took note
 that the fire between his ears was burning hotter than that
 of most people. So hot in fact that Paul Graham included hi
m in his 2009 [essay](http://www.paulgraham.com/5founders.ht
ml?viewfullsite=1) about the five founders who influenced hi
m the most.

He listed young Sam Altman next to Steve Jobs, 
Larry & Sergey from Google, and Paul Buchheit (creator of GM
ail and AdSense). He went on to describe him as a strategic 
mastermind whose sheer force of will was going to get him wh
atever he wanted.

And Sam Altman played his hand well!

He 
parleyed his new connections into raising $21M from Peter Th
iel and others to start investing. Within four years he 10x-
ed the money \[2\]. In addition, Paul Graham made him his su
ccessor as president of YC in 2014.

Within one decade of se
lling his first startup for $5M, he grew his net worth to a 
mind-bending $250M and rose to the circle of the most influe
ntial people in Silicon Valley.

Today, he is the CEO of Ope
nAI — one of the most exciting and impactful organizations i
n all of tech.

However, OpenAI — the rocket ship of AI inno
vation — is in dire straights.

# OpenAI is Bleeding Cash

B
ack in 2015, OpenAI was kickstarted with $1B in donations fr
om famous donors such as Elon Musk.

That money is long gone
.

In 2022 OpenAI is projecting a revenue of $36M. At the sa
me time, they spent roughly $544M. Hence the company has los
t >$500M over the last year alone.

This is probably not an 
outlier year. OpenAI is headquartered in San Francisco and h
as a stable of 375 employees of mostly machine learning rock
stars. Hence, salaries alone probably come out to be roughly
 $200M p.a.

In addition to high salaries their compute cost
s are stupendous. Considering it cost them $4.6M to train GP
T3 once, it is likely that their cloud bill is in a very hea
lthy nine-figure range as well \[4\].

So, where does this l
eave them today?

Before the Microsoft investment of $10B, O
penAI had received a total of $4B over its lifetime. With $4
B in funding, a burn rate of $0.5B, and eight years of compa
ny history it doesn’t take a genius to figure out that they 
are running low on cash.

It would be reasonable to think: O
penAI is sitting on ChatGPT and other great models. Can’t th
ey just lease them and make a killing?

Yes and no. OpenAI i
s projecting a revenue of $1B for 2024. However, it is unlik
ely that they could pull this off without significantly incr
easing their costs as well.

*Here are some reasons why!*

#
 The Tough Business Of Machine Learning

Machine learning co
mpanies are distinct from regular software companies. On the
 outside they look and feel similar: people are creating pro
ducts using code, but on the inside things can be very diffe
rent.

To start off, machine learning companies are usually 
way less profitable. Their gross margins land in the 50%-60%
 range, much lower than those of SaaS businesses, which can 
be as high as 80% \[7\].

On the one hand, the massive compu
te requirements and thorny data management problems drive up
 costs.

On the other hand, the work itself can sometimes re
semble consulting more than it resembles software engineerin
g. Everyone who has worked in the field knows that training 
models requires deep domain knowledge and loads of manual wo
rk on data.

To illustrate the latter point, imagine the uns
peakable complexity of performing content moderation on Chat
GPT’s outputs. If OpenAI scales the usage of GPT in producti
on, they will need large teams of moderators to filter and l
abel hate speech, slurs, tutorials on killing people, you na
me it.

*Alright, alright, alright! Machine learning is hard
.*

*OpenAI already has ChatGPT working. That’s gotta be wor
th something?*

# Foundation Models Might Become Commodities
:

In order to monetize GPT or any of their other models, Op
enAI can go two different routes.

First, they could pick on
e or more verticals and sell directly to consumers. They cou
ld for example become the ultimate copywriting tool and blow
 [Jasper](https://app.convertkit.com/campaigns/10748016/jasp
er.ai) or [copy.ai](https://app.convertkit.com/campaigns/107
48016/copy.ai) out of the water.

This is not going to happe
n. Reasons for it include:

1. To support their mission of b
uilding competitive foundational AI tools, and their huge(!)
 burn rate, they would need to capture one or more very larg
e verticals.
2. They fundamentally need to re-brand themselv
es and diverge from their original mission. This would likel
y scare most of the talent away.
3. They would need to build
 out sales and marketing teams. Such a step would fundamenta
lly change their culture and would inevitably dilute their f
ocus on research.

The second option OpenAI has is to keep d
oing what they are doing and monetize access to their models
 via API. Introducing a [pro version](https://www.searchengi
nejournal.com/openai-chatgpt-professional/476244/) of ChatGP
T is a step in this direction.

This approach has its own ch
allenges. Models like GPT do have a defensible moat. They ar
e just large transformer models trained on very large open-s
ource datasets.

As an example, last week Andrej Karpathy re
leased a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY
) of him coding up a version of GPT in an afternoon. Nothing
 could stop e.g. Google, StabilityAI, or HuggingFace from op
en-sourcing their own GPT.

As a result GPT inference would 
become a common good. This would melt OpenAI’s profits down 
to a tiny bit of nothing.

In this scenario, they would also
 have a very hard time leveraging their branding to generate
 returns. Since companies that integrate with OpenAI’s API c
ontrol the interface to the customer, they would likely end 
up capturing all of the value.

An argument can be made that
 this is a general problem of foundation models. Their high 
fixed costs and lack of differentiation could end up making 
them akin to the [steel industry](https://www.thediff.co/arc
hive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum
 it up:

* They don’t have a way to sustainably monetize the
ir models.
* They do not want and probably should not build 
up internal sales and marketing teams to capture verticals
*
 They need a lot of money to keep funding their research wit
hout getting bogged down by details of specific product deve
lopment

*So, what should they do?*

# The Microsoft Deal

O
penAI and Microsoft [announced](https://blogs.microsoft.com/
blog/2023/01/23/microsoftandopenaiextendpartnership/) the ex
tension of their partnership with a $10B investment, on Mond
ay.

At this point, Microsoft will have invested a total of 
$13B in OpenAI. Moreover, new VCs are in on the deal by buyi
ng up shares of employees that want to take some chips off t
he table.

However, the astounding size is not the only extr
aordinary thing about this deal.

First off, the ownership w
ill be split across three groups. Microsoft will hold 49%, V
Cs another 49%, and the OpenAI foundation will control the r
emaining 2% of shares.

If OpenAI starts making money, the p
rofits are distributed differently across four stages:

1. F
irst, early investors (probably Khosla Ventures and Reid Hof
fman’s foundation) get their money back with interest.
2. Af
ter that Microsoft is entitled to 75% of profits until the $
13B of funding is repaid
3. When the initial funding is repa
id, Microsoft and the remaining VCs each get 49% of profits.
 This continues until another $92B and $150B are paid out to
 Microsoft and the VCs, respectively.
4. Once the aforementi
oned money is paid to investors, 100% of shares return to th
e foundation, which regains total control over the company. 
\[3\]

# What This Means

This is absolutely crazy!

OpenAI 
managed to solve all of its problems at once. They raised a 
boatload of money and have access to all the compute they ne
ed.

On top of that, they solved their distribution problem.
 They now have access to Microsoft’s sales teams and their m
odels will be integrated into MS Office products.

Microsoft
 also benefits heavily. They can play at the forefront AI, b
rush up their tools, and have OpenAI as an exclusive partner
 to further compete in a [bitter cloud war](https://www.proj
ectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-
cloud-war/401) against AWS.

The synergies do not stop there
.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. 
will likely benefit heavily from the partnership as they con
tinue to develop[ GitHub Copilot](https://github.com/feature
s/copilot).

The deal creates a beautiful win-win situation,
 but that is not even the best part.

Sam Altman and his tea
m at OpenAI essentially managed to place a giant hedge. If O
penAI does not manage to create anything meaningful or we en
ter a new AI winter, Microsoft will have paid for the party.


However, if OpenAI creates something in the direction of A
GI — whatever that looks like — the value of it will likely 
be huge.

In that case, OpenAI will quickly repay the dept t
o Microsoft and the foundation will control 100% of whatever
 was created.

*Wow!*

Whether you agree with the path OpenA
I has chosen or would have preferred them to stay donation-b
ased, you have to give it to them.

*This deal is an absolut
e power move!*

I look forward to the future. Such exciting 
times to be alive!

As always, I really enjoyed making this 
for you and I sincerely hope you found it useful!

*Thank yo
u for reading!*

Would you like to receive an article such a
s this one straight to your inbox every Thursday? Consider s
igning up for **The Decoding** ⭕.

I send out a thoughtful n
ewsletter about ML research and the data economy once a week
. No Spam. No Nonsense. [Click here to sign up!](https://the
decoding.net/)

**References:**

\[1\] [https://golden.com/w
iki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J
5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/1
0/sam-altmans-manifest-destiny](https://www.newyorker.com/ma
gazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Art
icle in Fortune magazine ](https://fortune.com/2023/01/11/st
ructure-openai-investment-microsoft/?verification_code=DOVCV
S8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbG
Q2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMD
AzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRT
kxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW
55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6Nj
MyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/210
4.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5
\] [https://www.crunchbase.com/organization/openai/company\_
financials](https://www.crunchbase.com/organization/openai/c
ompany_financials)​

\[6\] Elon Musk donation [https://www.i
nverse.com/article/52701-openai-documents-elon-musk-donation
-a-i-research](https://www.inverse.com/article/52701-openai-
documents-elon-musk-donation-a-i-research)​

\[7\] [https://
a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-diffe
rent-from-traditional-software-2/](https://a16z.com/2020/02/
16/the-new-business-of-ai-and-how-its-different-from-traditi
onal-software-2/)"	"https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/"	"16"	"1674816348.0"	"117"	"10mhyek"
52	"image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model"	""	"https://www.youtube.com/watch?v=FwXQ568_io0"	"6"	"1596625086.0"	"99"	"i437pt"
53	"I wrote a program with OpenAI's Codex that fixes errors"	""	"https://v.redd.it/jupdtry6vf881"	"6"	"1640765002.0"	"97"	"rr2wme"
54	"Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT"	"Vicuna : ChatGPT Alternative, Open-Source, High Quality and 
Low Cost 

&#x200B;

[ Relative Response Quality Assessed by
 GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599
&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a07
2b91)

Vicuna-13B has demonstrated competitive performance a
gainst other open-source models, such as Stanford Alpaca, by
 fine-tuning a LLaMA base model on user-shared conversations
 collected from ShareGPT.

Evaluation using GPT-4 as a judge
 shows that Vicuna-13B achieves more than 90% of the quality
 of OpenAI ChatGPT and Google Bard AI, while outperforming o
ther models such as Meta LLaMA (Large Language Model Meta AI
) and Stanford Alpaca in more than 90% of cases.

The cost o
f training Vicuna-13B is approximately $300.

The training a
nd serving code, along with an online demo, are publicly ava
ilable for non-commercial use.

&#x200B;

More Information :
 [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-sourc
e-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT]
(https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source
-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)


Discord Server : [https://discord.gg/h6kCZb72G7](https://di
scord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysor
g](https://twitter.com/lmsysorg)"	"https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/"	"19"	"1680658600.0"	"82"	"12c43uu"
55	"GPT-4 Will Be 500x Smaller Than People Think - Here Is Why"	"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://pre
view.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=web
p&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mil
l is buzzing around the release of GPT-4.

People are predic
ting the model will have 100 trillion parameters. That’s a *
trillion* with a “t”.

The often-used graphic above makes GP
T-3 look like a cute little breadcrumb that is about to have
 a live-ending encounter with a bowling ball.

Sure, OpenAI’
s new brainchild will certainly be mind-bending and language
 models have been getting bigger — fast!

But this time migh
t be different and it makes for a good opportunity to look a
t the research on scaling large language models (LLMs).

*Le
t’s go!*

Training 100 Trillion Parameters

The creation of 
GPT-3 was a marvelous feat of engineering. The training was 
done on 1024 GPUs, took 34 days, and cost $4.6M in compute a
lone \[1\].

Training a 100T parameter model on the same dat
a, using 10000 GPUs, would take 53 Years. To avoid overfitti
ng such a huge model the dataset would also need to be much(
!) larger.

So, where is this rumor coming from?

The Source
 Of The Rumor:

It turns out OpenAI itself might be the sour
ce of it.

In August 2021 the CEO of Cerebras told [wired](h
ttps://www.wired.com/story/cerebras-chip-cluster-neural-netw
orks-ai/): “From talking to OpenAI, GPT-4 will be about 100 
trillion parameters”.

A the time, that was most likely what
 they believed, but that was in 2021. So, basically forever 
ago when machine learning research is concerned.

Things hav
e changed a lot since then!

To understand what happened we 
first need to look at how people decide the number of parame
ters in a model.

Deciding The Number Of Parameters:

The en
ormous hunger for resources typically makes it feasible to t
rain an LLM only once.

In practice, the available compute b
udget (how much money will be spent, available GPUs, etc.) i
s known in advance. Before the training is started, research
ers need to accurately predict which hyperparameters will re
sult in the best model.

*But there’s a catch!*

Most resear
ch on neural networks is empirical. People typically run hun
dreds or even thousands of training experiments until they f
ind a good model with the right hyperparameters.

With LLMs 
we cannot do that. Training 200 GPT-3 models would set you b
ack roughly a billion dollars. Not even the deep-pocketed te
ch giants can spend this sort of money.

Therefore, research
ers need to work with what they have. Either they investigat
e the few big models that have been trained or they train sm
aller models in the hope of learning something about how to 
scale the big ones.

This process can very noisy and the com
munity’s understanding has evolved a lot over the last few y
ears.

What People Used To Think About Scaling LLMs

In 2020
, a team of researchers from OpenAI released a [paper](https
://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For N
eural Language Models”.

They observed a predictable decreas
e in training loss when increasing the model size over multi
ple orders of magnitude.

So far so good. But they made two 
other observations, which resulted in the model size balloon
ing rapidly.

1. To scale models optimally the parameters sh
ould scale quicker than the dataset size. To be exact, their
 analysis showed when increasing the model size 8x the datas
et only needs to be increased 5x.
2. Full model convergence 
is not compute-efficient. Given a fixed compute budget it is
 better to train large models shorter than to use a smaller 
model and train it longer.

Hence, it seemed as if the way t
o improve performance was to scale models faster than the da
taset size \[2\].

And that is what people did. The models g
ot larger and larger with GPT-3 (175B), [Gopher](https://arx
iv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](htt
ps://arxiv.org/pdf/2201.11990) (530B) just to name a few.

B
ut the bigger models failed to deliver on the promise.

*Rea
d on to learn why!*

What We know About Scaling Models Today


It turns out you need to scale training sets and models in
 equal proportions. So, every time the model size doubles, t
he number of training tokens should double as well.

This wa
s published in DeepMind’s 2022 [paper](https://arxiv.org/pdf
/2203.15556.pdf): “Training Compute-Optimal Large Language M
odels”

The researchers fitted over 400 language models rang
ing from 70M to over 16B parameters. To assess the impact of
 dataset size they also varied the number of training tokens
 from 5B-500B tokens.

The findings allowed them to estimate
 that a compute-optimal version of GPT-3 (175B) should be tr
ained on roughly 3.7T tokens. That is more than 10x the data
 that the original model was trained on.

To verify their re
sults they trained a fairly small model on vastly more data.
 Their model, called Chinchilla, has 70B parameters and is t
rained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 b
ut trained on almost 5x the data.

Chinchilla outperforms GP
T-3 and other much larger models by a fair margin \[3\].

Th
is was a great breakthrough!The model is not just better, bu
t its smaller size makes inference cheaper and finetuning ea
sier.

*So What Will Happen?*

What GPT-4 Might Look Like:


To properly fit a model with 100T parameters, open OpenAI ne
eds a dataset of roughly 700T tokens. Given 1M GPUs and usin
g the calculus from above, it would still take roughly 2650 
years to train the model \[1\].

So, here is what GPT-4 coul
d look like:

* Similar size to GPT-3, but trained optimally
 on 10x more data
* ​[Multi-modal](https://thealgorithmicbri
dge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputt
ing text, images, and sound
* Output conditioned on document
 chunks from a memory bank that the model has access to duri
ng prediction \[4\]
* Doubled context size allows longer pre
dictions before the model starts going off the rails​

Regar
dless of the exact design, it will be a solid step forward. 
However, it will not be the 100T token human-brain-like AGI 
that people make it out to be.

Whatever it will look like, 
I am sure it will be amazing and we can all be excited about
 the release.

Such exciting times to be alive!

If you got 
down here, thank you! It was a privilege to make this for yo
u. At **TheDecoding** ⭕, I send out a thoughtful newsletter 
about ML research and the data economy once a week. No Spam.
 No Nonsense. [Click here to sign up!](https://thedecoding.n
et/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Ca
sper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbran
d, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee
 , M. Zaharia, [Efficient Large-Scale Language Model Trainin
g on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2
104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. 
Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Sc
aling laws for neural language model](https://arxiv.org/abs/
2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. B
orgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D
. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Tra
ining Compute-Optimal Large Language Models](https://arxiv.o
rg/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*
.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rut
herford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A.
 Clark, D. Casas, [Improving language models by retrieving f
rom trillions of tokens](https://arxiv.org/abs/2112.04426) (
2021). *arXiv preprint arXiv:2112.04426*.Vancouver"	"https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/"	"11"	"1674114949.0"	"69"	"10fw22o"
56	"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks"	"OpenAI released their newest language, [Triton](https://gith
ub.com/openai/triton). This open-source programming language
 that enables researchers to write highly efficient GPU code
 for AI workloads is Python-compatible and comes with the ab
ility of a user to write in as few as 25 lines, something on
 par with what an expert could achieve. OpenAI claims this m
akes it possible to reach peak hardware performance without 
much effort, making creating more complex workflows easier t
han ever before!

Researchers in the field of Deep Learning 
often rely on native framework operators. However, this can 
be problematic because it requires many temporary tensors to
 work, which may hurt performance at scale for neural networ
ks. Writing specialized GPU kernels is a more convenient sol
ution, but surprisingly difficult due to intricacies when pr
ogramming them according to GPUs. It was challenging to find
 a system that provides the flexibility and speed required w
hile also being easy enough for developers to understand. Th
is has led researchers at OpenAI in improving Triton, which 
was initially founded by one of their teammates.

Quick Read
: [https://www.marktechpost.com/2021/07/28/openai-releases-t
riton-an-open-source-python-like-gpu-programming-language-fo
r-neural-networks/](https://www.marktechpost.com/2021/07/28/
openai-releases-triton-an-open-source-python-like-gpu-progra
mming-language-for-neural-networks/) 

Paper: http://www.eec
s.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pd
f

Github: https://github.com/openai/triton"	"https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/"	"5"	"1627494357.0"	"65"	"otf0fs"
57	"Open AI and Microsoft Can Generate Python Code"	""	"https://youtu.be/y5-wzgIySb4"	"6"	"1590161008.0"	"66"	"golbq4"
58	"Everything you need to know about computer vision in one repo"	"*This post was co-authored by JS Tan, Patrick Buehler, Anupa
m Sharma and Jun Ki Min.*

In recent years, we’ve seen extra
ordinary growth in Computer Vision, with applications in ima
ge understanding, search, mapping, semi-autonomous or autono
mous vehicles and many more .

The ability for models to und
erstand actions in a video , a task that was unthinkable jus
t a few years ago , is now something that we can achieve wit
h relatively high accuracy and in near real-time.

However, 
the field is not particularly welcoming for newcomers. Witho
ut prior experience or guidance, building an accurate classi
fier can easily take weeks. Unless you’re ready to spend a l
ong-time learning computer vision, it’s extremely hard to ma
ster the basics, let alone begin to explore some of the cutt
ing-edge technologies in the field. Even for computer vision
 experts, building a quick Proof of Concept (POC) can be non
 trivial and could easily end up taking many days to put tog
ether.

At [Microsoft ](https://docs.microsoft.com/en-us/azu
re/machine-learning/?WT.mc_id=medium-article-lazzeri), we ha
ve been working for many years on diverse Computer Vision so
lutions for our customers and collected our learning into ou
r new public [Microsoft](https://docs.microsoft.com/en-us/az
ure/machine-learning/?WT.mc_id=medium-article-lazzeri) repos
itory: [Custom vision repo](https://github.com/microsoft/Com
puterVision-recipes?WT.mc_id=medium-article-lazzeri).

The g
oal of [this repository](https://github.com/microsoft/Comput
erVision-recipes?WT.mc_id=medium-article-lazzeri) is to prov
ide examples and best practice guidelines for building compu
ter vision systems on [Azure](https://docs.microsoft.com/en-
us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) 
, and to share this with the open-source community . More sp
ecifically, our goal was to create a [repository](https://do
cs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medi
um-article-lazzeri) that will help us to provide solutions r
apidly to the community and to customers that we work with ,
 or with on-boarding new team members who may have expertise
 in data science, but not specifically in computer vision. F
rom mastering some of the most common scenarios in the field
, like image classification, object detection , and image si
milarity, to exploring cutting edge scenarios like activity 
recognition and crowd counting, [this repo](https://docs.mic
rosoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-art
icle-lazzeri) will guide you through building models, fine-t
uning them, and using them in real-world scenarios.

We’re k
icking off our repo with **5 scenarios.** You can find the l
inks to the repos here:

* [Classification](https://github.c
om/microsoft/computervision-recipes/tree/master/scenarios/cl
assification?WT.mc_id=medium-article-lazzeri)
* [Similarity]
(https://github.com/microsoft/computervision-recipes/tree/ma
ster/scenarios/similarity?WT.mc_id=medium-article-lazzeri)
*
 [Detection](https://github.com/microsoft/computervision-rec
ipes/tree/master/scenarios/detection?WT.mc_id=medium-article
-lazzeri)
* [Action Recognition](https://github.com/microsof
t/computervision-recipes/tree/master/contrib/action_recognit
ion?WT.mc_id=medium-article-lazzeri)
* [Crowd Counting](http
s://github.com/microsoft/computervision-recipes/tree/master/
contrib/crowd_counting?WT.mc_id=medium-article-lazzeri)

Rat
her than creating implementations from scratch, we draw from
 popular state-of-the-art libraries (e.g. fast.ai and [torch
vision ](https://pytorch.org/docs/stable/torchvision/index.h
tml)), and we build additional utility around loading image 
data, optimizing models , and evaluating models. In addition
, we aim to answer the frequently asked questions, try to ex
plain the deep learning intuitions, and highlight common pit
falls.

Whether you a re an expert in computer vision or jus
t getting your hands wet, we believe [this repository](https
://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id
=medium-article-lazzeri) offers something for you . For the 
beginner, [this repo](https://docs.microsoft.com/en-us/azure
/machine-learning/?WT.mc_id=medium-article-lazzeri) will gui
de you through building a state-of-the-art model and help yo
u develop an intuition for the craft. For the experts, this 
repository can quickly get you to a strong baseline model wh
ich is easy to extend using custom Python/PyTorch code. In a
ddition, the repository also aims to provide support with:


1. [The full data science process](https://docs.microsoft.co
m/azure/machine-learning/team-data-science-process/overview?
WT.mc_id=medium-article-lazzeri).
2. [The tooling to succeed
 on Azure](https://docs.microsoft.com/en-us/azure/machine-le
arning/?WT.mc_id=medium-article-lazzeri).

We hope that thes
e examples and utilities will make it easier and faster for 
developers to create custom vision applications.

# The Data
 Science Process

The [Computer Vision Recipes GitHub reposi
tory](https://github.com/microsoft/ComputerVision-recipes?WT
.mc_id=medium-article-lazzeri) shows you how to approach the
 five key steps of the data science process and provides uti
lities to enrich each of the steps :

1. **Evaluating** — Ev
aluate your model. Depending on the metric you’re interested
 in optimizing, you may want to explore different methods of
 evaluation.
2. **Model selection and optimization** — Tun e
 and optimize hyperparameters to get the highest performing 
model. Because Computer Vision models are often computationa
lly costly, we show you how to seamlessly scale your paramet
er tuning into Azure .
3. **Operationalizing** — Operational
ize models in a production environment on Azure by deploying
 it onto Kubernetes.

Inside the computer vision recipes [re
po,](https://github.com/microsoft/ComputerVision-recipes?WT.
mc_id=medium-article-lazzeri) we have added a lot of utility
 to support common tasks such as loading data sets in the fo
rmat expected by different algorithms, splitting training/te
st data, and evaluating model outputs .

This computer visio
n repository also has deep integration with the [Azure Machi
ne Learning](https://docs.microsoft.com/en-us/azure/machine-
learning/?WT.mc_id=medium-article-lazzeri) to complement you
r work locally. We provide code examples on how you can opti
onally and easily scale your training into the cloud, and ho
w you can deploy your models for production workloads.

**Az
ure Cognitive Services**

Note that for certain computer vis
ion problems, you may not need to build your own models. Ins
tead, pre-built or easily customizable solutions exist which
 do not require any custom coding or machine learning expert
ise.

* [Vision Services](https://docs.microsoft.com/en-us/a
zure/cognitive-services/computer-vision/?WT.mc_id=medium-art
icle-lazzeri) are a set of pre-trained REST APIs which can b
e called for image tagging, OCR, video analytics, and more. 
These APIs work out of the box and require minimal expertise
 in machine learning but have limited customization capabili
ties. See the various demos available to get a feel for the 
functionality (e.g. Computer Vision).
* [Custom Vision](http
s://docs.microsoft.com/en-us/azure/cognitive-services/custom
-vision-service/?WT.mc_id=medium-article-lazzeri) is a SaaS 
service to train and deploy a model as a REST API given a us
er-provided training set. All steps including image upload, 
annotation, and model deployment can be performed using eith
er the UI or a Python SDK. Training image classification or 
object detection models can be achieved with minimal machine
 learning expertise. The Custom Vision offers more flexibili
ty than using the pre-trained cognitive services APIs but re
quires the user to bring and annotate their own data.

Befor
e using the Computer Vision repository, we strongly recommen
d evaluating if these can sufficiently solve your problem.


To give you a sense of how you can use our repo to build a s
tate of the art (SOTA) model, here is a preview of how simpl
e it is to create an Object Detection model. Of course, you 
can go much deeper and add custom PyTorch code, but getting 
started is as simple as this :

**1. Load your data**

The f
irst step is to load your data — we help you do this with a 
simple object that automatically parses your data and the an
notations:

`from utils_cv.detection.data import DetectionLo
ader data = DetectionLoader('path/to/data')`

**2. Train/fin
e-tune your model**

Then we create a ‘learner’ object that 
helps you manage and train your model. By default, it will u
se torchvision’s Faster R-CNN model. But you can easily swit
ch it out.

`from utils_cv.detection.model import DetectionL
earner detector = DetectionLearner(data) detector.fit()`

**
3. Evaluate**

Finally, lets evaluate our model using the bu
ilt-in helper functions. We can look at the precision and re
call curves to give us a sense of how our model is performin
g.

`from utils_cv.detection.plot import plot_pr_curves eval
 = detector.evaluate() plot_pr_curves(eval)`

As we continue
 to build out of repository, we will be looking for new comp
uter vision scenarios to unlock . Feel free to reach out to 
[cvbp@microsoft.com](mailto:cvbp@microsoft.com) or post an i
ssue if you wish to see us cover a scenario .

# Additional 
resources to learn more

To learn more, you can read the fol
lowing articles and notebooks:

* [Custom vision repo](https
://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medi
um-article-lazzeri)
* Original article: [https://techcommuni
ty.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-k
now-about-computer-vision-in-one/ba-p/1070311](https://techc
ommunity.microsoft.com/t5/azure-ai/nearly-everything-you-nee
d-to-know-about-computer-vision-in-one/ba-p/1070311)
* [Visi
on Services](https://docs.microsoft.com/en-us/azure/cognitiv
e-services/computer-vision/?WT.mc_id=medium-article-lazzeri)
 on Azure
* [Custom Vision](https://docs.microsoft.com/en-us
/azure/cognitive-services/custom-vision-service/?WT.mc_id=me
dium-article-lazzeri) on Azure
* Portfolio of Azure Machine 
Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://ak
a.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms
/AzureMLservice](https://aka.ms/AzureMLservice)
* Get starte
d with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/G
etStartedAzureML)
* Automated Machine Learning Documentation
: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
*
 What is Automated Machine Learning? [aka.ms/AutomatedML](ht
tps://aka.ms/AutomatedML)
* Python Microsoft: [aka.ms/Python
MS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms
/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)"	"https://www.reddit.com/r/deeplearning/comments/f8t2cf/everything_you_need_to_know_about_computer_vision/"	"5"	"1582559065.0"	"67"	"f8t2cf"
59	"*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!)"	""	"https://i.redd.it/as278qmzuqt61.gif"	"9"	"1618669867.0"	"64"	"msrsc4"
60	"Meta’s LLaMa weights leaked on torrent... and the best thing about it is someone put up a PR to replace the google form in the repo with it 😂"	""	"https://i.redd.it/olnsv438alla1.jpg"	"23"	"1677877700.0"	"187"	"11hezvk"
61	"⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI"	"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of
 The Decade](https://preview.redd.it/sg24cw3zekea1.png?width
=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2
fff989a1)

Microsoft is investing $10B into OpenAI!

There i
s lots of frustration in the community about OpenAI not bein
g all that open anymore. They appear to abandon their ethos 
of developing AI for everyone, [free](https://openai.com/blo
g/introducing-openai/) of economic pressures.

The fear is t
hat OpenAI’s models are going to become fancy MS Office plug
ins. Gone would be the days of open research and innovation.


However, the specifics of the deal tell a different story.


To understand what is going on, we need to peek behind the
 curtain of the tough business of machine learning. We will 
find that Sam Altman might have just orchestrated the coup o
f the decade!

To appreciate better why there is some three-
dimensional chess going on, let’s first look at Sam Altman’s
 backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sa
m Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt
) and was part of the first-ever YC batch. He raised a total
 of $30M in funding, but the company failed to gain traction
. Seven years into the business Loopt was basically dead in 
the water and had to be shut down.

Instead of caving, he ma
naged to sell his startup for $[43M](https://golden.com/wiki
/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https
://www.greendot.com/). Investors got their money back and he
 personally made $5M from the sale.

By YC standards, this w
as a pretty unimpressive outcome.

However, people took note
 that the fire between his ears was burning hotter than that
 of most people. So hot in fact that Paul Graham included hi
m in his 2009 [essay](http://www.paulgraham.com/5founders.ht
ml?viewfullsite=1) about the five founders who influenced hi
m the most.

He listed young Sam Altman next to Steve Jobs, 
Larry & Sergey from Google, and Paul Buchheit (creator of GM
ail and AdSense). He went on to describe him as a strategic 
mastermind whose sheer force of will was going to get him wh
atever he wanted.

And Sam Altman played his hand well!

He 
parleyed his new connections into raising $21M from Peter Th
iel and others to start investing. Within four years he 10x-
ed the money \[2\]. In addition, Paul Graham made him his su
ccessor as president of YC in 2014.

Within one decade of se
lling his first startup for $5M, he grew his net worth to a 
mind-bending $250M and rose to the circle of the most influe
ntial people in Silicon Valley.

Today, he is the CEO of Ope
nAI — one of the most exciting and impactful organizations i
n all of tech.

However, OpenAI — the rocket ship of AI inno
vation — is in dire straights.

# OpenAI is Bleeding Cash

B
ack in 2015, OpenAI was kickstarted with $1B in donations fr
om famous donors such as Elon Musk.

That money is long gone
.

In 2022 OpenAI is projecting a revenue of $36M. At the sa
me time, they spent roughly $544M. Hence the company has los
t >$500M over the last year alone.

This is probably not an 
outlier year. OpenAI is headquartered in San Francisco and h
as a stable of 375 employees of mostly machine learning rock
stars. Hence, salaries alone probably come out to be roughly
 $200M p.a.

In addition to high salaries their compute cost
s are stupendous. Considering it cost them $4.6M to train GP
T3 once, it is likely that their cloud bill is in a very hea
lthy nine-figure range as well \[4\].

So, where does this l
eave them today?

Before the Microsoft investment of $10B, O
penAI had received a total of $4B over its lifetime. With $4
B in funding, a burn rate of $0.5B, and eight years of compa
ny history it doesn’t take a genius to figure out that they 
are running low on cash.

It would be reasonable to think: O
penAI is sitting on ChatGPT and other great models. Can’t th
ey just lease them and make a killing?

Yes and no. OpenAI i
s projecting a revenue of $1B for 2024. However, it is unlik
ely that they could pull this off without significantly incr
easing their costs as well.

*Here are some reasons why!*

#
 The Tough Business Of Machine Learning

Machine learning co
mpanies are distinct from regular software companies. On the
 outside they look and feel similar: people are creating pro
ducts using code, but on the inside things can be very diffe
rent.

To start off, machine learning companies are usually 
way less profitable. Their gross margins land in the 50%-60%
 range, much lower than those of SaaS businesses, which can 
be as high as 80% \[7\].

On the one hand, the massive compu
te requirements and thorny data management problems drive up
 costs.

On the other hand, the work itself can sometimes re
semble consulting more than it resembles software engineerin
g. Everyone who has worked in the field knows that training 
models requires deep domain knowledge and loads of manual wo
rk on data.

To illustrate the latter point, imagine the uns
peakable complexity of performing content moderation on Chat
GPT’s outputs. If OpenAI scales the usage of GPT in producti
on, they will need large teams of moderators to filter and l
abel hate speech, slurs, tutorials on killing people, you na
me it.

*Alright, alright, alright! Machine learning is hard
.*

*OpenAI already has ChatGPT working. That’s gotta be wor
th something?*

# Foundation Models Might Become Commodities
:

In order to monetize GPT or any of their other models, Op
enAI can go two different routes.

First, they could pick on
e or more verticals and sell directly to consumers. They cou
ld for example become the ultimate copywriting tool and blow
 [Jasper](https://app.convertkit.com/campaigns/10748016/jasp
er.ai) or [copy.ai](https://app.convertkit.com/campaigns/107
48016/copy.ai) out of the water.

This is not going to happe
n. Reasons for it include:

1. To support their mission of b
uilding competitive foundational AI tools, and their huge(!)
 burn rate, they would need to capture one or more very larg
e verticals.
2. They fundamentally need to re-brand themselv
es and diverge from their original mission. This would likel
y scare most of the talent away.
3. They would need to build
 out sales and marketing teams. Such a step would fundamenta
lly change their culture and would inevitably dilute their f
ocus on research.

The second option OpenAI has is to keep d
oing what they are doing and monetize access to their models
 via API. Introducing a [pro version](https://www.searchengi
nejournal.com/openai-chatgpt-professional/476244/) of ChatGP
T is a step in this direction.

This approach has its own ch
allenges. Models like GPT do have a defensible moat. They ar
e just large transformer models trained on very large open-s
ource datasets.

As an example, last week Andrej Karpathy re
leased a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY
) of him coding up a version of GPT in an afternoon. Nothing
 could stop e.g. Google, StabilityAI, or HuggingFace from op
en-sourcing their own GPT.

As a result GPT inference would 
become a common good. This would melt OpenAI’s profits down 
to a tiny bit of nothing.

In this scenario, they would also
 have a very hard time leveraging their branding to generate
 returns. Since companies that integrate with OpenAI’s API c
ontrol the interface to the customer, they would likely end 
up capturing all of the value.

An argument can be made that
 this is a general problem of foundation models. Their high 
fixed costs and lack of differentiation could end up making 
them akin to the [steel industry](https://www.thediff.co/arc
hive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum
 it up:

* They don’t have a way to sustainably monetize the
ir models.
* They do not want and probably should not build 
up internal sales and marketing teams to capture verticals
*
 They need a lot of money to keep funding their research wit
hout getting bogged down by details of specific product deve
lopment

*So, what should they do?*

# The Microsoft Deal

O
penAI and Microsoft [announced](https://blogs.microsoft.com/
blog/2023/01/23/microsoftandopenaiextendpartnership/) the ex
tension of their partnership with a $10B investment, on Mond
ay.

At this point, Microsoft will have invested a total of 
$13B in OpenAI. Moreover, new VCs are in on the deal by buyi
ng up shares of employees that want to take some chips off t
he table.

However, the astounding size is not the only extr
aordinary thing about this deal.

First off, the ownership w
ill be split across three groups. Microsoft will hold 49%, V
Cs another 49%, and the OpenAI foundation will control the r
emaining 2% of shares.

If OpenAI starts making money, the p
rofits are distributed differently across four stages:

1. F
irst, early investors (probably Khosla Ventures and Reid Hof
fman’s foundation) get their money back with interest.
2. Af
ter that Microsoft is entitled to 75% of profits until the $
13B of funding is repaid
3. When the initial funding is repa
id, Microsoft and the remaining VCs each get 49% of profits.
 This continues until another $92B and $150B are paid out to
 Microsoft and the VCs, respectively.
4. Once the aforementi
oned money is paid to investors, 100% of shares return to th
e foundation, which regains total control over the company. 
\[3\]

# What This Means

This is absolutely crazy!

OpenAI 
managed to solve all of its problems at once. They raised a 
boatload of money and have access to all the compute they ne
ed.

On top of that, they solved their distribution problem.
 They now have access to Microsoft’s sales teams and their m
odels will be integrated into MS Office products.

Microsoft
 also benefits heavily. They can play at the forefront AI, b
rush up their tools, and have OpenAI as an exclusive partner
 to further compete in a [bitter cloud war](https://www.proj
ectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-
cloud-war/401) against AWS.

The synergies do not stop there
.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. 
will likely benefit heavily from the partnership as they con
tinue to develop[ GitHub Copilot](https://github.com/feature
s/copilot).

The deal creates a beautiful win-win situation,
 but that is not even the best part.

Sam Altman and his tea
m at OpenAI essentially managed to place a giant hedge. If O
penAI does not manage to create anything meaningful or we en
ter a new AI winter, Microsoft will have paid for the party.


However, if OpenAI creates something in the direction of A
GI — whatever that looks like — the value of it will likely 
be huge.

In that case, OpenAI will quickly repay the dept t
o Microsoft and the foundation will control 100% of whatever
 was created.

*Wow!*

Whether you agree with the path OpenA
I has chosen or would have preferred them to stay donation-b
ased, you have to give it to them.

*This deal is an absolut
e power move!*

I look forward to the future. Such exciting 
times to be alive!

As always, I really enjoyed making this 
for you and I sincerely hope you found it useful!

*Thank yo
u for reading!*

Would you like to receive an article such a
s this one straight to your inbox every Thursday? Consider s
igning up for **The Decoding** ⭕.

I send out a thoughtful n
ewsletter about ML research and the data economy once a week
. No Spam. No Nonsense. [Click here to sign up!](https://the
decoding.net/)

**References:**

\[1\] [https://golden.com/w
iki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J
5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/1
0/sam-altmans-manifest-destiny](https://www.newyorker.com/ma
gazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Art
icle in Fortune magazine ](https://fortune.com/2023/01/11/st
ructure-openai-investment-microsoft/?verification_code=DOVCV
S8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbG
Q2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMD
AzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRT
kxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW
55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6Nj
MyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/210
4.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5
\] [https://www.crunchbase.com/organization/openai/company\_
financials](https://www.crunchbase.com/organization/openai/c
ompany_financials)​

\[6\] Elon Musk donation [https://www.i
nverse.com/article/52701-openai-documents-elon-musk-donation
-a-i-research](https://www.inverse.com/article/52701-openai-
documents-elon-musk-donation-a-i-research)​

\[7\] [https://
a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-diffe
rent-from-traditional-software-2/](https://a16z.com/2020/02/
16/the-new-business-of-ai-and-how-its-different-from-traditi
onal-software-2/)"	"https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/"	"16"	"1674816348.0"	"119"	"10mhyek"
62	"image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model"	""	"https://www.youtube.com/watch?v=FwXQ568_io0"	"6"	"1596625086.0"	"97"	"i437pt"
63	"I wrote a program with OpenAI's Codex that fixes errors"	""	"https://v.redd.it/jupdtry6vf881"	"6"	"1640765002.0"	"100"	"rr2wme"
64	"Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT"	"Vicuna : ChatGPT Alternative, Open-Source, High Quality and 
Low Cost 

&#x200B;

[ Relative Response Quality Assessed by
 GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599
&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a07
2b91)

Vicuna-13B has demonstrated competitive performance a
gainst other open-source models, such as Stanford Alpaca, by
 fine-tuning a LLaMA base model on user-shared conversations
 collected from ShareGPT.

Evaluation using GPT-4 as a judge
 shows that Vicuna-13B achieves more than 90% of the quality
 of OpenAI ChatGPT and Google Bard AI, while outperforming o
ther models such as Meta LLaMA (Large Language Model Meta AI
) and Stanford Alpaca in more than 90% of cases.

The cost o
f training Vicuna-13B is approximately $300.

The training a
nd serving code, along with an online demo, are publicly ava
ilable for non-commercial use.

&#x200B;

More Information :
 [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-sourc
e-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT]
(https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source
-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)


Discord Server : [https://discord.gg/h6kCZb72G7](https://di
scord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysor
g](https://twitter.com/lmsysorg)"	"https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/"	"19"	"1680658600.0"	"82"	"12c43uu"
65	"GPT-4 Will Be 500x Smaller Than People Think - Here Is Why"	"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://pre
view.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=web
p&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mil
l is buzzing around the release of GPT-4.

People are predic
ting the model will have 100 trillion parameters. That’s a *
trillion* with a “t”.

The often-used graphic above makes GP
T-3 look like a cute little breadcrumb that is about to have
 a live-ending encounter with a bowling ball.

Sure, OpenAI’
s new brainchild will certainly be mind-bending and language
 models have been getting bigger — fast!

But this time migh
t be different and it makes for a good opportunity to look a
t the research on scaling large language models (LLMs).

*Le
t’s go!*

Training 100 Trillion Parameters

The creation of 
GPT-3 was a marvelous feat of engineering. The training was 
done on 1024 GPUs, took 34 days, and cost $4.6M in compute a
lone \[1\].

Training a 100T parameter model on the same dat
a, using 10000 GPUs, would take 53 Years. To avoid overfitti
ng such a huge model the dataset would also need to be much(
!) larger.

So, where is this rumor coming from?

The Source
 Of The Rumor:

It turns out OpenAI itself might be the sour
ce of it.

In August 2021 the CEO of Cerebras told [wired](h
ttps://www.wired.com/story/cerebras-chip-cluster-neural-netw
orks-ai/): “From talking to OpenAI, GPT-4 will be about 100 
trillion parameters”.

A the time, that was most likely what
 they believed, but that was in 2021. So, basically forever 
ago when machine learning research is concerned.

Things hav
e changed a lot since then!

To understand what happened we 
first need to look at how people decide the number of parame
ters in a model.

Deciding The Number Of Parameters:

The en
ormous hunger for resources typically makes it feasible to t
rain an LLM only once.

In practice, the available compute b
udget (how much money will be spent, available GPUs, etc.) i
s known in advance. Before the training is started, research
ers need to accurately predict which hyperparameters will re
sult in the best model.

*But there’s a catch!*

Most resear
ch on neural networks is empirical. People typically run hun
dreds or even thousands of training experiments until they f
ind a good model with the right hyperparameters.

With LLMs 
we cannot do that. Training 200 GPT-3 models would set you b
ack roughly a billion dollars. Not even the deep-pocketed te
ch giants can spend this sort of money.

Therefore, research
ers need to work with what they have. Either they investigat
e the few big models that have been trained or they train sm
aller models in the hope of learning something about how to 
scale the big ones.

This process can very noisy and the com
munity’s understanding has evolved a lot over the last few y
ears.

What People Used To Think About Scaling LLMs

In 2020
, a team of researchers from OpenAI released a [paper](https
://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For N
eural Language Models”.

They observed a predictable decreas
e in training loss when increasing the model size over multi
ple orders of magnitude.

So far so good. But they made two 
other observations, which resulted in the model size balloon
ing rapidly.

1. To scale models optimally the parameters sh
ould scale quicker than the dataset size. To be exact, their
 analysis showed when increasing the model size 8x the datas
et only needs to be increased 5x.
2. Full model convergence 
is not compute-efficient. Given a fixed compute budget it is
 better to train large models shorter than to use a smaller 
model and train it longer.

Hence, it seemed as if the way t
o improve performance was to scale models faster than the da
taset size \[2\].

And that is what people did. The models g
ot larger and larger with GPT-3 (175B), [Gopher](https://arx
iv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](htt
ps://arxiv.org/pdf/2201.11990) (530B) just to name a few.

B
ut the bigger models failed to deliver on the promise.

*Rea
d on to learn why!*

What We know About Scaling Models Today


It turns out you need to scale training sets and models in
 equal proportions. So, every time the model size doubles, t
he number of training tokens should double as well.

This wa
s published in DeepMind’s 2022 [paper](https://arxiv.org/pdf
/2203.15556.pdf): “Training Compute-Optimal Large Language M
odels”

The researchers fitted over 400 language models rang
ing from 70M to over 16B parameters. To assess the impact of
 dataset size they also varied the number of training tokens
 from 5B-500B tokens.

The findings allowed them to estimate
 that a compute-optimal version of GPT-3 (175B) should be tr
ained on roughly 3.7T tokens. That is more than 10x the data
 that the original model was trained on.

To verify their re
sults they trained a fairly small model on vastly more data.
 Their model, called Chinchilla, has 70B parameters and is t
rained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 b
ut trained on almost 5x the data.

Chinchilla outperforms GP
T-3 and other much larger models by a fair margin \[3\].

Th
is was a great breakthrough!The model is not just better, bu
t its smaller size makes inference cheaper and finetuning ea
sier.

*So What Will Happen?*

What GPT-4 Might Look Like:


To properly fit a model with 100T parameters, open OpenAI ne
eds a dataset of roughly 700T tokens. Given 1M GPUs and usin
g the calculus from above, it would still take roughly 2650 
years to train the model \[1\].

So, here is what GPT-4 coul
d look like:

* Similar size to GPT-3, but trained optimally
 on 10x more data
* ​[Multi-modal](https://thealgorithmicbri
dge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputt
ing text, images, and sound
* Output conditioned on document
 chunks from a memory bank that the model has access to duri
ng prediction \[4\]
* Doubled context size allows longer pre
dictions before the model starts going off the rails​

Regar
dless of the exact design, it will be a solid step forward. 
However, it will not be the 100T token human-brain-like AGI 
that people make it out to be.

Whatever it will look like, 
I am sure it will be amazing and we can all be excited about
 the release.

Such exciting times to be alive!

If you got 
down here, thank you! It was a privilege to make this for yo
u. At **TheDecoding** ⭕, I send out a thoughtful newsletter 
about ML research and the data economy once a week. No Spam.
 No Nonsense. [Click here to sign up!](https://thedecoding.n
et/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Ca
sper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbran
d, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee
 , M. Zaharia, [Efficient Large-Scale Language Model Trainin
g on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2
104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. 
Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Sc
aling laws for neural language model](https://arxiv.org/abs/
2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. B
orgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D
. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Tra
ining Compute-Optimal Large Language Models](https://arxiv.o
rg/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*
.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rut
herford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A.
 Clark, D. Casas, [Improving language models by retrieving f
rom trillions of tokens](https://arxiv.org/abs/2112.04426) (
2021). *arXiv preprint arXiv:2112.04426*.Vancouver"	"https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/"	"11"	"1674114949.0"	"69"	"10fw22o"
66	"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks"	"OpenAI released their newest language, [Triton](https://gith
ub.com/openai/triton). This open-source programming language
 that enables researchers to write highly efficient GPU code
 for AI workloads is Python-compatible and comes with the ab
ility of a user to write in as few as 25 lines, something on
 par with what an expert could achieve. OpenAI claims this m
akes it possible to reach peak hardware performance without 
much effort, making creating more complex workflows easier t
han ever before!

Researchers in the field of Deep Learning 
often rely on native framework operators. However, this can 
be problematic because it requires many temporary tensors to
 work, which may hurt performance at scale for neural networ
ks. Writing specialized GPU kernels is a more convenient sol
ution, but surprisingly difficult due to intricacies when pr
ogramming them according to GPUs. It was challenging to find
 a system that provides the flexibility and speed required w
hile also being easy enough for developers to understand. Th
is has led researchers at OpenAI in improving Triton, which 
was initially founded by one of their teammates.

Quick Read
: [https://www.marktechpost.com/2021/07/28/openai-releases-t
riton-an-open-source-python-like-gpu-programming-language-fo
r-neural-networks/](https://www.marktechpost.com/2021/07/28/
openai-releases-triton-an-open-source-python-like-gpu-progra
mming-language-for-neural-networks/) 

Paper: http://www.eec
s.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pd
f

Github: https://github.com/openai/triton"	"https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/"	"5"	"1627494357.0"	"68"	"otf0fs"
67	"Open AI and Microsoft Can Generate Python Code"	""	"https://youtu.be/y5-wzgIySb4"	"6"	"1590161008.0"	"69"	"golbq4"
68	"Everything you need to know about computer vision in one repo"	"*This post was co-authored by JS Tan, Patrick Buehler, Anupa
m Sharma and Jun Ki Min.*

In recent years, we’ve seen extra
ordinary growth in Computer Vision, with applications in ima
ge understanding, search, mapping, semi-autonomous or autono
mous vehicles and many more .

The ability for models to und
erstand actions in a video , a task that was unthinkable jus
t a few years ago , is now something that we can achieve wit
h relatively high accuracy and in near real-time.

However, 
the field is not particularly welcoming for newcomers. Witho
ut prior experience or guidance, building an accurate classi
fier can easily take weeks. Unless you’re ready to spend a l
ong-time learning computer vision, it’s extremely hard to ma
ster the basics, let alone begin to explore some of the cutt
ing-edge technologies in the field. Even for computer vision
 experts, building a quick Proof of Concept (POC) can be non
 trivial and could easily end up taking many days to put tog
ether.

At [Microsoft ](https://docs.microsoft.com/en-us/azu
re/machine-learning/?WT.mc_id=medium-article-lazzeri), we ha
ve been working for many years on diverse Computer Vision so
lutions for our customers and collected our learning into ou
r new public [Microsoft](https://docs.microsoft.com/en-us/az
ure/machine-learning/?WT.mc_id=medium-article-lazzeri) repos
itory: [Custom vision repo](https://github.com/microsoft/Com
puterVision-recipes?WT.mc_id=medium-article-lazzeri).

The g
oal of [this repository](https://github.com/microsoft/Comput
erVision-recipes?WT.mc_id=medium-article-lazzeri) is to prov
ide examples and best practice guidelines for building compu
ter vision systems on [Azure](https://docs.microsoft.com/en-
us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) 
, and to share this with the open-source community . More sp
ecifically, our goal was to create a [repository](https://do
cs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medi
um-article-lazzeri) that will help us to provide solutions r
apidly to the community and to customers that we work with ,
 or with on-boarding new team members who may have expertise
 in data science, but not specifically in computer vision. F
rom mastering some of the most common scenarios in the field
, like image classification, object detection , and image si
milarity, to exploring cutting edge scenarios like activity 
recognition and crowd counting, [this repo](https://docs.mic
rosoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-art
icle-lazzeri) will guide you through building models, fine-t
uning them, and using them in real-world scenarios.

We’re k
icking off our repo with **5 scenarios.** You can find the l
inks to the repos here:

* [Classification](https://github.c
om/microsoft/computervision-recipes/tree/master/scenarios/cl
assification?WT.mc_id=medium-article-lazzeri)
* [Similarity]
(https://github.com/microsoft/computervision-recipes/tree/ma
ster/scenarios/similarity?WT.mc_id=medium-article-lazzeri)
*
 [Detection](https://github.com/microsoft/computervision-rec
ipes/tree/master/scenarios/detection?WT.mc_id=medium-article
-lazzeri)
* [Action Recognition](https://github.com/microsof
t/computervision-recipes/tree/master/contrib/action_recognit
ion?WT.mc_id=medium-article-lazzeri)
* [Crowd Counting](http
s://github.com/microsoft/computervision-recipes/tree/master/
contrib/crowd_counting?WT.mc_id=medium-article-lazzeri)

Rat
her than creating implementations from scratch, we draw from
 popular state-of-the-art libraries (e.g. fast.ai and [torch
vision ](https://pytorch.org/docs/stable/torchvision/index.h
tml)), and we build additional utility around loading image 
data, optimizing models , and evaluating models. In addition
, we aim to answer the frequently asked questions, try to ex
plain the deep learning intuitions, and highlight common pit
falls.

Whether you a re an expert in computer vision or jus
t getting your hands wet, we believe [this repository](https
://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id
=medium-article-lazzeri) offers something for you . For the 
beginner, [this repo](https://docs.microsoft.com/en-us/azure
/machine-learning/?WT.mc_id=medium-article-lazzeri) will gui
de you through building a state-of-the-art model and help yo
u develop an intuition for the craft. For the experts, this 
repository can quickly get you to a strong baseline model wh
ich is easy to extend using custom Python/PyTorch code. In a
ddition, the repository also aims to provide support with:


1. [The full data science process](https://docs.microsoft.co
m/azure/machine-learning/team-data-science-process/overview?
WT.mc_id=medium-article-lazzeri).
2. [The tooling to succeed
 on Azure](https://docs.microsoft.com/en-us/azure/machine-le
arning/?WT.mc_id=medium-article-lazzeri).

We hope that thes
e examples and utilities will make it easier and faster for 
developers to create custom vision applications.

# The Data
 Science Process

The [Computer Vision Recipes GitHub reposi
tory](https://github.com/microsoft/ComputerVision-recipes?WT
.mc_id=medium-article-lazzeri) shows you how to approach the
 five key steps of the data science process and provides uti
lities to enrich each of the steps :

1. **Evaluating** — Ev
aluate your model. Depending on the metric you’re interested
 in optimizing, you may want to explore different methods of
 evaluation.
2. **Model selection and optimization** — Tun e
 and optimize hyperparameters to get the highest performing 
model. Because Computer Vision models are often computationa
lly costly, we show you how to seamlessly scale your paramet
er tuning into Azure .
3. **Operationalizing** — Operational
ize models in a production environment on Azure by deploying
 it onto Kubernetes.

Inside the computer vision recipes [re
po,](https://github.com/microsoft/ComputerVision-recipes?WT.
mc_id=medium-article-lazzeri) we have added a lot of utility
 to support common tasks such as loading data sets in the fo
rmat expected by different algorithms, splitting training/te
st data, and evaluating model outputs .

This computer visio
n repository also has deep integration with the [Azure Machi
ne Learning](https://docs.microsoft.com/en-us/azure/machine-
learning/?WT.mc_id=medium-article-lazzeri) to complement you
r work locally. We provide code examples on how you can opti
onally and easily scale your training into the cloud, and ho
w you can deploy your models for production workloads.

**Az
ure Cognitive Services**

Note that for certain computer vis
ion problems, you may not need to build your own models. Ins
tead, pre-built or easily customizable solutions exist which
 do not require any custom coding or machine learning expert
ise.

* [Vision Services](https://docs.microsoft.com/en-us/a
zure/cognitive-services/computer-vision/?WT.mc_id=medium-art
icle-lazzeri) are a set of pre-trained REST APIs which can b
e called for image tagging, OCR, video analytics, and more. 
These APIs work out of the box and require minimal expertise
 in machine learning but have limited customization capabili
ties. See the various demos available to get a feel for the 
functionality (e.g. Computer Vision).
* [Custom Vision](http
s://docs.microsoft.com/en-us/azure/cognitive-services/custom
-vision-service/?WT.mc_id=medium-article-lazzeri) is a SaaS 
service to train and deploy a model as a REST API given a us
er-provided training set. All steps including image upload, 
annotation, and model deployment can be performed using eith
er the UI or a Python SDK. Training image classification or 
object detection models can be achieved with minimal machine
 learning expertise. The Custom Vision offers more flexibili
ty than using the pre-trained cognitive services APIs but re
quires the user to bring and annotate their own data.

Befor
e using the Computer Vision repository, we strongly recommen
d evaluating if these can sufficiently solve your problem.


To give you a sense of how you can use our repo to build a s
tate of the art (SOTA) model, here is a preview of how simpl
e it is to create an Object Detection model. Of course, you 
can go much deeper and add custom PyTorch code, but getting 
started is as simple as this :

**1. Load your data**

The f
irst step is to load your data — we help you do this with a 
simple object that automatically parses your data and the an
notations:

`from utils_cv.detection.data import DetectionLo
ader data = DetectionLoader('path/to/data')`

**2. Train/fin
e-tune your model**

Then we create a ‘learner’ object that 
helps you manage and train your model. By default, it will u
se torchvision’s Faster R-CNN model. But you can easily swit
ch it out.

`from utils_cv.detection.model import DetectionL
earner detector = DetectionLearner(data) detector.fit()`

**
3. Evaluate**

Finally, lets evaluate our model using the bu
ilt-in helper functions. We can look at the precision and re
call curves to give us a sense of how our model is performin
g.

`from utils_cv.detection.plot import plot_pr_curves eval
 = detector.evaluate() plot_pr_curves(eval)`

As we continue
 to build out of repository, we will be looking for new comp
uter vision scenarios to unlock . Feel free to reach out to 
[cvbp@microsoft.com](mailto:cvbp@microsoft.com) or post an i
ssue if you wish to see us cover a scenario .

# Additional 
resources to learn more

To learn more, you can read the fol
lowing articles and notebooks:

* [Custom vision repo](https
://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medi
um-article-lazzeri)
* Original article: [https://techcommuni
ty.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-k
now-about-computer-vision-in-one/ba-p/1070311](https://techc
ommunity.microsoft.com/t5/azure-ai/nearly-everything-you-nee
d-to-know-about-computer-vision-in-one/ba-p/1070311)
* [Visi
on Services](https://docs.microsoft.com/en-us/azure/cognitiv
e-services/computer-vision/?WT.mc_id=medium-article-lazzeri)
 on Azure
* [Custom Vision](https://docs.microsoft.com/en-us
/azure/cognitive-services/custom-vision-service/?WT.mc_id=me
dium-article-lazzeri) on Azure
* Portfolio of Azure Machine 
Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://ak
a.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms
/AzureMLservice](https://aka.ms/AzureMLservice)
* Get starte
d with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/G
etStartedAzureML)
* Automated Machine Learning Documentation
: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
*
 What is Automated Machine Learning? [aka.ms/AutomatedML](ht
tps://aka.ms/AutomatedML)
* Python Microsoft: [aka.ms/Python
MS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms
/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)"	"https://www.reddit.com/r/deeplearning/comments/f8t2cf/everything_you_need_to_know_about_computer_vision/"	"5"	"1582559065.0"	"65"	"f8t2cf"
69	"*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!)"	""	"https://i.redd.it/as278qmzuqt61.gif"	"9"	"1618669867.0"	"66"	"msrsc4"
70	"Meta’s LLaMa weights leaked on torrent... and the best thing about it is someone put up a PR to replace the google form in the repo with it 😂"	""	"https://i.redd.it/olnsv438alla1.jpg"	"23"	"1677877700.0"	"185"	"11hezvk"
71	"⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI"	"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of
 The Decade](https://preview.redd.it/sg24cw3zekea1.png?width
=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2
fff989a1)

Microsoft is investing $10B into OpenAI!

There i
s lots of frustration in the community about OpenAI not bein
g all that open anymore. They appear to abandon their ethos 
of developing AI for everyone, [free](https://openai.com/blo
g/introducing-openai/) of economic pressures.

The fear is t
hat OpenAI’s models are going to become fancy MS Office plug
ins. Gone would be the days of open research and innovation.


However, the specifics of the deal tell a different story.


To understand what is going on, we need to peek behind the
 curtain of the tough business of machine learning. We will 
find that Sam Altman might have just orchestrated the coup o
f the decade!

To appreciate better why there is some three-
dimensional chess going on, let’s first look at Sam Altman’s
 backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sa
m Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt
) and was part of the first-ever YC batch. He raised a total
 of $30M in funding, but the company failed to gain traction
. Seven years into the business Loopt was basically dead in 
the water and had to be shut down.

Instead of caving, he ma
naged to sell his startup for $[43M](https://golden.com/wiki
/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https
://www.greendot.com/). Investors got their money back and he
 personally made $5M from the sale.

By YC standards, this w
as a pretty unimpressive outcome.

However, people took note
 that the fire between his ears was burning hotter than that
 of most people. So hot in fact that Paul Graham included hi
m in his 2009 [essay](http://www.paulgraham.com/5founders.ht
ml?viewfullsite=1) about the five founders who influenced hi
m the most.

He listed young Sam Altman next to Steve Jobs, 
Larry & Sergey from Google, and Paul Buchheit (creator of GM
ail and AdSense). He went on to describe him as a strategic 
mastermind whose sheer force of will was going to get him wh
atever he wanted.

And Sam Altman played his hand well!

He 
parleyed his new connections into raising $21M from Peter Th
iel and others to start investing. Within four years he 10x-
ed the money \[2\]. In addition, Paul Graham made him his su
ccessor as president of YC in 2014.

Within one decade of se
lling his first startup for $5M, he grew his net worth to a 
mind-bending $250M and rose to the circle of the most influe
ntial people in Silicon Valley.

Today, he is the CEO of Ope
nAI — one of the most exciting and impactful organizations i
n all of tech.

However, OpenAI — the rocket ship of AI inno
vation — is in dire straights.

# OpenAI is Bleeding Cash

B
ack in 2015, OpenAI was kickstarted with $1B in donations fr
om famous donors such as Elon Musk.

That money is long gone
.

In 2022 OpenAI is projecting a revenue of $36M. At the sa
me time, they spent roughly $544M. Hence the company has los
t >$500M over the last year alone.

This is probably not an 
outlier year. OpenAI is headquartered in San Francisco and h
as a stable of 375 employees of mostly machine learning rock
stars. Hence, salaries alone probably come out to be roughly
 $200M p.a.

In addition to high salaries their compute cost
s are stupendous. Considering it cost them $4.6M to train GP
T3 once, it is likely that their cloud bill is in a very hea
lthy nine-figure range as well \[4\].

So, where does this l
eave them today?

Before the Microsoft investment of $10B, O
penAI had received a total of $4B over its lifetime. With $4
B in funding, a burn rate of $0.5B, and eight years of compa
ny history it doesn’t take a genius to figure out that they 
are running low on cash.

It would be reasonable to think: O
penAI is sitting on ChatGPT and other great models. Can’t th
ey just lease them and make a killing?

Yes and no. OpenAI i
s projecting a revenue of $1B for 2024. However, it is unlik
ely that they could pull this off without significantly incr
easing their costs as well.

*Here are some reasons why!*

#
 The Tough Business Of Machine Learning

Machine learning co
mpanies are distinct from regular software companies. On the
 outside they look and feel similar: people are creating pro
ducts using code, but on the inside things can be very diffe
rent.

To start off, machine learning companies are usually 
way less profitable. Their gross margins land in the 50%-60%
 range, much lower than those of SaaS businesses, which can 
be as high as 80% \[7\].

On the one hand, the massive compu
te requirements and thorny data management problems drive up
 costs.

On the other hand, the work itself can sometimes re
semble consulting more than it resembles software engineerin
g. Everyone who has worked in the field knows that training 
models requires deep domain knowledge and loads of manual wo
rk on data.

To illustrate the latter point, imagine the uns
peakable complexity of performing content moderation on Chat
GPT’s outputs. If OpenAI scales the usage of GPT in producti
on, they will need large teams of moderators to filter and l
abel hate speech, slurs, tutorials on killing people, you na
me it.

*Alright, alright, alright! Machine learning is hard
.*

*OpenAI already has ChatGPT working. That’s gotta be wor
th something?*

# Foundation Models Might Become Commodities
:

In order to monetize GPT or any of their other models, Op
enAI can go two different routes.

First, they could pick on
e or more verticals and sell directly to consumers. They cou
ld for example become the ultimate copywriting tool and blow
 [Jasper](https://app.convertkit.com/campaigns/10748016/jasp
er.ai) or [copy.ai](https://app.convertkit.com/campaigns/107
48016/copy.ai) out of the water.

This is not going to happe
n. Reasons for it include:

1. To support their mission of b
uilding competitive foundational AI tools, and their huge(!)
 burn rate, they would need to capture one or more very larg
e verticals.
2. They fundamentally need to re-brand themselv
es and diverge from their original mission. This would likel
y scare most of the talent away.
3. They would need to build
 out sales and marketing teams. Such a step would fundamenta
lly change their culture and would inevitably dilute their f
ocus on research.

The second option OpenAI has is to keep d
oing what they are doing and monetize access to their models
 via API. Introducing a [pro version](https://www.searchengi
nejournal.com/openai-chatgpt-professional/476244/) of ChatGP
T is a step in this direction.

This approach has its own ch
allenges. Models like GPT do have a defensible moat. They ar
e just large transformer models trained on very large open-s
ource datasets.

As an example, last week Andrej Karpathy re
leased a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY
) of him coding up a version of GPT in an afternoon. Nothing
 could stop e.g. Google, StabilityAI, or HuggingFace from op
en-sourcing their own GPT.

As a result GPT inference would 
become a common good. This would melt OpenAI’s profits down 
to a tiny bit of nothing.

In this scenario, they would also
 have a very hard time leveraging their branding to generate
 returns. Since companies that integrate with OpenAI’s API c
ontrol the interface to the customer, they would likely end 
up capturing all of the value.

An argument can be made that
 this is a general problem of foundation models. Their high 
fixed costs and lack of differentiation could end up making 
them akin to the [steel industry](https://www.thediff.co/arc
hive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum
 it up:

* They don’t have a way to sustainably monetize the
ir models.
* They do not want and probably should not build 
up internal sales and marketing teams to capture verticals
*
 They need a lot of money to keep funding their research wit
hout getting bogged down by details of specific product deve
lopment

*So, what should they do?*

# The Microsoft Deal

O
penAI and Microsoft [announced](https://blogs.microsoft.com/
blog/2023/01/23/microsoftandopenaiextendpartnership/) the ex
tension of their partnership with a $10B investment, on Mond
ay.

At this point, Microsoft will have invested a total of 
$13B in OpenAI. Moreover, new VCs are in on the deal by buyi
ng up shares of employees that want to take some chips off t
he table.

However, the astounding size is not the only extr
aordinary thing about this deal.

First off, the ownership w
ill be split across three groups. Microsoft will hold 49%, V
Cs another 49%, and the OpenAI foundation will control the r
emaining 2% of shares.

If OpenAI starts making money, the p
rofits are distributed differently across four stages:

1. F
irst, early investors (probably Khosla Ventures and Reid Hof
fman’s foundation) get their money back with interest.
2. Af
ter that Microsoft is entitled to 75% of profits until the $
13B of funding is repaid
3. When the initial funding is repa
id, Microsoft and the remaining VCs each get 49% of profits.
 This continues until another $92B and $150B are paid out to
 Microsoft and the VCs, respectively.
4. Once the aforementi
oned money is paid to investors, 100% of shares return to th
e foundation, which regains total control over the company. 
\[3\]

# What This Means

This is absolutely crazy!

OpenAI 
managed to solve all of its problems at once. They raised a 
boatload of money and have access to all the compute they ne
ed.

On top of that, they solved their distribution problem.
 They now have access to Microsoft’s sales teams and their m
odels will be integrated into MS Office products.

Microsoft
 also benefits heavily. They can play at the forefront AI, b
rush up their tools, and have OpenAI as an exclusive partner
 to further compete in a [bitter cloud war](https://www.proj
ectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-
cloud-war/401) against AWS.

The synergies do not stop there
.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. 
will likely benefit heavily from the partnership as they con
tinue to develop[ GitHub Copilot](https://github.com/feature
s/copilot).

The deal creates a beautiful win-win situation,
 but that is not even the best part.

Sam Altman and his tea
m at OpenAI essentially managed to place a giant hedge. If O
penAI does not manage to create anything meaningful or we en
ter a new AI winter, Microsoft will have paid for the party.


However, if OpenAI creates something in the direction of A
GI — whatever that looks like — the value of it will likely 
be huge.

In that case, OpenAI will quickly repay the dept t
o Microsoft and the foundation will control 100% of whatever
 was created.

*Wow!*

Whether you agree with the path OpenA
I has chosen or would have preferred them to stay donation-b
ased, you have to give it to them.

*This deal is an absolut
e power move!*

I look forward to the future. Such exciting 
times to be alive!

As always, I really enjoyed making this 
for you and I sincerely hope you found it useful!

*Thank yo
u for reading!*

Would you like to receive an article such a
s this one straight to your inbox every Thursday? Consider s
igning up for **The Decoding** ⭕.

I send out a thoughtful n
ewsletter about ML research and the data economy once a week
. No Spam. No Nonsense. [Click here to sign up!](https://the
decoding.net/)

**References:**

\[1\] [https://golden.com/w
iki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J
5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/1
0/sam-altmans-manifest-destiny](https://www.newyorker.com/ma
gazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Art
icle in Fortune magazine ](https://fortune.com/2023/01/11/st
ructure-openai-investment-microsoft/?verification_code=DOVCV
S8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbG
Q2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMD
AzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRT
kxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW
55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6Nj
MyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/210
4.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5
\] [https://www.crunchbase.com/organization/openai/company\_
financials](https://www.crunchbase.com/organization/openai/c
ompany_financials)​

\[6\] Elon Musk donation [https://www.i
nverse.com/article/52701-openai-documents-elon-musk-donation
-a-i-research](https://www.inverse.com/article/52701-openai-
documents-elon-musk-donation-a-i-research)​

\[7\] [https://
a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-diffe
rent-from-traditional-software-2/](https://a16z.com/2020/02/
16/the-new-business-of-ai-and-how-its-different-from-traditi
onal-software-2/)"	"https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/"	"16"	"1674816348.0"	"114"	"10mhyek"
72	"image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model"	""	"https://www.youtube.com/watch?v=FwXQ568_io0"	"6"	"1596625086.0"	"96"	"i437pt"
73	"I wrote a program with OpenAI's Codex that fixes errors"	""	"https://v.redd.it/jupdtry6vf881"	"6"	"1640765002.0"	"97"	"rr2wme"
74	"Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT"	"Vicuna : ChatGPT Alternative, Open-Source, High Quality and 
Low Cost 

&#x200B;

[ Relative Response Quality Assessed by
 GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599
&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a07
2b91)

Vicuna-13B has demonstrated competitive performance a
gainst other open-source models, such as Stanford Alpaca, by
 fine-tuning a LLaMA base model on user-shared conversations
 collected from ShareGPT.

Evaluation using GPT-4 as a judge
 shows that Vicuna-13B achieves more than 90% of the quality
 of OpenAI ChatGPT and Google Bard AI, while outperforming o
ther models such as Meta LLaMA (Large Language Model Meta AI
) and Stanford Alpaca in more than 90% of cases.

The cost o
f training Vicuna-13B is approximately $300.

The training a
nd serving code, along with an online demo, are publicly ava
ilable for non-commercial use.

&#x200B;

More Information :
 [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-sourc
e-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT]
(https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source
-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)


Discord Server : [https://discord.gg/h6kCZb72G7](https://di
scord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysor
g](https://twitter.com/lmsysorg)"	"https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/"	"19"	"1680658600.0"	"84"	"12c43uu"
75	"GPT-4 Will Be 500x Smaller Than People Think - Here Is Why"	"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://pre
view.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=web
p&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mil
l is buzzing around the release of GPT-4.

People are predic
ting the model will have 100 trillion parameters. That’s a *
trillion* with a “t”.

The often-used graphic above makes GP
T-3 look like a cute little breadcrumb that is about to have
 a live-ending encounter with a bowling ball.

Sure, OpenAI’
s new brainchild will certainly be mind-bending and language
 models have been getting bigger — fast!

But this time migh
t be different and it makes for a good opportunity to look a
t the research on scaling large language models (LLMs).

*Le
t’s go!*

Training 100 Trillion Parameters

The creation of 
GPT-3 was a marvelous feat of engineering. The training was 
done on 1024 GPUs, took 34 days, and cost $4.6M in compute a
lone \[1\].

Training a 100T parameter model on the same dat
a, using 10000 GPUs, would take 53 Years. To avoid overfitti
ng such a huge model the dataset would also need to be much(
!) larger.

So, where is this rumor coming from?

The Source
 Of The Rumor:

It turns out OpenAI itself might be the sour
ce of it.

In August 2021 the CEO of Cerebras told [wired](h
ttps://www.wired.com/story/cerebras-chip-cluster-neural-netw
orks-ai/): “From talking to OpenAI, GPT-4 will be about 100 
trillion parameters”.

A the time, that was most likely what
 they believed, but that was in 2021. So, basically forever 
ago when machine learning research is concerned.

Things hav
e changed a lot since then!

To understand what happened we 
first need to look at how people decide the number of parame
ters in a model.

Deciding The Number Of Parameters:

The en
ormous hunger for resources typically makes it feasible to t
rain an LLM only once.

In practice, the available compute b
udget (how much money will be spent, available GPUs, etc.) i
s known in advance. Before the training is started, research
ers need to accurately predict which hyperparameters will re
sult in the best model.

*But there’s a catch!*

Most resear
ch on neural networks is empirical. People typically run hun
dreds or even thousands of training experiments until they f
ind a good model with the right hyperparameters.

With LLMs 
we cannot do that. Training 200 GPT-3 models would set you b
ack roughly a billion dollars. Not even the deep-pocketed te
ch giants can spend this sort of money.

Therefore, research
ers need to work with what they have. Either they investigat
e the few big models that have been trained or they train sm
aller models in the hope of learning something about how to 
scale the big ones.

This process can very noisy and the com
munity’s understanding has evolved a lot over the last few y
ears.

What People Used To Think About Scaling LLMs

In 2020
, a team of researchers from OpenAI released a [paper](https
://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For N
eural Language Models”.

They observed a predictable decreas
e in training loss when increasing the model size over multi
ple orders of magnitude.

So far so good. But they made two 
other observations, which resulted in the model size balloon
ing rapidly.

1. To scale models optimally the parameters sh
ould scale quicker than the dataset size. To be exact, their
 analysis showed when increasing the model size 8x the datas
et only needs to be increased 5x.
2. Full model convergence 
is not compute-efficient. Given a fixed compute budget it is
 better to train large models shorter than to use a smaller 
model and train it longer.

Hence, it seemed as if the way t
o improve performance was to scale models faster than the da
taset size \[2\].

And that is what people did. The models g
ot larger and larger with GPT-3 (175B), [Gopher](https://arx
iv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](htt
ps://arxiv.org/pdf/2201.11990) (530B) just to name a few.

B
ut the bigger models failed to deliver on the promise.

*Rea
d on to learn why!*

What We know About Scaling Models Today


It turns out you need to scale training sets and models in
 equal proportions. So, every time the model size doubles, t
he number of training tokens should double as well.

This wa
s published in DeepMind’s 2022 [paper](https://arxiv.org/pdf
/2203.15556.pdf): “Training Compute-Optimal Large Language M
odels”

The researchers fitted over 400 language models rang
ing from 70M to over 16B parameters. To assess the impact of
 dataset size they also varied the number of training tokens
 from 5B-500B tokens.

The findings allowed them to estimate
 that a compute-optimal version of GPT-3 (175B) should be tr
ained on roughly 3.7T tokens. That is more than 10x the data
 that the original model was trained on.

To verify their re
sults they trained a fairly small model on vastly more data.
 Their model, called Chinchilla, has 70B parameters and is t
rained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 b
ut trained on almost 5x the data.

Chinchilla outperforms GP
T-3 and other much larger models by a fair margin \[3\].

Th
is was a great breakthrough!The model is not just better, bu
t its smaller size makes inference cheaper and finetuning ea
sier.

*So What Will Happen?*

What GPT-4 Might Look Like:


To properly fit a model with 100T parameters, open OpenAI ne
eds a dataset of roughly 700T tokens. Given 1M GPUs and usin
g the calculus from above, it would still take roughly 2650 
years to train the model \[1\].

So, here is what GPT-4 coul
d look like:

* Similar size to GPT-3, but trained optimally
 on 10x more data
* ​[Multi-modal](https://thealgorithmicbri
dge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputt
ing text, images, and sound
* Output conditioned on document
 chunks from a memory bank that the model has access to duri
ng prediction \[4\]
* Doubled context size allows longer pre
dictions before the model starts going off the rails​

Regar
dless of the exact design, it will be a solid step forward. 
However, it will not be the 100T token human-brain-like AGI 
that people make it out to be.

Whatever it will look like, 
I am sure it will be amazing and we can all be excited about
 the release.

Such exciting times to be alive!

If you got 
down here, thank you! It was a privilege to make this for yo
u. At **TheDecoding** ⭕, I send out a thoughtful newsletter 
about ML research and the data economy once a week. No Spam.
 No Nonsense. [Click here to sign up!](https://thedecoding.n
et/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Ca
sper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbran
d, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee
 , M. Zaharia, [Efficient Large-Scale Language Model Trainin
g on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2
104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. 
Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Sc
aling laws for neural language model](https://arxiv.org/abs/
2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. B
orgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D
. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Tra
ining Compute-Optimal Large Language Models](https://arxiv.o
rg/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*
.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rut
herford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A.
 Clark, D. Casas, [Improving language models by retrieving f
rom trillions of tokens](https://arxiv.org/abs/2112.04426) (
2021). *arXiv preprint arXiv:2112.04426*.Vancouver"	"https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/"	"11"	"1674114949.0"	"69"	"10fw22o"
76	"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks"	"OpenAI released their newest language, [Triton](https://gith
ub.com/openai/triton). This open-source programming language
 that enables researchers to write highly efficient GPU code
 for AI workloads is Python-compatible and comes with the ab
ility of a user to write in as few as 25 lines, something on
 par with what an expert could achieve. OpenAI claims this m
akes it possible to reach peak hardware performance without 
much effort, making creating more complex workflows easier t
han ever before!

Researchers in the field of Deep Learning 
often rely on native framework operators. However, this can 
be problematic because it requires many temporary tensors to
 work, which may hurt performance at scale for neural networ
ks. Writing specialized GPU kernels is a more convenient sol
ution, but surprisingly difficult due to intricacies when pr
ogramming them according to GPUs. It was challenging to find
 a system that provides the flexibility and speed required w
hile also being easy enough for developers to understand. Th
is has led researchers at OpenAI in improving Triton, which 
was initially founded by one of their teammates.

Quick Read
: [https://www.marktechpost.com/2021/07/28/openai-releases-t
riton-an-open-source-python-like-gpu-programming-language-fo
r-neural-networks/](https://www.marktechpost.com/2021/07/28/
openai-releases-triton-an-open-source-python-like-gpu-progra
mming-language-for-neural-networks/) 

Paper: http://www.eec
s.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pd
f

Github: https://github.com/openai/triton"	"https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/"	"5"	"1627494357.0"	"69"	"otf0fs"
77	"Open AI and Microsoft Can Generate Python Code"	""	"https://youtu.be/y5-wzgIySb4"	"6"	"1590161008.0"	"64"	"golbq4"
78	"Everything you need to know about computer vision in one repo"	"*This post was co-authored by JS Tan, Patrick Buehler, Anupa
m Sharma and Jun Ki Min.*

In recent years, we’ve seen extra
ordinary growth in Computer Vision, with applications in ima
ge understanding, search, mapping, semi-autonomous or autono
mous vehicles and many more .

The ability for models to und
erstand actions in a video , a task that was unthinkable jus
t a few years ago , is now something that we can achieve wit
h relatively high accuracy and in near real-time.

However, 
the field is not particularly welcoming for newcomers. Witho
ut prior experience or guidance, building an accurate classi
fier can easily take weeks. Unless you’re ready to spend a l
ong-time learning computer vision, it’s extremely hard to ma
ster the basics, let alone begin to explore some of the cutt
ing-edge technologies in the field. Even for computer vision
 experts, building a quick Proof of Concept (POC) can be non
 trivial and could easily end up taking many days to put tog
ether.

At [Microsoft ](https://docs.microsoft.com/en-us/azu
re/machine-learning/?WT.mc_id=medium-article-lazzeri), we ha
ve been working for many years on diverse Computer Vision so
lutions for our customers and collected our learning into ou
r new public [Microsoft](https://docs.microsoft.com/en-us/az
ure/machine-learning/?WT.mc_id=medium-article-lazzeri) repos
itory: [Custom vision repo](https://github.com/microsoft/Com
puterVision-recipes?WT.mc_id=medium-article-lazzeri).

The g
oal of [this repository](https://github.com/microsoft/Comput
erVision-recipes?WT.mc_id=medium-article-lazzeri) is to prov
ide examples and best practice guidelines for building compu
ter vision systems on [Azure](https://docs.microsoft.com/en-
us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) 
, and to share this with the open-source community . More sp
ecifically, our goal was to create a [repository](https://do
cs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medi
um-article-lazzeri) that will help us to provide solutions r
apidly to the community and to customers that we work with ,
 or with on-boarding new team members who may have expertise
 in data science, but not specifically in computer vision. F
rom mastering some of the most common scenarios in the field
, like image classification, object detection , and image si
milarity, to exploring cutting edge scenarios like activity 
recognition and crowd counting, [this repo](https://docs.mic
rosoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-art
icle-lazzeri) will guide you through building models, fine-t
uning them, and using them in real-world scenarios.

We’re k
icking off our repo with **5 scenarios.** You can find the l
inks to the repos here:

* [Classification](https://github.c
om/microsoft/computervision-recipes/tree/master/scenarios/cl
assification?WT.mc_id=medium-article-lazzeri)
* [Similarity]
(https://github.com/microsoft/computervision-recipes/tree/ma
ster/scenarios/similarity?WT.mc_id=medium-article-lazzeri)
*
 [Detection](https://github.com/microsoft/computervision-rec
ipes/tree/master/scenarios/detection?WT.mc_id=medium-article
-lazzeri)
* [Action Recognition](https://github.com/microsof
t/computervision-recipes/tree/master/contrib/action_recognit
ion?WT.mc_id=medium-article-lazzeri)
* [Crowd Counting](http
s://github.com/microsoft/computervision-recipes/tree/master/
contrib/crowd_counting?WT.mc_id=medium-article-lazzeri)

Rat
her than creating implementations from scratch, we draw from
 popular state-of-the-art libraries (e.g. fast.ai and [torch
vision ](https://pytorch.org/docs/stable/torchvision/index.h
tml)), and we build additional utility around loading image 
data, optimizing models , and evaluating models. In addition
, we aim to answer the frequently asked questions, try to ex
plain the deep learning intuitions, and highlight common pit
falls.

Whether you a re an expert in computer vision or jus
t getting your hands wet, we believe [this repository](https
://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id
=medium-article-lazzeri) offers something for you . For the 
beginner, [this repo](https://docs.microsoft.com/en-us/azure
/machine-learning/?WT.mc_id=medium-article-lazzeri) will gui
de you through building a state-of-the-art model and help yo
u develop an intuition for the craft. For the experts, this 
repository can quickly get you to a strong baseline model wh
ich is easy to extend using custom Python/PyTorch code. In a
ddition, the repository also aims to provide support with:


1. [The full data science process](https://docs.microsoft.co
m/azure/machine-learning/team-data-science-process/overview?
WT.mc_id=medium-article-lazzeri).
2. [The tooling to succeed
 on Azure](https://docs.microsoft.com/en-us/azure/machine-le
arning/?WT.mc_id=medium-article-lazzeri).

We hope that thes
e examples and utilities will make it easier and faster for 
developers to create custom vision applications.

# The Data
 Science Process

The [Computer Vision Recipes GitHub reposi
tory](https://github.com/microsoft/ComputerVision-recipes?WT
.mc_id=medium-article-lazzeri) shows you how to approach the
 five key steps of the data science process and provides uti
lities to enrich each of the steps :

1. **Evaluating** — Ev
aluate your model. Depending on the metric you’re interested
 in optimizing, you may want to explore different methods of
 evaluation.
2. **Model selection and optimization** — Tun e
 and optimize hyperparameters to get the highest performing 
model. Because Computer Vision models are often computationa
lly costly, we show you how to seamlessly scale your paramet
er tuning into Azure .
3. **Operationalizing** — Operational
ize models in a production environment on Azure by deploying
 it onto Kubernetes.

Inside the computer vision recipes [re
po,](https://github.com/microsoft/ComputerVision-recipes?WT.
mc_id=medium-article-lazzeri) we have added a lot of utility
 to support common tasks such as loading data sets in the fo
rmat expected by different algorithms, splitting training/te
st data, and evaluating model outputs .

This computer visio
n repository also has deep integration with the [Azure Machi
ne Learning](https://docs.microsoft.com/en-us/azure/machine-
learning/?WT.mc_id=medium-article-lazzeri) to complement you
r work locally. We provide code examples on how you can opti
onally and easily scale your training into the cloud, and ho
w you can deploy your models for production workloads.

**Az
ure Cognitive Services**

Note that for certain computer vis
ion problems, you may not need to build your own models. Ins
tead, pre-built or easily customizable solutions exist which
 do not require any custom coding or machine learning expert
ise.

* [Vision Services](https://docs.microsoft.com/en-us/a
zure/cognitive-services/computer-vision/?WT.mc_id=medium-art
icle-lazzeri) are a set of pre-trained REST APIs which can b
e called for image tagging, OCR, video analytics, and more. 
These APIs work out of the box and require minimal expertise
 in machine learning but have limited customization capabili
ties. See the various demos available to get a feel for the 
functionality (e.g. Computer Vision).
* [Custom Vision](http
s://docs.microsoft.com/en-us/azure/cognitive-services/custom
-vision-service/?WT.mc_id=medium-article-lazzeri) is a SaaS 
service to train and deploy a model as a REST API given a us
er-provided training set. All steps including image upload, 
annotation, and model deployment can be performed using eith
er the UI or a Python SDK. Training image classification or 
object detection models can be achieved with minimal machine
 learning expertise. The Custom Vision offers more flexibili
ty than using the pre-trained cognitive services APIs but re
quires the user to bring and annotate their own data.

Befor
e using the Computer Vision repository, we strongly recommen
d evaluating if these can sufficiently solve your problem.


To give you a sense of how you can use our repo to build a s
tate of the art (SOTA) model, here is a preview of how simpl
e it is to create an Object Detection model. Of course, you 
can go much deeper and add custom PyTorch code, but getting 
started is as simple as this :

**1. Load your data**

The f
irst step is to load your data — we help you do this with a 
simple object that automatically parses your data and the an
notations:

`from utils_cv.detection.data import DetectionLo
ader data = DetectionLoader('path/to/data')`

**2. Train/fin
e-tune your model**

Then we create a ‘learner’ object that 
helps you manage and train your model. By default, it will u
se torchvision’s Faster R-CNN model. But you can easily swit
ch it out.

`from utils_cv.detection.model import DetectionL
earner detector = DetectionLearner(data) detector.fit()`

**
3. Evaluate**

Finally, lets evaluate our model using the bu
ilt-in helper functions. We can look at the precision and re
call curves to give us a sense of how our model is performin
g.

`from utils_cv.detection.plot import plot_pr_curves eval
 = detector.evaluate() plot_pr_curves(eval)`

As we continue
 to build out of repository, we will be looking for new comp
uter vision scenarios to unlock . Feel free to reach out to 
[cvbp@microsoft.com](mailto:cvbp@microsoft.com) or post an i
ssue if you wish to see us cover a scenario .

# Additional 
resources to learn more

To learn more, you can read the fol
lowing articles and notebooks:

* [Custom vision repo](https
://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medi
um-article-lazzeri)
* Original article: [https://techcommuni
ty.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-k
now-about-computer-vision-in-one/ba-p/1070311](https://techc
ommunity.microsoft.com/t5/azure-ai/nearly-everything-you-nee
d-to-know-about-computer-vision-in-one/ba-p/1070311)
* [Visi
on Services](https://docs.microsoft.com/en-us/azure/cognitiv
e-services/computer-vision/?WT.mc_id=medium-article-lazzeri)
 on Azure
* [Custom Vision](https://docs.microsoft.com/en-us
/azure/cognitive-services/custom-vision-service/?WT.mc_id=me
dium-article-lazzeri) on Azure
* Portfolio of Azure Machine 
Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://ak
a.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms
/AzureMLservice](https://aka.ms/AzureMLservice)
* Get starte
d with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/G
etStartedAzureML)
* Automated Machine Learning Documentation
: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
*
 What is Automated Machine Learning? [aka.ms/AutomatedML](ht
tps://aka.ms/AutomatedML)
* Python Microsoft: [aka.ms/Python
MS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms
/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)"	"https://www.reddit.com/r/deeplearning/comments/f8t2cf/everything_you_need_to_know_about_computer_vision/"	"5"	"1582559065.0"	"64"	"f8t2cf"
79	"*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!)"	""	"https://i.redd.it/as278qmzuqt61.gif"	"9"	"1618669867.0"	"65"	"msrsc4"
